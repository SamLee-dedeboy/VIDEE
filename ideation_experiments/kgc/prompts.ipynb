{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, RateLimitError, APITimeoutError\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import concurrent\n",
    "import time\n",
    "import json\n",
    "from pprint import pprint\n",
    "openai_client = OpenAI(api_key=open(\"api_key\").read().strip())\n",
    "def save_json(data, filename):\n",
    "    with open (filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_gpt(\n",
    "    client, messages, model=\"gpt-4o-mini\", temperature=0.5, format=None, seed=None\n",
    "):\n",
    "    with open(\"request_log.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"model: {model}, temperature: {temperature}, format: {format}\\n\")\n",
    "        f.write(json.dumps(messages, ensure_ascii=False) + \"\\n\")\n",
    "        f.write(\"=====================================\\n\")\n",
    "    try:\n",
    "        if format == \"json\":\n",
    "            response = (\n",
    "                client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    response_format={\"type\": \"json_object\"},\n",
    "                    temperature=temperature,\n",
    "                    seed=seed,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model, messages=messages, temperature=temperature, seed=seed\n",
    "            )\n",
    "        return response.choices[0].message.content\n",
    "    except RateLimitError as e:\n",
    "        print(\"RateLimitError\")\n",
    "        print(e)\n",
    "        time.sleep(5)\n",
    "        return request_gpt(client, messages, model, temperature, format)\n",
    "    except APITimeoutError as e:\n",
    "        print(\"APITimeoutError\")\n",
    "        print(messages)\n",
    "        time.sleep(5)\n",
    "        return request_gpt(client, messages, model, temperature, format)\n",
    "\n",
    "def multithread_prompts(\n",
    "    client,\n",
    "    prompts,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5,\n",
    "    response_format=None,\n",
    "    seed=None,\n",
    "):\n",
    "    l = len(prompts)\n",
    "    # results = np.zeros(l)\n",
    "    with tqdm(total=l) as pbar:\n",
    "        executor = concurrent.futures.ThreadPoolExecutor(max_workers=100)\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                request_gpt, client, prompt, model, temperature, response_format, seed\n",
    "            )\n",
    "            for prompt in prompts\n",
    "        ]\n",
    "        for _ in concurrent.futures.as_completed(futures):\n",
    "            pbar.update(1)\n",
    "    concurrent.futures.wait(futures)\n",
    "    return [future.result() for future in futures]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompose a goal into semantic steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"steps\": [\n",
      "        {\n",
      "            \"label\": \"Data Collection\",\n",
      "            \"description\": \"Gather the relevant Wikipedia documents that you want to use for constructing the knowledge graph.\",\n",
      "            \"explanation\": \"This step is essential to ensure that you have all the necessary data at hand before starting the knowledge graph construction process.\"\n",
      "        },\n",
      "        {\n",
      "            \"label\": \"Text Preprocessing\",\n",
      "            \"description\": \"Clean and preprocess the text data by removing unnecessary elements such as HTML tags, special characters, and stop words.\",\n",
      "            \"explanation\": \"Preprocessing helps improve the quality of the data by making it easier to extract meaningful information and relationships from the text.\"\n",
      "        },\n",
      "        {\n",
      "            \"label\": \"Entity Recognition\",\n",
      "            \"description\": \"Use Named Entity Recognition (NER) techniques to identify and extract entities (such as people, places, organizations) from the text.\",\n",
      "            \"explanation\": \"Identifying entities is crucial for building nodes in the knowledge graph, as these entities will represent the key components of your graph.\"\n",
      "        },\n",
      "        {\n",
      "            \"label\": \"Relationship Extraction\",\n",
      "            \"description\": \"Extract relationships between the identified entities using techniques such as dependency parsing or co-reference resolution.\",\n",
      "            \"explanation\": \"Understanding how entities relate to each other is fundamental to constructing the edges in the knowledge graph, which represent the connections between the nodes.\"\n",
      "        },\n",
      "        {\n",
      "            \"label\": \"Graph Construction\",\n",
      "            \"description\": \"Construct the knowledge graph by creating nodes for the entities and edges for the relationships extracted.\",\n",
      "            \"explanation\": \"This step is where the actual knowledge graph is built, visualizing the data in a structured format that can be easily queried and analyzed.\"\n",
      "        },\n",
      "        {\n",
      "            \"label\": \"Graph Optimization\",\n",
      "            \"description\": \"Optimize the graph by removing duplicates, merging similar entities, and refining relationships.\",\n",
      "            \"explanation\": \"Optimization improves the quality and usability of the knowledge graph, ensuring that it accurately represents the information without redundancy.\"\n",
      "        },\n",
      "        {\n",
      "            \"label\": \"Visualization\",\n",
      "            \"description\": \"Use graph visualization tools to represent the knowledge graph visually.\",\n",
      "            \"explanation\": \"Visualization helps in understanding the structure and insights of the knowledge graph, making it easier to communicate findings to others.\"\n",
      "        },\n",
      "        {\n",
      "            \"label\": \"Validation and Testing\",\n",
      "            \"description\": \"Validate the knowledge graph by checking the accuracy of entities and relationships, and test it with sample queries.\",\n",
      "            \"explanation\": \"Validation ensures that the knowledge graph is reliable and can be used effectively for further analysis or applications.\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a text analytics task planner. \n",
    "Users have collected their own dataset, and they need help with text analytics tasks.\n",
    "Users will describe a task to you, and you need to help them decompose the task into subtasks, and then provide a plan to complete the task. \n",
    "You can ask questions to clarify the task, and you can also ask for more information if needed. \n",
    "You can also provide examples to help the user understand the task better.\n",
    "Reply with this JSON format:\n",
    "{\n",
    "    \"steps\": [\n",
    "        {\n",
    "            \"label\": (string)\n",
    "            \"description\": (string)\n",
    "            \"explanation\": (string, explain why this step is needed)\n",
    "        },\n",
    "        {\n",
    "            \"label\": (string)\n",
    "            \"description\": (string)\n",
    "            \"explanation\": (string, explain why this step is needed)\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "user_goal_kgc = \"\"\"I need to construct a knowledge graph from a collection of documents from wikipedia.\"\"\"\n",
    "user_goal_bsa = \"\"\"I need to perform business management analysis using the SWOT strategy on a dataset of company financial reports.\"\"\"\n",
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_goal_kgc,\n",
    "    }\n",
    "]\n",
    "response = request_gpt(openai_client, prompt, model=\"gpt-4o-mini\", temperature=0.5)\n",
    "print(response)\n",
    "steps = json.loads(response)[\"steps\"]\n",
    "save_json(steps, \"kgc_semantic.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompose a semantic step into more semantic steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"steps\": [\n",
      "    {\n",
      "      \"label\": \"Text Preprocessing\",\n",
      "      \"description\": \"Clean and preprocess the text data to remove noise and irrelevant information.\",\n",
      "      \"explanation\": \"This step is essential to ensure that the text is in a suitable format for analysis. Preprocessing may include lowercasing, removing punctuation, and tokenization, which helps improve the accuracy of subsequent NER, RE, and EE processes.\"\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"Named Entity Recognition (NER)\",\n",
      "      \"description\": \"Apply NER techniques to identify and classify entities in the text into predefined categories such as persons, organizations, locations, dates, etc.\",\n",
      "      \"explanation\": \"NER helps in pinpointing the key entities present in the text, which is the first step in understanding the content and context of the data. Identifying these entities is crucial for further analysis and relationship extraction.\"\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"Relation Extraction (RE)\",\n",
      "      \"description\": \"Analyze the identified entities to extract relationships between them, determining how they are related based on the context.\",\n",
      "      \"explanation\": \"RE is important because it helps in understanding the interactions and connections between entities, which can provide deeper insights into the text and facilitate knowledge discovery.\"\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"Event Extraction (EE)\",\n",
      "      \"description\": \"Identify and extract events mentioned in the text, including the entities involved, the actions taken, and the timeframes.\",\n",
      "      \"explanation\": \"EE allows for capturing dynamic aspects of the text, such as actions and occurrences, which are critical for understanding narratives and timelines. This step is vital for applications like summarization and knowledge graph construction.\"\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"Post-Processing and Validation\",\n",
      "      \"description\": \"Review the extracted entities, relations, and events to ensure accuracy and consistency, making necessary adjustments.\",\n",
      "      \"explanation\": \"Post-processing is necessary to refine the extraction results, as automated methods may produce errors. Validating the output ensures that the information extracted is reliable and can be used for further analysis.\"\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"Output Formatting\",\n",
      "      \"description\": \"Format the extracted information into a structured format (e.g., JSON, CSV) for easy access and analysis.\",\n",
      "      \"explanation\": \"Output formatting is important for making the extracted data usable in downstream applications, such as databases, analytics tools, or visualization platforms, ensuring that it can be effectively leveraged.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "semantic_steps = json.load(open(\"kgc_semantic.json\"))\n",
    "test_index = 0\n",
    "test_target_step = semantic_steps[test_index]['label'] + \": \" + semantic_steps[test_index]['description']\n",
    "system_prompt = \"\"\"You are a text analytics task planner. \n",
    "The use will describe a task to you, you need to help them decompose the task into subtasks, and then provide a plan to complete the task. \n",
    "You can also provide examples to help the user understand the task better. \n",
    "Reply with this JSON format: \n",
    "{ \"steps\": [ \n",
    "        { \n",
    "            \"label\": (string) \n",
    "            \"description\": (string) \n",
    "            \"explanation\": (string, explain why this step is needed)\n",
    "        }, \n",
    "        { \n",
    "            \"label\": (string) \n",
    "            \"description\": (string) \n",
    "            \"explanation\": (string, explain why this step is needed)\n",
    "        }, \n",
    "        ... \n",
    "    ] \n",
    "}\"\"\"\n",
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": test_target_step,\n",
    "    }\n",
    "]\n",
    "response = request_gpt(openai_client, prompt, model=\"gpt-4o-mini\", temperature=0.5)\n",
    "print(response)\n",
    "more_steps = json.loads(response)[\"steps\"]\n",
    "save_json(more_steps, \"kgc_semantic_step_0.json\")\n",
    "# merge into original steps\n",
    "original_steps = json.load(open(\"kgc_semantic.json\"))\n",
    "original_steps[test_index] = more_steps\n",
    "# flatten\n",
    "original_steps = [step for steps in original_steps for step in steps]\n",
    "save_json(original_steps, \"kgc_semantic_decomposed.json\")\n",
    "# merge into original steps\n",
    "original_steps = json.load(open(\"kgc_semantic.json\"))\n",
    "original_steps[test_index:test_index + 1] = more_steps\n",
    "# flatten\n",
    "save_json(original_steps, \"kgc_semantic_decomposed.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Flags to semantic steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"steps\": [\n",
      "    {\n",
      "      \"label\": \"Text Preprocessing\",\n",
      "      \"analytical\": true,\n",
      "      \"explanation\": \"Text preprocessing is a fundamental step in text analytics, as it prepares the raw text data for analysis by removing noise and irrelevant information.\"\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"Named Entity Recognition (NER)\",\n",
      "      \"analytical\": true,\n",
      "      \"explanation\": \"NER is a core text analytics task that identifies and classifies entities within the text, allowing for a better understanding of the content.\"\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"Relation Extraction (RE)\",\n",
      "      \"analytical\": true,\n",
      "      \"explanation\": \"RE is a critical aspect of text analytics that focuses on determining relationships between identified entities, contributing to deeper insights from the text.\"\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"Event Extraction (EE)\",\n",
      "      \"analytical\": true,\n",
      "      \"explanation\": \"EE is an important text analytics task that identifies events within the text, capturing dynamic information essential for understanding narratives.\"\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"Post-Processing and Validation\",\n",
      "      \"analytical\": true,\n",
      "      \"explanation\": \"This step refines the results of extraction processes, ensuring accuracy and reliability of the information before further analysis.\"\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"Output Formatting\",\n",
      "      \"analytical\": true,\n",
      "      \"explanation\": \"Output formatting is crucial for structuring the extracted data, making it accessible for downstream applications and analysis.\"\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"Relationship Extraction\",\n",
      "      \"analytical\": true,\n",
      "      \"explanation\": \"This task involves extracting relationships between entities, which is essential for knowledge graph construction and understanding entity interactions.\"\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"Graph Construction\",\n",
      "      \"analytical\": true,\n",
      "      \"explanation\": \"Constructing a knowledge graph is a direct application of text analytics, where entities and their relationships are visualized in a structured format.\"\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"Graph Optimization\",\n",
      "      \"analytical\": true,\n",
      "      \"explanation\": \"Optimization of the knowledge graph is necessary to enhance its quality, ensuring that it accurately represents the information without redundancy.\"\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"Visualization\",\n",
      "      \"analytical\": true,\n",
      "      \"explanation\": \"Visualization is an important step in text analytics to communicate findings effectively, helping stakeholders understand the structure of the knowledge graph.\"\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"Validation and Testing\",\n",
      "      \"analytical\": true,\n",
      "      \"explanation\": \"Validation and testing ensure that the knowledge graph is accurate and reliable, which is essential for its effectiveness in further analysis or applications.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "semantic_steps = json.load(open(\"kgc_semantic_decomposed.json\"))\n",
    "system_prompt = \"\"\"You are a text analytics task helper. \n",
    "The user will describe a list practical tasks for you, you need to determine if each of the tasks can be done with typical text analytics tasks, and give an explanation.\n",
    "You can also provide examples to help the user understand the task better. \n",
    "Reply with this JSON format: \n",
    "{ \"steps\": [ \n",
    "        { \n",
    "            \"label\": (string) \n",
    "            \"analytical\": (boolean, true if the task can be done with text analytics tasks)\n",
    "            \"explanation\": (string, explain why this step is needed) \n",
    "        },\n",
    "        , ... \n",
    "    ] \n",
    "}\"\"\"\n",
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": json.dumps(semantic_steps),\n",
    "    }\n",
    "]\n",
    "formulate_flags = request_gpt(openai_client, prompt, model=\"gpt-4o-mini\", temperature=0.5)\n",
    "print(formulate_flags)\n",
    "save_json(json.loads(formulate_flags)[\"steps\"], \"kgc_flags.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An attempt to add previous and next steps when decomposing a semantic task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need to perform business management analysis using the SWOT strategy.I have already done the following steps: Define the Objective, Gather Relevant Data,  Now I need to List the internal strengths of the business, such as resources, capabilities, and competitive advantages.which means Identifying strengths allows you to understand what the business excels at and can leverage for growth or competitive advantage.\n",
      "{\n",
      "    \"steps\": [\n",
      "        {\n",
      "            \"label\": \"Identify Resources\",\n",
      "            \"description\": \"List all tangible and intangible resources available to the business, such as financial resources, human capital, technology, and physical assets.\",\n",
      "            \"explanation\": \"Identifying resources is crucial because it provides a foundation for understanding what the business has at its disposal to achieve its objectives and compete effectively in the market.\"\n",
      "        },\n",
      "        {\n",
      "            \"label\": \"Assess Capabilities\",\n",
      "            \"description\": \"Evaluate the skills, competencies, and processes that the business possesses, including the expertise of employees and operational efficiencies.\",\n",
      "            \"explanation\": \"Assessing capabilities helps to pinpoint areas where the business excels and can leverage these strengths to improve performance and gain a competitive edge.\"\n",
      "        },\n",
      "        {\n",
      "            \"label\": \"Analyze Competitive Advantages\",\n",
      "            \"description\": \"Determine what sets the business apart from its competitors, such as unique selling propositions, brand reputation, or proprietary technology.\",\n",
      "            \"explanation\": \"Understanding competitive advantages is essential for identifying strengths that can be leveraged to attract customers and outperform rivals in the market.\"\n",
      "        },\n",
      "        {\n",
      "            \"label\": \"Gather Input from Stakeholders\",\n",
      "            \"description\": \"Engage with employees, management, and other stakeholders to gain insights into perceived strengths of the business.\",\n",
      "            \"explanation\": \"Gathering input from various stakeholders provides a broader perspective on strengths and can uncover strengths that may not be immediately obvious.\"\n",
      "        },\n",
      "        {\n",
      "            \"label\": \"Compile and Prioritize Strengths\",\n",
      "            \"description\": \"Organize the identified strengths into a comprehensive list and prioritize them based on their significance to the business's objectives.\",\n",
      "            \"explanation\": \"Compiling and prioritizing strengths allows for a clear focus on the most impactful areas, enabling the business to strategically leverage these strengths for growth.\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "previous_steps = steps[0][\"label\"] + \", \" + steps[1]['label'] + \", \"\n",
    "step_3 = steps[2]\n",
    "user_prompt = user_goal_kgc + \"\"\"I have already done the following steps: \"\"\" + previous_steps + \"\"\" Now I need to \"\"\" + step_3[\"description\"] + \"which means \" + step_3[\"explanation\"]\n",
    "prompts = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    }\n",
    "]\n",
    "subtask_response = request_gpt(openai_client, prompts, model=\"gpt-4o-mini\", temperature=0.5)\n",
    "print(user_prompt)\n",
    "print(subtask_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert a semantic task into analytical tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'analytical': True,\n",
      "  'explanation': 'Text preprocessing is a fundamental step in text analytics, '\n",
      "                 'as it prepares the raw text data for analysis by removing '\n",
      "                 'noise and irrelevant information.',\n",
      "  'label': 'Text Preprocessing'},\n",
      " {'analytical': True,\n",
      "  'explanation': 'NER is a core text analytics task that identifies and '\n",
      "                 'classifies entities within the text, allowing for a better '\n",
      "                 'understanding of the content.',\n",
      "  'label': 'Named Entity Recognition (NER)'},\n",
      " {'analytical': True,\n",
      "  'explanation': 'RE is a critical aspect of text analytics that focuses on '\n",
      "                 'determining relationships between identified entities, '\n",
      "                 'contributing to deeper insights from the text.',\n",
      "  'label': 'Relation Extraction (RE)'},\n",
      " {'analytical': True,\n",
      "  'explanation': 'EE is an important text analytics task that identifies '\n",
      "                 'events within the text, capturing dynamic information '\n",
      "                 'essential for understanding narratives.',\n",
      "  'label': 'Event Extraction (EE)'},\n",
      " {'analytical': True,\n",
      "  'explanation': 'This step refines the results of extraction processes, '\n",
      "                 'ensuring accuracy and reliability of the information before '\n",
      "                 'further analysis.',\n",
      "  'label': 'Post-Processing and Validation'},\n",
      " {'analytical': True,\n",
      "  'explanation': 'Output formatting is crucial for structuring the extracted '\n",
      "                 'data, making it accessible for downstream applications and '\n",
      "                 'analysis.',\n",
      "  'label': 'Output Formatting'},\n",
      " {'analytical': True,\n",
      "  'explanation': 'This task involves extracting relationships between '\n",
      "                 'entities, which is essential for knowledge graph '\n",
      "                 'construction and understanding entity interactions.',\n",
      "  'label': 'Relationship Extraction'},\n",
      " {'analytical': True,\n",
      "  'explanation': 'Constructing a knowledge graph is a direct application of '\n",
      "                 'text analytics, where entities and their relationships are '\n",
      "                 'visualized in a structured format.',\n",
      "  'label': 'Graph Construction'},\n",
      " {'analytical': True,\n",
      "  'explanation': 'Optimization of the knowledge graph is necessary to enhance '\n",
      "                 'its quality, ensuring that it accurately represents the '\n",
      "                 'information without redundancy.',\n",
      "  'label': 'Graph Optimization'},\n",
      " {'analytical': True,\n",
      "  'explanation': 'Visualization is an important step in text analytics to '\n",
      "                 'communicate findings effectively, helping stakeholders '\n",
      "                 'understand the structure of the knowledge graph.',\n",
      "  'label': 'Visualization'},\n",
      " {'analytical': True,\n",
      "  'explanation': 'Validation and testing ensure that the knowledge graph is '\n",
      "                 'accurate and reliable, which is essential for its '\n",
      "                 'effectiveness in further analysis or applications.',\n",
      "  'label': 'Validation and Testing'}]\n",
      "I need to construct a knowledge graph from a collection of documents from wikipedia.. I have done Data Collection. I need to Text Preprocessing, which means Clean and preprocess the text data by removing unnecessary elements such as HTML tags, special characters, and stop words., which prepares me for the next step Entity Recognition.\n",
      "{\n",
      "    \"subtasks\": [\n",
      "        {\n",
      "            \"name\": \"Text Preprocessing\",\n",
      "            \"description\": \"A technique that involves cleaning and preparing text data for further analysis.\",\n",
      "            \"explanation\": \"Text preprocessing is essential to remove noise from the data, such as HTML tags, special characters, and stop words, which can interfere with the accuracy of subsequent analysis such as entity recognition.\",\n",
      "            \"input\": \"Raw text data collected from Wikipedia documents.\",\n",
      "            \"output\": \"Cleaned text data that is free from unnecessary elements.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Named Entity Recognition (NER)\",\n",
      "            \"description\": \"A technique used to identify and classify key entities in the text into predefined categories such as people, organizations, locations, etc.\",\n",
      "            \"explanation\": \"NER is needed to extract relevant entities from the cleaned text, which will serve as the foundational components for constructing the knowledge graph.\",\n",
      "            \"input\": \"Cleaned text data obtained from the text preprocessing step.\",\n",
      "            \"output\": \"A list of identified entities along with their types and associated metadata.\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "semantic_steps = json.load(open(\"kgc_flags.json\"))\n",
    "semantic_analytics_tasks = list(filter(lambda x: x['analytical'] is True, semantic_steps))\n",
    "assert(all(x['label'] in list(map(lambda x: x['label'], steps))) for x in semantic_analytics_tasks)\n",
    "pprint(semantic_analytics_tasks)\n",
    "system_prompt = \"\"\"You are a text analytics task helper. \n",
    "The user will give you a practical task, you need to decompose it into subtasks that can be done with text analytics techniques.\n",
    "Here are some examples of text analytics techniques: Named Entity Recognition, Annotation, Text Classification, Summarization, Sentiment Analysis, etc.\n",
    "You can add more techniques as needed, but the techniques need to be strictly defined from the perspective of natural language processing.\n",
    "Reply with the following JSON format:\n",
    "{\n",
    "    \"subtasks\": [\n",
    "        {\n",
    "            \"name\": (string, the name of the technique), \n",
    "            \"description\": (string, the description of the technique),\n",
    "            \"explanation\": (string, explain why this technique is needed)\n",
    "            \"input\": (string, the input needed for this technique)\n",
    "            \"output\": (string, the output of this technique)\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "step_index = 1\n",
    "assert(step_index > 0 and step_index < len(steps) - 1)\n",
    "user_prompt = f\"\"\"{user_goal_kgc}. I have done {steps[step_index-1]['label']}. I need to {steps[step_index]['label']}, which means {steps[step_index]['description']}, which prepares me for the next step {steps[step_index + 1]['label']}.\"\"\"\n",
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    }\n",
    "]\n",
    "analytics_tasks = request_gpt(openai_client, prompt, model=\"gpt-4o-mini\", temperature=0.5)\n",
    "print(user_prompt)\n",
    "print(analytics_tasks)\n",
    "save_json(json.loads(analytics_tasks)[\"subtasks\"], f\"kgc_analytical_tasks_step_{step_index}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement an analytical task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Named Entity Recognition (NER)', 'description': 'A technique used to identify and classify key entities in the text into predefined categories such as people, organizations, locations, etc.', 'explanation': 'NER is needed to extract relevant entities from the cleaned text, which will serve as the foundational components for constructing the knowledge graph.', 'input': 'Cleaned text data obtained from the text preprocessing step.', 'output': 'A list of identified entities along with their types and associated metadata.'}\n",
      "I want to implement Named Entity Recognition (NER): A technique used to identify and classify key entities in the text into predefined categories such as people, organizations, locations, etc.. The input is Cleaned text data obtained from the text preprocessing step., and the output is A list of identified entities along with their types and associated metadata..\n",
      "{\n",
      "    \"code\": \"import spacy\\n\\n# Load the pre-trained spaCy model for NER\\nnlp = spacy.load('en_core_web_sm')\\n\\ndef extract_entities(cleaned_text):\\n    \\\"\\\"\\\"\\n    Extracts named entities from the cleaned text and returns a list of entities\\n    along with their types and associated metadata.\\n    \\\"\\\"\\\"\\n    # Process the cleaned text with the spaCy NLP pipeline\\n    doc = nlp(cleaned_text)\\n    \\n    # Initialize a list to hold the entities\\n    entities = []\\n    \\n    # Iterate over the identified entities in the doc\\n    for ent in doc.ents:\\n        # Append each entity and its type to the entities list\\n        entities.append({'text': ent.text, 'label': ent.label_, 'start': ent.start_char, 'end': ent.end_char})\\n    \\n    return entities\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "step_index = 1\n",
    "analytical_tasks = json.load(open(f\"kgc_analytical_tasks_step_{step_index}.json\"))\n",
    "print(analytical_tasks[0])\n",
    "system_prompt = \"\"\"You are a text analytics task code generator. \n",
    "The user will give you a text analytics task, you need to generate code snippet for the task.\n",
    "Write a function that takes the input and returns the output. You don't need to include examples or main function.\n",
    "Add comments in the code to explain the codes.\n",
    "Reply with the following JSON format:\n",
    "{\n",
    "    \"code\": (string, in python)\n",
    "}\"\"\"\n",
    "for analytics_task in analytical_tasks:\n",
    "    task_name = analytics_task['name']\n",
    "    description = analytics_task['description']\n",
    "    input = analytics_task['input']\n",
    "    output = analytics_task['output']\n",
    "    user_prompt = f\"\"\"I want to implement {task_name}: {description}. The input is {input}, and the output is {output}.\"\"\"\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt\n",
    "        }\n",
    "    ]\n",
    "    code = request_gpt(openai_client, prompt, model=\"gpt-4o-mini\", temperature=0.5)\n",
    "    print(user_prompt)\n",
    "    print(code)\n",
    "    analytics_task['code'] = json.loads(code)[\"code\"]\n",
    "save_json(analytical_tasks, f\"kgc_analytical_tasks_step_{step_index}_codes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting failed: 'str' object has no attribute 'suffix'\n"
     ]
    }
   ],
   "source": [
    "import black\n",
    "\n",
    "# String of Python code\n",
    "code_string = analytical_tasks[0]['code']\n",
    "# Specify the file name\n",
    "file_name = \"test_output.py\"\n",
    "\n",
    "# Step 1: Write the code string to a file\n",
    "with open(file_name, \"w\") as file:\n",
    "    file.write(code_string)\n",
    "\n",
    "# Step 2: Format the file using black\n",
    "try:\n",
    "    black.format_file_in_place(\n",
    "        src=file_name,\n",
    "        fast=False,\n",
    "        mode=black.FileMode(),\n",
    "        write_back=black.WriteBack.YES,\n",
    "    )\n",
    "    print(f\"Code has been formatted and saved to {file_name}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Formatting failed: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taskdecomposition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
