{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'orm_mode' has been renamed to 'from_attributes'\n",
      "  warnings.warn(message, UserWarning)\n",
      "/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3047, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3102, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3306, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3489, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3549, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/l4/65dfr4t56k1g5w3mbltb8tx40000gn/T/ipykernel_15388/3011130425.py\", line 13, in <module>\n",
      "    import server.executor as executor\n",
      "  File \"/Users/samytlee/Documents/projects/TaskDecomposition/server/executor/__init__.py\", line 1, in <module>\n",
      "    from .langgraph_utils import (\n",
      "  File \"/Users/samytlee/Documents/projects/TaskDecomposition/server/executor/langgraph_utils.py\", line 5, in <module>\n",
      "    from server.executor.tools import embedding_tool\n",
      "  File \"/Users/samytlee/Documents/projects/TaskDecomposition/server/executor/tools/__init__.py\", line 1, in <module>\n",
      "    from .clustering_tool import (\n",
      "  File \"/Users/samytlee/Documents/projects/TaskDecomposition/server/executor/tools/clustering_tool.py\", line 8, in <module>\n",
      "    from bertopic import BERTopic\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/bertopic/__init__.py\", line 3, in <module>\n",
      "    from bertopic._bertopic import BERTopic\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/bertopic/_bertopic.py\", line 58, in <module>\n",
      "    from bertopic.backend import BaseEmbedder\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/bertopic/backend/__init__.py\", line 22, in <module>\n",
      "    from bertopic.backend._multimodal import MultiModalBackend\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/bertopic/backend/_multimodal.py\", line 5, in <module>\n",
      "    from sentence_transformers import SentenceTransformer\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/sentence_transformers/__init__.py\", line 9, in <module>\n",
      "    from sentence_transformers.backend import (\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/sentence_transformers/backend.py\", line 11, in <module>\n",
      "    from sentence_transformers.util import disable_datasets_caching, is_datasets_available\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/sentence_transformers/util.py\", line 17, in <module>\n",
      "    import torch\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/samytlee/opt/anaconda3/envs/task_decomposition_311/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Get the parent directory of the current script\n",
    "parent_dir = os.path.abspath(\"..\")  # Move one level up\n",
    "sibling1_path = os.path.join(parent_dir, \"server\")\n",
    "# Add sibling directory to sys.path\n",
    "sys.path.append(sibling1_path)\n",
    "os.chdir(\"/Users/samytlee/Documents/projects/TaskDecomposition\")\n",
    "import server.custom_types as custom_types\n",
    "import server.executor as executor\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import concurrent\n",
    "import tiktoken\n",
    "api_key = open(\"evaluation_experiments/openai_api_key\", \"r\").read().strip()\n",
    "\n",
    "def request_gpt(messages, model, api_key, format=None):\n",
    "    openai = OpenAI(api_key=api_key)\n",
    "    try:\n",
    "        if format == \"json\":\n",
    "            response = openai.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    response_format={\"type\": \"json_object\"},\n",
    "                )\n",
    "            \n",
    "            try:\n",
    "                response = json.loads(response.choices[0].message.content)\n",
    "                return response\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"JSON Decode Error\")\n",
    "                print(e)\n",
    "                time.sleep(5)\n",
    "                return request_gpt(messages, model, api_key, format)\n",
    "\n",
    "        else:\n",
    "            response = openai.chat.completions.create(\n",
    "                model=model, messages=messages\n",
    "            )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        time.sleep(5)\n",
    "        pass\n",
    "        # return request_gpt(messages, model, api_key, format)\n",
    "\n",
    "def multithread_embedding(client, texts):\n",
    "    l = len(texts)\n",
    "    with tqdm(total=l) as pbar:\n",
    "        executor = concurrent.futures.ThreadPoolExecutor(max_workers=100)\n",
    "        futures = [executor.submit(get_embedding, client, text) for text in texts]\n",
    "        for _ in concurrent.futures.as_completed(futures):\n",
    "            pbar.update(1)\n",
    "    concurrent.futures.wait(futures)\n",
    "    return [future.result() for future in futures]\n",
    "\n",
    "# def get_embedding(client, text, model=\"text-embedding-3-small\"):\n",
    "def get_embedding(client, text, model=\"text-embedding-ada-002\"):\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    # print(\"tokens: \", len(enc.encode(text)), len(enc.encode(text)) > 8191)\n",
    "    while len(enc.encode(text)) > 8191:\n",
    "        text = text[:-100]\n",
    "        print(\"truncated: \", len(enc.encode(text)))\n",
    "    try:\n",
    "        return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return get_embedding(client, text, model)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import random\n",
    "def read_jsonl(file_path):\n",
    "    \"\"\"Reads a JSONL file and returns a list of dictionaries.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                json_object = json.loads(line)\n",
    "                data.append(json_object)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping invalid JSON line: {line.strip()}\")\n",
    "    return data\n",
    "def save_json(data, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "wiki = read_jsonl('evaluation_experiments/data/wiki/test.metadata.jsonl')\n",
    "# sampling documents\n",
    "grouped_by_supercategory = {}\n",
    "for doc in wiki:\n",
    "    supercategory = doc['supercategory']\n",
    "    if supercategory not in grouped_by_supercategory:\n",
    "        grouped_by_supercategory[supercategory] = []\n",
    "    grouped_by_supercategory[supercategory].append(doc)\n",
    "sampled_documents = []\n",
    "for supercategory in grouped_by_supercategory:\n",
    "    random_sample_num = max(5, round(len(grouped_by_supercategory[supercategory])/40))\n",
    "    sampled_documents.extend(random.sample(grouped_by_supercategory[supercategory], random_sample_num))\n",
    "random.shuffle(sampled_documents)\n",
    "print(len(sampled_documents))\n",
    "input_documents = [{'id': doc['id'], 'content': doc['text'] } for doc in sampled_documents]\n",
    "lloom_pipeline = json.load(open('evaluation_experiments/data/lloom/execution_plan.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "def get_node_config(app, thread_config, node_id, execution_version=None):\n",
    "    state_history = list(\n",
    "        reversed([step for step in app.get_state_history(thread_config)])\n",
    "    )\n",
    "    node_configs = [\n",
    "        # step.config for step in state_history if step.next[0] == f\"{node_id}_evaluation\"\n",
    "        step.config\n",
    "        for step in state_history\n",
    "        if step.next[0] == f\"{node_id}\"\n",
    "    ]\n",
    "    if len(node_configs) == 0:\n",
    "        return None\n",
    "    if execution_version is None:\n",
    "        return node_configs[-1]\n",
    "    else:\n",
    "        return node_configs[execution_version]\n",
    "\n",
    "async def execute_node(app, thread_config, node_id, execution_version=None, state=None, parallelize=False):\n",
    "    # if this is the first node executed in the graph\n",
    "    # then we need to invoke with the initial state\n",
    "    if len(list(app.get_state_history(thread_config))) == 0:\n",
    "        new_state = await app.ainvoke(state, config=thread_config)\n",
    "        return new_state\n",
    "\n",
    "    # if this is not the first node executed in the graph\n",
    "    # check if this node is executed before\n",
    "    node_config = get_node_config(app, thread_config, node_id, execution_version)\n",
    "    if node_config is None:  # first time executing this node\n",
    "        if parallelize:\n",
    "            new_state = await app.ainvoke(Command(resume=True), config=thread_config)\n",
    "        else:\n",
    "            new_state = app.invoke(Command(resume=True), config=thread_config)\n",
    "    else:  # if this node is executed before\n",
    "        if parallelize:\n",
    "            new_state = app.ainvoke(\n",
    "                Command(goto=node_id, update=state),\n",
    "                config=node_config,\n",
    "            )\n",
    "        else:\n",
    "            new_state = app.invoke(\n",
    "                Command(goto=node_id, update=state),\n",
    "                config=node_config,\n",
    "            )\n",
    "    return new_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_experiment(intermediate_filepath, final_filepath):\n",
    "    execution_graph, checkpointer = executor.create_graph(lloom_pipeline)\n",
    "    initial_state = {\"documents\": input_documents}\n",
    "    state = initial_state\n",
    "    thread_config = {\"configurable\": {\"thread_id\": 42}}\n",
    "    for index, node in enumerate(lloom_pipeline):\n",
    "        parent_node = lloom_pipeline[index - 1]['id'] if index > 0 else None\n",
    "        parallelizable = node['execution']['tool'] not in [\"clustering_tool\", \"data_transform_tool\"]\n",
    "        print(\"executing node: \", node['label'], node['id'], parent_node)\n",
    "        last_state = executor.find_last_state(execution_graph, parent_node, thread_config)\n",
    "        last_state = last_state if last_state is not None else initial_state\n",
    "        state = await execute_node(\n",
    "            execution_graph,\n",
    "            thread_config,\n",
    "            node['id'],\n",
    "            None,\n",
    "            state=last_state,\n",
    "            parallelize=parallelizable\n",
    "        )\n",
    "        # state = executor.execute_node(\n",
    "        #     execution_graph,\n",
    "        #     thread_config,\n",
    "        #     node['id'],\n",
    "        #     None,\n",
    "        #     state=last_state,\n",
    "        # )\n",
    "        save_json(state, intermediate_filepath + f\"{node['label']}.json\")\n",
    "    final_result = {\n",
    "        \"grouped_summary\": state['global_store']['grouped_summary'],\n",
    "        \"cluster_labels\": [obj['cluster_labels'] for obj in state['global_store']['cluster_labels']]\n",
    "    }\n",
    "    save_json(final_result, final_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    print(\"Running experiment: \", i)\n",
    "    os.makedirs(f\"evaluation_experiments/results/execution/intermediate/{i}/\", exist_ok=True)\n",
    "    intermediate_filepath = f\"evaluation_experiments/results/execution/intermediate/{i}/\"\n",
    "    final_filepath = f\"evaluation_experiments/results/execution/finals/final_{i}.json\"\n",
    "    await run_experiment(intermediate_filepath, final_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 15 supercategories, which matches the paper introduction of 15 topics\n",
    "all_topics = list(set([doc['supercategory'] for doc in sampled_documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage_evaluation_prompts(ground_truth_concepts, generated_concepts):\n",
    "    coverage_prompt = \"\"\"I have this set of CONCEPTS: {ground_truth_concepts}\n",
    "            I have this set of TEXTS: {generated_concepts}\n",
    "            Please match at most ONE TEXT to each CONCEPT.\n",
    "            To perform a match, the text must EXACTLY match the meaning of the concept.\n",
    "            Do NOT match the same TEXT to multiple CONCEPTS. \n",
    "            Here are examples of VALID matches: \n",
    "            - Global Diplomacy, International Relations ; rationale: \"The text is about diplomacy between countries .\"\n",
    "            - Statistical Data, Quantitative Evidence ; rationale: \"The text is about data and quantitative measures .\" \n",
    "            - Policy and Regulation, Policy issues and legislation ; rationale : \" The text is about policy, laws, and legislation .\" \n",
    "\n",
    "            Here are examples of INVALID matches:\n",
    "            - Reputation Impact, Immigration - Environment, Politics and Law - Interdisciplinary Politics, Economy \n",
    "            If there are no valid matches, please EXCLUDE the concept from the list. \n",
    "            Please provide a 1-sentence RATIONALE for your decision for any matches. \n",
    "            Please respond with a list of each concept and either the item it matches or NONE if no item matches in this JSON format: \n",
    "            {{ \n",
    "                \"concept_matches\": [ \n",
    "                    {{ \n",
    "                        \"concept_id\": \"<concept_id_number>\" \n",
    "                        \"item_id\": \"<item_id_number or NONE >\" \n",
    "                        \"rationale\": \"<rationale for match >\" \n",
    "                    }} \n",
    "                ] \n",
    "            }}\n",
    "            \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistant that follows the instructions of the user. Please follow the instructions and provide the requested information.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": coverage_prompt.format(\n",
    "                ground_truth_concepts=\", \".join([f\"{index}: {concept}\" for index, concept in enumerate(ground_truth_concepts)]),\n",
    "                generated_concepts=\", \".join(generated_concepts)\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4\n",
      "1 1.0\n",
      "2 1.0\n",
      "3 0.4666666666666667\n",
      "4 1.0\n",
      "5 0.6\n",
      "6 1.0\n",
      "7 0.8666666666666667\n",
      "8 1.0\n",
      "9 1.0\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "avg_coverage = 0\n",
    "for i in range(10):\n",
    "    final_result = json.load(open(f\"evaluation_experiments/results/execution/finals/final_{i}.json\"))\n",
    "    coverage_metric_prompts = coverage_evaluation_prompts(all_topics, final_result['cluster_labels'])\n",
    "    response = request_gpt(coverage_metric_prompts, \"gpt-4o-mini\", api_key, format=\"json\")\n",
    "    if 'concept_matches' not in response:\n",
    "        print(\"Error in response\")\n",
    "        print(response)\n",
    "        continue\n",
    "    response = response['concept_matches'] \n",
    "    matched_concepts = [obj['concept_id'] for obj in response if obj['item_id'] != \"NONE\" and obj['item_id'] is not None]\n",
    "    matched_concepts = list(set(matched_concepts))\n",
    "    coverage = len(matched_concepts) / len(all_topics)\n",
    "    print(i, coverage)\n",
    "    avg_coverage += coverage\n",
    "avg_coverage = avg_coverage / 10\n",
    "print(avg_coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## baselines -- bertopic\n",
    "openai_client = OpenAI(api_key=api_key)\n",
    "# precalculate embeddings\n",
    "sampled_documents = json.load(open(\"evaluation_experiments/data/wiki/sampled_documents.json\"))\n",
    "openai_client = OpenAI(api_key=api_key)\n",
    "embeddings = multithread_embedding(openai_client, [doc['text'] for doc in sampled_documents])\n",
    "save_json(embeddings, \"evaluation_experiments/data/wiki/sampled_documents_embeddings.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=5, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "topic_model = BERTopic(vectorizer_model=vectorizer_model, hdbscan_model=hdbscan_model)\n",
    "sampled_documents = json.load(open(\"evaluation_experiments/data/wiki/sampled_documents.json\"))\n",
    "embeddings = json.load(open(\"evaluation_experiments/data/wiki/sampled_documents_embeddings.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5333333333333333\n",
      "1 0.6\n",
      "2 0.5333333333333333\n",
      "3 0.5333333333333333\n",
      "4 0.6\n",
      "5 0.6666666666666666\n",
      "6 0.4\n",
      "7 0.6\n",
      "8 0.4\n",
      "9 0.4\n",
      "0.5266666666666666\n"
     ]
    }
   ],
   "source": [
    "avg_coverage = 0\n",
    "for i in range(10):\n",
    "    topics, probs = topic_model.fit_transform([doc['text'] for doc in sampled_documents], np.array(embeddings))\n",
    "    topic_df =  topic_model.get_topic_info()\n",
    "    generated_concepts = []\n",
    "    for index, row in topic_df.iterrows():\n",
    "        generated_concepts.append(row['Name'].replace(str(row['Topic']), \"\").strip().replace(\"_\", \" \").strip())\n",
    "    coverage_metric_prompts = coverage_evaluation_prompts(all_topics, generated_concepts)\n",
    "    response = request_gpt(coverage_metric_prompts, \"gpt-4o-mini\", api_key, format=\"json\")\n",
    "    response = response['concept_matches'] \n",
    "    matched_concepts = [obj['concept_id'] for obj in response if obj['item_id'] != \"NONE\" and obj['item_id'] is not None]\n",
    "    matched_concepts = list(set(matched_concepts))\n",
    "    coverage = len(matched_concepts) / len(all_topics)\n",
    "    print(i, coverage)\n",
    "    avg_coverage += coverage\n",
    "avg_coverage = avg_coverage / 10\n",
    "print(avg_coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4\n",
      "1 0.3333333333333333\n",
      "2 0.6\n",
      "3 0.9333333333333333\n",
      "4 0.2\n",
      "5 0.26666666666666666\n",
      "6 0.9333333333333333\n",
      "7 0.3333333333333333\n",
      "8 0.8\n",
      "9 0.5333333333333333\n",
      "0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "# baseline -- gpt-4o-mini\n",
    "avg_coverage = 0\n",
    "for i in range (10):\n",
    "    docs_within_context_window = random.sample(sampled_documents, 19)\n",
    "    docs_json = json.dumps(docs_within_context_window)\n",
    "    system_prompt = \"\"\"\n",
    "    I have this set of text examples: {docs_json}\n",
    "    Please write a summary of {n_concepts} unifying patterns for these examples. \n",
    "    For each high-level pattern, write a {n_name_words} word NAME for the pattern.\n",
    "    Please respond ONLY with a valid JSON in the following format: \n",
    "        {{ \n",
    "            \"patterns\": [ \n",
    "                {{ \n",
    "                    \"name\": \"<PATTERN_NAME_1 >\" \n",
    "                }} \n",
    "                {{ \n",
    "                    \"name\": \"<PATTERN_NAME_2 >\" \n",
    "                }} \n",
    "            ] \n",
    "        }}\n",
    "    \"\"\".format(\n",
    "        docs_json=docs_json,\n",
    "        n_concepts=len(all_topics),\n",
    "        n_name_words=3,\n",
    "        n_example_ids=2\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistant that follows the instructions of the user. Please follow the instructions and provide the requested information.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": system_prompt\n",
    "        }\n",
    "    ]\n",
    "    response = request_gpt(messages, \"gpt-4o-mini\", api_key, format=\"json\")\n",
    "    generated_concepts = [pattern['name'] for pattern in response['patterns']]\n",
    "    coverage_metric_prompts = coverage_evaluation_prompts(all_topics, generated_concepts)\n",
    "    response = request_gpt(coverage_metric_prompts, \"gpt-4o-mini\", api_key, format=\"json\")\n",
    "    response = response['concept_matches'] \n",
    "    matched_concepts = [obj['concept_id'] for obj in response if obj['item_id'] != \"NONE\" and obj['item_id'] is not None]\n",
    "    matched_concepts = list(set(matched_concepts))\n",
    "    coverage = len(matched_concepts) / len(all_topics)\n",
    "    print(i, coverage)\n",
    "    avg_coverage += coverage\n",
    "avg_coverage = avg_coverage / 10\n",
    "print(avg_coverage)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "task_decomposition_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
