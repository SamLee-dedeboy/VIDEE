{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samytlee/opt/anaconda3/envs/taskdecomposition/lib/python3.10/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'orm_mode' has been renamed to 'from_attributes'\n",
      "  warnings.warn(message, UserWarning)\n",
      "/Users/samytlee/opt/anaconda3/envs/taskdecomposition/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Get the parent directory of the current script\n",
    "parent_dir = os.path.abspath(\"..\")  # Move one level up\n",
    "sibling1_path = os.path.join(parent_dir, \"server\")\n",
    "# Add sibling directory to sys.path\n",
    "sys.path.append(sibling1_path)\n",
    "os.chdir(\"/Users/samytlee/Documents/projects/TaskDecomposition\")\n",
    "import server.custom_types as custom_types\n",
    "import server.decomposer as decomposer\n",
    "import server.executor as executor\n",
    "import server.evaluator as evaluator\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "async def iter_response(\n",
    "        root, node_dict, goal, next_selection, eval_definitions, eval_few_shot_examples, model, api_key, save_file_path\n",
    "    ):  # (1)\n",
    "        async for (\n",
    "            new_root,\n",
    "            node_dict,\n",
    "            next_selection,\n",
    "            max_value_path,\n",
    "        ) in decomposer.stream_MCTS(\n",
    "            root,\n",
    "            node_dict,\n",
    "            goal,\n",
    "            next_selection=next_selection,\n",
    "            eval_definitions=eval_definitions,\n",
    "            eval_few_shot_examples=eval_few_shot_examples,\n",
    "            model=model,\n",
    "            api_key=api_key,\n",
    "        ):\n",
    "            if next_selection is None:\n",
    "                break\n",
    "            root = new_root\n",
    "            result = {\n",
    "                \"node_dict\": {\n",
    "                    k: v.model_dump(mode=\"json\") for k, v in node_dict.items()\n",
    "                },\n",
    "                \"next_node\": next_selection.model_dump(mode=\"json\"),\n",
    "                \"max_value_path\": max_value_path,\n",
    "            }\n",
    "            save_json(result, save_file_path)\n",
    "            yield result\n",
    "        yield None\n",
    "\n",
    "def save_json(data, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "api_key = open(\"evaluation_experiments/openai_api_key\").read()\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = open(\"server/anthropic_api_key\").read()\n",
    "os.environ[\"GEMINI_API_KEY\"] = open(\"server/gemini_api_key\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_max_value_path(node_dict, max_value_path):\n",
    "    max_value_path_nodes = [node_dict[node_id] for node_id in max_value_path if node_id != \"-1\"]\n",
    "    # sort by depth\n",
    "    max_value_path_nodes.sort(key=lambda x: x['level'])\n",
    "    max_value_path_nodes = [\n",
    "        {k: node[k] for k in ['label', 'description', 'explanation', 'level']} for node in max_value_path_nodes\n",
    "    ]\n",
    "    return max_value_path_nodes\n",
    "\n",
    "async def decomposition_to_primitive_tasks(semantic_tasks, save_filepath, api_key, model=\"gpt-4o-mini\"):\n",
    "    semantic_tasks = [custom_types.Node.model_validate(t) for t in semantic_tasks]\n",
    "    semantic_tasks = list(\n",
    "        filter(lambda t: t.label != \"END\" and t.id != \"root\", semantic_tasks)\n",
    "    )\n",
    "    primitive_task_list = json.load(\n",
    "        open(\"server/decomposer/primitive_task_defs.json\")\n",
    "    )\n",
    "    primitive_tasks = await decomposer.one_shot_decomposition_to_primitive_task(\n",
    "        semantic_tasks=semantic_tasks,\n",
    "        primitive_task_list=primitive_task_list,\n",
    "        model=model,\n",
    "        api_key=api_key,\n",
    "    )\n",
    "    save_json(primitive_tasks, save_filepath)\n",
    "    # save_json(primitive_tasks, \"evaluation_experiments/results/searching/primitive_tasks/LLooM/1.json\")\n",
    "import random\n",
    "def ask_oracle(goal, manual_pipeline_str, generated_pipeline_str, client):\n",
    "    revert = random.choice([True, False])\n",
    "    if revert:\n",
    "        node_path_strings = [manual_pipeline_str, generated_pipeline_str]\n",
    "    else:\n",
    "        node_path_strings = [generated_pipeline_str, manual_pipeline_str]\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"I have a text analytics goal: {goal}.\n",
    "            I have come up with two solutions: \n",
    "            Solution 1: {node_path_string_1}.\n",
    "            Solution 2: {node_path_string_2}.\n",
    "            The solutions intentionally stay at a high level and not get into the technical details.\n",
    "            What do you think about my solutions? Give me the pros and cons of each solution in a bullet list, each bullet should have a clear criteria explicitly specified.\n",
    "            \"\"\".format(goal=goal, node_path_string_1=node_path_strings[0], node_path_string_2=node_path_strings[1])\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"o3-mini\", \n",
    "        reasoning_effort=\"medium\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content, revert\n",
    "\n",
    "node_path_string_generator = lambda node: f\"\"\"\n",
    "<node>\n",
    "    <label> {node['label']} </label>\n",
    "    <description> {node['description']} </description>\n",
    "    <explanation> {node['explanation']} </explanation>\n",
    "    <step> {node['level']} </step>\n",
    "</node>\n",
    "\"\"\"\n",
    "openai_client = OpenAI(api_key=api_key)\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chose an evaluation target (LLooM or TnT-LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lloom_pipeline = json.load(open(\"evaluation_experiments/data/lloom/LLooM_semantic_path_manual.json\"))\n",
    "tnt_llm_pipeline = json.load(open(\"evaluation_experiments/data/tntllm/tntllm_semantic_path_manual.json\"))\n",
    "evaluation_target, goal, save_file_path = lloom_pipeline, \\\n",
    "    \"I have a dataset of UIST paper abstracts. I want to extract high-level concepts from the abstracts to understand research topics.\", \\\n",
    "    \"evaluation_experiments/results/searching/LLooM_full_tree.json\"\n",
    "# evaluation_target, goal, save_file_path = tnt_llm_pipeline, \\\n",
    "#     \"I have a dataset of user conversation with the Microsoft's Bing Consumer Copilot system. I want to understand the user's intents when interacting with the chatbot.\", \\\n",
    "#     \"evaluation_experiments/results/searching/tnt_llm_semantic_path_manual_evaluation.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Evaluate the MCTS pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword Extraction\n",
      "Concept Clustering\n",
      "Topic Modeling\n",
      "Concept Extraction\n",
      "Keyword Analysis\n",
      "Concept Clustering\n",
      "Concept Extraction\n",
      "Concept Clustering\n",
      "Concept Summarization\n",
      "Concept Summarization\n",
      "Semantic Analysis\n",
      "Concept Ranking\n",
      "Concept Summarization\n",
      "Topic Modeling\n",
      "Concept Refinement\n",
      "Concept Clustering\n",
      "Research Gap Analysis\n",
      "Concept Clustering\n",
      "Topic Modeling\n",
      "Concept Mapping\n",
      "Topic Identification\n",
      "Topic Modeling\n",
      "Concept Summarization\n",
      "Theme Consolidation\n",
      "Concept Mapping\n",
      "Concept Evaluation\n",
      "Clustering Analysis\n",
      "Concept Mapping\n",
      "Thematic Synthesis\n",
      "Concept Synthesis\n",
      "Concept Summarization\n",
      "Concept Clustering\n",
      "Concept Interpretation\n",
      "Concept Synthesis\n",
      "Concept Hierarchy\n",
      "Concept Summarization\n",
      "Concept Refinement\n",
      "Concept Mapping\n",
      "Concept Analysis\n",
      "Topic Synthesis\n",
      "Concept Synthesis\n",
      "Theme Analysis\n",
      "Research Prioritization\n",
      "Concept Validation\n",
      "Concept Validation\n",
      "Concept Validation\n",
      "Research Gap Analysis\n"
     ]
    }
   ],
   "source": [
    "# Generate the pipeline through MCTS\n",
    "async def generate_MCTS_pipeline(goal, save_file_path):\n",
    "    user_root = decomposer.init_MCTS()\n",
    "    node_dict = {user_root.MCT_id: user_root}\n",
    "    next_selection = None\n",
    "    eval_definitions = {\n",
    "                \"complexity\": evaluator.complexity_definition,\n",
    "                \"coherence\": evaluator.coherence_definition,\n",
    "                \"importance\": evaluator.importance_definition,\n",
    "            }\n",
    "    eval_few_shot_examples = []\n",
    "    model = \"gpt-4o-mini\"\n",
    "\n",
    "    while True:\n",
    "        response = await anext(iter_response(\n",
    "            user_root, node_dict, goal, next_selection, eval_definitions, eval_few_shot_examples, model, api_key, save_file_path\n",
    "        ))\n",
    "        if response is None:\n",
    "            break\n",
    "        print(response[\"next_node\"][\"label\"])\n",
    "await generate_MCTS_pipeline(goal, save_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the generated pipeline with the manual one\n",
    "def generated_pipeline_nodes(max_value_path_nodes):\n",
    "    node_path_string = \"\\n\".join([node_path_string_generator(node) for node in max_value_path_nodes])\n",
    "    return node_path_string\n",
    "\n",
    "def save_max_value_path(full_tree, save_filepath):\n",
    "    node_dict = full_tree['node_dict']\n",
    "    max_value_path = full_tree['max_value_path'][0]\n",
    "    max_value_path_nodes = extract_max_value_path(node_dict, max_value_path)\n",
    "    save_json(max_value_path_nodes, save_filepath)\n",
    "\n",
    "# save the max value path of the full tree\n",
    "full_tree = json.load(open(\"evaluation_experiments/results/searching/LLooM_full_tree.json\"))\n",
    "index = 0\n",
    "save_max_value_path(full_tree, \"evaluation_experiments/results/searching/semantic_tasks/{i}.json\".format(i=index))\n",
    "\n",
    "# read a max value path\n",
    "max_value_path = json.load(open(\"evaluation_experiments/results/searching/semantic_tasks/{i}.json\").format(index))\n",
    "generated_pipeline = generated_pipeline_nodes(max_value_path)\n",
    "# read a manual pipeline\n",
    "manual_pipeline = \"\\n\".join([node_path_string_generator(node) for node in evaluation_target])\n",
    "\n",
    "# compare the two pipelines\n",
    "evaluation, revert = ask_oracle(goal, generated_pipeline, manual_pipeline, openai_client)\n",
    "# print the evaluation\n",
    "if revert:\n",
    "    order = [\"generated\", \"manual\"]\n",
    "else:\n",
    "    order = [\"manual\", \"generated\"]\n",
    "print(\"\"\"\n",
    "Solution 1: {}\n",
    "Solution 2: {}\n",
    "\"\"\".format(order[0], order[1]))\n",
    "pprint(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_definitions = {\n",
    "            \"complexity\": evaluator.complexity_definition,\n",
    "            \"coherence\": evaluator.coherence_definition,\n",
    "            \"importance\": evaluator.importance_definition,\n",
    "        }\n",
    "eval_few_shot_examples = []\n",
    "model = \"gpt-4o-mini\"\n",
    "api_key = open(\"evaluation_experiments/openai_api_key\").read()\n",
    "root_node = {\n",
    "    \"id\": \"-1\",\n",
    "    \"label\": \"Root\",\n",
    "    \"description\": \"Root node\",\n",
    "    \"explanation\": \"Root node\",\n",
    "    \"parentIds\": [],\n",
    "    \"MCT_id\": \"-1\",\n",
    "    \"MCT_parent_id\": None,\n",
    "    \"level\": 0\n",
    "}\n",
    "results = []\n",
    "for index, node in enumerate(lloom_pipeline):\n",
    "    parent_node = root_node if index == 0 else lloom_pipeline[index - 1]\n",
    "    eval_params = [\n",
    "        [goal, node, parent_node]\n",
    "    ]\n",
    "    eval_results, eval_reasons = await evaluator.run_all_evaluations(goal, eval_params, eval_definitions=eval_definitions, eval_few_shot_examples=eval_few_shot_examples, model=model, api_key=api_key)\n",
    "    results.append([eval_results, eval_reasons])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taskdecomposition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
