{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, TypedDict\n",
    "import tools\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Annotated, List, Optional\n",
    "import operator\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda, RunnablePassthrough, RunnableAssign\n",
    "from tools import prompt_tool\n",
    "\n",
    "class Doc(TypedDict):\n",
    "    id: str\n",
    "    content: str\n",
    "    summary: Optional[str]\n",
    "    explanation: Optional[str]\n",
    "    # category: Optional[str]\n",
    "class TaxonomyGenerationState(TypedDict):\n",
    "    documents: List[Doc]\n",
    "    summaries: List[str]\n",
    "    # The raw docs; we inject summaries within them in the first step\n",
    "    # Indices to be concise\n",
    "    minibatches: List[List[int]]\n",
    "    # Candidate Taxonomies (full trajectory)\n",
    "    clusters: Annotated[List[List[dict]], operator.add]\n",
    "    \n",
    "\n",
    "# Now combine as a \"map\" operation in a map-reduce chain\n",
    "# Input: state\n",
    "# Output: state U summaries\n",
    "# Processes docs in parallel\n",
    "def get_input_func(state: TaxonomyGenerationState, state_input_key: str, doc_input_key: str):\n",
    "    docs = state[state_input_key] # how to get the list of docs from the state (e.g. state[\"documents\"])\n",
    "    return [{doc_input_key: doc[doc_input_key]} for doc in docs] # how to get the input field from a doc (e.g. doc[\"content\"])\n",
    "\n",
    "def fake_llm(state):\n",
    "    print(state)\n",
    "    return [{\"summary\": \"This is a summary\", \"explanation\": \"This is an explanation\"} for _ in state]\n",
    "\n",
    "def reduce_func(combined: dict, state_input_key: str, state_output_key: str):\n",
    "    outputs = combined[state_output_key] # \"summaries\"\n",
    "    documents = combined[state_input_key] # \"documents\"\n",
    "    return {\n",
    "        \"documents\": [\n",
    "            {\n",
    "                **doc,\n",
    "                **{k: v for k, v in output.items()}\n",
    "            }\n",
    "            for doc, output in zip(documents, outputs)\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# map_step = RunnablePassthrough.assign(\n",
    "#     # This effectively creates a \"map\" operation # Note you can make this more robust by handling individual errors\n",
    "#     summaries=get_content | RunnableLambda(func=summary_chain.batch, afunc=summary_chain.abatch)\n",
    "# )\n",
    "json_schema = \"{{summary: str, explanation: str}}\"\n",
    "summary_prompt = tools.parse_template([\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You are a summarization system. Please summarize the following text and give an explanation. Reply with the following JSON format: {json_schema}\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"human\",\n",
    "        \"content\": \"{content}\"\n",
    "    }\n",
    "])\n",
    "summary_chain = tools.prompt_tool(\n",
    "    tool_name=\"summarization\",\n",
    "    prompt_template=summary_prompt,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=open(\"../api_key\").read(),\n",
    "    format=\"json\"\n",
    ")\n",
    "\n",
    "state_output_key = \"summaries\"\n",
    "get_input = lambda x: get_input_func(x, \"documents\", \"content\")\n",
    "map_step = RunnableAssign({state_output_key: get_input | RunnableLambda(func=summary_chain.batch, afunc=summary_chain.abatch)})\n",
    "reduce_step = lambda x: reduce_func(x, \"documents\", state_output_key)\n",
    "# This is actually the node itself!\n",
    "map_reduce_chain = map_step | reduce_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "graph = StateGraph(TaxonomyGenerationState)\n",
    "graph.add_node(\"summarize\", map_reduce_chain)\n",
    "graph.add_edge(START, \"summarize\")\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "import json\n",
    "docs = json.load(open(\"docs.json\"))\n",
    "final_state = app.invoke(\n",
    "    {\"documents\": docs},\n",
    "    config={\"configurable\": {\"thread_id\": 42}},\n",
    ")\n",
    "from pprint import pprint\n",
    "pprint(final_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "json_schema = \"{{summary: str, explanation: str}}\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You are a summarization system. Please summarize the following text and give an explanation. Reply with the following JSON format: {json_schema}\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"human\",\n",
    "        \"content\": \"{content}\"\n",
    "    }\n",
    "]\n",
    "summary_prompt = tools.parse_template(messages)\n",
    "print(summary_prompt)\n",
    "template = ChatPromptTemplate(summary_prompt)\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = open(\"api_key\").read()\n",
    "test_steps = [\n",
    "    {\n",
    "        \"id\": \"0\",\n",
    "        \"label\": \"Entity Extraction\",\n",
    "        \"state_input_key\": \"documents\",\n",
    "        \"doc_input_keys\": [\"content\"],\n",
    "        \"state_output_key\": \"entities\",\n",
    "        \"parentIds\": [],\n",
    "        \"execution\": {\n",
    "            \"tool\": \"prompt_tool\",\n",
    "            \"parameters\": {\n",
    "                \"name\": \"entity_extraction\",\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"api_key\": api_key,\n",
    "                \"format\": \"json\",\n",
    "                \"prompt_template\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"\"\"\n",
    "                            ** Context **\n",
    "                            You are an entity extraction system. The user will give you a piece of text.\n",
    "                            ** Task **\n",
    "                            Your task is to extract the entities from the text. \n",
    "                            ** Requirements **\n",
    "                            Reply with the following JSON format: {{ \"entities\": [\"entity1\", \"entity2\", ...] }}\n",
    "                        \"\"\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"human\",\n",
    "                        \"content\": \"{content}\"\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"label\": \"Relationship Extraction\",\n",
    "        \"state_input_key\": \"documents\",\n",
    "        \"doc_input_keys\": [\"content\", \"entities\"],\n",
    "        \"state_output_key\": \"relationships\",\n",
    "        \"parentIds\": [\"0\"],\n",
    "        \"execution\": {\n",
    "            \"tool\": \"prompt_tool\",\n",
    "            \"parameters\": {\n",
    "                \"name\": \"relationship_extraction\",\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"api_key\": api_key,\n",
    "                \"format\": \"json\",\n",
    "                \"prompt_template\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"\"\"\n",
    "                        ** Context **\n",
    "                        You are a relationship extraction system. The user will give you a piece of text and the entities extracted from it.\n",
    "                        ** Task **\n",
    "                        Your task is to extract the relationships between entities from the text. \n",
    "                        ** Requirements **\n",
    "                        Reply with the following JSON format: \n",
    "                            {{ \"relationships\": [\n",
    "                                {{\n",
    "                                    \"label\": (str, label for the relationship), \n",
    "                                    \"source\": (str, entity1),\n",
    "                                    \"target\": (str, entity2),\n",
    "                                    \"explanation\": (str, explanation of the relationship)\n",
    "                                }}, \n",
    "                                {{\n",
    "                                    \"label\": (str, label for the relationship), \n",
    "                                    \"source\": (str, entity1),\n",
    "                                    \"target\": (str, entity2),\n",
    "                                    \"explanation\": (str, explanation of the relationship)\n",
    "                                }}, \n",
    "                                ...\n",
    "                                ] \n",
    "                            }}\n",
    "                        \"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"human\",\n",
    "                        \"content\": \"Content: {content}\\n Entities: {entities}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "from langgraph_utils import create_graph\n",
    "graph = create_graph(test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "app = graph.compile()\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "app = graph.compile()\n",
    "docs = json.load(open(\"docs.json\"))\n",
    "final_state = app.invoke(\n",
    "    {\"documents\": docs},\n",
    "    config={\"configurable\": {\"thread_id\": 42}},\n",
    ")\n",
    "from pprint import pprint\n",
    "pprint(final_state.keys())\n",
    "pprint(final_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taskdecomposition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
