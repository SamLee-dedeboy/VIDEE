{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, RateLimitError, APITimeoutError\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import concurrent\n",
    "import time\n",
    "import json\n",
    "from pprint import pprint\n",
    "openai_client = OpenAI(api_key=open(\"api_key\").read().strip())\n",
    "def save_json(data, filename):\n",
    "    with open (filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def request_gpt(\n",
    "    client, messages, model=\"gpt-4o-mini\", temperature=0.5, format=None, seed=None\n",
    "):\n",
    "    with open(\"request_log.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"model: {model}, temperature: {temperature}, format: {format}\\n\")\n",
    "        f.write(json.dumps(messages, ensure_ascii=False) + \"\\n\")\n",
    "        f.write(\"=====================================\\n\")\n",
    "    try:\n",
    "        if format == \"json\":\n",
    "            response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    response_format={\"type\": \"json_object\"},\n",
    "                    temperature=temperature,\n",
    "                    seed=seed,\n",
    "                )\n",
    "            \n",
    "            try:\n",
    "                json.loads(response.choices[0].message.content)\n",
    "                return response.choices[0].message.content\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"JSON Decode Error\")\n",
    "                print(e)\n",
    "                time.sleep(5)\n",
    "                return request_gpt(client, messages, model, temperature=1.0, format=format)\n",
    "\n",
    "        else:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model, messages=messages, temperature=temperature, seed=seed\n",
    "            )\n",
    "        return response.choices[0].message.content\n",
    "    except RateLimitError as e:\n",
    "        print(\"RateLimitError\")\n",
    "        print(e)\n",
    "        time.sleep(5)\n",
    "        return request_gpt(client, messages, model, temperature, format)\n",
    "    except APITimeoutError as e:\n",
    "        print(\"APITimeoutError\")\n",
    "        print(messages)\n",
    "        time.sleep(5)\n",
    "        return request_gpt(client, messages, model, temperature, format)\n",
    "\n",
    "def multithread_prompts(\n",
    "    client,\n",
    "    prompts,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5,\n",
    "    response_format=None,\n",
    "    seed=None,\n",
    "):\n",
    "    l = len(prompts)\n",
    "    # results = np.zeros(l)\n",
    "    with tqdm(total=l) as pbar:\n",
    "        executor = concurrent.futures.ThreadPoolExecutor(max_workers=100)\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                request_gpt, client, prompt, model, temperature, response_format, seed\n",
    "            )\n",
    "            for prompt in prompts\n",
    "        ]\n",
    "        for _ in concurrent.futures.as_completed(futures):\n",
    "            pbar.update(1)\n",
    "    concurrent.futures.wait(futures)\n",
    "    return [future.result() for future in futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_task = [\n",
    "    {\n",
    "        \"label\": \"Document Preprocessing\",\n",
    "        \"description\": \"Clean and preprocess the text from the Wikipedia documents.\",\n",
    "        \"explanation\": \"Preprocessing is essential to remove noise and irrelevant information, which will enhance the quality of the extraction process and ensure consistency in the analysis.\",\n",
    "        \"depend_on\": [],\n",
    "        \"id\": \"0\",\n",
    "        \"parentIds\": []\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Entity Recognition\",\n",
    "        \"description\": \"Identify key entities (like people, places, organizations) within the preprocessed documents.\",\n",
    "        \"explanation\": \"Entity recognition is crucial as it helps in identifying the main subjects or nodes that will form the foundation of the knowledge graph.\",\n",
    "        \"depend_on\": [\n",
    "            \"Document Preprocessing\"\n",
    "        ],\n",
    "        \"id\": \"1\",\n",
    "        \"parentIds\": [\n",
    "            \"0\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Relationship Extraction\",\n",
    "        \"description\": \"Extract relationships between the identified entities.\",\n",
    "        \"explanation\": \"Understanding how entities relate to each other is key to building the connections in the knowledge graph, which provides context and meaning.\",\n",
    "        \"depend_on\": [\n",
    "            \"Entity Recognition\"\n",
    "        ],\n",
    "        \"id\": \"2\",\n",
    "        \"parentIds\": [\n",
    "            \"1\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Ontology Definition\",\n",
    "        \"description\": \"Define the structure and schema of the knowledge graph, including classes and properties.\",\n",
    "        \"explanation\": \"Creating an ontology is needed to formalize the relationships and hierarchy of the entities, ensuring the knowledge graph is organized and interpretable.\",\n",
    "        \"depend_on\": [\n",
    "            \"Entity Recognition\",\n",
    "            \"Relationship Extraction\"\n",
    "        ],\n",
    "        \"id\": \"3\",\n",
    "        \"parentIds\": [\n",
    "            \"1\",\n",
    "            \"2\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Graph Construction\",\n",
    "        \"description\": \"Construct the knowledge graph using the identified entities, relationships, and defined ontology.\",\n",
    "        \"explanation\": \"This step is where the actual graph is built, combining all the previous outputs into a structured format that can be utilized for querying and analysis.\",\n",
    "        \"depend_on\": [\n",
    "            \"Ontology Definition\"\n",
    "        ],\n",
    "        \"id\": \"4\",\n",
    "        \"parentIds\": [\n",
    "            \"3\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Validation and Refinement\",\n",
    "        \"description\": \"Validate the knowledge graph for accuracy and completeness, and refine it as necessary.\",\n",
    "        \"explanation\": \"Validation ensures the knowledge graph accurately represents the information extracted from the documents; refinement helps improve its quality and usability.\",\n",
    "        \"depend_on\": [\n",
    "            \"Graph Construction\"\n",
    "        ],\n",
    "        \"id\": \"5\",\n",
    "        \"parentIds\": [\n",
    "            \"4\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "semantic_tasks_str = json.dumps(semantic_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read defs\n",
    "elementary_task_defs = json.load(open(\"elementary_task_defs.json\"))\n",
    "elementary_task_defs_str = \"\"\n",
    "for elementary_task in elementary_task_defs:\n",
    "    elementary_task_defs_str += \"<elementary_task>\\n\"\n",
    "    for key, value in elementary_task.items():\n",
    "        elementary_task_defs_str += f\"<{key}>{value}</{key}>\\n\"\n",
    "    elementary_task_defs_str += \"</elementary_task>\\n\"\n",
    "# Decompose a tree of semantic tasks into elementary tasks\n",
    "prompts = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "        ** Context **\n",
    "        You are a Natural Language Processing (NLP) assistant. You are given a list of elementary NLP tasks that could be used.\n",
    "        Here is the list of elementary NLP tasks:\n",
    "        {elementary_task_defs}\n",
    "        ** Task **\n",
    "        The user will describe a series of real-world tasks for you. First, for each of the task, decide if it can be formulated as an NLP task. If yes, you need to find the proper elementary NLP tasks and arrange them to accomplish user's goal. \n",
    "        You can ignore the need to handle formats or evaluate outputs.\n",
    "        ** Requirements **\n",
    "        The labels of the elementary task must match exactly as those provided above. Reply with the following JSON format: \n",
    "        {{ \"elementary_tasks\": [ \n",
    "                {{ \n",
    "                    \"id\": int,\n",
    "                    \"label\": (string) (one of the above)\n",
    "                    \"description\": (string, describe implementation procedure)\n",
    "                    \"explanation\": (string, explain why this task is needed)\n",
    "                    \"depend_on\": (string[], the tasks task this step depends on)\n",
    "                    \"input\": string,\n",
    "                    \"output\": string,\n",
    "                    \"example_output\": string,\n",
    "                }}, \n",
    "                {{ \n",
    "                    \"id\": int,\n",
    "                    \"label\": (string) (one of the above)\n",
    "                    \"description\": (string, describe implementation procedure)\n",
    "                    \"explanation\": (string, explain why this task is needed)\n",
    "                    \"depend_on\": (int[], the task ids that this task depends on)\n",
    "                    \"input\": string,\n",
    "                    \"output\": string,\n",
    "                    \"example_output\": string,\n",
    "                }}, \n",
    "                ... \n",
    "            ] \n",
    "        }}\n",
    "        \"\"\".format(elementary_task_defs=elementary_task_defs)\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"{semantic_tasks_str}\"\n",
    "    }\n",
    "]\n",
    "response = request_gpt(openai_client, prompts, model=\"gpt-4o-mini\", temperature=0.0, format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response)\n",
    "import json\n",
    "test_response = \"\"\"\n",
    "{\n",
    "  \"elementary_tasks\": [\n",
    "    {\n",
    "      \"label\": \"Information Extraction\",\n",
    "      \"description\": \"Identify and extract key entities (e.g., people, places, organizations) from the documents using defined techniques.\",\n",
    "      \"explanation\": \"Entity extraction is the first step in building a knowledge graph, identifying primary components that will form the nodes of the graph.\",\n",
    "      \"depend_on\": [],\n",
    "      \"input\": \"Documents\",\n",
    "      \"output\": \"Extracted Entities\",\n",
    "      \"example_output\": \"{'Person': 'Barack Obama', 'Location': 'Honolulu', 'Organization': 'United States'}\"\n",
    "    },\n",
    "    {\n",
    "      \"label\": \"Information Extraction\",\n",
    "      \"description\": \"Determine the relationships between the extracted entities based on the context of the documents.\",\n",
    "      \"explanation\": \"Understanding the relationships allows us to connect the entities and define the edges of the knowledge graph, which is crucial for representing the semantic structure.\",\n",
    "      \"depend_on\": [\"Extract Entities\"],\n",
    "      \"input\": \"Extracted Entities and Context\",\n",
    "      \"output\": \"Identified Relationships\",\n",
    "      \"example_output\": \"{'Entity A': 'Barack Obama', 'Entity B': 'United States', 'Relationship': 'Presidency'}\"\n",
    "    },\n",
    "    {\n",
    "      \"label\": \"Define Ontology\",\n",
    "      \"description\": \"Establish a schema or ontology that categorizes the types of entities and relationships defined from the previous steps.\",\n",
    "      \"explanation\": \"An ontology helps in standardizing how entities and relationships are represented, making the knowledge graph more structured and interpretable.\",\n",
    "      \"depend_on\": [\"Extract Entities\", \"Identify Relationships\"],\n",
    "      \"input\": \"Entity Types and Relationships\",\n",
    "      \"output\": \"Defined Ontology\",\n",
    "      \"example_output\": \"{'Entity Types': ['Person', 'Organization', 'Location'], 'Relationships': ['Works At', 'Lives In']}\"\n",
    "    },\n",
    "    {\n",
    "      \"label\": \"Construct Knowledge Graph\",\n",
    "      \"description\": \"Create the knowledge graph by integrating the extracted entities and their relationships according to the defined ontology.\",\n",
    "      \"explanation\": \"This step involves the actual creation of the graph structure that can be visualized or queried, which is the end goal of the task.\",\n",
    "      \"depend_on\": [\"Define Ontology\", \"Identify Relationships\"],\n",
    "      \"input\": \"Extracted Entities and Relationships\",\n",
    "      \"output\": \"Knowledge Graph\",\n",
    "      \"example_output\": \"{'Nodes': ['Barack Obama', 'United States'], 'Edges': ['Barack Obama - Presidency -> United States']}\"\n",
    "    },\n",
    "    {\n",
    "      \"label\": \"Iterate and Refine\",\n",
    "      \"description\": \"Review and refine the knowledge graph to improve accuracy and completeness.\",\n",
    "      \"explanation\": \"It’s essential to validate the graph to ensure it accurately represents the information from the documents and to address any gaps or errors.\",\n",
    "      \"depend_on\": [\"Construct Knowledge Graph\"],\n",
    "      \"input\": \"Initial Knowledge Graph\",\n",
    "      \"output\": \"Refined Knowledge Graph\",\n",
    "      \"example_output\": \"{'Refined Nodes': ['Barack Obama', 'United States', 'Democrat'], 'Refined Edges': ['Barack Obama - Presidency -> United States']}\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "elementary_tasks = json.loads(test_response)[\"elementary_tasks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elementary_task_defs_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [{'id': '1', 'label': 'Extract Entities', 'description': 'Identify and extract key entities (people, places, organizations, etc.) from the documents.', 'explanation': 'This step is crucial for populating the knowledge graph with relevant and meaningful nodes, which represent different concepts and facts from the documents.', 'depend_on': [], 'parentIds': [], 'children': ['2', '3']}, {'id': '2', 'label': 'Identify Relationships', 'description': 'Determine relationships between the extracted entities based on the context of the documents.', 'explanation': 'Identifying relationships helps in defining how entities are connected in the knowledge graph, enabling a better representation of the information landscape.', 'depend_on': ['1'], 'parentIds': ['1'], 'children': ['3', '4']}, {'id': '3', 'label': 'Define Schema', 'description': 'Create a schema that outlines the types of entities and their possible relationships.', 'explanation': 'A well-defined schema is necessary to ensure that the knowledge graph is structured, understandable, and queryable, facilitating accurate representation of the data.', 'depend_on': ['1', '2'], 'parentIds': ['1', '2'], 'children': ['4']}, {'id': '4', 'label': 'Construct Graph', 'description': 'Assemble the entities and relationships into a graphical representation based on the schema.', 'explanation': 'Constructing the graph is the final step in visually organizing the extracted entities and relationships into a coherent structure that can be analyzed and queried.', 'depend_on': ['2', '3'], 'parentIds': ['2', '3'], 'children': []}]\n",
    "\n",
    "decomposed_semantic_tasks = list(\n",
    "    map(\n",
    "        lambda t: t.update({\"parentIds\": t[\"depend_on\"], \"children\": []}),\n",
    "        test,\n",
    "    )\n",
    ")\n",
    "decomposed_semantic_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "all_rows = []\n",
    "elementary_task_defs = json.load(open(\"elementary_task_defs.json\"))\n",
    "for task_def in elementary_task_defs:\n",
    "    all_rows.append({\n",
    "        \"label\": task_def['label'],\n",
    "        \"definition\": task_def['definition'],\n",
    "        \"input\": task_def['input'],\n",
    "        \"output\": task_def['output'],\n",
    "    })\n",
    "\n",
    "with open('elementary_task_defs.csv', 'w',) as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(list(all_rows[0].keys()))\n",
    "    for row in all_rows:\n",
    "        writer.writerow(list(row.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Node(BaseModel):\n",
    "    id: str\n",
    "    label: str\n",
    "    description: str\n",
    "    explanation: str\n",
    "    depend_on: list[str]\n",
    "    parentIds: list[str]\n",
    "    children: list[\"Node\"] = []\n",
    "    confidence: float\n",
    "    complexity: float\n",
    "\n",
    "    class Config:\n",
    "        orm_mode = True\n",
    "\n",
    "\n",
    "async def run_prompt_generation_agent(task: Node, existing_keys: str, model: str, api_key: str):\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=model,\n",
    "        api_key=api_key,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    prompt_generation_agent = AssistantAgent(\n",
    "        name=\"prompt_generation_agent\",\n",
    "        model_client=model_client,\n",
    "        system_message= \"\"\"\n",
    "        ** Context **\n",
    "        You are an expert in writing prompts for Large Language Models, especially for generating prompts that analyze a given piece of text.\n",
    "        ** Task **\n",
    "        The user will describe the task and provide a piece of text. You need to generate a prompt that can be used to analyze the text for the given task.\n",
    "        ** Requirements **\n",
    "        First, decide what data in each document is needed to complete the task. Then, generate a prompt that instructs an LLM to analyze the text for the given task.\n",
    "        Organize the prompt into these three sections:\n",
    "        1. Input Keys: List the keys that are required from the document to complete the task.\n",
    "        2. Context: Give instructions on what the user is trying to do.\n",
    "        3. Task: Give instructions on how to analyze the text.\n",
    "        4. Requirements: Provide any specific requirements or constraints for the prompt.\n",
    "        In addition, give a key name suitable to store the result of the prompt, and define a JSON format for the output.\n",
    "        Here are the data already exists in the dataset: {existing_keys}\n",
    "        They do not need to be included in the JSON format.\n",
    "        Reply with this JSON format:\n",
    "            {{\n",
    "               \"prompt\": {{\n",
    "                    \"Context\": str,\n",
    "                    \"Task\": str,\n",
    "                    \"Requirements\": str\n",
    "                    \"JSON_format\": str\n",
    "                }}\n",
    "            }}\n",
    "        \"\"\".format(existing_keys=existing_keys),\n",
    "    )\n",
    "    task_message = f\"\"\"\n",
    "        <task_name> {task['label']} </task_name>\n",
    "        <description> {task['description']} </description>\n",
    "        <explanation> {task['explanation']} </explanation>\n",
    "    \"\"\"\n",
    "    response = await prompt_generation_agent.on_messages(\n",
    "        [TextMessage(content=task_message, source=\"user\")],\n",
    "        cancellation_token=CancellationToken(),\n",
    "    )\n",
    "    return json.loads(response.chat_message.content)[\"prompt\"]\n",
    "\n",
    "async def run_input_key_generation_agent(task: Node, existing_keys: str, model: str, api_key: str):\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=model,\n",
    "        api_key=api_key,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    prompt_generation_agent = AssistantAgent(\n",
    "        name=\"input_key_generation_agent\",\n",
    "        model_client=model_client,\n",
    "        system_message= \"\"\"\n",
    "        ** Context **\n",
    "        You are an expert in text analytics.\n",
    "        ** Task **\n",
    "        The user will describe a task for you, and what data is available in the dataset. Your task is to pick the keys that are required from the dataset to complete the task.\n",
    "        ** Requirements **\n",
    "        Reply with this JSON format:\n",
    "            {{\n",
    "               \"required keys\": str[]\n",
    "            }}\n",
    "        \"\"\",\n",
    "    )\n",
    "    task_message = f\"existing keys: {existing_keys}\"\n",
    "    response = await prompt_generation_agent.on_messages(\n",
    "        [TextMessage(content=task_message, source=\"user\")],\n",
    "        cancellation_token=CancellationToken(),\n",
    "    )\n",
    "    return json.loads(response.chat_message.content)[\"required keys\"]\n",
    "\n",
    "api_key = open(\"api_key\").read().strip()\n",
    "primitive_tasks = json.load(open(\"../test_primitive_tasks.json\"))\n",
    "test_task, existing_keys = primitive_tasks[0], [\"content\"]\n",
    "test_task, existing_keys = primitive_tasks[1], [\"content\", \"entities\"]\n",
    "input_keys = await run_input_key_generation_agent(test_task, existing_keys, \"gpt-4o-mini\", api_key)\n",
    "print(input_keys)\n",
    "prompt = await run_prompt_generation_agent(test_task, input_keys, \"gpt-4o-mini\", api_key)\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{content}\"\n",
    "keys = [\"content\", \"entities\"]\n",
    "\"\\n\".join([f\"{key}: {{{key}}}\" for key in keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_structured = {'Input Keys': 'content', 'Context': 'You are tasked with extracting structured information from a given text document. The goal is to identify key entities that can be used as nodes in a knowledge graph.', 'Task': 'Analyze the provided text and extract relevant entities such as names, organizations, locations, dates, and other significant terms. Ensure that the extracted entities are categorized appropriately.', 'Requirements': 'The extracted entities should be presented in a structured format, clearly indicating their types. Avoid including any irrelevant information or context from the original text.', 'JSON_format': \"{ 'entities': [ { 'name': 'entity_name', 'type': 'entity_type' } ] }\"}\n",
    "print(str(prompt_structured['JSON_format']).replace(\"'\", '\"'))\n",
    "list(json.loads(str(prompt_structured['JSON_format']).replace(\"'\", '\"')).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "format_schema = \"{ 'relationships': [ { 'name': 'entity_name', 'type': 'entity_type' }, ... ] }\"\n",
    "pattern = r\"\\{\\s*'(\\w+)'\\s*:\\s*\\[\"\n",
    "\n",
    "# Search for the match\n",
    "match = re.search(pattern, format_schema)\n",
    "\n",
    "# Extract and print the result\n",
    "if match:\n",
    "    print(match.group(1))  # Output: entities\n",
    "else:\n",
    "    print(\"No match found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_schema = \"{ 'relationships': [ { 'entity1': 'string', 'entity2': 'string', 'relationship': 'string', 'context': 'string' } ] }\"\n",
    "format_schema = re.sub(\n",
    "    r\"\\{\", r\"{{\", prompt_structured[\"JSON_format\"]\n",
    ")\n",
    "format_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_schema = {'JSON_format': \"{'entities': [{'name': 'string', 'category': 'string'}...]}\"}\n",
    "print(isinstance(format_schema['JSON_format'], dict))\n",
    "print(isinstance(format_schema['JSON_format'], str))\n",
    "# format_schema['JSON_format'] = json.dumps(format_schema['JSON_format'])\n",
    "pattern = r'\\{\\s*\"(\\w+)\"\\s*:\\s*\\['\n",
    "print(format_schema['JSON_format'])\n",
    "format_schema['JSON_format'] = format_schema['JSON_format'].replace(\"'\", '\"')\n",
    "match = re.search(pattern, format_schema['JSON_format'])\n",
    "match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def run_json_schema_correction_agent(schema: str, model: str, api_key: str):\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=model,\n",
    "        api_key=api_key,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    json_schema_correction_agent = AssistantAgent(\n",
    "        name=\"json_schema_correction_agent\",\n",
    "        model_client=model_client,\n",
    "        system_message= \"\"\"\n",
    "        ** Context **\n",
    "        You are an expert in JSON schema\n",
    "        ** Task **\n",
    "        The user will give you an invalid JSON schema. Your task is to correct the JSON schema to make it valid.\n",
    "        ** Requirements **\n",
    "        Reply with this JSON format:\n",
    "            {{\n",
    "               \"corrected_schema\": str[]\n",
    "            }}\n",
    "        \"\"\",\n",
    "    )\n",
    "    task_message = f\"existing keys: {existing_keys}\"\n",
    "    response = await prompt_generation_agent.on_messages(\n",
    "        [TextMessage(content=task_message, source=\"user\")],\n",
    "        cancellation_token=CancellationToken(),\n",
    "    )\n",
    "    return json.loads(response.chat_message.content)[\"required keys\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "import concurrent\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "\n",
    "\n",
    "async def run_goal_decomposition_agent_stepped(goal: str, previous_steps: list, model: str, api_key: str, n=1):\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=model,\n",
    "        api_key=api_key,\n",
    "        temperature=0.0,\n",
    "        model_capabilities={\n",
    "            \"vision\": False,\n",
    "            \"function_calling\": False,\n",
    "            \"json_output\": True,\n",
    "        },\n",
    "    )\n",
    "    goal_decomposition_agent = AssistantAgent(\n",
    "        name=\"goal_decomposition_agent\",\n",
    "        model_client=model_client,\n",
    "        system_message=\"\"\"\n",
    "        ** Context **\n",
    "        You are a text analytics task planner. \n",
    "        Users have collected a dataset of documents. The user will describe a goal to achieve through some text analytics, and what they have done already.\n",
    "        ** Task **\n",
    "        Your task is to provide a single next step based on what the user have done so far.\n",
    "        ** Requirements **\n",
    "        Please specify the logical next step to take.\n",
    "        Ignore the practical steps such as data collection, cleaning or visualization.\n",
    "        Focus on the conceptual next step. If no further steps are needed, label the next step with \"END\".\n",
    "        Reply with this JSON format:\n",
    "            {\n",
    "                \"next_step\": {\n",
    "                        \"id\": (int),\n",
    "                        \"label\": (string) or \"END\"\n",
    "                        \"description\": (string)\n",
    "                        \"explanation\": (string, explain why this step is needed)\n",
    "                        \"depend_on\": (int[], ids of the steps that this step depends on)\n",
    "                    },\n",
    "            }  \"\"\",\n",
    "    )\n",
    "    user_message = \"My goal is: {goal}\".format(goal=goal) + \"\\n\"\n",
    "    if len(previous_steps) > 0:\n",
    "        previous_steps_str = \"\\n\".join(list(map(lambda s: f\"{s['label']}: {s['description']}\", previous_steps)))\n",
    "        user_message += \"Here are the steps that I have done so far: \\n{previous_steps}\".format(previous_steps=previous_steps_str)\n",
    "    if n == 1:\n",
    "        response = await goal_decomposition_agent.on_messages(\n",
    "            [TextMessage(content=user_message, source=\"user\")],\n",
    "            cancellation_token=CancellationToken(),\n",
    "        )\n",
    "        return json.loads(response.chat_message.content)[\"next_step\"]\n",
    "    else:\n",
    "        responses = await parallel_call_agents(n, goal_decomposition_agent, user_message)\n",
    "        responses = [json.loads(response.chat_message.content)[\"next_step\"] for response in responses]\n",
    "        return responses\n",
    "\n",
    "async def run_decomposition_self_evaluation_agent(goal: str, previous_steps: list, next_step: str, model: str, api_key: str, n=1):\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=model,\n",
    "        api_key=api_key,\n",
    "        temperature=0.0,\n",
    "        model_capabilities={\n",
    "            \"vision\": False,\n",
    "            \"function_calling\": False,\n",
    "            \"json_output\": True,\n",
    "        },\n",
    "    )\n",
    "    decomposition_self_evaluation_agent = AssistantAgent(\n",
    "        name=\"decomposition_self_evaluation_agent\",\n",
    "        model_client=model_client,\n",
    "        system_message=\"\"\"\n",
    "        ** Context **\n",
    "        You are a text analytics expert.\n",
    "        Users will describe a text analytics goal and the steps they have taken to achieve it.\n",
    "        ** Task **\n",
    "        Your task is to evaluate the correctness of the next step provided by the user.\n",
    "        ** Requirements **\n",
    "        Give a score from 0 to 5, where 0 is completely incorrect and 5 is perfect.\n",
    "        Reply with this JSON format:\n",
    "            {\n",
    "                \"evaluation_score\": (int) 0-5\n",
    "            }  \"\"\",\n",
    "    )\n",
    "    user_message = \"My goal is: {goal}\".format(goal=goal) + \"\\n\" \n",
    "    if len(previous_steps) > 0:\n",
    "        previous_steps_str = \"\\n\".join(list(map(lambda s: f\"{s['label']}: {s['description']}\", previous_steps)))\n",
    "        user_message += \"Here are the steps that I have done so far: \\n{previous_steps}\".format(previous_steps=previous_steps_str) \n",
    "    user_message += \"\\nHere is the next step that I think I should take: {next_step}\".format(next_step=next_step)\n",
    "\n",
    "    \n",
    "    if n == 1:\n",
    "        response = await decomposition_self_evaluation_agent.on_messages(\n",
    "            [TextMessage(content=user_message, source=\"user\")],\n",
    "            cancellation_token=CancellationToken(),\n",
    "        )\n",
    "        return json.loads(response.chat_message.content)[\"evaluation_score\"]\n",
    "    else:\n",
    "        responses = await parallel_call_agents(n, decomposition_self_evaluation_agent, user_message)\n",
    "        responses = [json.loads(response.chat_message.content)[\"evaluation_score\"] for response in responses]\n",
    "        return responses\n",
    "\n",
    "async def call_agent(agent, user_message):\n",
    "    response = await agent.on_messages(\n",
    "        [TextMessage(content=user_message, source=\"user\")],\n",
    "        cancellation_token=CancellationToken(),\n",
    "    )\n",
    "    return response\n",
    "\n",
    "async def parallel_call_agents(n, agent, user_message):\n",
    "    tasks = [call_agent(agent, user_message) for _ in range(n)]\n",
    "    \n",
    "    results = []\n",
    "    with tqdm(total=n) as pbar:\n",
    "        for coro in asyncio.as_completed(tasks):\n",
    "            result = await coro\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_steps = []\n",
    "api_key = open(\"api_key\").read().strip()\n",
    "model = \"gpt-4o-mini\"\n",
    "goal = \"I need to construct a knowledge graph from a collection of documents from wikipedia.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_step = await run_goal_decomposition_agent_stepped(goal, all_steps, model, api_key)\n",
    "evaluation_score = await run_decomposition_self_evaluation_agent(goal, all_steps, next_step, model, api_key)\n",
    "all_steps.append(next_step)\n",
    "print(evaluation_score)\n",
    "next_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "async def decode_n_samples(goal: str, previous_steps: list, model: str, api_key: str, n: int=2):\n",
    "    candidate_steps = []\n",
    "    next_steps = await run_goal_decomposition_agent_stepped(goal, previous_steps, model, api_key, n)\n",
    "    evaluation_scores = await run_decomposition_self_evaluation_agent(goal, previous_steps, next_steps, model, api_key, n)\n",
    "    for next_step, evaluation_score in zip(next_steps, evaluation_scores):\n",
    "        new_beam = copy.deepcopy(previous_steps)\n",
    "        new_beam.append(next_step)\n",
    "        candidate_steps.append((new_beam, evaluation_score))\n",
    "    return candidate_steps\n",
    "\n",
    "async def beam_search_decomposition(goal: str, model: str, api_key: str, k: int = 2, n: int = 2):\n",
    "    candidate_steps = []\n",
    "    iteration = 0\n",
    "    candidate_steps += await decode_n_samples(goal, [], model, api_key, n)\n",
    "    print(f\"candidate steps after iteration {iteration}: \\n{candidate_steps}\")\n",
    "    iteration += 1\n",
    "    while True:\n",
    "        candidate_steps = await beam_search_decomposition_step(goal, candidate_steps, model, api_key, k, n)\n",
    "        print(\"iteration: \", iteration)\n",
    "        print(list(map(lambda s: s[0][-1]['label'], candidate_steps)))\n",
    "        iteration += 1\n",
    "        # check if all beams are END\n",
    "        if len(candidate_steps) > 0 and all([beam[-1][\"label\"] == \"END\" for beam, _ in candidate_steps]):\n",
    "            break\n",
    "    return candidate_steps\n",
    "\n",
    "async def beam_search_decomposition_step(goal: str, candidate_steps: list, model: str, api_key: str, k: int = 2, n: int = 2):\n",
    "    new_candidates = []\n",
    "    for beam, score in candidate_steps:\n",
    "        previous_steps = beam\n",
    "        if previous_steps[-1][\"label\"] == \"END\":\n",
    "            continue\n",
    "        new_candidates += await decode_n_samples(goal, previous_steps, model, api_key, n)\n",
    "    candidate_steps = new_candidates\n",
    "    # sort by score\n",
    "    candidate_steps.sort(key=lambda x: x[1], reverse=True)\n",
    "    # select top k steps\n",
    "    candidate_steps = candidate_steps[:k]\n",
    "    return candidate_steps\n",
    "\n",
    "# api_key = open(\"api_key\").read().strip()\n",
    "# goal = \"I need to construct a knowledge graph from a collection of documents from wikipedia.\"\n",
    "# model = \"gpt-4o-mini\"\n",
    "# top_k_decomposition = await beam_search_decomposition(goal, model, api_key, k=2, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.09it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from fastapi.responses import StreamingResponse\n",
    "\n",
    "api_key = open(\"api_key\").read().strip()\n",
    "goal = \"I need to construct a knowledge graph from a collection of documents from wikipedia.\"\n",
    "model = \"gpt-4o-mini\"\n",
    "k=2\n",
    "n=2\n",
    "async def iter_response(candidate_steps):\n",
    "    while True:\n",
    "        candidate_steps = await beam_search_decomposition_step(goal, candidate_steps, model, api_key, k=k, n=n)\n",
    "        yield candidate_steps\n",
    "        if len(candidate_steps) > 0 and all([beam[-1][\"label\"] == \"END\" for beam, _ in candidate_steps]):\n",
    "            break\n",
    "        # time.sleep(5)  # Simulating processing time\n",
    "\n",
    "candidate_steps = []\n",
    "candidate_steps += await decode_n_samples(goal, [], model, api_key, n)\n",
    "stream = iter_response(candidate_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 4\n",
      "[{'id': 1, 'label': 'Entity Extraction', 'description': 'Extract entities and their relationships from the documents.', 'explanation': 'This step is crucial as it allows you to identify the key components (entities) and how they are related to each other, which is foundational for constructing a knowledge graph.', 'depend_on': []}] 4\n",
      "==================================\n",
      "iteration 1 5\n",
      "[{'id': 1, 'label': 'Entity Extraction', 'description': 'Extract entities and their relationships from the documents.', 'explanation': 'Entity extraction is crucial for constructing a knowledge graph as it identifies the key components (entities) and their interconnections (relationships) within the text, which will form the nodes and edges of the graph.', 'depend_on': []}] 5\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "for i, (semantic_tasks, eval_score) in enumerate(candidate_steps):\n",
    "    print(f\"iteration {i}\", eval_score)\n",
    "    print(semantic_tasks, eval_score)\n",
    "    print(\"==================================\")\n",
    "# candidate_steps = await anext(stream)\n",
    "# async for steps in stream:\n",
    "#     print(steps[0][-1], steps[1][-1])\n",
    "#     print(\"==================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([{'id': -1,\n",
       "    'label': 'Entity Extraction',\n",
       "    'description': 'Extract entities and their relationships from the documents.',\n",
       "    'explanation': 'This step is crucial as it allows you to identify the key components (entities) and how they are related to each other, which is foundational for constructing a knowledge graph.',\n",
       "    'depend_on': []},\n",
       "   {'id': 2,\n",
       "    'label': 'Relation Mapping',\n",
       "    'description': 'Map the extracted entities to their relationships to form a structured representation.',\n",
       "    'explanation': 'This step is crucial as it will help in organizing the extracted entities into a coherent structure that reflects their interconnections, which is essential for constructing a knowledge graph.',\n",
       "    'depend_on': [1]}],\n",
       "  5),\n",
       " ([{'id': -1,\n",
       "    'label': 'Entity Extraction',\n",
       "    'description': 'Extract entities and their relationships from the documents.',\n",
       "    'explanation': 'This step is crucial as it allows you to identify the key components (entities) and how they are related to each other, which is foundational for constructing a knowledge graph.',\n",
       "    'depend_on': []},\n",
       "   {'id': 1,\n",
       "    'label': 'Entity Linking',\n",
       "    'description': 'Link the extracted entities to a knowledge base or ontology.',\n",
       "    'explanation': 'Entity linking is crucial to ensure that the extracted entities are connected to a broader context, allowing for the construction of a more meaningful and interconnected knowledge graph.',\n",
       "    'depend_on': [1]}],\n",
       "  5)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\".\")) \n",
    "sys.path.append(os.path.abspath(\"../\")) \n",
    "sys.path.append(os.path.abspath(\"../../\")) \n",
    "\n",
    "from server.AutoGenUtils import query\n",
    "class MCT_Node(BaseModel):\n",
    "    # Task properties\n",
    "    id: str\n",
    "    label: str\n",
    "    description: str\n",
    "    explanation: str\n",
    "    parentIds: list[str]    \n",
    "    # MCT properties\n",
    "    print_label: str = \"N/A\"\n",
    "    MCT_id: str\n",
    "    MCT_parent_id: Optional[str]\n",
    "    MCT_children_ids: list[str] = []\n",
    "    visits: int = 0\n",
    "    value: float = 0.0\n",
    "\n",
    "async def MCTS_step(root: MCT_Node, node_dict: dict, goal: str, model: str, api_key: str) -> MCT_Node:\n",
    "    node = select(root, node_dict)\n",
    "    child = await expand(node, node_dict, goal, model, api_key)\n",
    "    reward_value = await reward(child)\n",
    "    backpropagate(child, reward_value, node_dict)\n",
    "    return root\n",
    "\n",
    "def UCT(node: MCT_Node, parent_node: MCT_Node | None, exploration_weight=1.41) -> float:\n",
    "    \"\"\" Upper Confidence Bound for Trees (UCT) selection \"\"\"\n",
    "    if node.visits == 0:\n",
    "        return float('inf')  # Prioritize unvisited nodes\n",
    "    if node.label == \"END\":\n",
    "        return float('-inf')\n",
    "    if parent_node is None:\n",
    "        parent_visits = 1\n",
    "    else:\n",
    "        parent_visits = parent_node.visits\n",
    "    return (node.value / node.visits) + exploration_weight * (math.sqrt(math.log(parent_visits) / node.visits))\n",
    "\n",
    "def select(node: MCT_Node, node_dict: dict) -> MCT_Node:\n",
    "    while node.MCT_children_ids:\n",
    "        parent_node = node_dict[node.MCT_parent_id] if node.MCT_parent_id else None\n",
    "        node = max(list(map(lambda node_id: node_dict[node_id], node.MCT_children_ids)), key=lambda node: UCT(node, parent_node))\n",
    "    return node\n",
    "\n",
    "async def expand(parent_node: MCT_Node, node_dict: dict, goal: str, model: str, api_key: str, n=2) -> MCT_Node:\n",
    "    \"\"\" Expands the node by adding one of its possible children \"\"\"\n",
    "\n",
    "    # new_node = MCT_Node(id=f\"{parent_node.MCT_id}/{-1}\", label=\"END\", description=\"END\", explanation=\"END\", parentIds=[parent_node.MCT_id], MCT_id=f\"{parent_node.MCT_id}/{-1}\", MCT_parent_id=parent_node.MCT_id)\n",
    "    # node_dict[new_node.MCT_id] = new_node\n",
    "    # parent_node.MCT_children_ids.append(new_node.MCT_id)\n",
    "    # return new_node\n",
    "    previous_steps = get_previous_steps(parent_node, node_dict)\n",
    "    if not is_END(parent_node):\n",
    "        children = await query.run_goal_decomposition_agent_stepped(goal, previous_steps, model=model, api_key=api_key, temperature=1.0, n=n)\n",
    "        for index, child_node in enumerate(children):\n",
    "            print(child_node)\n",
    "            child_as_MCT_node = MCT_Node(**child_node, \n",
    "                                         MCT_id=f\"{parent_node.MCT_id}/{index}\", \n",
    "                                         print_label=f\"{child_node['label']} (0/0)\", \n",
    "                                         MCT_parent_id=parent_node.MCT_id\n",
    "                                        )\n",
    "            node_dict[child_as_MCT_node.MCT_id] = child_as_MCT_node\n",
    "            parent_node.MCT_children_ids.append(child_as_MCT_node.MCT_id)\n",
    "        return node_dict[random.choice(parent_node.MCT_children_ids)]\n",
    "    return parent_node  # No expansion if node is terminal\n",
    "\n",
    "async def reward(node: MCT_Node) -> float:\n",
    "    return random.random()\n",
    "    # evaluation_score = await run_decomposition_self_evaluation_agent(goal, all_steps, node, model, api_key)\n",
    "    return evaluation_score\n",
    "\n",
    "def backpropagate(node: MCT_Node, reward: float, node_dict: dict) -> None:\n",
    "    \"\"\" Updates the tree with the simulation results \"\"\"\n",
    "    while node is not None:\n",
    "        node.visits += 1\n",
    "        node.value += reward  # Should we do some normalization here to avoid inflation?\n",
    "        node.print_label = f\"{node.label} ({node.value}/{node.visits})\"\n",
    "        node = node_dict[node.MCT_parent_id] if node.MCT_parent_id else None\n",
    "\n",
    "# def best_child(node: MCT_Node, node_dict: dict) -> MCT_Node:\n",
    "#     return max(list(map(lambda id: node_dict[id], node.MCT_children_ids)), key=lambda c: c.visits) # most visits or highest value?\n",
    "\n",
    "def get_previous_steps(node: MCT_Node, node_dict: dict) -> list[dict]:\n",
    "    steps = []\n",
    "    while node.MCT_parent_id:\n",
    "        steps.append(dict(node))\n",
    "        node = node_dict[node.MCT_parent_id]\n",
    "    return steps\n",
    "\n",
    "def is_END(node: MCT_Node):\n",
    "    return node.label == \"END\"\n",
    "    \n",
    "from treelib import Node, Tree\n",
    "def visualize_tree(root: MCT_Node, node_dict: dict):\n",
    "    tree = Tree()\n",
    "    # bread-first traversal\n",
    "    queue = [root]\n",
    "    while queue:\n",
    "        node = queue.pop(0)\n",
    "        tree.create_node(node.print_label, node.MCT_id, parent=node.MCT_parent_id)\n",
    "        queue += list(map(lambda id: node_dict[id], node.MCT_children_ids))\n",
    "    print(tree.show(stdout=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = open(\"api_key\").read().strip()\n",
    "model = \"gpt-4o-mini\"\n",
    "goal = \"I need to construct a knowledge graph from a collection of documents from wikipedia.\"\n",
    "root = MCT_Node(id=\"root\", label=\"Root\", MCT_id=\"-1\", print_label=\"Root\", description=\"Root node\", explanation=\"Root node\", parentIds=[], MCT_parent_id=None)\n",
    "node_dict = {root.MCT_id: root}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"next_step\": {\n",
      "        \"id\": \"END\",\n",
      "        \"label\": \"END\",\n",
      "        \"description\": \"You have successfully completed the necessary steps to construct a knowledge graph from the Wikipedia documents.\",\n",
      "        \"explanation\": \"Since you have extracted entities and relationships, constructed the knowledge graph, validated it, and analyzed it for insights, all essential tasks for achieving your goal are completed.\",\n",
      "        \"parentIds\": []\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"next_step\": {\n",
      "        \"id\": \"step5\",\n",
      "        \"label\": \"Update the knowledge graph\",\n",
      "        \"description\": \"Incorporate new information, corrections, or additional relationships as needed and iteratively refine the knowledge graph.\",\n",
      "        \"explanation\": \"This step is needed to ensure that the knowledge graph remains accurate and relevant over time as new data becomes available or as existing data is validated.\",\n",
      "        \"parentIds\": [\"step4\", \"step3\"]\n",
      "    }\n",
      "}\n",
      "{'id': 'END', 'label': 'END', 'description': 'You have successfully completed the necessary steps to construct a knowledge graph from the Wikipedia documents.', 'explanation': 'Since you have extracted entities and relationships, constructed the knowledge graph, validated it, and analyzed it for insights, all essential tasks for achieving your goal are completed.', 'parentIds': []}\n",
      "{'id': 'step5', 'label': 'Update the knowledge graph', 'description': 'Incorporate new information, corrections, or additional relationships as needed and iteratively refine the knowledge graph.', 'explanation': 'This step is needed to ensure that the knowledge graph remains accurate and relevant over time as new data becomes available or as existing data is validated.', 'parentIds': ['step4', 'step3']}\n",
      "Root (2.999588340678221/7)\n",
      "├── Entity Extraction (0.7836371491167023/2)\n",
      "│   ├── Construct Knowledge Graph (0/0)\n",
      "│   └── Data Structuring for Knowledge Graph (0.30417881958375725/1)\n",
      "└── Extract entities and relationships (2.215951191561519/5)\n",
      "    ├── Construct the knowledge graph (0.9052573057604403/2)\n",
      "    │   ├── Validate the knowledge graph (0.026695462798569713/1)\n",
      "    │   └── Validate the knowledge graph (0/0)\n",
      "    └── Construct the knowledge graph (1.3106938858010784/3)\n",
      "        ├── Evaluate and refine the knowledge graph (0.28727190905480193/1)\n",
      "        └── Validate the knowledge graph (1.0234219767462764/2)\n",
      "            ├── Analyze the knowledge graph (0.30405631463632543/1)\n",
      "            │   ├── END (0/0)\n",
      "            │   └── Update the knowledge graph (0.30405631463632543/1)\n",
      "            └── END (0.719365662109951/1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "root = await MCTS_step(root, node_dict, goal, model, api_key)\n",
    "visualize_tree(root, node_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def all_END(node: MCT_Node, node_dict: dict):\n",
    "    # dfs to check if all paths end in END\n",
    "    if not node.MCT_children_ids:\n",
    "        return is_END(node)\n",
    "    return all(all_END(node_dict[child], node_dict) for child in node.MCT_children_ids)\n",
    "all_END(root, node_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\": {\"id\": \"root\", \"label\": \"Root\", \"print_label\": \"Root (2.999588340678221/7)\", \"description\": \"Root node\", \"explanation\": \"Root node\", \"parentIds\": [], \"MCT_id\": \"-1\", \"MCT_parent_id\": null, \"MCT_children_ids\": [\"-1/0\", \"-1/1\"], \"visits\": 7, \"value\": 2.999588340678221}}'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.dumps({\"id\": root.model_dump(mode=\"json\")})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taskdecomposition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
