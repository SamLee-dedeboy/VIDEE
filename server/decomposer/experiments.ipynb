{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, RateLimitError, APITimeoutError\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import concurrent\n",
    "import time\n",
    "import json\n",
    "from pprint import pprint\n",
    "openai_client = OpenAI(api_key=open(\"api_key\").read().strip())\n",
    "def save_json(data, filename):\n",
    "    with open (filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def request_gpt(\n",
    "    client, messages, model=\"gpt-4o-mini\", temperature=0.5, format=None, seed=None\n",
    "):\n",
    "    with open(\"request_log.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"model: {model}, temperature: {temperature}, format: {format}\\n\")\n",
    "        f.write(json.dumps(messages, ensure_ascii=False) + \"\\n\")\n",
    "        f.write(\"=====================================\\n\")\n",
    "    try:\n",
    "        if format == \"json\":\n",
    "            response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    response_format={\"type\": \"json_object\"},\n",
    "                    temperature=temperature,\n",
    "                    seed=seed,\n",
    "                )\n",
    "            \n",
    "            try:\n",
    "                json.loads(response.choices[0].message.content)\n",
    "                return response.choices[0].message.content\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"JSON Decode Error\")\n",
    "                print(e)\n",
    "                time.sleep(5)\n",
    "                return request_gpt(client, messages, model, temperature=1.0, format=format)\n",
    "\n",
    "        else:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model, messages=messages, temperature=temperature, seed=seed\n",
    "            )\n",
    "        return response.choices[0].message.content\n",
    "    except RateLimitError as e:\n",
    "        print(\"RateLimitError\")\n",
    "        print(e)\n",
    "        time.sleep(5)\n",
    "        return request_gpt(client, messages, model, temperature, format)\n",
    "    except APITimeoutError as e:\n",
    "        print(\"APITimeoutError\")\n",
    "        print(messages)\n",
    "        time.sleep(5)\n",
    "        return request_gpt(client, messages, model, temperature, format)\n",
    "\n",
    "def multithread_prompts(\n",
    "    client,\n",
    "    prompts,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5,\n",
    "    response_format=None,\n",
    "    seed=None,\n",
    "):\n",
    "    l = len(prompts)\n",
    "    # results = np.zeros(l)\n",
    "    with tqdm(total=l) as pbar:\n",
    "        executor = concurrent.futures.ThreadPoolExecutor(max_workers=100)\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                request_gpt, client, prompt, model, temperature, response_format, seed\n",
    "            )\n",
    "            for prompt in prompts\n",
    "        ]\n",
    "        for _ in concurrent.futures.as_completed(futures):\n",
    "            pbar.update(1)\n",
    "    concurrent.futures.wait(futures)\n",
    "    return [future.result() for future in futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_task = [\n",
    "    {\n",
    "        \"label\": \"Document Preprocessing\",\n",
    "        \"description\": \"Clean and preprocess the text from the Wikipedia documents.\",\n",
    "        \"explanation\": \"Preprocessing is essential to remove noise and irrelevant information, which will enhance the quality of the extraction process and ensure consistency in the analysis.\",\n",
    "        \"depend_on\": [],\n",
    "        \"id\": \"0\",\n",
    "        \"parentIds\": []\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Entity Recognition\",\n",
    "        \"description\": \"Identify key entities (like people, places, organizations) within the preprocessed documents.\",\n",
    "        \"explanation\": \"Entity recognition is crucial as it helps in identifying the main subjects or nodes that will form the foundation of the knowledge graph.\",\n",
    "        \"depend_on\": [\n",
    "            \"Document Preprocessing\"\n",
    "        ],\n",
    "        \"id\": \"1\",\n",
    "        \"parentIds\": [\n",
    "            \"0\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Relationship Extraction\",\n",
    "        \"description\": \"Extract relationships between the identified entities.\",\n",
    "        \"explanation\": \"Understanding how entities relate to each other is key to building the connections in the knowledge graph, which provides context and meaning.\",\n",
    "        \"depend_on\": [\n",
    "            \"Entity Recognition\"\n",
    "        ],\n",
    "        \"id\": \"2\",\n",
    "        \"parentIds\": [\n",
    "            \"1\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Ontology Definition\",\n",
    "        \"description\": \"Define the structure and schema of the knowledge graph, including classes and properties.\",\n",
    "        \"explanation\": \"Creating an ontology is needed to formalize the relationships and hierarchy of the entities, ensuring the knowledge graph is organized and interpretable.\",\n",
    "        \"depend_on\": [\n",
    "            \"Entity Recognition\",\n",
    "            \"Relationship Extraction\"\n",
    "        ],\n",
    "        \"id\": \"3\",\n",
    "        \"parentIds\": [\n",
    "            \"1\",\n",
    "            \"2\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Graph Construction\",\n",
    "        \"description\": \"Construct the knowledge graph using the identified entities, relationships, and defined ontology.\",\n",
    "        \"explanation\": \"This step is where the actual graph is built, combining all the previous outputs into a structured format that can be utilized for querying and analysis.\",\n",
    "        \"depend_on\": [\n",
    "            \"Ontology Definition\"\n",
    "        ],\n",
    "        \"id\": \"4\",\n",
    "        \"parentIds\": [\n",
    "            \"3\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Validation and Refinement\",\n",
    "        \"description\": \"Validate the knowledge graph for accuracy and completeness, and refine it as necessary.\",\n",
    "        \"explanation\": \"Validation ensures the knowledge graph accurately represents the information extracted from the documents; refinement helps improve its quality and usability.\",\n",
    "        \"depend_on\": [\n",
    "            \"Graph Construction\"\n",
    "        ],\n",
    "        \"id\": \"5\",\n",
    "        \"parentIds\": [\n",
    "            \"4\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "semantic_tasks_str = json.dumps(semantic_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read defs\n",
    "elementary_task_defs = json.load(open(\"elementary_task_defs.json\"))\n",
    "elementary_task_defs_str = \"\"\n",
    "for elementary_task in elementary_task_defs:\n",
    "    elementary_task_defs_str += \"<elementary_task>\\n\"\n",
    "    for key, value in elementary_task.items():\n",
    "        elementary_task_defs_str += f\"<{key}>{value}</{key}>\\n\"\n",
    "    elementary_task_defs_str += \"</elementary_task>\\n\"\n",
    "# Decompose a tree of semantic tasks into elementary tasks\n",
    "prompts = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "        ** Context **\n",
    "        You are a Natural Language Processing (NLP) assistant. You are given a list of elementary NLP tasks that could be used.\n",
    "        Here is the list of elementary NLP tasks:\n",
    "        {elementary_task_defs}\n",
    "        ** Task **\n",
    "        The user will describe a series of real-world tasks for you. First, for each of the task, decide if it can be formulated as an NLP task. If yes, you need to find the proper elementary NLP tasks and arrange them to accomplish user's goal. \n",
    "        You can ignore the need to handle formats or evaluate outputs.\n",
    "        ** Requirements **\n",
    "        The labels of the elementary task must match exactly as those provided above. Reply with the following JSON format: \n",
    "        {{ \"elementary_tasks\": [ \n",
    "                {{ \n",
    "                    \"id\": int,\n",
    "                    \"label\": (string) (one of the above)\n",
    "                    \"description\": (string, describe implementation procedure)\n",
    "                    \"explanation\": (string, explain why this task is needed)\n",
    "                    \"depend_on\": (string[], the tasks task this step depends on)\n",
    "                    \"input\": string,\n",
    "                    \"output\": string,\n",
    "                    \"example_output\": string,\n",
    "                }}, \n",
    "                {{ \n",
    "                    \"id\": int,\n",
    "                    \"label\": (string) (one of the above)\n",
    "                    \"description\": (string, describe implementation procedure)\n",
    "                    \"explanation\": (string, explain why this task is needed)\n",
    "                    \"depend_on\": (int[], the task ids that this task depends on)\n",
    "                    \"input\": string,\n",
    "                    \"output\": string,\n",
    "                    \"example_output\": string,\n",
    "                }}, \n",
    "                ... \n",
    "            ] \n",
    "        }}\n",
    "        \"\"\".format(elementary_task_defs=elementary_task_defs)\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"{semantic_tasks_str}\"\n",
    "    }\n",
    "]\n",
    "response = request_gpt(openai_client, prompts, model=\"gpt-4o-mini\", temperature=0.0, format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"elementary_tasks\": [\n",
      "        {\n",
      "            \"id\": 0,\n",
      "            \"label\": \"Text Mining\",\n",
      "            \"description\": \"Analyze the Wikipedia documents to discover patterns and insights, focusing on cleaning and preprocessing the text.\",\n",
      "            \"explanation\": \"Preprocessing is essential to remove noise and irrelevant information, which will enhance the quality of the extraction process and ensure consistency in the analysis.\",\n",
      "            \"depend_on\": [],\n",
      "            \"input\": \"Wikipedia Documents\",\n",
      "            \"output\": \"Cleaned Text\",\n",
      "            \"example_output\": \"The text has been cleaned of irrelevant information and noise.\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": 1,\n",
      "            \"label\": \"Information Extraction\",\n",
      "            \"description\": \"Identify key entities (like people, places, organizations) within the preprocessed documents.\",\n",
      "            \"explanation\": \"Entity recognition is crucial as it helps in identifying the main subjects or nodes that will form the foundation of the knowledge graph.\",\n",
      "            \"depend_on\": [0],\n",
      "            \"input\": \"Cleaned Text\",\n",
      "            \"output\": \"Identified Entities\",\n",
      "            \"example_output\": {\"Person\": \"Albert Einstein\", \"Location\": \"Germany\", \"Organization\": \"Princeton University\"}\n",
      "        },\n",
      "        {\n",
      "            \"id\": 2,\n",
      "            \"label\": \"Information Extraction\",\n",
      "            \"description\": \"Extract relationships between the identified entities.\",\n",
      "            \"explanation\": \"Understanding how entities relate to each other is key to building the connections in the knowledge graph, which provides context and meaning.\",\n",
      "            \"depend_on\": [1],\n",
      "            \"input\": \"Identified Entities\",\n",
      "            \"output\": \"Extracted Relationships\",\n",
      "            \"example_output\": {\"Entity1\": \"Albert Einstein\", \"Entity2\": \"Princeton University\", \"Relationship\": \"affiliated with\"}\n",
      "        },\n",
      "        {\n",
      "            \"id\": 3,\n",
      "            \"label\": \"Argument Mining\",\n",
      "            \"description\": \"Define the structure and schema of the knowledge graph, including classes and properties.\",\n",
      "            \"explanation\": \"Creating an ontology is needed to formalize the relationships and hierarchy of the entities, ensuring the knowledge graph is organized and interpretable.\",\n",
      "            \"depend_on\": [1, 2],\n",
      "            \"input\": \"Extracted Relationships and Identified Entities\",\n",
      "            \"output\": \"Defined Ontology\",\n",
      "            \"example_output\": {\"Classes\": [\"Person\", \"Location\", \"Organization\"], \"Properties\": [\"affiliated with\", \"located in\"]}\n",
      "        },\n",
      "        {\n",
      "            \"id\": 4,\n",
      "            \"label\": \"Text Generation\",\n",
      "            \"description\": \"Construct the knowledge graph using the identified entities, relationships, and defined ontology.\",\n",
      "            \"explanation\": \"This step is where the actual graph is built, combining all the previous outputs into a structured format that can be utilized for querying and analysis.\",\n",
      "            \"depend_on\": [3],\n",
      "            \"input\": \"Defined Ontology, Extracted Relationships, Identified Entities\",\n",
      "            \"output\": \"Knowledge Graph\",\n",
      "            \"example_output\": \"A structured knowledge graph representing entities and their relationships.\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": 5,\n",
      "            \"label\": \"Text Mining\",\n",
      "            \"description\": \"Validate the knowledge graph for accuracy and completeness, and refine it as necessary.\",\n",
      "            \"explanation\": \"Validation ensures the knowledge graph accurately represents the information extracted from the documents; refinement helps improve its quality and usability.\",\n",
      "            \"depend_on\": [4],\n",
      "            \"input\": \"Knowledge Graph\",\n",
      "            \"output\": \"Validated Knowledge Graph\",\n",
      "            \"example_output\": \"The knowledge graph has been validated and refined for accuracy.\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response)\n",
    "import json\n",
    "test_response = \"\"\"\n",
    "{\n",
    "  \"elementary_tasks\": [\n",
    "    {\n",
    "      \"label\": \"Information Extraction\",\n",
    "      \"description\": \"Identify and extract key entities (e.g., people, places, organizations) from the documents using defined techniques.\",\n",
    "      \"explanation\": \"Entity extraction is the first step in building a knowledge graph, identifying primary components that will form the nodes of the graph.\",\n",
    "      \"depend_on\": [],\n",
    "      \"input\": \"Documents\",\n",
    "      \"output\": \"Extracted Entities\",\n",
    "      \"example_output\": \"{'Person': 'Barack Obama', 'Location': 'Honolulu', 'Organization': 'United States'}\"\n",
    "    },\n",
    "    {\n",
    "      \"label\": \"Information Extraction\",\n",
    "      \"description\": \"Determine the relationships between the extracted entities based on the context of the documents.\",\n",
    "      \"explanation\": \"Understanding the relationships allows us to connect the entities and define the edges of the knowledge graph, which is crucial for representing the semantic structure.\",\n",
    "      \"depend_on\": [\"Extract Entities\"],\n",
    "      \"input\": \"Extracted Entities and Context\",\n",
    "      \"output\": \"Identified Relationships\",\n",
    "      \"example_output\": \"{'Entity A': 'Barack Obama', 'Entity B': 'United States', 'Relationship': 'Presidency'}\"\n",
    "    },\n",
    "    {\n",
    "      \"label\": \"Define Ontology\",\n",
    "      \"description\": \"Establish a schema or ontology that categorizes the types of entities and relationships defined from the previous steps.\",\n",
    "      \"explanation\": \"An ontology helps in standardizing how entities and relationships are represented, making the knowledge graph more structured and interpretable.\",\n",
    "      \"depend_on\": [\"Extract Entities\", \"Identify Relationships\"],\n",
    "      \"input\": \"Entity Types and Relationships\",\n",
    "      \"output\": \"Defined Ontology\",\n",
    "      \"example_output\": \"{'Entity Types': ['Person', 'Organization', 'Location'], 'Relationships': ['Works At', 'Lives In']}\"\n",
    "    },\n",
    "    {\n",
    "      \"label\": \"Construct Knowledge Graph\",\n",
    "      \"description\": \"Create the knowledge graph by integrating the extracted entities and their relationships according to the defined ontology.\",\n",
    "      \"explanation\": \"This step involves the actual creation of the graph structure that can be visualized or queried, which is the end goal of the task.\",\n",
    "      \"depend_on\": [\"Define Ontology\", \"Identify Relationships\"],\n",
    "      \"input\": \"Extracted Entities and Relationships\",\n",
    "      \"output\": \"Knowledge Graph\",\n",
    "      \"example_output\": \"{'Nodes': ['Barack Obama', 'United States'], 'Edges': ['Barack Obama - Presidency -> United States']}\"\n",
    "    },\n",
    "    {\n",
    "      \"label\": \"Iterate and Refine\",\n",
    "      \"description\": \"Review and refine the knowledge graph to improve accuracy and completeness.\",\n",
    "      \"explanation\": \"Itâ€™s essential to validate the graph to ensure it accurately represents the information from the documents and to address any gaps or errors.\",\n",
    "      \"depend_on\": [\"Construct Knowledge Graph\"],\n",
    "      \"input\": \"Initial Knowledge Graph\",\n",
    "      \"output\": \"Refined Knowledge Graph\",\n",
    "      \"example_output\": \"{'Refined Nodes': ['Barack Obama', 'United States', 'Democrat'], 'Refined Edges': ['Barack Obama - Presidency -> United States']}\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "elementary_tasks = json.loads(test_response)[\"elementary_tasks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elementary_task_defs_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [{'id': '1', 'label': 'Extract Entities', 'description': 'Identify and extract key entities (people, places, organizations, etc.) from the documents.', 'explanation': 'This step is crucial for populating the knowledge graph with relevant and meaningful nodes, which represent different concepts and facts from the documents.', 'depend_on': [], 'parentIds': [], 'children': ['2', '3']}, {'id': '2', 'label': 'Identify Relationships', 'description': 'Determine relationships between the extracted entities based on the context of the documents.', 'explanation': 'Identifying relationships helps in defining how entities are connected in the knowledge graph, enabling a better representation of the information landscape.', 'depend_on': ['1'], 'parentIds': ['1'], 'children': ['3', '4']}, {'id': '3', 'label': 'Define Schema', 'description': 'Create a schema that outlines the types of entities and their possible relationships.', 'explanation': 'A well-defined schema is necessary to ensure that the knowledge graph is structured, understandable, and queryable, facilitating accurate representation of the data.', 'depend_on': ['1', '2'], 'parentIds': ['1', '2'], 'children': ['4']}, {'id': '4', 'label': 'Construct Graph', 'description': 'Assemble the entities and relationships into a graphical representation based on the schema.', 'explanation': 'Constructing the graph is the final step in visually organizing the extracted entities and relationships into a coherent structure that can be analyzed and queried.', 'depend_on': ['2', '3'], 'parentIds': ['2', '3'], 'children': []}]\n",
    "\n",
    "decomposed_semantic_tasks = list(\n",
    "    map(\n",
    "        lambda t: t.update({\"parentIds\": t[\"depend_on\"], \"children\": []}),\n",
    "        test,\n",
    "    )\n",
    ")\n",
    "decomposed_semantic_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "all_rows = []\n",
    "elementary_task_defs = json.load(open(\"elementary_task_defs.json\"))\n",
    "for task_def in elementary_task_defs:\n",
    "    all_rows.append({\n",
    "        \"label\": task_def['label'],\n",
    "        \"definition\": task_def['definition'],\n",
    "        \"input\": task_def['input'],\n",
    "        \"output\": task_def['output'],\n",
    "    })\n",
    "\n",
    "with open('elementary_task_defs.csv', 'w',) as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(list(all_rows[0].keys()))\n",
    "    for row in all_rows:\n",
    "        writer.writerow(list(row.values()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taskdecomposition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
