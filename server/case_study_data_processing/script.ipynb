{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal agent interface based on dynamical dialogue model: MAICO: multimodal agent interface for communication\n",
      "In this paper, we describe a multimodal interface prototype system based on Dynamical Dialogue Model. This system not only integrates information of speech and gestures, but also controls the response timing in order to realize a smooth interaction between user and computer. Our approach consists of human-human dialogue analysis, and computational modeling of dialogue.\n",
      "=============================\n",
      "Rapid Prototyping of Multimodal Interactive Applications Based on Off-The-Shelf Heterogeneous Components\n",
      "OpenInterface Kernel is a lightweight open-source plat-form designed for supporting the effective prototyping of multimodal interactive systems. Iterative design of such applications requires the easy integration, replacement, interconnection or upgrade of components. OpenInterface provides a thin integration platform able to manage these key elements with little programming knowledge, and thus provide the research community a tool to fill the gap in the current support for multimodal applications implementation. The platform offers non-intrusive tools and techniques to assemble various modalities developed with different implementation technologies, while keeping a high level of performance of the integrated system.\n",
      "=============================\n",
      "User Interaction Models for Disambiguation in Programming by Example\n",
      "Programming by Examples (PBE) has the potential to revolutionize end-user programming by enabling end users, most of whom are non-programmers, to create small scripts for automating repetitive tasks. However, examples, though often easy to provide, are an ambiguous specification of the user's intent. Because of that, a key impedance in adoption of PBE systems is the lack of user confidence in the correctness of the program that was synthesized by the system. We present two novel user interaction models that communicate actionable information to the user to help resolve ambiguity in the examples. One of these models allows the user to effectively navigate between the huge set of programs that are consistent with the examples provided by the user. The other model uses active learning to ask directed example-based questions to the user on the test input data over which the user intends to run the synthesized program. Our user studies show that each of these models significantly reduces the number of errors in the performed task without any difference in completion time. Moreover, both models are perceived as useful, and the proactive active-learning based model has a slightly higher preference regarding the users' confidence in the result.\n",
      "=============================\n",
      "SceneSkim: Searching and Browsing Movies Using Synchronized Captions, Scripts and Plot Summaries\n",
      "Searching for scenes in movies is a time-consuming but crucial task for film studies scholars, film professionals, and new media artists. In pilot interviews we have found that such users search for a wide variety of clips---e.g., actions, props, dialogue phrases, character performances, locations---and they return to particular scenes they have seen in the past. Today, these users find relevant clips by watching the entire movie, scrubbing the video timeline, or navigating via DVD chapter menus. Increasingly, users can also index films through transcripts---however, dialogue often lacks visual context, character names, and high level event descriptions. We introduce SceneSkim, a tool for searching and browsing movies using synchronized captions, scripts and plot summaries. Our interface integrates information from such sources to allow expressive search at several levels of granularity: Captions provide access to accurate dialogue, scripts describe shot-by-shot actions and settings, and plot summaries contain high-level event descriptions. We propose new algorithms for finding word-level caption to script alignments, parsing text scripts, and aligning plot summaries to scripts. Film studies graduate students evaluating SceneSkim expressed enthusiasm about the usability of the proposed system for their research and teaching.\n",
      "=============================\n",
      "Makers' Marks: Physical Markup for Designing and Fabricating Functional Objects\n",
      "To fabricate functional objects, designers create assemblies combining existing parts (e.g., mechanical hinges, electronic components) with custom-designed geometry (e.g., enclosures). Modeling complex assemblies is outside the reach of the growing number of novice ``makers' with access to digital fabrication tools. We aim to allow makers to design and 3D print functional mechanical and electronic assemblies. Based on a formative exploration, we created Makers' Marks, a system based on physically authoring assemblies with sculpting materials and annotation stickers. Makers physically sculpt the shape of an object and attach stickers to place existing parts or high-level features (such as parting lines). Our tool extracts the 3D pose of these annotations from a scan of the design, then synthesizes the geometry needed to support integrating desired parts using a library of clearance and mounting constraints. The resulting designs can then be easily 3D printed and assembled. Our approach enables easy creation of complex objects such as TUIs, and leverages physical materials for tangible manipulation and understanding scale. We validate our tool through several design examples: a custom game controller, an animated toy figure, a friendly baby monitor, and a hinged box with integrated alarm.\n",
      "=============================\n",
      "EverybodyLovesSketch: 3D sketching for a broader audience\n",
      "We present EverybodyLovesSketch, a gesture-based 3D curve sketching system for rapid ideation and visualization of 3D forms, aimed at a broad audience. We first analyze traditional perspective drawing in professional practice. We then design a system built upon the paradigm of ILoveSketch, a 3D curve drawing system for design professionals. The new system incorporates many interaction aspects of perspective drawing with judicious automation to enable novices with no perspective training to proficiently create 3D curve sketches. EverybodyLovesSketch supports a number of novel interactions: tick-based sketch plane selection, single view definition of arbitrary extrusion vectors, multiple extruded surface sketching, copy-and-project of 3D curves, freeform surface sketching, and an interactive perspective grid. Finally, we present a study involving 49 high school students (with no formal artistic training) who each learned and used the system over 11 days, which provides detailed insights into the popularity, power and usability of the various techniques, and shows our system to be easily learnt and effectively used, with broad appeal.\n",
      "=============================\n",
      "Interactive visualization of serial periodic data\n",
      "Serial periodic data exhibit both serial and periodic properties. For example, time continues forward serially, but weeks, months, and years are periods that recur. While there are extensive visualization techniques for exploring serial data, and a few for exploring periodic data, no existing technique simultaneously displays serial and periodic attributes of a data set. We introduce a spiral visualization technique, which displays data along a spiral to highlight serial attributes along the spiral axis and periodic ones along the radii. We show several applications of the spiral visualization to data exploration tasks, present our implementation, discuss the capacity for data analysis, and present findings of our informal study with users in data-rich scientific domains.\n",
      "=============================\n",
      "High rate, low-latency multi-touch sensing with simultaneous orthogonal multiplexing\n",
      "We present \"Fast Multi-Touch\" (FMT), an extremely high frame rate and low-latency multi-touch sensor based on a novel projected capacitive architecture that employs simultaneous orthogonal signals. The sensor has a frame rate of 4000 Hz and a touch-to-data output latency of only 40 microseconds, providing unprecedented responsiveness. FMT is demonstrated with a high-speed DLP projector yielding a touch-to-light latency of 110 microseconds.\n",
      "=============================\n",
      "Speech for multimedia information retrieval\n",
      "We describe the Informediatm News-on-Demand system. News-on-Demand is an innovative example of indexing and searching broadcast video and audio material by text content. The fully-automatic system monitors TV news and allows selective retrieval of news items based on spoken queries. The user then plays the appropriate video \"paragraph\". The system runs on a Pentium PC using MPEG-I video compression and the Sphinx-II continuous speech recognition system [6].\n",
      "=============================\n",
      "Area-based photo-plethysmographic sensing method for the surfaces of handheld devices\n",
      "Capturing the user's vital signs is an urgent goal in the HCI community. Photo-plethysmography (PPG) is one approach; it can collect data from the finger tips that indicate the user's autonomic nervous system (ANS) and offers new potentials such as mental stress measurement and drowsy state detection. Our goal is to set PPG sensors on the surfaces of ordinary devices such as mice, smartphones, and steering wheels. This will offer smart monitoring without the burden of additional wearable sensors. Unfortunately, current PPG sensors are very narrow, and even if the sensor is attached to the surface of a device, the user is forced to align and hold the finger to the sensor point, which degrades device usability. To solve this problem, we propose an area-based sensing method that relaxes the alignment requirement. The proposed method uses two thin acrylic plates, a diffuser plate and a detection plate, as an IR waveguide. The proposed method can yield very thin sensing surfaces and gentle curvatures are possible. An experiment compares the proposed method to the conventional point-sensor in terms of LF/HF discrimination performance with the participant in the resting state, and the proposed method is shown to offer comparable sensing performance with superior usability.\n",
      "=============================\n",
      "The dog programming language\n",
      "Today, most popular software applications are deployed in the cloud, interact with many users, and run on multiple platforms from Web browsers to mobile operating systems. While these applications confer a number of benefits to their users, building them brings many challenges: manually managing state between asynchronous user actions, creating and maintaining separate code bases for each desired client platform and gracefully scaling to handle a large number of concurrent users. Dog is a new programming language that provides a solution to these challenges and others through a unique runtime model that allows developers to model scalable cross-client applications as an imperative control-flow --- simplifying many development tasks. In this paper we describe the key features of Dog and show its utility through several applications that are difficult and time-consuming to write in existing languages, but are simple and easily written in Dog in a few lines of code.\n",
      "=============================\n",
      "Grizzly Bear: a demonstrational learning tool for a user interface specification language\n",
      "Grizzly Bear is a new demonstrational tool for specifying user interface behavior. It can handle multiple application windows, dynamic object instantiation and deletion, changes to any object attribute, and operations on sets of objects. It enables designers to experiment with rubber-banding, deletion by dragging to a trashcan and many other interactive techniques. To the author’s best knowledge it is currently the most complete demonstrational user interface design tool that does not base its in ferencing on rule-based guessing. There are inherent limitations to the range of user interfaces that can ever be built by demonstration alone. Grizzly Bear is therefore designed to work hand-in-hand with a user interface specification language called the Elements, Events & Transitions model. As designers demonstrate behavior, they can watch Grizzly Bear incrementally build the corresponding textual specification, letting them learn the language on the fly. They can then apply their knowledge by modifying Grizzly Bear’s textual inferences, which reduces the need for repetitive demonstrations and provides an escape mechanism for behavior that cannot be demonstrated.\n",
      "=============================\n",
      "Druid: a system for demonstrational rapid user interface development\n",
      "The Druid user interface management system aims to help user interface designers create, modify, and maintain interactive, graphical, direct-manipulation user interfaces. Druid allows thecreationof the complete user interface in highly-interactive and demonstrational manner. In Druid,the designer creates the layout of the interface using interactive graphical facilities and demonstrates to the UlMS how the end-user might interact with the interface. From this demonstration, the UIMS determines the details of the interface and automatically produces an implementation of it.\n",
      "=============================\n",
      "A mark-based interaction paradigm for free-hand drawing\n",
      "We propose an interaction technique for editing splines that is aimed at professional graphic designers. These users do not take full advantage of existing spline editing software because their mental representations of drawings do not match the underlying conceptual model of the software. Although editing splines by specifying control points and tangents may be appropriate for engineers, graphic designers think more in terms of strokes, shapes, and gestures appropriate for editing drawings. Our interaction technique matches the latter model: curves can be edited by means of marks, similar to the way strokes are naturally overloaded when drawing on paper. We describe this interaction technique and the algorithms used for its implementation.\n",
      "=============================\n",
      "PoliTel: mobile remote presence system that autonomously adjusts the interpersonal distance\n",
      "Mobile Remote Presence (MRP) system that uses a smart device such as smartphone and tablet pc as video conferencing equipment is getting popular. There are varieties of smart devices, and the appearance of a smart device varies from one to another. We assumed that the appropriate interpersonal distance for an MRP system varies depending on the appearance of the smart device. To confirm our assumption, we conducted a preliminary experiment. The result of the experiment suggested that the value of the proper interpersonal distance increases as the video size increases. It is known that the task load of the remote operator of the MRP system increases if the operator is forced to manually control the MRP system to keep the interpersonal distance to the appropriate level, which adversely affects the quality of the communication through MRP. To resolve the problem, we propose PoliTel, a novel MRP system which autonomously adjusts the interpersonal distance according to the appearance of the smart device by controlling the position or video size of MRP, and allows the operator to concentrate more on the conversation with the person facing to the MRP system.\n",
      "=============================\n",
      "Directness and liveness in the morphic user interface construction environment\n",
      "Morphic is a user interface construction environment that strives to embody directness and liveness. Directness means a user interface designer can initiate the process of examining or changing the attributes, structure, and behavior of user interface components by pointing at their graphical representations directly. Liveness means the user interface is always active and reactive-+bjects respond to user actions, animations run, layout happens, and information displays update continuously. Four implementation techniques work together to support directness and liveness in Morphic: structural reification, layout reification, ubiquitous animation, and live editing.\n",
      "=============================\n",
      "SDM: selective dynamic manipulation of visualizations\n",
      "In this paper we present a new set of interactive techniques for 2D and 3D visualizations. This set of techniques is called SDM (Selective Dynamic Manipulation). Selective, indicating our goal for providing a high degree of user control in selecting an object set, in selecting interactive techniques and the properties they affect, and in the degree to which a user action affects the visualization. Dynamic, indicating that the interactions all occur in real-time and that interactive animation is used to provide better contextual information to users in response to an action or operation. Manipulation, indicating the types of interactions we provide, where users can directly move objects and transform their appearance to perform different tasks. While many other approaches only provide interactive techniques in isolation, SDM supports a suite of techniques which users can combine to solve a wide variety of problems.\n",
      "=============================\n",
      "Inky: a sloppy command line for the web with rich visual feedback\n",
      "We present Inky, a command line for shortcut access to common web tasks. Inky aims to capture the efficiency benefits of typed commands while mitigating their usability problems. Inky commands have little or no new syntax to learn, and the system displays rich visual feedback while the user is typing, including missing parameters and contextual information automatically clipped from the target web site. Inky is an example of a new kind of hybrid between a command line and a GUI interface. We describe the design and implementation of two prototypes of this idea, and report the results of a preliminary user study.\n",
      "=============================\n",
      "The information percolator: ambient information display in a decorative object\n",
      "Most current interface designs require that the user focus their attention on them in order to be of value. However, as the price of computation falls, and computational capabilities make their way into many everyday objects, the demand for attention from many different directions may begin to seriously reduce the usefulness of these computational objects. Ambient information displays are intended to fit in a part of the interface design space that does not have this property. They are designed to convey background or context information that the user may or may not wish to attend to at any given time. Ambient Displays are designed to work primarily in the periphery of a user's awareness, moving to the center of attention only when appropriate and desirable. This paper describes a new ambient information display that is designed to give a rich medium of expression placed within an aesthetically pleasing decorative object. This display — the Information Percolator — is formed by air bubbles rising up tubes of water. By properly controlling the release of air, a set of pixels which scroll up the display is created. This allows a rendition of any (small, black and white) image to be displayed. The detailed design and construction of this display device will be considered, along with several applications.\n",
      "=============================\n",
      "Multi-finger and whole hand gestural interaction techniques for multi-user tabletop displays\n",
      "Recent advances in sensing technology have enabled a new generation of tabletop displays that can sense multiple points of input from several users simultaneously. However, apart from a few demonstration techniques [17], current user interfaces do not take advantage of this increased input bandwidth. We present a variety of multifinger and whole hand gestural interaction techniques for these displays that leverage and extend the types of actions that people perform when interacting on real physical tabletops. Apart from gestural input techniques, we also explore interaction and visualization techniques for supporting shared spaces, awareness, and privacy. These techniques are demonstrated within a prototype room furniture layout application, called RoomPlanner.\n",
      "=============================\n",
      "Transmogrification: causal manipulation of visualizations\n",
      "A transmogrifier is a novel interface that enables quick, on-the-fly graphic transformations. A region of a graphic can be specified by a shape and transformed into a destination shape with real-time, visual feedback. Both origin and destination shapes can be circles, quadrilaterals or arbitrary shapes defined through touch. Transmogrifiers are flexible, fast and simple to create and invite use in casual InfoVis scenarios, opening the door to alternative ways of exploring and displaying existing visualizations (e.g., rectifying routes or rivers in maps), and enabling free-form prototyping of new visualizations (e.g., lenses).\n",
      "=============================\n",
      "Mobile interaction using paperweight metaphor\n",
      "Conventional scrolling methods for small sized display in PDAs or mobile phones are difficult to use when frequent switching of scrolling and editing operations are required, for example, browsing and operating large sized WWW pages.In this paper, we propose a new user-interface method to provide seamless switching between scrolling and other operations such as editing, based on \"Paperweight Metaphor\". A sheet of paper that has been placed on a slippery table is difficult to draw on. Therefore, in order to write or draw something on the sheet of paper, a person must secure the paper with his/her palm to avoid the paper from moving. This will be a good metaphor to design switching operation of scroll and editing modes.We have made prototype systems by placing a touch sensor under each PDA display where user's palm will be hit. Three application programs - map browser, WWW browser, and photograph browser - that switch between scrolling and other operation modes depending on sensor output have been developed. We have carried out user tests on this mode switching method and have received favorable feedback on the same.\n",
      "=============================\n",
      "Eden: supporting home network management through interactive visual tools\n",
      "As networking moves into the home, home users are increasingly being faced with complex network management chores. Previous research, however, has demonstrated the difficulty many users have in managing their networks. This difficulty is compounded by the fact that advanced network management tools - such as those developed for the enterprise - are generally too complex for home users, do not support the common tasks they face, and are not a good fit for the technical peculiarities of the home. This paper presents Eden, an interactive, direct manipulation home network management system aimed at end users. Eden supports a range of common tasks, and provides a simple conceptual model that can help users understand key aspects of networking better. The system leverages a novel home network router that acts as a \"dropin\" replacement for users' current router. We demonstrate that Eden not only improves the user experience of networking, but also aids users in forming workable conceptual models of how the network works.\n",
      "=============================\n",
      "Transparency and awareness in a real-time groupware system\n",
      "This article explores real-time groupware systems from the perspective of both the users and the designer. This exploration is carried out through the description of GroupDesign, a real-time multi-user drawing tool that we have developed. From the perspective of the users, we present a number of functionalities that we feel necessary in any real-time groupware system: Graphic & Audio Echo, Localization, Identification, Age, and History. From the perspective of the designer, we demonstrate the possibility of creating a multi-user application from a single-user one, and we introduce the notion of purely replicated architecture.\n",
      "=============================\n",
      "Paper3D: bringing casual 3D modeling to a multi-touch interface\n",
      "A 3D modeling system that provides all-inclusive functionality is generally too demanding for a casual 3D modeler to learn. In recent years, there has been a shift towards developing more approachable systems, with easy-to-learn, intuitive interfaces. However, most modeling systems still employ mouse and keyboard interfaces, despite the ubiquity of tablet devices, and the benefits of multi-touch interfaces applied to 3D modeling. In this paper, we introduce an alternative 3D modeling paradigm for creating developable surfaces, inspired by traditional papercrafting, and implemented as a system designed from the start for a multi-touch tablet. We demonstrate the process of assembling complex 3D scenes from a collection of simpler models, in turn shaped through operations applied to sheets of virtual paper. The modeling and assembling operations mimic familiar, real-world operations performed on paper, allowing users to quickly learn our system with very little guidance. We outline key design decisions made throughout the development process, based on feedback obtained through collaboration with target users. Finally, we include a range of models created in our system.\n",
      "=============================\n",
      "A model for input and output of multilingual text in a windowing environment\n",
      "The layered multilingual input/output(I/O) sytems we designed, based on typological studies of major-language writing conventions, unifies common features of such conventions to enable international and local utilization. The internationalization layer input module converts keystroke sequences to phonograms and ideograms. The corresponding output module displays position-independent and dependent characters. The localization layer positions language-specific functions outside the structure, integrating them as tables used by finite automaton interpreters and servers to add new languages and code sets without recompilation. The I/O system generates and displays stateful and stateless code sets, enabling interactive language switching. Going beyond POSIX locale model bounds, the system generates ISO 2022, ISO/DIS 10646 (1990), and Compound Text, defined for the interchange encoding format in X11 protocols, for basic polyglot text communication and processing. Able to generate multilingual code sets, the I/O system clearly demonstrates that code sets should be selected by applications which have purposes beyond selecting one element from a localization set. Functionality and functions related to text manipulation in an operating sytem (OS) must also be determined by such applications. A subset of this I/O system was implemented in the X window system as a basic use of X11R5 I/O by supplying basic code set generation and string manipulation to eliminate OS interference. To ensure polyglot string manipulation, the I/O system must clearly be implemented separately from an OS and its limitations.\n",
      "=============================\n",
      "Dirty desktops: using a patina of magnetic mouse dust to make common interactor targets easier to select\n",
      "A common task in graphical user interfaces is controlling onscreen elements using a pointer. Current adaptive pointing techniques require applications to be built using accessibility libraries that reveal information about interactive targets, and most do not handle path/menu navigation. We present a pseudo-haptic technique that is OS and application independent, and can handle both dragging and clicking. We do this by associating a small force with each past click or drag. When a user frequently clicks in the same general area (e.g., on a button), the patina of past clicks naturally creates a pseudo-haptic magnetic field with an effect similar to that ofsnapping or sticky icons. Our contribution is a bottom-up approach to make targets easier to select without requiring prior knowledge of them.\n",
      "=============================\n",
      "Animating direct manipulation interfaces\n",
      "ABSTRACT If judiciously applied, the techniques of cartoon anima-tion can enhance the illusion of direct manipulation thatmany human computer interfaces strive to present. Inparticular, animation can convey a feeling of substancein the objects that a user manipulates, strengtheningthe sense that real work is being done. This paper sug-gests some techniques that application programmers canuse to animate direct manipulation interfaces, and it de-scribes tools that programmers can use to easily incor-porate the effects into their code.Our approach is based on suggesting a range of ani-mation effects by distorting the view of the manipu-lated object. To explore the idea, we added a warpingtransformation capability to the Interviews user inter-face toolkit and used the new transformation to build asimple drawing editor that uses animated feedback. Theeditor demonstrates the effectiveness of the animationfor simple operations, and it shows that the techniqueis practical even on standard workstation hardware.\n",
      "=============================\n",
      "Madgets: actuating widgets on interactive tabletops\n",
      "We present a system for the actuation of tangible magnetic widgets (Madgets) on interactive tabletops. Our system combines electromagnetic actuation with fiber optic tracking to move and operate physical controls. The presented mechanism supports actuating complex tangibles that consist of multiple parts. A grid of optical fibers transmits marker positions past our actuation hardware to cameras below the table. We introduce a visual tracking algorithm that is able to detect objects and touches from the strongly sub-sampled video input of that grid. Six sample Madgets illustrate the capabilities of our approach, ranging from tangential movement and height actuation to inductive power transfer. Madgets combine the benefits of passive, untethered, and translucent tangibles with the ability to actuate them with multiple degrees of freedom.\n",
      "=============================\n",
      "Matter matters: offloading machine computation to material computation for shape changing interfaces\n",
      "This paper introduces material computation to offload computing from machine to material, in the process of creating shape-changing output. It contains the explanation on the mechanism of transformation, the concept of material computation, the summary and analysis of literature research within and beyond the HCI field, the interaction loop integrating material computation, and my own practice in material computation technics and applications.\n",
      "=============================\n",
      "H-Studio: an authoring tool for adding haptic and motion effects to audiovisual content\n",
      "Haptic and motion effects have been widely used for virtual reality applications in order to provide a physical feedback from the virtual world. Such feedback was recently studied to improve the user experience in audiovisual entertainment applications. But the creation of haptic and motion effects is a main issue and requires dedicated editing tool. This paper describes a user-friendly authoring tool to create and synchronize such effects with audiovisual content. More precisely we focus on the edition of motion effects. Authoring is simplified thanks to a dedicated graphical user interface, allowing either to import external data or to synthesize effects thanks to a force-feedback device. Another key feature of this editor is the playback function which enables to preview the motion effect. Hence this new tool allows non expert users to create immersive haptic-audiovisual experiences.\n",
      "=============================\n",
      "A modular geometric constraint solver for user interface applications\n",
      "Constraints have been playing an important role in the user interface field since its infancy. A prime use of constraints in this field is to automatically maintain geometric layouts of graphical objects. To facilitate the construction of constraint-based user interface applications, researchers have proposed various constraint satisfaction methods and constraint solvers. Most previous research has focused on either local propagation or linear constraints, excluding more general nonlinear ones. However, nonlinear geometric constraints are practically useful to various user interfaces, e.g., drawing editors and information visualization systems. In this paper, we propose a novel constraint solver called Chorus, which realizes various powerful nonlinear geometric constraints such as Euclidean geometric, non-overlapping, and graph layout constraints. A key feature of Chorus is its module mechanism that allows users to define new kinds of geometric constraints. Also, Chorus supports \"soft\" constraints with hierarchical strengths or preferences (i.e., constraint hierarchies). We describe its framework, algorithm, implementation, and experimental results.\n",
      "=============================\n",
      "FingerFlux: near-surface haptic feedback on tabletops\n",
      "We introduce FingerFlux, an output technique to generate near-surface haptic feedback on interactive tabletops. Our system combines electromagnetic actuation with permanent magnets attached to the user's hand. FingerFlux lets users feel the interface before touching, and can create both attracting and repelling forces. This enables applications such as reducing drifting, adding physical constraints to virtual controls, and guiding the user without visual output. We show that users can feel vibration patterns up to 35 mm above our table, and that FingerFlux can significantly reduce drifting when operating on-screen buttons without looking.\n",
      "=============================\n",
      "Tangential force input for touch panels using bezel-aligned elastic pillars and a transparent sheet\n",
      "This research aims to enable tangential force input for touch panels by measuring the tangential force. The system is composed of a plastic sheet on a touch panel, urethane pillars on the panel that are aligned at the four corners of the bezel, and a case on top of the pillars. When the sheet moves with a finger, the pillars deform so that a tangential force can be obtained by measuring the movement of the finger. We evaluated the method and found that the system showed realistic force sensing accuracy in any direction. This input method will enable development of new applications for touch panels such as using any part of the touch panel surface as joysticks, or modeling virtual objects by deforming them with the fingers.\n",
      "=============================\n",
      "Stacksplorer: call graph navigation helps increasing code maintenance efficiency\n",
      "We present Stacksplorer, a new tool to support source code navigation and comprehension. Stacksplorer computes the call graph of a given piece of code, visualizes relevant parts of it, and allows developers to interactively traverse it. This augments the traditional code editor by offering an additional layer of navigation. Stacksplorer is particularly useful to understand and edit unknown source code because branches of the call graph can be explored and backtracked easily. Visualizing the callers of a method reduces the risk of introducing unintended side effects. In a quantitative study, programmers using Stacksplorer performed three of four software maintenance tasks significantly faster and with higher success rates, and Stacksplorer received a System Usability Scale rating of 85.4 from participants.\n",
      "=============================\n",
      "Trends in the computer industry: life-long subscriptions, magical cures, and profits along the information highway (invited talk)\n",
      "It doesn’t work the way you think it works. Technical, business, and social factors affect the way that new technologies are deployed, Once ideas me let out of the laboratory, common sense disappears, especially in the rush to show that one company’s products are superior to another’s almost equal, very similar ones. The easy part of interface design is the technology and the science. The hard parts are the social aspects: negotiating the multiple constraints on products, including cost, business models, the sales story, time to market, and those well known impediments to progress: the installed base and industry standards. The race is to the swift and the clever, not to the best. Customers purchase what they are told they want. Wants are not the same things as needs, customers are not the same people as users. Don’t believe everything you read. In fact, don’t believe anything. How much science and research actually impacts products? Less than you might think, less than you might hope, but often for good reasons. November 2-4, 1994 UIST ’94 193\n",
      "=============================\n",
      "Traceband: locating missing items by visual remembrance\n",
      "Finding missing items has always been troublesome. To tackle the hassle, several systems have been suggested; yet they are inflexible due to excessive setup time, operational cost, and effectiveness. We present Traceband; a lightweight and portable bracelet, which keeps track of every targeted commonly used object that a user interacts with. Users can find the location of missing items via a web-based software portal.\n",
      "=============================\n",
      "Contelli: a user-controllable intelligent keyboard for watch-sized small touchscreens\n",
      "Intelligent keyboards aid fast text entry by correcting user's erroneous input, but there is a big problem that a user always has to watch and judge of their suggestion results. Contelli, a user-controllable intelligent keyboard, monitors the duration of each key-tapping, and analyzes the possibility of mis-typing only for short-tapped letters. A long-tapped letter is regarded as a precise input and excluded in the process of candidate generation from a lexicon. Using Contelli, a user may actively \"control\" the intelligent keyboards. S/he may type ordinary words quickly on watch-sized small touchscreens. Also, s/he may input a word as typed without switching off the automatic replacement or performing additional actions for the replaced result. In addition, long-tapping a part of a string reduces the number of replacement candidates, which contributes the more precise word replacement for highly erroneous input typed on small touchscreens.\n",
      "=============================\n",
      "Real-time captioning by groups of non-experts\n",
      "Real-time captioning provides deaf and hard of hearing people immediate access to spoken language and enables participation in dialogue with others. Low latency is critical because it allows speech to be paired with relevant visual cues. Currently, the only reliable source of real-time captions are expensive stenographers who must be recruited in advance and who are trained to use specialized keyboards. Automatic speech recognition (ASR) is less expensive and available on-demand, but its low accuracy, high noise sensitivity, and need for training beforehand render it unusable in real-world situations. In this paper, we introduce a new approach in which groups of non-expert captionists (people who can hear and type) collectively caption speech in real-time on-demand. We present Legion:Scribe, an end-to-end system that allows deaf people to request captions at any time. We introduce an algorithm for merging partial captions into a single output stream in real-time, and a captioning interface designed to encourage coverage of the entire audio stream. Evaluation with 20 local participants and 18 crowd workers shows that non-experts can provide an effective solution for captioning, accurately covering an average of 93.2% of an audio stream with only 10 workers and an average per-word latency of 2.9 seconds. More generally, our model in which multiple workers contribute partial inputs that are automatically merged in real-time may be extended to allow dynamic groups to surpass constituent individuals (even experts) on a variety of human performance tasks.\n",
      "=============================\n",
      "Video object annotation, navigation, and composition\n",
      "We explore the use of tracked 2D object motion to enable novel approaches to interacting with video. These include moving annotations, video navigation by direct manipulation of objects, and creating an image composite from multiple video frames. Features in the video are automatically tracked and grouped in an off-line preprocess that enables later interactive manipulation. Examples of annotations include speech and thought balloons, video graffiti, path arrows, video hyperlinks, and schematic storyboards. We also demonstrate a direct-manipulation interface for random frame access using spatial constraints, and a drag-and-drop interface for assembling still images from videos. Taken together, our tools can be employed in a variety of applications including film and video editing, visual tagging, and authoring rich media such as hyperlinked video.\n",
      "=============================\n",
      "MuSE: a multiscale editor\n",
      "Information worlds are getting ever more vast. We need, not only better environments for dealing with this vast scale, but better tools for authoring information in those environments. This paper describes a new type of tool for authoring objects in infinite pan/zoom (so-called “multiscale”) environments, like PAD++. Called the MultiScale Editor (MuSE) it provides a direct way to manipulate objects in scale, simplifying important operations for authoring with large, multiscale information worlds.\n",
      "=============================\n",
      "Cross-device eye-based interaction\n",
      "Eye-tracking technology is envisaged to become part of our daily life, as its development progresses it becomes more wearable. Additionally there is a wealth of digital content around us, either close to us, on our personal devices or out-of-reach on public displays. The scope of this work aims to combine gaze with mobile input modalities to enable the transfer of content between public and close proximity personal displays. The work contributes enabling technologies, novel interaction techniques, and poses bigger questions that move toward a formalisation of this design space to develop guidelines for the development of future cross-device eye-based interaction methods.\n",
      "=============================\n",
      "ParaFrustum: visualization techniques for guiding a user to a constrained set of viewing positions and orientations\n",
      "Many tasks in real or virtual environments require users to view a target object or location from one of a set of strategic viewpoints to see it in context, avoid occlusions, or view it at an appropriate angle or distance. We introduce ParaFrustum, a geometric construct that represents this set of strategic viewpoints and viewing directions. ParaFrustum is inspired by the look-from and look-at points of a computer graphics camera specification, which precisely delineate a location for the camera and a direction in which it looks. We generalize this approach by defining a ParaFrustum in terms of a look-from volume and a look-at volume, which establish constraints on a range of acceptable locations for the user's eyes and a range of acceptable angles in which the user's head can be oriented. Providing tolerance in the allowable viewing positions and directions avoids burdening the user with the need to assume a tightly constrained 6DoF pose when it is not required by the task. We describe two visualization techniques for virtual or augmented reality that guide a user to assume one of the poses defined by a ParaFrustum, and present the results of a user study measuring the performance of these techniques. The study shows that the constraints of a tightly constrained ParaFrustum (e.g., approximating a conventional camera frustum) require significantly more time to satisfy than those of a loosely constrained one. The study also reveals interesting differences in participant trajectories in response to the two techniques.\n",
      "=============================\n",
      "An asymmetric communications platform for knowledge sharing with low-end mobile phones\n",
      "We present Awaaz.De (\"give voice\"), a social platform for communities to access and share knowledge using low-end mobile phones. Awaaz.De features a configurable mobile voice application organized into asynchronous voice mes-sage boards. For poor, remote and marginal communities, the voice-touchtone interface addresses the constraints of low literacy, language diversity, and affordability of only basic mobile devices. Voice content also presents a low barrier to content authoring, encouraging otherwise disconnected communities to actively participate in knowledge exchange. Awaaz.De includes a web-based administration interface for Internet-connected community managers to moderate, annotate, categorize, route, and narrow-cast voice messages. In this paper we describe the platform's design, implementation, and future directions.\n",
      "=============================\n",
      "Pinstripe: eyes-free continuous input anywhere on interactive clothing\n",
      "We present Pinstripe, a textile user interface element for eyes-free, continuous value input on smart garments that uses pinching and rolling a piece of cloth between your fingers. Input granularity can be controlled by the amount of cloth pinched. Pinstripe input elements are invisible, and can be included across large areas of a garment. Pinstripe thus addresses several problems previously identified in the placement and operation of textile UI elements on smart clothing.\n",
      "=============================\n",
      "Empirical measurements of intrabody communication performance under varied physical configurations\n",
      "Intrabody communication (IBC) is a wireless communications technology that uses a person's body as the transmission medium for imperceptible electrical signals. Because communication is limited to the vicinity of a person's body, ambiguities arising from communication between personal devices and environmental devices when multiple people are present can, in theory, be solved simply. Intrabody communication also potentially allows data to be transferred when a person touches an IBC-enabled device. We have designed and constructed an intrabody communication system, modeled after Zimmerman's original design, and extended it to operate up to 38.4Kbps and to calculate signal strength. In this paper, we present quantitative measurements of data error rates and signal strength while varying hand distance to transceiver plate, electrode location on the body, touch plate size and shape, and several other factors. We find that plate size and shape have only minor effects, but that the distance to plate and the coupling mechanism significantly effect signal strength. We also find that portable devices, with poor ground coupling, suffer more significant signal attenuation. Our goal is to promote design guidelines for this technology and identify the best contexts for its effective deployment.\n",
      "=============================\n",
      "PhotoMesa: a zoomable image browser using quantum treemaps and bubblemaps\n",
      "PhotoMesa is a zoomable image browser that uses a novel treemap algorithm to present large numbers of images grouped by directory, or other available metadata. It uses a new interaction technique for zoomable user interfaces designed for novices and family use that makes it straightforward to navigate through the space of images, and impossible to get lost.PhotoMesa groups images using one of two new algorithms that lay out groups of objects in a 2D space-filling manner. Quantum treemaps are designed for laying out images or other objects of indivisible (quantum) size. They are a variation on existing treemap algorithms in that they guarantee that every generated rectangle will have a width and height that are an integral multiple of an input object size. Bubblemaps also fill space with groups of quantum-sized objects, but generate non-rectangular blobs, and utilize space more efficiently.\n",
      "=============================\n",
      "Coupling a UI framework with automatic generation of context-sensitive animated help\n",
      "Animated help can assist users in understanding how to use computer application interfaces. An animated help facility integrated into a runtime user interface support tool requires information pertaining to user interfaces, the applications being supported, the relationships between interface and application and precise detailed information sufficient for accurate illustrations of interface components. This paper presents aknowledge model developed to support such an animated help facility. Continuing our research efforts towards automatic generation of user interfaces from specifications, a framework has been developed to utilize one knowledge model to automatically generate animated help at runtime and to assist the management of user interfaces. Cartoonist is a system implemented based on the framework. Without the help facility, Cartoonist functions as a knowledge-driven user interface. With the help facility added to Cartoonist’s user interface architecture, we demonstrate how animation of user’s actions can be simulated by superimposing animation on the actual interface. The animation sequences imitate user actionsandCartoonist’s user interface dialogue controller responds to animation “inputs”exactly as if they were from a user. The user interface runtime information managed by Cartoonist is shared with the help facility to furnish animation scenarios and to vary scenarios to suit the current user context. The Animator and the UI controller are modeled so that the Animator incorporates what is essential to the animation task and the UI controller assumes responsibility of the rest of the interactions an approach which maintains consistency between help animation and the actual user interface.\n",
      "=============================\n",
      "Scratch input: creating large, inexpensive, unpowered and mobile finger input surfaces\n",
      "We present Scratch Input, an acoustic-based input technique that relies on the unique sound produced when a fingernail is dragged over the surface of a textured material, such as wood, fabric, or wall paint. We employ a simple sensor that can be easily coupled with existing surfaces, such as walls and tables, turning them into large, unpowered and ad hoc finger input surfaces. Our sensor is sufficiently small that it could be incorporated into a mobile device, allowing any suitable surface on which it rests to be appropriated as a gestural input surface. Several example applications were developed to demonstrate possible interactions. We conclude with a study that shows users can perform six Scratch Input gestures at about 90% accuracy with less than five minutes of training and on wide variety of surfaces.\n",
      "=============================\n",
      "Adding rule-based reasoning to a demonstrational interface builder\n",
      "This paper presents a demonstrational interface builder with improved reasoning capabilities. The system is comprised of two major components: an interactive display manager and a rule-based reasoner. The display manager provides facilities to draw the physical appearance of an interface and define interface behavior by graphical demonstration. The behavior is defined using a technique of stimulus-response demonstrations. With this technique, an interface developer first demonstrates a stimulus that represents an action that an end user will perform on the interface. After the stimulus, the developer demonstrates the response(s) that should result from the given stimulus. As the behavior is demonstrated, the reasoner observes the demonstrations and draws inferences to expedite behavior definition. The inferences entail generalizing from specific behavior demonstrations and identifying constraints that define the generalized behavior. Once behavior constraints are identified, the reasoner sends them to the display manager to complete the definition process. When the interface is executed by an end-user, the display manager uses the constraints to implement the run-time behavior of the interface.\n",
      "=============================\n",
      "PhotoelasticTouch: transparent rubbery tangible interface using an LCD and photoelasticity\n",
      "PhotoelasticTouch is a novel tabletop system designed to intuitively facilitate touch-based interaction via real objects made from transparent elastic material. The system utilizes vision-based recognition techniques and the photoelastic properties of the transparent rubber to recognize deformed regions of the elastic material. Our system works with elastic materials over a wide variety of shapes and does not require any explicit visual markers. Compared to traditional interactive surfaces, our 2.5 dimensional interface system enables direct touch interaction and soft tactile feedback. In this paper we present our force sensing technique using photoelasticity and describe the implementation of our prototype system. We also present three practical applications of PhotoelasticTouch, a force-sensitive touch panel, a tangible face application, and a paint application.\n",
      "=============================\n",
      "Wheels in motion: inertia sensing in roller derby\n",
      "The recent resurgence of Roller Derby has seen the game progress to an elite level with leagues becoming increasingly competitive and taking a more structured and athletic approach to training. Leagues that the authors are involved in have expressed a desire for an objective measure of basic skills and a way to monitor improvements in performance especially amongst junior skaters. This paper details the construction of an inertia-sensing platform designed to be safe to wear by skaters. We have identified a skating manoeuvre, the \"crossover\" that can be automatically detected using a simple filtering and thresholding procedure. We also report on some initial results in automatically detecting when a crossover occurs and provide details of our future work.\n",
      "=============================\n",
      "Model-based user interfaces: what are they and why should we care?\n",
      "INTRODUCTION Model-based user interface design refers to a paradigm which uses an explicit, largely declarative representation capturing application semantics and other knowledge needed to specify the appearance and behavior of an interactive system. In this paradigm, an application developer instead of writing a large procedural program defines a model of facts, which controls behavior of reusable code, and a much smaller procedural program. The goal of the model-based UI design is to identify reusable components of a UI and to capture more knowledge in the model, while reducing the amount of new (procedural) code that has to be written for each new application.\n",
      "=============================\n",
      "SATIN: a toolkit for informal ink-based applications\n",
      "Software support for making effective pen-based applications is currently rudimentary. To facilitate the creation of such applications, we have developed SATIN, a Java-based toolkit designed to support the creation of applications that leverage the informal nature of pens. This support includes a scenegraph for manipulating and rendering objects; support for zooming and rotating objects, switching between multiple views of an object, integration of pen input with interpreters, libraries for manipulating ink strokes, widgets optimized for pens, and compatibility with Java’s Swing toolkit. SATIN includes a generalized architecture for handling pen input, consisting of recognizers, interpreters, and multi-interpreters. In this paper, we describe the functionality and architecture of SATIN, using two applications built with SATIN as examples.\n",
      "=============================\n",
      "The PICASSO applications framework\n",
      "PICASSO is a graphical user interface development system that includes an interface toolkit end an application framework. The application framework provides high-level abstractions including modal dialog boxes end non-modal frames end partels simh.r to conventional programming language procedures and co-routines. These abstractions can be used to define objects that have local variables and that can be called with parameters. PICASSO also has a constraint system that is used to bind program variables to widgets, to implement triggered behaviors, and to itnplement multiple views of data. The system is implemented in Common Lisp using the Common Lisp Object System and the CLX interface to the X Window System.\n",
      "=============================\n",
      "Haptic techniques for media control\n",
      "We introduce a set of techniques for haptically manipulating digital media such as video, audio, voicemail and computer graphics, utilizing virtual mediating dynamic models based on intuitive physical metaphors. For example, a video sequence can be modeled by linking its motion to a heavy spinning virtual wheel: the user browses by grasping a physical force-feedback knob and engaging the virtual wheel through a simulated clutch to spin or brake it, while feeling the passage of individual frames. These systems were implemented on a collection of single axis actuated displays (knobs and sliders), equipped with orthogonal force sensing to enhance their expressive potential. We demonstrate how continuous interaction through a haptically actuated device rather than discrete button and key presses can produce simple yet powerful tools that leverage physical intuition.\n",
      "=============================\n",
      "Window real objects: a distributed shared memory for distributed implementation of GUI applications\n",
      "This paper proposes a distributed shared memory, Window Real-Object (WROL which facilitates the construction of GUI applications with a set of cooperating parallel units running on multiple machines. To support unit cooperation, the WRO provides a shared data structure storing interaction objects displayed in a window among multiple machines. It also provides an event mechanism called absfrac[ everm to support control transfer among these units. Abstract events are generated when the shared data structure is updated, and invoke the units sharing the WRo. Both the function and mechanism of the WRO are tailored for GUI applications. The WRO provides spatia~ addressing to the data structure, which is implemented efficiently using an R-tree. It also adopts aflli-replication algorithm as a shared-memory coherenee scheme. Consequently, the WRO can be implemented with adequate performance on a usual workstation environment.\n",
      "=============================\n",
      "Supporting awareness and interaction through collaborative virtual interfaces\n",
      "This paper explores interfaces to virtual environments supporting multiple users. An interface to an environment allowing interaction with virtual artefacts is constructed, drawing on previous proposals for 'desktop' virtual environments. These include the use of Peripheral Lenses to support peripheral awareness in collaboration; and extending the ways in which users' actions are represented for each other. Through a qualitative analysis of a design task, the effect of the proposals is outlined. Observations indicate that, whilst these designs go some way to re-constructing physical co-presence in terms of awareness and interaction through the environment, some issues remain. Notably, peripheral distortion in supporting awareness may cause problematic interactions with and through the virtual world; and extended representations of actions may still allow problems in re-assembling the composition of others' actions. We discuss the potential for: designing representations for distorted peripheral perception; and explicitly displaying the course of action in object-focused interaction.\n",
      "=============================\n",
      "Interaction techniques for ambiguity resolution in recognition-based interfaces\n",
      "Because of its promise of natural interaction, recognition is coming into its own as a mainstream technology for use with computers. Both commercial and research applications are beginning to use it extensively. However the errors made by recognizers can be quite costly, and this is increasingly becoming a focus for researchers. We present a survey of existing error correction techniques in the user interface. These mediation techniques most commonly fall into one of two strategies, repetition and choice. Based on the needs uncovered by this survey, we have developed OOPS, a toolkit that supports resolution of input ambiguity through mediation. This paper describes four new interaction techniques built using OOPS, and the toolkit mechanisms required to build them. These interaction techniques each address problems not directly handled by standard approaches to mediation, and can all be re-used in a variety of settings.\n",
      "=============================\n",
      "Mediating photo collage authoring\n",
      "The medium of collage supports the visualization of meaningful event summaries using photographs. It can however be rather tedious to author a collage from a large collection of photographs. In this work we present an approach that supports efficient construction of a collage by assisting the user with an automatic layout procedure that can be controlled at a high level. Our layout method utilizes a pre-designed template which consists of cells for photos and annotations applied to these cells. The layout is then filled by matching the metadata of photos to the annotations in the cells using an optimization algorithm. The user exercises flexibility in the authoring process by (a) maintaining high-level control through the types of constraints applied and (b) leveraging visual emphases supported by the layout algorithm. The user can of course provide fine-grained control of the final collage through direct manipulation. Off-loading the tedium of collage construction to a user controlled yet automated process clears the way for rapidly generating different views of the same album and could also support the increased sharing of digital photos in the form of compact collages.\n",
      "=============================\n",
      "Interactions speak louder than words: shared user models and adaptive interfaces\n",
      "Touch-screens are becoming increasingly ubiquitous. They have great appeal due to their capabilities to support new forms of human interaction, including their abilities to interpret rich gestural inputs, render flexible user interfaces and enable multi-user interactions. However, the technology creates new challenges and barriers for users with limited levels of vision and motor abilities. The PhD work described in this paper proposes a technique combining Shared User Models (SUM) and adaptive interfaces to improve the accessibility of touch-screen devices for people with low levels of vision and motor ability. SUM, built from an individual's interaction data across multiple applications and devices, is used to infer new knowledge of their abilities and characteristics, without the need for continuous calibration exercises or user configurations. This approach has been realized through the development of an open source software framework to support the creation of applications that make use of SUM to adapt interfaces that match the needs of individual users.\n",
      "=============================\n",
      "Augmenting braille input through multitouch feedback\n",
      "Current touch interfaces lack the rich tactile feedback that allows blind users to detect and correct errors. This is especially relevant for multitouch interactions, such as Braille input. We propose HoliBraille, a system that combines touch input and multi-point vibrotactile output on mobile devices. We believe this technology can offer several benefits to blind users; namely, convey feedback for complex multitouch gestures, improve input performance, and support inconspicuous interactions. In this paper, we present the design of our unique prototype, which allows users to receive multitouch localized vibrotactile feedback. Preliminary results on perceptual discrimination show an average of 100% and 82% accuracy for single-point and chord discrimination, respectively. Finally, we discuss a text-entry application with rich tactile feedback.\n",
      "=============================\n",
      "Suede: a Wizard of Oz prototyping tool for speech user interfaces\n",
      "Speech-based user interfaces are growing in popularity. Unfortunately, the technology expertise required to build speech UIs precludes many individuals from participating in the speech interface design process. Furthermore, the time and knowledge costs of building even simple speech systems make it difficult for designers to iteratively design speech UIs. SUEDE, the speech interface prototyping tool we describe in this paper, allows designers to rapidly create prompt/response speech interfaces. It offers an electronically supported Wizard of Oz (WOz) technique that captures test data, allowing designers to analyze the interface after testing. This informal tool enables speech user interface designers, even non-experts, to quickly create, test, and analyze speech user interface prototypes.\n",
      "=============================\n",
      "KinÊtre: animating the world with the human body\n",
      "KinÊtre allows novice users to scan arbitrary physical objects and bring them to life in seconds. The fully interactive system allows diverse static meshes to be animated using the entire human body. Traditionally, the process of mesh animation is laborious and requires domain expertise, with rigging specified manually by an artist when designing the character. KinÊtre makes creating animations a more playful activity, conducted by novice users interactively \"at runtime\". This paper describes the KinÊtre system in full, highlighting key technical contributions and demonstrating many examples of users animating meshes of varying shapes and sizes. These include non-humanoid meshes and incomplete surfaces produced by 3D scanning - two challenging scenarios for existing mesh animation systems. Rather than targeting professional CG animators, KinÊtre is intended to bring mesh animation to a new audience of novice users. We demonstrate potential uses of our system for interactive storytelling and new forms of physical gaming.\n",
      "=============================\n",
      "Hairlytop interface: a basic tool for active interfacing\n",
      "The Hairlytop Interface is a high scalability interface composed of hair-like units called smart hairs. The original version of the smart hair comprised a shape-memory alloy, drive circuits, and a light sensor. Simply placing the smart hair above a light display device enabled each smart hair to be bent and controlled by modulating the intensity of light from the display. Various prototypes of the Hairlytop Interface have been created to show its high flexibility in configuration. This flexibility should help users to develop their own moving interfaces.\n",
      "=============================\n",
      "A testbed for characterizing dynamic response of virtual environment spatial sensors\n",
      "This paper describes a testbed and method for characterizing the dynamic response of the type of spatial displacement transducers commonly used in virtual environment (VE) applications. The testbed consists of a motorized rotary swing arm that imparts known displacement inputs to the VE sensor. The experimental method involves a series of tests in which the sensor is displaced back and forth at a number of controlled frequencies that span the bandwidth of volitional human movement. During the tests, actual swing arm angle and reported VE sensor displacements are collected and time stamped. Because of the time stamping technique, the response time of the sensor can be measured directly, independent of latencies in data transmission from the sensor unit and any processing by the interface applications running on the host computer. Analysis of these experimental results allows sensor time delay and gain characteristics to be determined as a function of input frequency. Results from tests of several differnt VE spatial sensors (Ascension, Logitech, and Polhemus) are presented here to demonstrate use of the testbed and method.\n",
      "=============================\n",
      "An interactive visual query environment for exploring data\n",
      "Direct manipulation of visualizations is a powerful technique for performing exploratory data operations such as navigation, aggregation, and filtering. Its immediacy facilitates rapid, incremental, and reversible forays into the data. However it does not provide for reuse or modification of exploration sessions. This paper describes a visual query language, VQE, that adds these capabilities to a direct manipulation exploration environment called Visage. Queries and visualizations are dynamically linked: operations on either one immediately update the other, in contrast to the feedforward sequence of database query followed by visualization of results common in traditional systems. These features are supported by the architectural concept of threads, which represent a sequence of navigation steps on particular objects. Because they are tied to particular data objects, they can be directly manipulated. Because they represent operations, they can be generalized into queries. We expect this technique to apply to direct manipulation interfaces to any objectoriented system that represents both objects and the relationships among them. NOTE: Color versions of the figures are at, e.g.,\n",
      "=============================\n",
      "That one there! Pointing to establish device identity\n",
      "Computing devices within current work and play environments are relatively static. As the number of 'networked' devices grows, and as people and their devices become more dynamic, situations will commonly arise where users will wish to use 'that device there' instead of navigating through traditional user interface widgets such as lists. This paper describes a process for identifying devices through a pointing gesture using custom tags and a custom stylus called the gesturePen. Implementation details for this system are provided along with qualitative and quantitative results from a formal user study. As ubiquitous computing environments become more pervasive, people will rapidly switch their focus between many computing devices. The results of our work demonstrate that our gesturePen method can improve the user experience in ubiquitous environments by facilitating significantly faster interactions between computing devices.\n",
      "=============================\n",
      "MISO: a context-sensitive multimodal interface for smart objects based on hand gestures and finger snaps\n",
      "We present an unobtrusive multimodal interface for smart objects (MISO) in an everyday indoor environment. MISO uses pointing for object selection and context-sensitive arm gestures for object control. Finger snaps are used to confirm object selections and to aid with gesture segmentation. Audio feedback is provided during the interaction. The use of a Kinect depth camera allows for a compact system and robustness in varying environments and lighting conditions at low cost.\n",
      "=============================\n",
      "Language-level support for exploratory programming of distributed virtual environments\n",
      "We describe COTERIE, a toolkit that provides languagelevel support for building distributed virtual environments. COTERIE is based on the distributed data-object paradigm for distributed shared memory. Any data object in COTERIE can be declared to be a Shared Object that is replicated fully in any process that is interested in it. These Shared Objects support asynchronous data propagation with atomic serializable updates, and asynchronous notification of updates. COTERIE is built in Modula-3 and uses existing Modula-3 packages that support an integrated interpreted language, multithreading, and 3D animation. Unlike other VE toolkits, COTERIE is based on a set of general-purpose parallel and distributed language concepts designed with the needs of virtual environments in mind. We summarize the requirements that we identified for COTERIE, describe its implementation, compare it with other toolkits, and provide examples that show COTERIE’s advantages.\n",
      "=============================\n",
      "HaCHIStick: simulating haptic sensation on tablet pc for musical instruments application\n",
      "In this paper, we propose a novel stick-type interface, the \"HaCHIStick,\" for musical performance on a tablet PC. The HaCHIStick is composed of a stick with an embedded vibrotactile actuator, a visual display, and an elastic sheet on the display. By combining the kinesthetic sensation induced by striking the elastic sheet with vibrotactile sensation, the system provides natural haptic cues that enable the user to feel what they strike with the stick, such as steel or wood. This haptic interaction would enrich the user's experience when playing the instruments. The interface is regarded as a type of haptic augmented reality (AR) system, with a relatively simple setup.\n",
      "=============================\n",
      "Constant density visualizations of non-uniform distributions of data\n",
      "The cartographic Principle of Constant Information Density suggests that the amount of information in an interactive visualization should remain constant as the user pans and zooms. In previous work, we presented a system, VIDA (Visual Information Density Adjuster), which helps users manually construct applications in which overall display density remains constant. In the context of semantic zoom systems, this approach ensures uniformity in the z dimension, but does not extend naturally to ensuring uniformity in the x and y dimensions. In this paper, we present a new approach that automatically creates displays that are uniform in the x, y, and z dimensions. In the new system, users express constraints about visual representations that should appear in the display. The system applies these constraints to subdivisions of the display such that each subdivision meets a target density value. We have implemented our technique in the DataSplash/VIDA database visualization environment. We describe our algorithm, implementation, and the advantages and disadvantages of our approach.\n",
      "=============================\n",
      "Tutorial-based interfaces for cloud-enabled applications\n",
      "Powerful image editing software like Adobe Photoshop and GIMP have complex interfaces that can be hard to master. To help users perform image editing tasks, we introduce tutorial-based applications (tapps) that retain the step-by-step structure and descriptive text of tutorials but can also automatically apply tutorial steps to new images. Thus, tapps can be used to batch process many images automatically, similar to traditional macros. Tapps also support interactive exploration of parameters, automatic variations, and direct manipulation (e.g., selection, brushing). Another key feature of tapps is that they execute on remote instances of Photoshop, which allows users to edit their images on any Web-enabled device. We demonstrate a working prototype system called TappCloud for creating, managing and using tapps. Initial user feedback indicates support for both the interactive features of tapps and their ability to automate image editing. We conclude with a discussion of approaches and challenges of pushing monolithic direct-manipulation GUIs to the cloud.\n",
      "=============================\n",
      "Aesthetic information collages: generating decorative displays that contain information\n",
      "Normally, the primary purpose of an information display is to convey information. If information displays can be aesthetically interesting, that might be an added bonus. This paper considers an experiment in reversing this imperative. It describes the Kandinsky system which is designed to create displays which are first aesthetically interesting, and then as an added bonus, able to convey information. The Kandinsky system works on the basis of aesthetic properties specified by an artist (in a visual form). It then explores a space of collages composed from information bearing images, using an optimization technique to find compositions which best maintain the properties of the artist's aesthetic expression.\n",
      "=============================\n",
      "Force gestures: augmenting touch screen gestures with normal and tangential forces\n",
      "Force gestures are touch screen gestures augmented by the normal and tangential forces on the screen. In order to study the feasibility of the force gestures on a mobile touch screen, we implemented a prototype touch screen device that can sense the normal and tangential forces of a touch gesture on the screen. We also designed two example applications, a web browser and an e-book reader, that utilize the force gestures for their primary actions. We conducted a user study with the prototype and the applications to study the characteristics of the force gestures and the effectiveness of their mapping to the primary actions. In the user study we could also discover interesting usability issues and collect useful user feedback about the force gestures and their mapping to GUI actions.\n",
      "=============================\n",
      "MenuOptimizer: interactive optimization of menu systems\n",
      "Menu systems are challenging to design because design spaces are immense, and several human factors affect user behavior. This paper contributes to the design of menus with the goal of interactively assisting designers with an optimizer in the loop. To reach this goal, 1) we extend a predictive model of user performance to account for expectations as to item groupings; 2) we adapt an ant colony optimizer that has been proven efficient for this class of problems; and 3) we present MenuOptimizer, a set of inter-actions integrated into a real interface design tool (QtDesigner). MenuOptimizer supports designers' abilities to cope with uncertainty and recognize good solutions. It allows designers to delegate combinatorial problems to the optimizer, which should solve them quickly enough without disrupting the design process. We show evidence that satisfactory menu designs can be produced for complex problems in minutes.\n",
      "=============================\n",
      "Eyes-free text entry interface based on contact area for people with visual impairment\n",
      "We developed an eyes-free text entry interface using contact area to determine pressed state for mobile device with touchscreen. The interface gives audio feedback for a touched character similar to VoiceOver of iPhone, but audio feedbacks of two simultaneous touches are considered. A desired character is entered by pressing once. Independent entry of two fingers can reduce movement distance for searching a character. Whole interaction occurs in touched states, additional tactile feedback can be augmented.\n",
      "=============================\n",
      "Blinkbot: look at, blink and move\n",
      "In this paper we present BlinkBot - a hands free input interface to control and command a robot. BlinkBot explores the natural modality of gaze and blink to direct a robot to move an object from a location to another. The paper also explains detailed hardware and software implementation of the prototype system.\n",
      "=============================\n",
      "RhythmLink: securely pairing I/O-constrained devices by tapping\n",
      "We present RhythmLink, a system that improves the wireless pairing user experience. Users can link devices such as phones and headsets together by tapping a known rhythm on each device. In contrast to current solutions, RhythmLink does not require user interaction with the host device during the pairing process; and it only requires binary input on the peripheral, making it appropriate for small devices with minimal physical affordances. We describe the challenges in enabling this user experience and our solution, an algorithm that allows two devices to compare imprecisely-entered tap sequences while maintaining the secrecy of those sequences. We also discuss our prototype implementation of RhythmLink and review the results of initial user tests.\n",
      "=============================\n",
      "DoubleFlip: a motion gesture delimiter for interaction\n",
      "In order to use motion gestures with mobile devices it is imperative that the device be able to distinguish between input motion and everyday motion. In this abstract we present DoubleFlip, a unique motion gesture designed to act as an input delimiter for mobile motion gestures. We demonstrate that the DoubleFlip gesture is extremely resistant to false positive conditions, while still achieving high recognition accuracy. Since DoubleFlip is easy to perform and less likely to be accidentally invoked, it provides an always-active input event for mobile interaction.\n",
      "=============================\n",
      "bioPrint: an automatic deposition system for bacteria spore actuators\n",
      "We propose an automatic deposition method of bacteria spores, which deform thin soft materials under environmental humidity change. We describe the process of two-dimensional printing the spore solution as well as a design application. This research intends to contribute to the understanding of the control and pre-programming the transformation of future interfaces.\n",
      "=============================\n",
      "A marking based interface for collaborative writing\n",
      "We describe a system to support a particular model of document creation. In this model, the document flows from the primary author to one or more collaborators. They annotate it, then return it to the author who makes the final changes. Annotations are made using conventional marks, typically using a stylus. The intent is to match the flow and mark-up of paper documents observed in the everyday world, . The system is very much modeled on Wang FreeStyle (Perkins, Blatt, Workman and Ehrlich, 1989; Francik and Akagi, 1989; & Levine and Ehrlich, in press). Our contribution is to incorporate mark recognition into the system and to explore some novel navigation tools that are enabled by the higher-level data structures that we use. The system is described and the results of initial usertesting are reported. INTRODUCTION Collaborative writing occurs in many different forms (Posner, 1991). Our system supports a model of document creation in which the document flows from the primary author to one or more collaborators. They annotate it, then return it to the author who makes the final changes, This flow of the document is illustrated in Figure 1. Annotations are created by marking-up a copy of the document with a stylus. The intent is to match the flow and the way paper documents are marked-up in the everyday world. Other systems, such as Wang FreeStyle, emulate this model of document creation (Perkins, Blatt, Workman and Ehrlich, 1989; Francik and Akagi, 1989; & Levine and Ehrlich, in press). With FreeStyle, annotations are made not only with a stylus but also by recording speech. In our system annotations can only be made with the stylus. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for diract commercial advantage, tha ACM copyright notice and the dtle of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. . . @ 1993 ACM ().89791-62&x/93/0()1 1...$1 .SO With FreeStyle, the copy of the document that is distributed is only a “dumb” snap-shot of the original. Annotations are integrated as a separate layer to the “snapshot.” Furthermore, there is no computer recognition of the markings. To edit the final document, the user needs two J%ncloal Author Col~orat@ t Creates / Edits document Sends copies Annotate to collaborators document — —\n",
      "=============================\n",
      "Pinch-to-zoom-plus: an enhanced pinch-to-zoom that reduces clutching and panning\n",
      "Despite its popularity, the classic pinch-to-zoom gesture used in modern multi-touch interfaces has drawbacks: specifically, the need to support an extended range of scales and the need to keep content within the view window on the display can result in the need to clutch and pan. In two formative studies of unimanual and bimanual pinch-to-zoom, we found patterns: zooming actions follows a predictable ballistic velocity curve, and users tend to pan the point-of-interest towards the center of the screen. We apply these results to design an enhanced zooming technique called Pinch-to-Zoom-Plus (PZP) that reduces clutching and panning operations compared to standard pinch-to-zoom behaviour.\n",
      "=============================\n",
      "The engineering of personhood\n",
      "Any subset of reality can potentially be interpreted as a computer, so when we speak about a particular computer, we are merely speaking about a portion of reality we can understand computationally. That means that computation is only identifiable through the human experience of it. User interface is ultimately the only grounding for the abstractions of computation, in the same way that the measurement of physical phenomena provides the only legitimate basis for physics. But user interface also changes humans. As computation is perceived, the natures of self and personhood are transformed. This process, when designers are aware of it, can be understood as an emerging form of applied philosophy or even applied spirituality.\n",
      "=============================\n",
      "Interactive calibration of a multi-projector system in a video-wall multi-touch environment\n",
      "Wall-sized interactive displays gain more and more attention as a valuable tool for multiuser applications, but typically require the adoption of projectors tiles. Projectors tend to display deformed images, due to lens distortion and/or imperfection, and because they are almost never perfectly aligned to the projection surface. Multi-projector video-walls are typically bounded to the video architecture and to the specific application to be displayed. This makes it harder to develop interactive applications, in which a fine grained control of the coordinate transformations (to and from user space and model space) is required. This paper presents a solution to such issues: implementing the blending functionalities at an application level allows seamless development of multi-display interactive applications with multi-touch capabilities. The description of the multi-touch interaction, guaranteed by an array of cameras on the baseline of the wall, is beyond the scope of this work which focuses on calibration.\n",
      "=============================\n",
      "Synchronizing clipboards of multiple computers\n",
      "This paper describes a new technique for transferring data between computers, the synchronized clipboard. Multiple computers can share a synchronized clipboard for all clipboard operations, so that data copied to the clipboard from one computer, using the standard Copy command, can be pasted directly on another computer using the standard Paste command. Synchronized clipboards are well-suited for a single user moving data among several computers in close proximity. We describe an implementation of synchronized clipboards that works across a wide range of existing systems, including 3Com PalmPilots, Microsoft Windows PCs, Unix workstations, and other Java-capable platforms. Our implementation adds no noticeable overhead to local copy and paste operations.\n",
      "=============================\n",
      "Head-tracked orbital viewing: an interaction technique for immersive virtual environments\n",
      "An interaction technique for immersive virtual environments called “head-tracked orbital viewing” is described. The user’s head orientation is tracked and mapped so as to move the viewpoint of the user about the surface of a virtual sphere surrounding a center of rotation. The technique is useful for object examination tasks in a virtual world, allowing the user to quickly and easily view an object from many perspectives.\n",
      "=============================\n",
      "Data visualization sliders\n",
      "Computer sliders are a generic user input mechanism for specifying a numeric value from a range. For data visualization, the effectiveness of sliders may be increased by using the space inside the slider as• an interactive color scale,• a barplot for discrete data, and• a density plot for continuous data.The idea is to show the selected values in relation to the data and its distribution. Furthermore, the selection mechanism may be generalized using a painting metaphor to specify arbitrary, disconnected intervals while maintaining an intuitive user-interface.\n",
      "=============================\n",
      "Stacksplorer: understanding dynamic program behavior\n",
      "To thoroughly comprehend application behavior, programmers need to understand the interactions of objects at runtime. Today, these interactions are often poorly visualized in common IDEs except during debugging. Stacksplorer allows visualizing and traversing potential call stacks in an application even when it is not running by showing callers and called methods in two columns next to the code editor. The relevant information is gathered from the source code automatically.\n",
      "=============================\n",
      "Supporting interaction in augmented reality in the presence of uncertain spatial knowledge\n",
      "A significant problem encountered when building Augmented Reality (AR) systems is that all spatial knowledge about the world has uncertainty associated with it. This uncertainty manifests itself as registration errors between the graphics and the physical world, and ambiguity in user interaction. In this paper, we show how estimates of the registration error can be leveraged to support predictable selection in the presence of uncertain 3D knowledge. These ideas are demonstrated in osgAR, an extension to OpenSceneGraph with explicit support for uncertainty in the 3D transformations. The osgAR runtime propagates this uncertainty throughout the scene graph to compute robust estimates of the probable location of all entities in the system from the user's viewpoint, in real-time. We discuss the implementation of selection in osgAR, and the issues that must be addressed when creating interaction techniques in such a system.\n",
      "=============================\n",
      "Design and technology for Collaborage: collaborative collages of information on physical walls\n",
      "A Collaborage is a collaborative collage of physically represented information on a surface that is connected with electronic information, such as a physical In/Out board connected to a people-locator database. The physical surface (board) contains items that are tracked by camera and computer vision technology. Events on the board trigger electronic services. This paper motivates this concept, presents three different applications, describes the system architecture and component technologies, and discusses several design issues.\n",
      "=============================\n",
      "The continuous zoom: a constrained fisheye technique for viewing and navigating large information spaces\n",
      "Navigating and viewing large information spaces, such as hierarchically-organized networks from complex realtime systems, suffer the problems of viewing a large space on a small screen. Distorted-view approaches, such as fisheye techniques, have great potential to reduce these problems by representing detail within its larger context but introduce new issues of focus, transition between views and user disorientation from excessive distortion. We present a fisheyebased method which supports multiple focus points, enhances continuity through smooth transitions between views, and maintains location constraints to reduce the user’s sense of spatial disorientation. These are important requirements for the representation and navigation of networked systems in supervisory control applications. The method consists of two steps: a global allocation of space to rectangular sections of the display, based on scale factors, followed by degree-of-interest adjustments. Previous versions of the algorithm relied solely on relative scale factors to assign size; we present a new version which allocates space more efficiently using a dynamically calculated degree of interest. In addition to the automatic system sizing, manual user control over the amount of space assigned each area is supported. The amount of detail shown in various parts of the network is controlled by pruning the hierarchy and presenting those sections in summary form.\n",
      "=============================\n",
      "The enhancement of hearing using a combination of sound and skin sensation to the pinna\n",
      "Recent development in sound technologies has enabled the realistic replay of real-life sounds. Thanks to these technologies, we can experience a virtual real sound environment. However, there are other types of sound technologies that enhance reality, such as acoustic filters, sound effects, and background music. They are quite effective if carefully prepared, but they also alter the sound itself. Consequently, sound is simultaneously used to reconstruct realistic environments and to enhance emotions, which are actually incompatible functions. With this background, we focused on using tactile modality to enhance emotions and propose a method that enhances the sound experience by a combination of sound and skin sensation to the pinna (earlobe). In this paper, we evaluate the effectiveness of this method.\n",
      "=============================\n",
      "Skin buttons: cheap, small, low-powered and clickable fixed-icon laser projectors\n",
      "Smartwatches are a promising new interactive platform, but their small size makes even basic actions cumbersome. Hence, there is a great need for approaches that expand the interactive envelope around smartwatches, allowing human input to escape the small physical confines of the device. We propose using tiny projectors integrated into the smartwatch to render icons on the user's skin. These icons can be made touch sensitive, significantly expanding the interactive region without increasing device size. Through a series of experiments, we show that these 'skin buttons' can have high touch accuracy and recognizability, while being low cost and power-efficient.\n",
      "=============================\n",
      "AdaptableGIMP: designing a socially-adaptable interface\n",
      "We introduce the concept of a socially-adaptable interface, an interface that provides instant access to task-specific interface customizations created, edited, and documented by the application's user community. We demonstrate this concept in AdaptableGIMP, a modified version of the GIMP image editor that we have developed.\n",
      "=============================\n",
      "Capturing on site laser annotations with smartphones to document construction work\n",
      "In the process of construction work, taking notes of real world objects like walls, pipes, cables and others is an important task. The ad hoc capturing of small information pieces on such objects on site can be challenging when there is no specialized technology available. Handwritten or hand drawn notes on paper are good for textual information like measurements whereas images are better to capture the physical state of objects. Without a proper combination however the benefit is limited. In this paper we present an interaction system for taking ad hoc notes on real world objects by using a combination of a smartphone and a laserpointer as input device. Our interface enables the user to directly annotate objects by drawing on them and to store these annotations for later reviewing. The deictic gestures of the user are then replayed on a stitched image of the scene. The users voice input is captured and analyzed to integrate additional Information. The user can mark positions and place hand taken measurements by pointing on the objects and speaking the corresponding voice commands.\n",
      "=============================\n",
      "From information visualization to direct manipulation: extending a generic visualization framework for the interactive editing of large datasets\n",
      "Today's generic data management applications such as accounting, CRM or logging and tracking software, rely on form and menu based interfaces. These applications take only marginal advantage of current graphical user interfaces. This is because the data they handle does not have intrinsic visual representations upon which direct manipulation principles can be used. This article presents how we have extended an Information Visualization framework with generic data manipulation functions. These new data editing capabilities are tuned to take advantage of the characteristics of each view. They enable us to generalize the direct manipulation mechanisms to address many abstract data manipulation needs. In this article we present five uses of the features we have implemented and deduce a general workflow applicable to a variety of contexts. The workflow comprises three steps and five editing actions. The steps are: adjust view, select, and edit. The editing actions are: edit a value or group of values, clone objects, remove objects, add attributes, and remove attributes. The workflow provides complete editing access to table and hierarchical data structures using particularly terse interaction methods. It defines a general data editing model that enables powerful data manipulation tasks without requiring end-user programming or scripting.\n",
      "=============================\n",
      "PapierCraft: a command system for interactive paper\n",
      "Knowledge workers use paper extensively for document reviewing and note-taking due to its versatility and simplicity of use. As users annotate printed documents and gather notes, they create a rich web of annotations and cross references. Unfortunately, as paper is a static media, this web often gets trapped in the physical world. While several digital solutions such as XLibris [15] and Digital Desk [18] have been proposed, they suffer from a small display size or onerous hardware requirements.To address these limitations, we propose PapierCraft, a gesture-based interface that allows users to manipulate digital documents directly using their printouts as proxies. Using a digital pen, users can annotate a printout or draw command gestures to indicate operations such as copying a document area, pasting an area previously copied, or creating a link. Upon pen synchronization, our infrastructure executes these commands and presents the result in a customized viewer. In this paper we describe the design and implementation of the PapierCraft command system, and report on early user feedback.\n",
      "=============================\n",
      "Pressages: augmenting phone calls with non-verbal messages\n",
      "ForcePhone is a mobile synchronous haptic communication system. During phone calls, users can squeeze the side of the device and the pressure level is mapped to vibrations on the recipient's device. The pressure/vibrotactile messages supported by ForcePhone are called pressages. Using a lab-based study and a small field study, this paper addresses the following questions: how can haptic interpersonal communication be integrated into a standard mobile device? What is the most appropriate feedback design for pressages? What types of non-verbal cues can be represented by pressages? Do users make use of pressages during their conversations? The results of this research indicate that such a system has value as a communication channel in real-world settings with users expressing greetings, presence and emotions through pressages.\n",
      "=============================\n",
      "GADGET: a toolkit for optimization-based approaches to interface and display generation\n",
      "Recent work is beginning to reveal the potential of numerical optimization as an approach to generating interfaces and displays. Optimization-based approaches can often allow a mix of independent goals and constraints to be blended in ways that would be difficult to describe algorithmically. While optimization-based techniques appear to offer several potential advantages, further research in this area is hampered by the lack of appropriate tools. This paper presents GADGET, an experimental toolkit to support optimization for interface and display generation. GADGET provides convenient abstractions of many optimization concepts. GADGET also provides mechanisms to help programmers quickly create optimizations, including an efficient lazy evaluation framework, a powerful and configurable optimization structure, and a library of reusable components. Together these facilities provide an appropriate tool to enable exploration of a new class of interface and display generation techniques.\n",
      "=============================\n",
      "LightWeight: wearable resistance visualizer for rehabilitation\n",
      "People recovering from arm injuries are often prescribed limits to the amount of strain they can place on their muscles at a given point during the recovery process. However, it is sometimes difficult for them to know when a given activity creates strain in excess of these limits. To inform this process, we have developed a prototype, the LightWeight, and describe it here. The aim of the LightWeight is to inform users of the strain on targeted muscles as the activity occurs, and to display the relationship of that strain to the aforementioned limits. LightWeight is embedded within a compression sleeve that measures muscle strain through conductive fabric and EMG while displaying that information through an intuitive circular LED display.\n",
      "=============================\n",
      "Enabling beyond-surface interactions for interactive surface with an invisible projection\n",
      "This paper presents a programmable infrared (IR) technique that utilizes invisible, programmable markers to support interaction beyond the surface of a diffused-illumination (DI) multi-touch system. We combine an IR projector and a standard color projector to simultaneously project visible content and invisible markers. Mobile devices outfitted with IR cameras can compute their 3D positions based on the markers perceived. Markers are selectively turned off to support multi-touch and direct on-surface tangible input. The proposed techniques enable a collaborative multi-display multi-touch tabletop system. We also present three interactive tools: i-m-View, i-m-Lamp, and i-m-Flashlight, which consist of a mobile tablet and projectors that users can freely interact with beyond the main display surface. Early user feedback shows that these interactive devices, combined with a large interactive display, allow more intuitive navigation and are reportedly enjoyable to use.\n",
      "=============================\n",
      "A tour of suite user interface software\n",
      "Suite offers high-level abstractions for developing both single-user and multi-user interfaces. An interactive application in Suite displays data structures to one or more users and asynchronously responds to user modifications to these data structures. Between each application and user is a dialogue manager, which offers the user an interface for manipulating displayed data. Dialogue managers and applications execute in separate address spaces, residing possibly on different computers, such as local workstations and remote hosts. Users and applications can customize several aspects of the user interface including how data are displayed, the kind of feedback given in response to user input, and how changes made by multiple users to a data structure are synchronized. An inheritance model offering both structural and type inheritance is provided for easing the task of customizing user interfaces. Typing is considered orthogonal to persistence, communication, and input/output, that is, a value of any type can be made persistent, communicated among applications, and input from and output to users. In this paper, we take the reader on a tour of the software, highlighting its distinguishing features.\n",
      "=============================\n",
      "Strategies for automatically incorporating metaphoric attributes in interface designs\n",
      "Recently a number of projects have addressed the problem of automatically producing interface designs for computer applications [Arens .et al., 1988; Weitzman, 1988; Beshers and Feiner, 1989; Wiecha et al., 1989; Kim and Foley, 19891. The core of each of these design tools takes as input a specification of the functionality and information that the application is to present to the user and produces as output an interface specified in terms of some toolkit of interface techniques. Each of these tools interacts, to some degree, with a human interface design expert in order to produce these interface designs. Some of these tools allow the designer to arbitrarily manipulate the components of a design to produce the desired result. Others require the design expert to specify the desired changes in the form of new rules which can be used on subsequent designs. Interaction with a designer is essential for each of these tools because there is no complete set of rules that can be used to compute all aspects of an interface design. Although there are a number of sets of guidelines for effective interface designs [Corporation, 1987; Smith and Mosier, 1986; Shneiderman, 19871, they are all inadequate for a completely automated interface design system. The most obvious inadequacy is in the area of Esthetic decisions; most sets of interface guidelines either lack guidelines for determining the specific visual appearances and organization of an interface, or else such guidelines are too vague to be used automatically. For example, advice like, “Allocate more screen space (or primary optical areas) to more important objects,” does not specify just how much more screen space should be allocated. Furthermore, such advice\n",
      "=============================\n",
      "Amortizing 3D graphics optimization across multiple frames\n",
      "Abstract : This paper describes a mechanism for improving rendering rates dynamically during runtime in an interactive three-dimensional graphics application. Well-known techniques such as transforming hierarchical geometry into a flat list and removing redundant graphics primitives are often performed off-line on static databases, or continuously every rendering frame. In addition, these optimizations are usually performed over the whole database. We observe that much of the database remains static for a fixed period of time, while other portions are modified continuously (e.g. the camera position), or are repeatedly modified during some finite interval (e.g. during user interaction). We have implemented a runtime optimization mechanism which is sensitive to repeated, local database changes. This mechanism employs timing strategies which optimize only when the cost of optimization will be amortized over a sufficient number of frames. Using this optimization scheme, we observe a rendering speedup of roughly 2.5 in existing applications. We discuss our initial implementation of this mechanism, the improved timing measurements, the issues and assumptions we made, and future improvements.\n",
      "=============================\n",
      "Supporting worker independence in collaboration transparency\n",
      "Conventional collaboration-transparency systems are inefficient in their use of network resources and lack support for key groupware principles: concurrent work, relaxed WYSIWIS, and group awareness. We present an alternative implementation approach to collaboration transparency that provides many features previously seen only in collaboration-aware applications. Our approach is based on an object-oriented replicated architecture where selected single-user interface objects are dynamically replaced by multi-user extensions. The replacement is transparent to the single-user application and its developer. As an instance of this approach, we describe its incorporation into a new Java-based collaboration-transparency system, called Flexible JAMM. We conducted an empirical study to evaluate the effectiveness of Flexible JAMM versus a representative conventional collaboration-transparency system, NetMeeting. Completion times were significantly faster in a loosely-coupled task using Flexible JAMM, and were not adversely affected in a tightly-coupled task. Accuracy was unaffected by the system used. Participants greatly preferred Flexible JAMM. The evaluation validates our aim of supporting multiple styles of collaboration.\n",
      "=============================\n",
      "Pause-and-play: automatically linking screencast video tutorials with applications\n",
      "Video tutorials provide a convenient means for novices to learn new software applications. Unfortunately, staying in sync with a video while trying to use the target application at the same time requires users to repeatedly switch from the application to the video to pause or scrub backwards to replay missed steps. We present Pause-and-Play, a system that helps users work along with existing video tutorials. Pause-and-Play detects important events in the video and links them with corresponding events in the target application as the user tries to replicate the depicted procedure. This linking allows our system to automatically pause and play the video to stay in sync with the user. Pause-and-Play also supports convenient video navigation controls that are accessible from within the target application and allow the user to easily replay portions of the video without switching focus out of the application. Finally, since our system uses computer vision to detect events in existing videos and leverages application scripting APIs to obtain real time usage traces, our approach is largely independent of the specific target application and does not require access or modifications to application source code. We have implemented Pause-and-Play for two target applications, Google SketchUp and Adobe Photoshop, and we report on a user study that shows our system improves the user experience of working with video tutorials.\n",
      "=============================\n",
      "Intelligent tagging interfaces: beyond folksonomy\n",
      "This paper summarizes our work on using tags to broaden the dialog between a recommender system and its users. We present two tagging applications that enrich this dialog: tagsplanations are tag-based explanations of recommendations provided by a system to its users, and Movie Tuner is a conversational recommender system that enables users to provide feedback on movie recommendations using tags. We discuss the design of both systems and the experimental methodology used to evaluate the design choices.\n",
      "=============================\n",
      "MUSE: reviving memories using email archives\n",
      "Email archives silently record our actions and thoughts over the years, forming a passively acquired and detailed life-log that contains rich material for reminiscing on our lives. However, exploratory browsing of archives containing thousands of messages is tedious without effective ways to guide the user towards interesting events and messages. We present Muse (Memories USing Email), a system that combines data mining techniques and an interactive interface to help users browse a long-term email archive. Muse analyzes the contents of the archive and generates a set of cues that help to spark users' memories: communication activity with inferred social groups, a summary of recurring named entities, occurrence of sentimental words, and image attachments. These cues serve as salient entry points into a browsing interface that enables faceted navigation and rapid skimming of email messages. In our user studies, we found that users generally enjoyed browsing their archives with Muse, and extracted a range of benefits, from summarizing work progress to renewing friendships and making serendipitous discoveries.\n",
      "=============================\n",
      "Retrieving electronic documents with real-world objects on InteractiveDESK\n",
      "We are developing a computerized desk which we have named InteractiveDESK [1]. One of the major features of the InteractiveDESK is reality awareness; that is, the ability to respond to situational changes in the real world in order to reduce users’ workloads. In this paper, we present a new method, as an example of the reality awareness, to retrieve electronic documents with real objects such as paper documents or folders. Users of the InteractiveDESK can retrieve electronic documents by just showing real objects which have links to the electronic documents. The links are made by the users through interactions with the InteractiveDESK. The advantage of this method is that the user can unify the arrangement of electronic documents into the arrangement of real objects.\n",
      "=============================\n",
      "The role of natural language in a multimodal interface\n",
      "Although graphics and direct manipulation are effective interface technologies for some classes of problems, they are limited in many ways. In particular, they provide little support for identifying objects not on the screen, for specifying temporal relations, for identifying and operating on large sets and subsets of entities, and for using the context of interaction. On the other hand, these are precisely strengths of natural language. This paper presents an interface that blends natural language processing and direct manipulation technologies, using each for their characteristic advantages. Specifically, the paper shows how to use natural language to describe objects and temporal relations, and how to use direct manipulation for overcoming hard natural language problems involving the establishment and use of context and pronominal reference. This work has been implemented in SRI's Shoptalk system, a prototype information and decision-support system for manufacturing.\n",
      "=============================\n",
      "An explanation-based, visual debugger for one-way constraints\n",
      "This paper describes a domain-specific debugger for one-way constraint solvers. The debugger makes use of several new techniques. First, the debugger displays only a portion of the dataflow graph, called a <i>constraint slice</i>, that is directly related to an incorrect variable. This technique helps the debugger scale to a system containing thousands of constraints. Second, the debugger presents a visual representation of the solver's data structures and uses color encodings to highlight changes to the data structures. Finally, the debugger allows the user to point to a variable that has an unexpected value and ask the debugger to suggest reasons for the unexpected value. The debugger makes use of information gathered during the constraint satisfaction process to generate plausible suggestions. Informal testing has shown that the explanatory capability and the color coding of the constraint solver's data structures are particularly useful in locating bugs in constraint code.\n",
      "=============================\n",
      "The VIEP system: interacting with collaborative multimedia\n",
      "This paper presents a survey of the Visual Information Environment Prototype (VIEP), a system which demonstrates the next generation of Command, Control, Communication, and Intelligence (C31) systems. In particular, VIEP provides a novel integration of user interaction techniques including wireless input and largescreen output to facilitate the task of collaborating with media such as large images, audio, and video. The prototype has been implemented and demonstrated over both local and wide area networks.\n",
      "=============================\n",
      "Traxion: a tactile interaction device with virtual force sensation\n",
      "This paper introduces a new mechanism to induce a virtual force based on human illusory sensations. An asymmetric signal is applied to a tactile actuator consisting of an electromagnetic coil, a metal weight, and a spring, such that the user feels that the device is being pulled (or pushed) in a particular direction, although it is not supported by any mechanical connection to other objects or the ground. The proposed tactile device is smaller (35.0 mm x 5.0 mm x 7.5 mm) and lighter (5.2 g) than any previous force-feedback devices, which have to be connected to the ground with mechanical links. This small form factor allows the device to be implemented in several novel interactive applications, such as a pedestrian navigation system that includes a finger-mounted tactile device or an (untethered) input device that features virtual force. Our experimental results indicate that this illusory sensation actually exists and the proposed device can switch the virtual force direction within a short period. We combined this new technology with visible light transmission via a digital micromirror device (DMD) projector and developed a position guiding input device with force perception.\n",
      "=============================\n",
      "Augmenting shared personal calendars\n",
      "In this paper, we describe Augur, a groupware calendar system to support personal calendaring practices, informal workplace communication, and the socio-technical evolution of the calendar system within a workgroup. Successful design and deployment of groupware calendar systems have been shown to depend on several converging, interacting perspectives. We describe calendar-based work practices as viewed from these perspectives, and present the Augur system in support of them. Augur allows users to retain the flexibility of personal calendars by anticipating and compensating for inaccurate calendar entries and idiosyncratic event names. We employ predictive user models of event attendance, intelligent processing of calendar text, and discovery of shared events to drive novel calendar visualizations that facilitate interpersonal communication. In addition, we visualize calendar access to support privacy management and long-term evolution of the calendar system.\n",
      "=============================\n",
      "FLANNEL: adding computation to electronic mail during transmission\n",
      "In this paper, we describe FLANNEL, an architecture for adding computational capabilities to email. FLANNEL allows email to be modified by an application while in transit between sender and receiver. This modification is done without modification to the endpoints---mail clients---at either end. This paper also describes interaction techniques that we have developed to allow senders of email to quickly and easily select computations to be performed by FLANNEL. Through, our experience, we explain the properties that applications must have in order to be successful in the context of FLANNEL.\n",
      "=============================\n",
      "Vermeer: direct interaction with a 360° viewable 3D display\n",
      "We present Vermeer, a novel interactive 360° viewable 3D display. Like prior systems in this area, Vermeer provides viewpoint-corrected, stereoscopic 3D graphics to simultaneous users, 360° around the display, without the need for eyewear or other user instrumentation. Our goal is to over-come an issue inherent in these prior systems which - typically due to moving parts - restrict interactions to outside the display volume. Our system leverages a known optical illusion to demonstrate, for the first time, how users can reach into and directly touch 3D objects inside the display volume. Vermeer is intended to be a new enabling technology for interaction, and we therefore describe our hardware implementation in full, focusing on the challenges of combining this optical configuration with an existing approach for creating a 360° viewable 3D display. Initially we demonstrate direct involume interaction by sensing user input with a Kinect camera placed above the display. However, by exploiting the properties of the optical configuration, we also demonstrate novel prototypes for fully integrated input sensing alongside simultaneous display. We conclude by discussing limitations, implications for interaction, and ideas for future work.\n",
      "=============================\n",
      "An interactive constraint-based system for drawing graphs\n",
      "Hwyx z {r| ( * (4 &'^}$1(o ,$) &J n Gf $)% &#GB `(+ $) A _` (4&5K &5K A $) ~K ]\\q$! 0( ^, ! A 3K^ &'K7$!L ^ A (4$!J&5K0  `( d < 7K7&5(o &J\\&J f/ `  0 $)'&'(/ & $! &' Gf $! ,$!  ^ & GB ) 1 _u n $)% &g^, &J ` $!  &'% $) L`(q(4*7(4 &'^ ('I ( K7$!( A $! 0 L $!( $! 0&5 L` &5( &# % 7GJ _ L 1 * -v( 3&5GB$1 !$)'&'K,GJ A (4 n $! n(0  :K7 n ]\\q$) 036 3K: ( $)^  !&GJ `(4 n $! A ( $1(4-l Gf $) ^ &'Gn 3 $1(4^ ` q ) ! \\#(Y &L3(4&' Y ^ $! L A 1 & &HK7 n ]\\q$) 0 \\q $) !&# &gGJ `(4 n $! (Y & GB $!% & I/ &5(4& &' L &'( &5(4L ) $!  0 n ` A K ]\\q$! 0~&'K7$)  3 $1( ( L u& A $) g$! Q^, * \\ ]* ( s ` ( & _` (4&5KW Q^ &0 &' &J n  `K u \\Y&' 4L vGJ `(4 n $! A ( $1(+-l Gf $) ^ &B ` K`(JI KEYWORDS  n  K7 n ]\\q$) 036 GB `(+ $) A _` (4&5K, 1 ]* L '6 K7 n ]\\q$) `0 !('6 GB ) 1 _u n $)% & $! &J -l GJ&'('I OVERVIEW &~K7 ^ $! ` @^ &J ` $)  & K7&'( $!0  -# L`^ A GB ^ A  L7 &J $) &J n Gf $) $!(r `&o% $)&'\\ -`GJ ^  L7 &J y (TJOJRf'k DN RfI/8# A ` L 0 $! &J -l GJ&'(_3 ( &'K , `$!(% $)&'\\ ` ]% &# % &'KL`(4&JL $! @% &J *^ *(4&J 4 $! 0 (J6  L` )$) $!% & 0 &'( (y$) @ &# &5 # L ^, A GB ^  L7 &' r$! &J n GB $! ^, ]* ]\\ $t v &Y _ $! )$) +* $! A &' Gf o\\q$)  &HGB ^`L7 &' Y (HnN ) m N R DN RfIo&H ` ]% &q_`L $) ) (4*7(4 &J^ 6 GJ ) !&'Knwyx z {y|7 ¡   ¡Z ]* L  &' Gf $!% & ¢ $! 0 n ^£V K7$) nB6 Y$! &' Gf $!% &g0 n   ! ]* L7 56¥¤ `  A 0 $!J&'( `&@$! &J n GB $! W$!  s^ &@GJ ! ! _3 $!% &^, ` &J 3 $! ` &'% $! L3(0 n ` A K ]\\q$! 0( * (4 &'^,(JI£ &wrx z {r| ( * (4 &'^¦$1(W §GB `(+ $) A _` (4&5KK7 n ]\\q$! 0§&'K7$) WK7&'( $)0 &'K ( 3&5GB$) ̈3GJ ) !*~ &' ` _` )&@L`(4&' ( s&' (4$! )*~ 7K7L`GB&(4^, ) / `K ^ &'K7$!L ^ A (4$!J&5K:K7$1 0 n ^,('I© &L`(4&' $1( &5(4u `( $!_ )&a4«u¬ ­ ̄®f°±n2]35 ́Yμ·¶y ̧q¶ ±n1 oh3]¶/15»'1⁄4]¶D1⁄2 ¶+1⁄4]®f¶15¶+o μ3⁄4»f° ̄¿ 1⁄4]ÀÁ±n®f° ̄±n ̧qÂuμrÀ¥1⁄4]¶+Ã ¬ Ä Âh¶41⁄4Yo ̄»Å'À¥Â Ä ±nÃ¥À¥Æ+¶vÇ]À¥1 ±n°l¬°h¶4ÃÁ± ohÀ¥»f1]Â4ÈJ±n1]1⁄4q1]»no·±n°hÇ]À oh°±n°h¬oÀ¥15É)»f° ̄ ̧#± ohÀ¥»f1 ±nÃ ®f°±n253]À¥Ê+Â ËvÌv3]¶+° ̄¶À¥Ây±#Ê+»f1]ÂhÀ¥1⁄4]¶+°±nÇ]Ã¥¶Ã¥À oh¶+°± o Ä ° ̄¶»f1®f°±n25351⁄2 1⁄4]° ̄±4μrÀ¥1]®q±nÃ 1⁄2 ®f»f° ̄À oh3] ̧qÂ4È Ç Ä oyÀ o25° ̄À¥ ̧#±n° ̄À¥Ã ¬ Ê4»f15Ê4¶+°h1]Â/1]»f1]À¥1Boh¶4° ̄±nÊDo ̄À ÅB¶oo ̄¶+Ê3]15À¥Í Ä ¶+Â/É)»f° ± Ä oh»f ̧#± o ̄À¥Ê®f° ̄±n2]3HÃÁ±+¬B» Ä oYÎ Ï5È Ð5È ÑfÑBÈ Ñ4Ï È Ñ4Ò Ó Ë Q ` ]Ô7$!^, & 1 ]* L7 - & 7K7&'('63⁄4 `K H( u&'GB$)* $) `0 K &'GB 1 n $)% &J !* & % &J n ! r% $1(4L3  0 `$)5 $!  - `&K7$1 A 0 ^ I &GJ ^  L7 &J #GJ % &' 4 n(o &'( & % $!( L` A 0 `$)5 $! &' L $! &J^ &J ($!  g( &B  -uGB `(4 n $! n(&BÔ7 &'( (4&5K$! &J ^ ( -y (4$!^  !&g *7( $!G' 3⁄4^ K &J ̄6 `K~GJ !GJL ! &5( & 3 (4$) $! `( -v0 n  `$!G' _7Õ+&'GB (Y% $1  *7( $!G' 3( $!^ L 1 $) vI\"# !$)e &# A K $t $) ` q &' Gf $!% & $! &J -l GJ&'($! Ö\\q `$!Gn Ö `&L`(4&' ,^, e &5( &' L &5(+  `K\\ $) (  &5(4u `( & 6qwrx z {r|$1( L ! )*$) A &J n GB $!% &@( *7(+ &J^×$! \\q `$!Gn _3 L`(4&' H `KQGJ ^  L7 &J GJ Øu&5Gf  & (4 &g -v `&HK7 n ]\\q$! 0 ( $)^@L t n &' L`( !* I & (4*7( A &J^ÙK &5( q` L`(4&H\\q $! )&H & L`(4&' # K K`(Y ^ % &'(Y 7K7&'( `KÚ&'K 0 &'('6/   !$)&5( @K7&' )&J &'(@GB `(+ $) n(JIWZr$)e &J\\q$1(4& 6 & L3(4&' YGJ ,$) &J % &J `& `K,GJ $! L &q $) &J n Gf Y\\q$t , & K ]\\q$! 0 (q `&( *7(+ &J^× K]Õ+L3(+ n( 7K7&@3 (4$) $! 3( $! Q  A &J^ 7 q ( $1(+*, &% $) L`(L`( &J A ( 3&5GB$) ̈`&'K~GB `(4 n $! n(JI UW &Q(4u&'GJ$t ̈uGJ ! !* 6 wyx z {r|§$! `GB u n &'(s GJ &JL ! )*ÛGn A ( &J Ú( &B  -4^, GJ  GB `(4 n $! n(J6· . $1(4L` YÜg 0 $!' $! Ý`&' L &'(  .gÜHÝ/(nf6v\\q `$!Gn  &@ !$!(4 &5KQ$! ÚÝy$!0 L` & ? Þ &,  A ` )$1GJ $!  -&5 Gn .gÜHÝ$!( $) ! )L3(+ &'K _ *mnOlßBN R OH `K 4ßn OJR 1 ]* L7 ('I àÛ & L`(4&' A ( 3&5GB$) ̈`&'K.gÜHÝ/( &   !$!&'K_ *GB A % &J $! 0 `&J^á$) @ (4&J  -v(  $! 0 A !$)e & nGB&5( Ø3⁄4&'GB $! 0 & ` K &'(H$) Ú s0 n  QK ]\\q$! 0`I8gK K7$) $! ` /nGB&5(g &$) A K L`GB&5K, L7 ^, $!G' ! )*  &'( &J % & (4* Gf $!GqGJ &5Gf `&'( (/ &qK7 n ]\\q$) 03Iy & L`( &J ^, ]* !( H   !*nGB& K7$! &5Gf )*  ` K &_ *sK7 n 0 0 $! 0 $t \\q$) &^ L3(4& IY & ` K &'('6`\\q $1Gn &g^ K &J !&'K (Yu $! Y^, ( ( &'('6 & ^ % &'K,$! @^$! $!^ L ^ A &' &J 0 *âGB  ̈`0 L n $) `( _ *â Û` *7( $!G' ( $)^@L 1 $! ã_` (4&5K ~ 0 &J &' )$!J&5KGJ& A K7$) &'GB &5K !0 $) `^åä d æ 6 \\q ` ( &H_u& A 3 ]% $! q$1(#&5 ( $) !*s   &'GJ$! &'K `K $) ç`L &J 3GB&'K _ *s & L`(4&' 'I `&( $)^@L 1 $!  - &  *7(4$1GJ ^ 7K7&' $1( $!^, &'KGB A $) L L`( )* 6r L3(  % $!K $) 0L3(4&JL o% $1(4L` &'&'K7_` GneW  & L3(4&' ä ? 6 ?'b æ Iq8# ) ` L 0 \\Y&5 e^ &'Gn 3 $1(4^Ù( $!(4* $) `0 GJ `(4 n $! ('6 &J `&J 0 *^ $! $!^$!' $! L 0 ` *7( $!G' r( $!^ A L` ! $! 3 `K7 !&'(y % &' A GB `(4 n $! &'K ( * (4 &'^,(y0 GB&JL ) !* 6 `K ` % $1K7&'( ~&' (4$! !*L `K7&' (4 7Ks^&J  &L3(4&' 'I è Ìv3]¶+Â ̄¶éêë Ây3]±4ÅB¶Ç ¶4¶+1À¥1]Ê+»f°h2 »f° ̄± o ̄¶+1⁄4 25° ̄¶DÅ5À¥» Ä Â ̄Ã ¬gÀ¥1go μ·»qÇ ± ohÊ3 Âh¬'Âhoh¶4 ̧qÂ3⁄4É)»f°v®f°±n2]3 ÃÁ±4¬B» Ä oÎ ì5È]í Ó Ë`Ìr35¶2]° ̄¶+Â ̄¶+1BorÂh¬'Âhoh¶4 ̧À¥Â·o ̄35¶/î ° ̄Âlo·o ̄» ±nÃ¥Ã¥» μ@À¥1Joh¶+°±nÊDo ̄À ÅB¶3Â ̄2 ¶+Ê+À î Ê4± o ̄À¥»f1Y±n1]1⁄4 ̧#±n1]À¥2 Ä ÃÁ± ohÀ¥»f1»nÉ]2 ¶+° ̄ÂhÀ¥Âhoh¶+1Jo735À¥®f351⁄2 Ã¥¶+ÅB¶+ÃBéêë`Â Â Ä Ê3o±nÂ oh3]¶+Â ̄¶fË7ï 1o° ̄¶+ÃÁ± oh¶41⁄4μ·»f°h¿ È ðy¶+1]°l¬±n1]1⁄4oð Ä 1⁄4]Â ̄»f1o3 ±4ÅB¶ 1⁄4]¶+Â ̄Ê+° ̄À¥Ç ¶+1⁄4 ±n1gÀ¥1Bo ̄¶+° ̄±nÊ+ohÀ ÅB¶/®f° ̄±n2]351⁄2 ÃÁ±+¬B» Ä ovÂh¬'Âhoh¶+ ̧oh3 ± o Ä Âh¶4Âv±1⁄4]À ñ ¶+° ̄¶+1Bo Â ̄¶Dor»nÉ 1]»f1]2 ¶+° ̄Â ̄À¥Âlo ̄¶+1BoÃÁ±+¬B» Ä oy»f2 ¶+° ̄± o ̄»f°hÂYÎ ò Ó Ë\n",
      "=============================\n",
      "FeelCraft: crafting tactile experiences for media using a feel effect library\n",
      "FeelCraft is a media plugin that monitors events and states in the media and associates them with expressive tactile content using a library of feel effects (FEs). A feel effect (FE) is a user-defined haptic pattern that, by virtue of its connection to a meaningful event, generates dynamic and expressive effects on the user's body. We compiled a library of more than fifty FEs associated with common events in games, movies, storybooks, etc., and used them in a sandbox-type gaming platform. The FeelCraft plugin allows a game designer to quickly generate haptic effects, associate them to events in the game, play them back for testing, save them and/or broadcast them to other users to feel the same haptic experience. Our demonstration shows an interactive procedure for authoring haptic media content using the FE library, playing it back during interactions in the game, and broadcasting it to a group of guests.\n",
      "=============================\n",
      "Sauron: embedded single-camera sensing of printed physical user interfaces\n",
      "3D printers enable designers and makers to rapidly produce physical models of future products. Today these physical prototypes are mostly passive. Our research goal is to enable users to turn models produced on commodity 3D printers into interactive objects with a minimum of required assembly or instrumentation. We present Sauron, an embedded machine vision-based system for sensing human input on physical controls like buttons, sliders, and joysticks. With Sauron, designers attach a single camera with integrated ring light to a printed prototype. This camera observes the interior portions of input components to determine their state. In many prototypes, input components may be occluded or outside the viewing frustum of a single camera. We introduce algorithms that generate internal geometry and calculate mirror placements to redirect input motion into the visible camera area. To investigate the space of designs that can be built with Sauron along with its limitations, we built prototype devices, evaluated the suitability of existing models for vision sensing, and performed an informal study with three CAD users. While our approach imposes some constraints on device design, results suggest that it is expressive and accessible enough to enable constructing a useful variety of devices.\n",
      "=============================\n",
      "Synchronous gestures for multiple persons and computers\n",
      "This research explores distributed sensing techniques for mobile devices using synchronous gestures. These are patterns of activity, contributed by multiple users (or one user with multiple devices), which take on a new meaning when they occur together in time, or in a specific sequence in time. To explore this new area of inquiry, this work uses tablet computers augmented with touch sensors and two-axis linear accelerometers (tilt sensors). The devices are connected via an 802.11 wireless network and synchronize their time-stamped sensor data. This paper describes a few practical examples of interaction techniques using synchronous gestures such as dynamically tiling together displays by physically bumping them together, discusses implementation issues, and speculates on further possibilities for synchronous gestures.\n",
      "=============================\n",
      "Transparent sharing of Java applets: a replicated approach\n",
      "People interact together in all aspects of life and, as computers have become prevalent, users seek computer support for their interactions. The WWW provides an unprecedented opportunity for users to interact with each other, and the advent of JavaThfl has created a consistent computing environment to support synchronous collaboration. We describe JAMM, a prototype Java runtime environment that supports the shared use of existing Java applets, thus leveraging the existing base of software for synchronous collaboration. Our approach is based on a replicated architecture, where each user maintains their own copy of the Java applet, and the users’ input events are broadcast to each applet copy. We discuss solutions to certain key problems, such as unanticipated sharing, supporting late-joiners, and replicating input sources other than user inputs (e.g., files, sockets, and random number generators).\n",
      "=============================\n",
      "Improvements to keyboard optimization with integer programming\n",
      "Keyboard optimization is concerned with the design of keyboards for different terminals, languages, user groups, and tasks. Previous work in HCI has used random search based methods, such as simulated annealing. These \"black box\" approaches are convenient, because good solutions are found quickly and no assumption must be made about the objective function. This paper contributes by developing integer programming (IP) as a complementary approach. To this end, we present IP formulations for the letter assignment problem and solve them by branch-and-bound. Although computationally expensive, we show that IP offers two strong benefits. First, its structured non-random search approach improves the out- comes. Second, it guarantees bounds, which increases the designer's confidence over the quality of results. We report improvements to three keyboard optimization cases.\n",
      "=============================\n",
      "Integrated videos and maps for driving directions\n",
      "While onboard navigation systems are gaining in importance, maps are still the medium of choice for laying out a route to a destination and for way finding. However, even with a map, one is almost always more comfortable navigating a route the second time due to the visual memory of the route. To make the first time navigating a route feel more familiar, we present a system that integrates a map with a video automatically constructed from panoramic imagery captured at close intervals along the route. The routing information is used to create a variable speed video depicting the route. During playback of the video, the frame and field of view are dynamically modulated to highlight salient features along the route and connect them back to the map. A user interface is demonstrated to allow exploration of the combined map, video, and textual driving directions. We discuss the construction of the hybrid map and video interface. Finally, we report the results of a study that provides evidence of the effectiveness of such a system for route following.\n",
      "=============================\n",
      "Parallel bargrams for consumer-based information exploration and choice\n",
      "In this paper we introduce multidimensional visualization and interaction techniques that are an extension to related work in parallel histograms and dynamic querying. Bargrams are, in effect, histograms whose bars have been tipped over and lined up end-to-end. We discuss affordances of parallel bargrams in the context of systems that support consumer-based information exploration and choice based on the attributes of the items in the choice set. Our tool called EZChooser has enabled a number of prototypes in such domains as Internet shopping, investment decisions, college choice, and so on, and a limited version has been deployed for car shopping. Evaluations of the techniques include an experiment indicating that trained users prefer EZChooser over static tables for choice tasks among sets of 50 items with 7-9 attributes.\n",
      "=============================\n",
      "ModelCraft: capturing freehand annotations and edits on physical 3D models\n",
      "With the availability of affordable new desktop fabrication techniques such as 3D printing and laser cutting, physical models are used increasingly often during the architectural and industrial design cycle. Models can easily be annotated to capture comments, edits and other forms of feedback. Unfortunately, these annotations remain in the physical world and cannot be easily transferred back to the digital world. Here we present a simple solution to this problem based on a tracking pattern printed on the surface of each model. Our solution is inexpensive, requires no tracking infrastructure or per object calibration, and can be used in the field without a computer nearby. It lets users not only capture annotations, but also edit the model using a simple yet versatile command system. Once captured, annotations and edits are merged into the original CAD models. There they can be easily edited or further refined. We present the design of a SolidWorks plug-in implementing this concept, and report initial feedback from potential users using our prototype. We also present how this prototype could be extended seamlessly to a fully functional system using current 3D printing technology.\n",
      "=============================\n",
      "Integrating gesture and snapping into a user interface toolkit\n",
      "This paper describes Artkit - the Arizona Retargetable Toolkit - an extensible object-oriented user interface toolkit. Artkit provides an extensible input model which is designed to support a wider range of interaction techniques than conventional user interface toolkits. In particular the system supports the implementation of interaction objects using dragging, snapping (or gravity fields), and gesture (or handwriting) inputs. Because these techniques are supported directly by the toolkit it is also possible to create interactions that mix these techniques within a single interface or even a single interactor object.\n",
      "=============================\n",
      "Dynamo: a public interactive surface supporting the cooperative sharing and exchange of media\n",
      "In this paper we propose a novel way of supporting occasional meetings that take place in unfamiliar public places, which promotes lightweight, visible and fluid collaboration. Our central idea is that the sharing and exchange of information occurs across public surfaces that users can easily access and interact with. To this end, we designed and implemented Dynamo, a communal multi-user interactive surface. The surface supports the cooperative sharing and exchange of a wide range of media that can be brought to the surface by users that are remote from their familiar organizational settings.\n",
      "=============================\n",
      "TextTearing: opening white space for digital ink annotation\n",
      "Having insufficient space for making annotations is a problem that afflicts both paper and digital documents. We introduce the TextTearing technique for in situ expansion of inter-line whitespace and pair it with a lightweight interaction for margin expansion as a way to address this problem. The full system leverages the dynamism of digital documents and employs a bimanual design that combines the precision of pen with the fluidity of touch. Our evaluation found that a simpler unimanual variant of TextTearing was preferred over direct annotation and margin-only expansion. Direct annotation in naturally occurring whitespace was least preferred.\n",
      "=============================\n",
      "Debugging lenses: a new class of transparent tools for user interface debugging\n",
      "The visual and event driven nature of modem user interfaces, while a boon to users, can also make them more difficult to debug than conventional programs. This is because only the very surface representation of interactive objects their final visual appearance is visible to the programmer on the screen. The remaining “programming details” of the object remain hidden. If the appearance or behavior of an object is incorrect, often few clues are visible to indicate the cause. One must usually turn to text oriented debugging techniques (debuggers or simply print statements) which are separate from the interface, and often cumbersome to use with event-driven control flow. This paper describes a new class of techniques designed to aid in the debugging of user interfaces by making more of the invisible, visible. This class of techniques: debugging lenses, makes use of transparent lens interaction techniques to show debugging information. It is designed to work in situ in the context of a running interface, without stopping or interfering with that interface.This paper describes and motivates the class of techniques, gives a number of specific examples of debugging lenses, and ,describes their implementation in’ the subArctic user interface toolkit. ”\n",
      "=============================\n",
      "The jabberwocky programming environment for structured social computing\n",
      "We present Jabberwocky, a social computing stack that consists of three components: a human and machine resource management system called Dormouse, a parallel programming framework for human and machine computation called ManReduce, and a high-level programming language on top of ManReduce called Dog. Dormouse is designed to enable cross-platform programming languages for social computation, so, for example, programs written for Mechanical Turk can also run on other crowdsourcing platforms. Dormouse also enables a programmer to easily combine crowdsourcing platforms or create new ones. Further, machines and people are both first-class citizens in Dormouse, allowing for natural parallelization and control flows for a broad range of data-intensive applications. And finally and importantly, Dormouse includes notions of real identity, heterogeneity, and social structure. We show that the unique properties of Dormouse enable elegant programming models for complex and useful problems, and we propose two such frameworks. ManReduce is a framework for combining human and machine computation into an intuitive parallel data flow that goes beyond existing frameworks in several important ways, such as enabling functions on arbitrary communication graphs between human and machine clusters. And Dog is a high-level procedural language written on top of ManReduce that focuses on expressivity and reuse. We explore two applications written in Dog: bootstrapping product recommendations without purchase data, and expert labeling of medical images.\n",
      "=============================\n",
      "Tools for expressive text-to-speech markup\n",
      "This paper describes handicapped accessible text-to-speech markup software developed for poetry and performance. Most text-to-speech software allows the user to select a voice, but provides no control over performance parameters such as rate, volume, and pitch. For users with vocal disabilities, the default \"computer voice\" is often dreaded since it provides no personalization. Evolving standards exist for text-to-speech markup (Sable, Java Speech Markup Language, Spoken Text Markup Language), but few tools exist for non-experts to modify documents using these prosody options [1, 5]. Furthermore, we could find fewer tools allowing for straightforward live performance using a synthesized voice [3]. Thus we created an easy to learn text-to-speech markup tool that requires little training to use.\n",
      "=============================\n",
      "On temporal-spatial realism in the virtual reality environment\n",
      "The Polhemus Isotrak is often used as an orientation and position tracking device in virtual reality environments. When it is used to dynamically determine the user's viewpoint and line of sight ( e.g. in the case of a head mounted display) the noise and delay in its measurement data causes temporal-spatial distortion, perceived by the user as jittering of images and lag between head movement and visual feedback. To tackle this problem, we rst examined the major cause of the distortion, and found that the lag felt by the user is mainly due to the delay in orientation data, and the jittering of images is caused mostly by the noise in position data. Based on these observations, a predictive Kalman lter was designed to compensate for the delay in orientation data, and an anisotropic low pass lter was devised to reduce the noise in position data. The e ectiveness and limitations of both approaches were then studied, and the results shown to be satisfactory.\n",
      "=============================\n",
      "LiveSphere: immersive experience sharing with 360 degrees head-mounted cameras\n",
      "Sharing full immersive experience in real-time has been the one of ultimate goals of telecommunication. Possible application can include various applications such as entertainment, sports viewing, education, social network and professional assistance. Recent head-worn wearable camera enables to shoot the first person video, however, view of angle is limited with the head direction of the person who is wearing, and also captured video is shaky that makes us dizzy. We propose LiveSphere, immersive experience sharing system with wearable camera headgear that provide 360 degrees spherical images of the user's surrounding environment. LiveSphere system performs spherical video stabilization and transmits it to other users, so that they are enable to view shared video comfortably and also look around at the scene from a different view angle independently from the first person. In this note, we explain the overview of the LiveSphere system implementation, stabilization and viewing experience.\n",
      "=============================\n",
      "Using taps to separate the user interface from the application code\n",
      "A new mechanism based on taps is introduced to separate the output from the application code in graphical interactive interfaces. The mechanism is implemented in GINA, an object-oriented application framework. Taps maintain a functional mapping from application data to interface objects that is described in a general-purpose programming language. Taps are triggered automatically by user actions. Compared to constraints or the MVC model, taps do not need execution or memory support from the application objects, at the expense of a performance penalty. Screen updates, which pose the largest performance problem, are minimized by checking for attribute changes and window visibility. A comparison operation is used to maintain structural consistency between hierarchies of application and interface objects. Taps can be defined interactively using formulas in a spreadsheet-like tool.\n",
      "=============================\n",
      "A pure reasoning engine for programming by demonstration\n",
      "We present an inference engine that can be used for creating Programming By Demonstration systems. The class of systems addressed are those which infer a state change description from examples of state [9, 11].The engine can easily be incorporated into an existing design environment that provides an interactive object editor.The main design goals of the inference engine are responsiveness and generality. All demonstrational systems must respond quickly because of their interactive use. They should also be general—they should be able to make inferences for any attribute that the user may want to define by demonstration, and they should be able to treat any other attributes as parameters of this definition.The first goal, responsiveness, is best accommodated by limiting the number of attributes that the inference engine takes into consideration. This, however, is in obvious conflict with the second goal, generality.This conflict is intrinsic to the class of demonstrational system described above. The challenge is to find an algorithm which responds quickly but does not heuristically limit the number of attributes it looks at. We present such an algorithm in this paper.A companion paper describes Inference Bear [4], an actual demonstrational system that we have built using this inference engine and an existing user interface builder [5].\n",
      "=============================\n",
      "Digital tape drawing\n",
      "Tape drawing is the art of creating sketches on large scale upright surfaces using black photographic tape. Typically used in the automotive industry, it is an important part of the automotive design process that is currently not computerized. We analyze and describe the unique aspects of tape drawing, and use this knowledge to design and implement a digital tape drawing system. Our system retains the fundamental interaction and visual affordances of the traditional media while leveraging the power of the digital media. Aside from the practical aspect of our work, the interaction techniques developed have interesting implications for current theories of human bimanual interaction.\n",
      "=============================\n",
      "EasySnap: real-time audio feedback for blind photography\n",
      "This demonstration presents EasySnap, an application that enables blind and low-vision users to take high-quality photos by providing real-time audio feedback as they point their existing camera phones. Users can readily follow the audio instructions to adjust their framing, zoom level and subject lighting appropriately. Real-time feedback is achieved on current hardware using computer vision in conjunction with use patterns drawn from current blind photographers.\n",
      "=============================\n",
      "Multi-perspective multi-layer interaction on mobile device\n",
      "We propose a novel multi-perspective multi-layer interaction using a mobile device, which provides an immersive experience of 3D navigation through an object. The mobile device serves as a window, through which the user can observe the object in detail from various perspectives by orienting the device differently. Various layers of the object can also be shown while users move the device away and toward themselves. Our approach is real-time, completely mobile (running on Android) and does not depend on external sensor/displays (e.g., camera and projector).\n",
      "=============================\n",
      "DON: user interface presentation design assistant\n",
      "We describe a design tool, DON, which assists user interface designers in generating menu and dialog box presentations. An integrated knowledge base model serves as the foundation for developing thesetofdesignrulestoautomatevariousactivitiesofthe designprccess.Usefulandreusableknowledgeabouttheorganization of menus and dialog boxes is identified andencapsulated in the form of design rules. The basic approach we take is embedding a top-down design methodology in a tool that assists designers in organizing the information, selecting appropriate interface object classes and their attributes, and placing selected interface objects in a dialog box or a menu in a meaningful, logical, and consistent manner. We let the designer specify the conceptual design of an application, maintain high-level style preference profiles, customize the appearances of interface object classes whichmakeup aninterface presentation. and controlthepriority of organizationrules. The tool then automatically generates the user interface presentation. INTRODUCTION A user interface management system (UIMS) provides a framework for producing quality interfaces faster and easier. The high cost of trying out interface designs to accommodate varying styles and preferences makes developing interfaces very difficult without design tools which speed up the process. Bottom-up interface building tools provide interactive ways to create, compose, and modify the objects which make up the user interfaces. Interface building tools like Trillium [Henderson 861, the dialog editor [Cardelli 881, Interviews [Linton 891, Prototyper [Cossey 89],theExperInterfaceBuilder[Hullot87],andTransportable Application Executive (TAE Plus) [ Szczur 881, are very effective in speeding up the iterative development cycle, but do not guide and assist the designer in choosing and organizing interface objects. Thesebottom-updesigntoolsreduceinitialcreationtimeandfacilitate modifying visual attributes, and some provide graphical constraints to assist in laying out interface objects. Difficulties arise when a designer tries to reorganize the content or to change the application objecttointerfaceobjectmappingwhenknowledgeoflinksbetween applicationknowledgeandtheinteractively laid-outinterfaceobjects Permission to copy without fee all or part of this matertial is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that the copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. @ 1990 ACM 0897914104/90/0010/0010 $1.50 is lacking. The major drawback is that the resulting interface quality is too dependent on a designer’s ability to organize information and lay out selected interface objects. There is a class of UIMS’s which attempt to create the user interface directly from a specification of the application semantic descriptions, and then allow the designer to modify the interface to improve it. The top-down development approach starts with conceptual representation, then analyzes, structures, develops, and fiia.lly allows modification to the interface design [Hartson 871. Examples of tools that automate some aspect of this process are the Menu Interaction Kontrol Environment (MIKE) [Olsen 871, Chisel [Singh 861, Generator [Petoud 881, Presentation Designer [Arens 881. and User Interface Design Environment (UIDE) [Foley 88, Foley 891. MIKE does not consider user characteristics or designer preferences during the initial design process. The Generator automatically creates an operational version of the user interface from the functional specification of the application, and the Presentation Designer applies a system of antecedent-consequent rules that maps objects from the applicationmodel into objects in the interface model. Both of these tools concentrate on the selection processfor thedatatointerfaceobjectmappingandusesimplelayout rules to compose the selected interface objects. Chisel lacks the content organization rules needed to generate a wide range of user interface styles. Our User Interface Presentation Design Assistant (DON) is an intelligent design tool that focuses on automatic organization and development of menus and dialog boxes using various types of knowledge as input and a set of design rules as an engine. DON means to put on (an article of clothing). in this case, to put on user interface objects. The selection of each piece of clothing is made basedoneachperson’sphysical attributes,personalstylepreferences, and appearance goal for each particular day. Similarly, designing user interfaces requires careful analysis of the conceptual design of each application, basic style preferences of interface objects, and high-level goals specific to the current design instance. DON is designed to go beyond the existing tools. It is baaed on User Interface Design Environment (UIDE), a knowledge-based system to assist in user interface design, evaluation, and implementation. DON extends the general UIDE research objective to identify knowledge, logical relations, and rules used by interface designers, and to embody them in the knowledge base as a way of further automating the design process. KNOWLEDGE BASE MODEL The architecture of thedesignenvironment (Figure 1) embodies theknowledge base model that unites various components needed for effectiveuser~terfacepresentation.Theintegratedknowledgebase\n",
      "=============================\n",
      "Model-based user interface design by example and by interview\n",
      "Model-based user interface design is centered around a description of application objects and operations at a level of abstraction higher than that of code. A good model can be used to support multiple interfaces, help separate interface and application, describe input sequencing in a simple way, check consistency and completeness of the interface, evaluate the interface’s speed-of-use, generate context-specific help and assist in designing the interface. However, designers rarely use computer-supported application modelling today and prefer less formal approaches such as story boards of user interface prototypes. One reason is that available tools often use cryptic languages for the model specification. Another reason is that these tools force the designers to specify the application model before they can start working on the visual interface, which is their main area of expertise. We present the Interactive User Interface Design Environment (Interactive UIDE), a novel framework for concurrent development of the application model and the user interface which combines story-boarding and model-based interface design. We also present Albert, an intelligent component within this framework, which is able to infer an application model from a user interface and from an interview process with the designer.\n",
      "=============================\n",
      "Page detection using embedded tags\n",
      "We describe a robust working prototype of a system for accurate page-ID detection from bound paper books. Our method uses a new RFID technology to recognize book page location. A thin flexible transponder tag with a unique ID is embedded in the paper of each page, and a tag reader is affixed to the binding of the back of the book. As the pages turn, the tag reader notices which tags are within its read range and which have moved out of its range (which is about four inches). The human interacts with the book naturally, and is not required to perform any actions for page detection that are not usual in book interaction. The page-detection data can be used to enhance the experience of the book, or to enable the book as a controller for another system. One such system, an interactive museum exhibit, is briefly described.\n",
      "=============================\n",
      "The importance of pointer variables in constraint models\n",
      "Graphical tools are increasingly using constraints to specify the graphical layout and behavior of many parts of an application. However, conventional constraints directly encode the objects they reference, and thus cannot provide support for the dynamic rttntime creation and manipulation of application objects. This paper discusses an extension to current constraint models that allows constraints to indirectly reference objects through pointer variables. Pointer variables permit programmers to create the constraint equivalent of procedures in traditional programming languages. This procedural abstraction allows constraints to model a wide array of dynamic application behavior, simplifies the implementation of structured object and demonstrational systems, and improves the storage and efficiency of highly interactive, graphical applications. It also promotes a simpler, more effective style of programming than conventional constraints. Constraints that use pointer variables are powerful enough to allow a comprehensive user interface toolkit to be built for the first time on top of a constraint system.\n",
      "=============================\n",
      "Gaze locking: passive eye contact detection for human-object interaction\n",
      "Eye contact plays a crucial role in our everyday social interactions. The ability of a device to reliably detect when a person is looking at it can lead to powerful human-object interfaces. Today, most gaze-based interactive systems rely on gaze tracking technology. Unfortunately, current gaze tracking techniques require active infrared illumination, calibration, or are sensitive to distance and pose. In this work, we propose a different solution-a passive, appearance-based approach for sensing eye contact in an image. By focusing on gaze *locking* rather than gaze tracking, we exploit the special appearance of direct eye gaze, achieving a Matthews correlation coefficient (MCC) of over 0.83 at long distances (up to 18 m) and large pose variations (up to ±30° of head yaw rotation) using a very basic classifier and without calibration. To train our detector, we also created a large publicly available gaze data set: 5,880 images of 56 people over varying gaze directions and head poses. We demonstrate how our method facilitates human-object interaction, user analytics, image filtering, and gaze-triggered photography.\n",
      "=============================\n",
      "GIST: a gestural interface for remote nonvisual spatial perception\n",
      "Spatial perception is a challenging task for people who are blind due to the limited functionality and sensing range of hands. We present GIST, a wearable gestural interface that offers spatial perception functionality through the novel appropriation of the user's hands into versatile sensing rods. Using a wearable depth-sensing camera, GIST analyzes the visible physical space and allows blind users to access spatial information about this space using different hand gestures. By allowing blind users to directly explore the physical space using gestures, GIST allows for the closest mapping between augmented and physical reality, which facilitates spatial interaction. A user study with eight blind users evaluates GIST in its ability to help perform everyday tasks that rely on spatial perception, such as grabbing an object or interacting with a person. Results of our study may help develop new gesture based assistive applications.\n",
      "=============================\n",
      "Edge-respecting brushes\n",
      "Digital paint is one of the more successful interactive applications of computing. Brushes that apply various effects to an image have been central to this success. Current painting techniques ignore the underlying image. By considering that image we can help the user paint more effectively. There are algorithms that assist in selecting regions to paint including flood fill, intelligent scissors and graph cut. Selected regions and the algorithms to create them introduce conceptual layers between the user and the painting task. We propose a series of \"edge-respecting brushes\" that spread paint or other effects according to the edges and texture of the image being modified. This restores the simple painting metaphor while providing assistance in working with the shapes already in the image. Our most successful fill brush algorithm uses competing least-cost-paths to identify what should be selected and what should not.\n",
      "=============================\n",
      "High-latency, low-bandwidth windowing in the Jupiter collaboration system\n",
      "Jupiter is a multi-user, multimedia virtual world intended to support long-term remote collaboration. In particular, it supports shared documents, shared tools, and, optionally, live audio/video communication. Users who program can, with only moderate effort, create new kinds of shared tools using a high-level windowing toolkit; the toolkit provides transparent support for fully-shared widgets by default. This paper describes the low-level communications facilities used by the implementation of the toolkit to enable that support. The state of the Jupiter virtual world, including application code written by users, is stored and (for code) executed in a central server shared by all of the users. This architecture, along with our desire to support multiple client platforms and high-latency networks, led us to a design in which the server and clients communicate in terms of high-level widgets and user events. As in other groupware toolkits, we need a concurrency-control algorithm to maintain common values for all instances of the shared widgets. Our algorithm is derived from a fully distributed, optimistic algorithm developed by Ellis and Gibbs [12]. Jupiter’s centralized architecture allows us to substantially simplify their algorithm. This combination of a centralized architecture and optimistic concurrency control gives us both easy serializability of concurrent update streams and fast response to user actions. The algorithm relies on operation transformations to fix up conflicting messages. The best transformations are not always obvious, though, and several conflicting concerns are involved in choosing them. We present our experience with choosing transformations for our widget set, which includes a text editor, a graphical drawing widget, and a number of simpler widgets such as buttons and sliders.\n",
      "=============================\n",
      "Sensor design and interaction techniques for gestural input to smart glasses and mobile devices\n",
      "Touchscreen interfaces for small display devices have several limitations: the act of touching the screen occludes the display, interface elements like keyboards consume precious display real estate, and even simple tasks like document navigation - which the user performs effortlessly using a mouse and keyboard - require repeated actions like pinch-and-zoom with touch input. More recently, smart glasses with limited or no touch input are starting to emerge commercially. However, the primary input to these systems has been voice. In this paper, we explore the space around the device as a means of touchless gestural input to devices with small or no displays. Capturing gestural input in the surrounding volume requires sensing the human hand. To achieve gestural input we have built Mime [3] -- a compact, low-power 3D sensor for short-range gestural control of small display devices. Our sensor is based on a novel signal processing pipeline and is built using standard off-the-shelf components. Using Mime we demonstrated a variety of application scenarios including 3D spatial input using close-range gestures, gaming, on-the-move interaction, and operation in cluttered environments and in broad daylight conditions. In my thesis, I will continue to extend sensor capabilities to support new interaction styles.\n",
      "=============================\n",
      "Pursuit calibration: making gaze calibration less tedious and more flexible\n",
      "Eye gaze is a compelling interaction modality but requires user calibration before interaction can commence. State of the art procedures require the user to fixate on a succession of calibration markers, a task that is often experienced as difficult and tedious. We present pursuit calibration, a novel approach that, unlike existing methods, is able to detect the user's attention to a calibration target. This is achieved by using moving targets, and correlation of eye movement and target trajectory, implicitly exploiting smooth pursuit eye movement. Data for calibration is then only sampled when the user is attending to the target. Because of its ability to detect user attention, pursuit calibration can be performed implicitly, which enables more flexible designs of the calibration task. We demonstrate this in application examples and user studies, and show that pursuit calibration is tolerant to interruption, can blend naturally with applications and is able to calibrate users without their awareness.\n",
      "=============================\n",
      "Spatial augmented reality for physical drawing\n",
      "Spatial augmented reality (SAR) makes possible the projection of virtual environments into the real world. In this demo, we propose to demonstrate our SAR tools dedicated to the creation of physical drawings. From the most simple tools: the projection on virtual guidelines enabling to trace lines and curves to more advanced techniques enabling stereoscopic drawing through the projection of a 3D scene. This demo presents how we can use computer graphics tools to ease the drawing, and how it will enable new kinds of physical drawings.\n",
      "=============================\n",
      "Programming with everybody: tightening the copy-modify-publish feedback loop\n",
      "People write more code than they ever share online. They also copy and tweak code more often than they contribute their modifications back to the public. These situations can lead to widespread duplication of effort. However, the copy-modify-publish feedback loop which could solve the problem is inhibited by the effort required to publish code online. In this paper we present our preliminary, ongoing effort to create Ditty, a programming environment that attacks the problem by sharing changes immediately, making all code public by default. Ditty tracks the changes users make to code they find and exposes the modified versions alongside the original so that commonly-used derivatives can eventually become canonical. Our work will examine mechanical and social methods to consolidate global effort on common code snippets, and the effects of designing a programming interface that inspires a feeling of the whole world programming together.\n",
      "=============================\n",
      "Tactile rendering of 3D features on touch surfaces\n",
      "We present a tactile-rendering algorithm for simulating 3D geometric features, such as bumps, on touch screen surfaces. This is achieved by modulating friction forces between the user's finger and the touch screen, instead of physically moving the touch surface. We proposed that the percept of a 3D bump is created when local gradients of the rendered virtual surface are mapped to lateral friction forces. To validate this approach, we first establish a psychophysical model that relates the perceived friction force to the controlled voltage applied to the tactile feedback device. We then use this model to demonstrate that participants are three times more likely to prefer gradient force profiles than other commonly used rendering profiles. Finally, we present a generalized algorithm and conclude the paper with a set of applications using our tactile rendering technology.\n",
      "=============================\n",
      "ENO: synthesizing structured sound spaces\n",
      "ENO is an audio server designed to make it easy for applications in the Unix environment to incorporate non-speech audio cues. At the physical level, ENO manages a shared resource, namely the audio hardware. At the logical level, it manages a sound space that is shared by various client applications. Instead of dealing with sound in terms of its physical description (i.e., sampled sounds), ENO allows sounds to be presented and controlled in terms of higher-level descriptions of sources, interactions, attributes, and sound space. Using this structure, ENO can facilitate the creation of consistent, rich systems of audio cues. In this paper, we discuss the justification, design, and implementation of ENO.\n",
      "=============================\n",
      "Spatial augmented reality to enhance physical artistic creation\n",
      "Spatial augmented reality (SAR) promises the integration of digital information in the real (physical) world through projection. In this doctoral symposium paper, I propose different tools to improve speed or ease the drawing by projecting photos, virtual construction lines and interactive 3D scenes. After describing the tools, I explain some future challenges to explore such as the creation of tools which helps to create drawings that are \"difficult\" to achieve for a human being, but easy to do by a computer. Furthermore, I propose some insights for the creation of digital games and programs which can take full advantages of physical drawings.\n",
      "=============================\n",
      "Depth based interaction and field of view manipulation for augmented reality\n",
      "In recent years, the market for portable devices has seen a large increase in the development of head mounted displays. While these displays provide many benefits to users, safety is still a concern. In particular, ensuring that content does not interfere with everyday activities and that users have adequate peripheral vision is very important for situational awareness. In this paper, I address these issues through the use of two novel display prototypes. The first is an optical see-through multi-focal plane display combined with an eye tracking interface. Through eye tracking and knowledge of the focal plane distances, I can calculate whether a user is looking at the environment or at a focal plane in the display. Any distracting text can then be quickly removed so that he or she has a clear view of the environment. The second prototype is a video see-through display which expands a user's environmental view through the use of 238° ultra wide field of view fisheye lenses. Based on the results of several initial evaluations, these new interfaces have the potential help users improve environmental awareness.\n",
      "=============================\n",
      "Enabling tangible interaction on capacitive touch panels\n",
      "We propose two approaches to sense tangible objects on capacitive touch screens, which are used in off-the-shelf multi-touch devices such as Apple iPad, iPhone, and 3M's multi-touch displays. We seek for the approaches that do not require modifications to the panels: spatial tag and frequency tag. Spatial tag is similar to fiducial tag used by tangible tabletop surface interaction, and uses multi-point, geometric patterns to encode object IDs. Frequency tag simulates high-frequency touches in the time domain to encode object IDs, using modulation circuits embedded inside tangible objects to simulate high-speed touches in varying frequency. We will show several demo applications. The first combines simultaneous tangible + touch input system. This explores how tangible inputs (e.g., pen, easer, etc.) and some simple gestures work together on capacitive touch panels.\n",
      "=============================\n",
      "Manipulating structured information in a visual workspace\n",
      "This paper describes the VITE system, a visual workspace that supports two-way mapping for projecting structured information to a two-dimensional workspace and updating the structured information based on user interactions in the workspace. This is related to information visualization, but reflecting visual edits in the structured data requires a two-way mapping from data to visualization and from visualization to data. VITE provides users with an interface for designing two-way mappings. Mappings are reusable on different datasets and may be switched within a task. An evaluation of VITE was conducted to study how people use two-way mapping and how two-way mapping can help in problem solving tasks. The results show that users could quickly design visual mappings to help their problem-solving tasks. Users developed more sophisticated strategies for visual problem-solving over time.\n",
      "=============================\n",
      "Physical user interfaces: what they are and how to build them\n",
      "Physical user interfaces are special purpose devices that can be situated in a real-world setting. Unlike general purpose computers, they are typically designed for particular contexts and uses. In this survey, I present an introductory tour of this new interface genre. First, I will summarize what they are by describing several design niches for these devices: ubiquitous computing, tangible media, foreground and ambient devices, collaborative devices, roomware, and physical controls. Examples will be plentiful, and will range from the playful, to the artistic, and to the serious. Second, I will introduce technologies that are suitable for software professionals who wish to prototype these physical user interfaces. The commercially available Phidgets (www.phidgets.com) are used as a case study of what is available and what can be done with them.\n",
      "=============================\n",
      "Evolutionary learning of graph layout constraints from examples\n",
      "We propose a new evolutionary method of extracting user preferences from examples shown to an automatic graph layout system. Using stochastic methods such as simulated annealing and genetic algorithms, automatic layout systems can find a good layout using an evaluation function which can calculate how good a given layout is. However, the evaluation function is usually not known beforehand, and it might vary from user to user. In our system, users show the system several pairs of good and bad layout examples, and the system infers the evaluation function from the examples using genetic programming technique. After the evaluation function evolves to reflect the preferences of the user, it is used as a general evaluation function for laying out graphs. The same technique can be used for a wide range of adaptive user interface systems.\n",
      "=============================\n",
      "Browsing the Web with a mail/news reader\n",
      "This TechNote introduces WebCard, an integrated maillnews reader and Web browser. As a maillnews reader, WebCard is fairly conventional; the innovation is that Web pages are fully integrated in the mail/news reader. The user interface is based on folders, where an “item” in a folder can be a mail message, news article or Web page. When displaying a Web page, users can follow links, and the new pages will appear as items in the current folder. Users can copy and move items between folders, forward items, and can also use folders to organize material on the Web, such as hotlists, query results, and breadth-first expansions. INTRODUCTION As people are spending increasing amounts of time sending e-mail, reading news, and surfing the Web, an integrated user interface to the three activities is increasingly appealing. Leading-edge Web browsers are moving toward this goal by starting to include features for handling e-mail and news. For instance, Netscape (version 1.IN) [4] provides a polished interface for reading news and posting articles. Netscape, like most Web browsers, allows a user to send e-mail; however, no browsers support reading e-mail. Leading-edge mail readers are moving in the same direction, by starting to provide access to the Web. For example, Z -Mai 1 [5] allows e-mail to reference HTML documents as MIME attachments. The text part of the mail message is shown in the standard fashion, and clicking on the “attachment” button invokes a Web browser of the user’s choice that displays the attachment. Once the Web browser is running, it is completely independent of the mail reader, however. This TechNote introduces WebCard, a folder-based mail/news reader (think of it as a combination of xmh and xrn) that has been enhanced to handle Web pages using the same inter-face that it already uses for handling mail messages and newsgroup posting. Moreover, folders of Web pages proPennission to make digitrd/hnrd copies of all or part of this material for personal or classroom use is grrtntecl without fee provided thnt lhe copies are not made or distributed for profit or commercial advantage, the copyright notice, the tine of the puhlicatimt and its date appear, and notice is given that crrpy right ia by pennissiun of IIIC ACM. Inc. TO copy otftenvise. to republish, to post on sewers or 10 redistribute to lists, requires specific permission and/or fee. UIST 95 Pittsburgh PA USA @1995 ACM O-89791-709-x/95/l 1..$3.50 vide a general-purpose way to organize Web material, such as hotlists, query results, and breadth-first expansions. WEBCARD At first blush, WebCard is a conventional folder-based e-mail reader. At any given time there is an open folder, and one message from the open folder, called the selected message, is displayed in the display pane of the WebCard window. The user can respond to the selected message, forward it, copy or move it to another folder, and so on. (Actually, more than one message can be selected, but some operations, like ‘<Reply,” are only valid when there is a single selected message.) Users can also perform standard folder operations, such as creating, renaming, and deleting folders. WebCard handles newsgroups in the same way: a folder whose name refers to a newsgroup contains the postings in the newsgroup. Users can respond to a news article by using the “Reply” button, copy postings to folders, and so on. The only difference between a newsgroup folder and any other folder is that newsgroup folders are “read-only;” users cannot delete messages from a news folder or copy messages into a news folder. WebCard uses the name of the folder to distinguish between newsgroup folders and ordinary folders; a period in the folder name indicates a newsgroup. The Andrew Messages System [3] is the only other system we know of that deals uniformly with e-mail and bulletin boards. WebCard also integrates Web pages in the same way: folders can contain Web pages. Web pages are rendered just as they would be in a stand-alone graphical Web browser, and displayed in WebCard’s display pane where the contents of a message normally are displayed (textually). The subject of the Web page “message” is the “<TITLE>” field of the Web page. The only operation that doesn’t make sense for a Web page “message” is “Reply.” There are three ways to display a Web page: clicking on a URL that appears in the currently displayed mail message or news article, primary-selecting a URL (in any window on the screen) and then middle-clicking on the “URE’ button, and using the “Open URL7 dialog. Once a Web page is displayed. clicking on a link fetches the new page. Depending on keyboard modifiers, the new page either replaces the existing page or is added as a “message” in the current folder. November 14-17, 1995 UIST ’95 197 These screen dumps show WebCard in action. The top part of the WebCard window contains a browser listing the subjects of mail messages or news postings and titles of Web pages. The bottom part of the window displays the mail message, news posting or Web page. At the left, WebCard is displaying a URL that had been mentioned in a news article posted by a user named “websurfer”. That article had been copied into the inbox as message 10. The image at the right shows WebCard after clicking on the “Mr. Showbiz” icon, and then on a link to retrieve a review of the movie Dangerous Minds. M, Stww+,zw,l, do wha&,,,ake$ roauermk?~. Grmttnam,cs F,Rm m,-, m ,, w,,, ,Iw w“ uc-,c-!k-ml,nl, enfert.”.mi m m Ikbrl,, O@ ,1= ..”,, re”leva ,elwlwm Mix,. WebCard allows users to “Detach” items from the folder, and temporarily display them in a separate pane in the WebCard window. When a detached item is a Web page, the user can click on links and the new page is added to the folder and it appears back on the display pane rather than obscuring the detached page. Thus, the user can have a page, such as a table of contents or index, visible for an extended period, even while following another chain of links on the main body of the folder. Folders provide a convenient way for users to organize material. For example, a user can keep the home pages of all of his or her colleagues together in a folder named “Colleagues,” or keep several “hotlists”, each in its own folder. WebCard also uses folders to return the results of certain operations. For example, WebCard has an “Expand One Level” command which traverses every link on a particular page, and returns all resulting pages in a new folder. IMPLEMENTATION WebCard is implemented in Modula–3 [2], and makes use of two existing Modula–3 applications, Postcard and DeckScape. Postcard is a folder-based maillnews reader that has been in daily use at SRC since 1988. DeckScape[l] is an experimental Web browser based on the metaphor of a deck, collections of Web pages only one of which is visible at a time. The DeckScape display consists of multiple decks, all in a single top-level window. Users can move, resize or iconify decks, move or copy pages between decks, and so on. Decks in DeckScape serve the function of folders in WebCard. WebCard support HTML2.0, including inline images (rendered in black-and-white), forms, and active maps. WebCard does not support external viewers. CONCLUSION WebCard has introduced a new interaction technique for browsing the Web. It integrates e-mail, news, and Web browsing into a single user interface, thereby avoiding the contextswitching inherent when using independent applications. As a Web browser, WebCard supports folders, a flexible way to organize, browse, and store large numbers of documents. We do not claim that WebCard the correct way to browse the Web, to organize material on the Web, to integrate mail and news, or even to integrate mailfnews with Web browsing. Discovering and quantifying the strengths and weaknesses of the interaction techniques introduced by WebCard are challenges for the future. ACKNOWLEDGMENTS Andrew Birrell implemented Postcard; Rob Shillner implemented a large part of DeckScape.\n",
      "=============================\n",
      "Personal computing in the 21st century\n",
      "Ever since the dawn of the digital computer, invention, innovation, and creativity have been a hallmark of the industry. The mainframe computer seemed for a while to be the real player with experts or at least highly trained professionals operating these large and expensive machines. Most users were allowed to see them through glass windows but \"hands on\" was a rare opportunity. In 1972, the Xerox Palo Alto Research Center (PARC), built a remarkable personal computer named the ALTO. Except for the visionaries at PARC and a few others, most people considered the personal computer a mere curiosity in this early period. Today, the personal computer has become a tool that very few imagined. What might be yet to come.While prognosticating about the future is a risky endeavor at best, perhaps we can obtain a look ahead with a straightforward review of the current status of personal computing. We will look at operating systems, application software and peripherals, however, the real goal of this talk is to see what the user interface, tools and interactions with this future computing environment might be or perhaps even should be. Will we still be using continuing variations of Doug Englebart's mouse in 2020 or might something new and much more advanced emerge? How might users seamlessly deal with terabytes of storage? How might multi-user environments be used and could multi-OS machines be an economic and generally available personal computing environment? Are there user experience issues that are critical in multi-OS environments? How might the user's display be different from today? Will tomorrow's displays be larger, have a significantly higher pixel density, be much more paper-like, etc.? Might electronic printers and their requisite paper output still be with us by 2025, for example? Will home and neighborhood network resources finally be a powerful ally of the computing environment? Many exciting opportunities and questions beg for answers and industry insight.This talk will attempt to peer into the near future to see what we might expect of the personal computing environment based on what we can extrapolate from current experience and technology directions. While the exactitude of such projections may be limited, taken as a whole, there is perhaps much that can be learned from such an exercise. Why do this? Charles Kettering, the great automotive inventor was asked why he spent so much time planning and thinking about the future. He wisely replied, \"Because I am going to spend the rest of my life there.\" Thirty years ago, very few could have imagined all the wonderful things that personal computing has enabled. Perhaps we have just begun our exciting journey.\n",
      "=============================\n",
      "A graphics toolkit based on differential constraints\n",
      "This paper describes Bramble, a toolkit for constructing graphical editing applications. The primary focus of Bramble is improve support for graphical manipulation by employing differential constraint techniques. A constraint engine capable of managing non-linear equations maps interactive controls and constraints to object parameters. This allows objects to provide mathematical outputs that are easily composed, rather than exposing their internal structure or requiring special purpose interaction techniques. The model of interaction used with the differential approach has a continuous notion of time, which provides the continuous motion required for graphical manipulation. Bramble provides a LISP-like extension language and support for other application features such as windows and buttons. The paper concludes with examples of interaction techniques defined in Bramble and applications built with Bramble.\n",
      "=============================\n",
      "A screen-space formulation for 2D and 3D direct manipulation\n",
      "Rotate-Scale-Translate (RST) interactions have become the de facto standard when interacting with two-dimensional (2D) contexts in single-touch and multi-touch environments. Because the use of RST has thus far focused almost entirely on 2D, there are not yet standard techniques for extending these principles into three dimensions. In this paper we describe a screen-space method which fully captures the semantics of the traditional 2D RST multi-touch interaction, but also allows us to extend these same principles into three-dimensional (3D) interaction. Just like RST allows users to directly manipulate 2D contexts with two or more points, our method allows the user to directly manipulate 3D objects with three or more points. We show some novel interactions, which take perspective into account and are thus not available in orthographic environments. Furthermore, we identify key ambiguities and unexpected behaviors that arise when performing direct manipulation in 3D and offer solutions to mitigate the difficulties each presents. Finally, we show how to extend our method to meet application-specific control objectives, as well as show our method working in some example environments.\n",
      "=============================\n",
      "Skyblue: a multi-way local propagation constraint solver for user interface construction\n",
      "Many user interface toolkits use constraint solvers to maintain geometric relationships between graphic objects, or to connect the graphics to the application data structures. One efficient and flexible technique for maintaining constraints is multi-way local propagation, where constraints are represented by sets of method procedures. To satisfy a set of constraints, a local propagation solver executes one method from each constraint.SkyBlue is an incremental constraint solver that uses local propagation to maintain a set of constraints as individual constraints are added and removed. If all of the constraints cannot be satisfied, SkyBlue leaves weaker constraints unsatisfied in order to satisfy stronger constraints (maintaining a constraint hierarchy). SkyBlue is a more general successor to the DeltaBlue algorithm that satisfies cycles of methods by calling external cycle solvers and supports multi-output methods. These features make SkyBlue more useful for constructing user interfaces, since cycles of constraints can occur frequently in user interface applications and multi-output methods are necessary to represent some useful constraints. This paper discusses some of applications that use SkyBlue, presents times for some user interface benchmarks and describes the SkyBlue algorithm in detail.\n",
      "=============================\n",
      "Individual variation in susceptibility to cybersickness\n",
      "We examined background characteristics of virtual reality participants in order to determine correlations to cybersickness. As 3D media and new VR display technologies from companies such as Occulus and Sony become more popular, the incidence of cybersickness is likely to increase. Understanding the impact of individual backgrounds on susceptibility can help shed light on which individuals are more likely to be impacted. Past history of motion sickness and video game play have the best predictive power of cybersickness of the factors studied. A model to estimate the likelihood of cybersickness using background characteristics is posed.\n",
      "=============================\n",
      "Graphical styles for building interfaces by demonstration\n",
      "Conventional interface builders allow the user interface designer to select widgets such as menus, buttons and scroll bars, and lay them out using a mouse. Although these are conceptually simple to use, in practice there are a number of problems. First, a typical widget will have dozens of properties which the designer might change. Insuring that these properties are consistent across multiple widgets in a dialog box and multiple dialog boxes in an application can be very difficult. Second, if the designer wants to change the properties, each widget must be edited individually. Third, getting the widgets laid out appropriately in a dialog box can be tedious. Grids and alignment commands are not sufficient. This paper describes Graphical Tabs and Graphical Styles in the Gild interface builder which solve all of these problems. A “graphical tab” is an absolute position in a window. A “graphical style” incorporates both property and layout information, and can be defined by example, named, applied to other widgets, edited, saved to a file, and read from a file. If a graphical style is edited, then all widgets defined using that style are modified. In addition, because appropriate styles are inferred, they do not have to be explicitly applied.\n",
      "=============================\n",
      "X toolkits: the lessons learned\n",
      "Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission.\n",
      "=============================\n",
      "Multi-user, multi-display interaction with a single-user, single-display geospatial application\n",
      "In this paper, we discuss our adaptation of a single-display, single-user commercial application for use in a multi-device, multi-user environment. We wrap Google Earth, a popular geospatial application, in a manner that allows for synchronized coordinated views among multiple instances running on different machines in the same co-located environment. The environment includes a touch-sensitive tabletop display, three vertical wall displays, and a TabletPC. A set of interaction techniques that allow a group to manage and exploit this collection of devices is presented.\n",
      "=============================\n",
      "Spreadsheet driven web applications\n",
      "Creating and publishing read-write-compute web applications requires programming skills beyond what most end users possess. But many end users know how to make spreadsheets that act as simple information management applications, some even with computation. We present a system for creating basic web applications using such spreadsheets in place of a server and using HTML to describe the client UI. Authors connect the two by placing spreadsheet references inside HTML attributes. Data computation is provided by spreadsheet formulas. The result is a reactive read-write-compute web page without a single line of Javascript code. Nearly all of the fifteen HTML novices we studied were able to connect HTML to spreadsheets using our method with minimal instruction. We draw conclusions from their experience and discuss future extensions to this programming model.\n",
      "=============================\n",
      "Sensing foot gestures from the pocket\n",
      "Visually demanding interfaces on a mobile phone can diminish the user experience by monopolizing the user's attention when they are focusing on another task and impede accessibility for visually impaired users. Because mobile devices are often located in pockets when users are mobile, explicit foot movements can be defined as eyes-and-hands-free input gestures for interacting with the device. In this work, we study the human capability associated with performing foot-based interactions which involve lifting and rotation of the foot when pivoting on the toe and heel. Building upon these results, we then developed a system to learn and recognize foot gestures using a single commodity mobile phone placed in the user's pocket or in a holster on their hip. Our system uses acceleration data recorded by a built-in accelerometer on the mobile device and a machine learning approach to recognizing gestures. Through a lab study, we demonstrate that our system can classify ten different foot gestures at approximately 86% accuracy.\n",
      "=============================\n",
      "A mechanism for supporting client migration in a shared window system\n",
      "Migrating collaborative applications to or near the workstations of active users can offer better performance in many scenarios. We have developed {3 client migration mechanism for centralized shared window systems that does not require changes to existing application and system software. It is based on logging input at the old site and replaying it at the new site. This approach raises several difficult questions: How should the log size be kept low? How should response time be kept low white migration is in progress? How should applications that depend on the rate at which input is received be accommodated? How should the transition from the replay phase to the play phase be detected at the new site? How should the software at the old and new sites be synchronized? We have developed a series of alternative approaches for answering these questions and implemented them in the XTV [1] shared window system. In this paper, we motivate, describe, illustrate and evaluate these approaches, and outline how they are implemented.\n",
      "=============================\n",
      "Mobile multi-display environments\n",
      "Mobile devices are increasingly being fitted with more than one display, presenting a new breed of Mobile Multi-Display Environments (MMDEs). It is however still unclear how the extra display fits within the mobile devices' ecology in terms of visualisation and interaction. My research explores the alignment between multiple displays in a mobile environment and how different alignments affect usability and the choice of a suitable interaction technique. In order to investigate those properties and adapt them to various use cases, I will build a steerable projection system to study different alignments, then analyse visual separation effects in MMDEs and finally explore the possibilities offered when the displays are overlapping.\n",
      "=============================\n",
      "Surface haptic interactions with a TPad tablet\n",
      "A TPad Tablet is a tablet computer with a variable friction touchscreen. It can create the perception of force, shape, and texture on a fingertip, enabling unique and novel haptic interactions on a flat touchscreen surface. We have created an affordable and easy to use variable friction device and have made it available through the open-hardware TPad Tablet Project. We present this device as a potential research platform as well as demonstrate two applications: remote touch communication and rapid haptic sketching.\n",
      "=============================\n",
      "Of Vampire mirrors and privacy lamps: privacy management in multi-user augmented environments\n",
      "We consider the problem of privacy in a 3D multi-user collaborative environment. We assume that information objects are represented by visual icons, and can either be public or private, and that users need effective methods for viewing and manipulating that state. We suggest two methods, which we call vampire mirrorsand privacy lamps, that are unobtrusive, simple, and natural.\n",
      "=============================\n",
      "Hanzi Lamp: an intelligent guide interface for Chinese character learning\n",
      "In recent years, an increasing number of people want to understand Chinese culture and Hanzi (Chinese characters) is a key to that. Learning Chinese characters as a second language can be quite challenging. What confuses learners is not only the meaning of Hanzi, but also the complicated writing rules since they are very different from the alphabetic ways English uses. Although many mobile applications and online learning systems provide Hanzi teaching interfaces, they are restricted to the two-dimensional screens and thus they offer little flexibility for practicing while learning. In this paper, we propose Hanzi Lamp, an intelligent guide interface which allows users to practice writing under real-time and adaptive projected guidance. Information captured by sensors has been utilized to perceive learners' behaviors and make appropriate response. We explore how we can enhance Chinese characters learning by improving the system's understanding of physical learning environment.\n",
      "=============================\n",
      "TeslaTouch: electrovibration for touch surfaces\n",
      "We present a new technology for enhancing touch interfaces with tactile feedback. The proposed technology is based on the electrovibration principle, does not use any moving parts and provides a wide range of tactile feedback sensations to fingers moving across a touch surface. When combined with an interactive display and touch input, it enables the design of a wide variety of interfaces that allow the user to feel virtual elements through touch. We present the principles of operation and an implementation of the technology. We also report the results of three controlled psychophysical experiments and a subjective user evaluation that describe and characterize users' perception of this technology. We conclude with an exploration of the design space of tactile touch screens using two comparable setups, one based on electrovibration and another on mechanical vibrotactile actuation.\n",
      "=============================\n",
      "Paper augmented digital documents\n",
      "Paper Augmented Digital Documents (PADDs) are digital documents that can be manipulated either on a computer screen or on paper. PADDs, and the infrastructure supporting them, can be seen as a bridge between the digital and the paper worlds. As digital documents, PADDs are easy to edit, distribute and archive; as paper documents, PADDs are easy to navigate, annotate and well accepted in social settings. The chimeric nature of PADDs make them well suited for many tasks such as proofreading, editing, and annotation of large format document like blueprints.We are presenting an architecture which supports the seamless manipulation of PADDs using today's technologies and reports on the lessons we learned while implementing the first PADD system.\n",
      "=============================\n",
      "Generating remote control interfaces for complex appliances\n",
      "The personal universal controller (PUC) is an approach for improving the interfaces to complex appliances by introducing an intermediary graphical or speech interface. A PUC engages in two-way communication with everyday appliances, first downloading a specification of the appliance's functions, and then automatically creating an interface for controlling that appliance. The specification of each appliance includes a high-level description of every function, a hierarchical grouping of those functions, and dependency information, which relates the availability of each function to the appliance's state. Dependency information makes it easier for designers to create specifications and helps the automatic interface generators produce a higher quality result. We describe the architecture that supports the PUC, and the interface generators that use our specification language to build high-quality graphical and speech interfaces.\n",
      "=============================\n",
      "A widget framework for augmented interaction in SCAPE\n",
      "We have previously developed a collaborative infrastructure called SCAPE - an acronym for Stereoscopic Collaboration in Augmented and Projective Environments - that integrates the traditionally separate paradigms of virtual and augmented reality. In this paper, we extend SCAPE by formalizing its underlying mathematical framework and detailing three augmented Widgets constructed via this framework: CoCylinder, Magnifier, and CoCube. These devices promote intuitive ways of selecting, examining, and sharing synthetic objects, and retrieving associated documentary text. Finally we present a testbed application to showcase SCAPE's capabilities for interaction in large, augmented virtual environments.\n",
      "=============================\n",
      "Memento: unifying content and context to aid webpage re-visitation\n",
      "While users often revisit pages on the Web, tool support for such re-visitation is still lacking. Current tools (such as browser histories) only provide users with basic information such as the date of the last visit and title of the page visited. In this paper, we describe a system that provides users with descriptive topic-phrases that aid re-finding. Unlike prior work, our system considers both the content of a webpage and the context in which the page was visited. Preliminary evaluation of this system suggests users find this approach of combining content with context useful.\n",
      "=============================\n",
      "Creating graphical interactive application objects by demonstration\n",
      "The Lapidary user interface tool allows all pictorial aspects of programs to be specified graphically. In addition, the behavior of these objects at run-time can be specified using dialogue boxes and by demonstration. In particular, Lapidary allows the designer to draw pictures of application-specific graphical objects which will be created and maintained at run-time by the application. This includes the graphical entities that the end user will manipulate (such as the components of the picture), the feedback that shows which objects are selected (such as small boxes on the sides and corners of an object), and the dynamic feedback objects (such as hair-line boxes to show where an object is being dragged). In addition, Lapidary supports the construction and use of “widgets” (sometimes called interaction techniques or gadgets) such as menus, scroll bars, buttons and icons. Lapidary therefore supports using a pre-defined library of widgets, and defining a new library with a unique “look and feel.” The run-time behavior of all these objects can be specified in a straightforward way using constraints and abstract descriptions of the interactive response to the input devices. Lapidary generalizes from the specific example pictures to allow the graphics and behaviors to be specified by demonstration.\n",
      "=============================\n",
      "DT controls: adding identity to physical interfaces\n",
      "In this paper, we show how traditional physical interface components such as switches, levers, knobs and touch screens can be easily modified to identify who is activating each control. This allows us to change the function per-formed by the control, and the sensory feedback provided by the control itself, dependent upon the user. An auditing function is also available that logs each user's actions. We describe a number of example usage scenarios for our tech-nique, and present two sample implementations.\n",
      "=============================\n",
      "AirPincher: a handheld device for recognizing delicate mid-air hand gestures\n",
      "We propose AirPincher, a handheld device for recognizing delicate mid-air hand gestures. AirPincher is designed to overcome disadvantages of the two kinds of existing hand gesture-aware techniques such as wearable sensor-based and external vision-based. The wearable sensor-based techniques cause cumbersomeness of wearing sensors every time and the external vision-based techniques incur performance dependence on distance between a user and a remote display. AirPincher allows a user to hold the device in one hand and to generate several delicate mid-air finger gestures. The gestures are captured by several sensors proximately embedded into AirPincher. These features help AirPincher avoid the aforementioned disadvantages of the existing techniques. It allows several delicate finger gestures, for example, rubbing a thumb against a middle finger, swiping with a thumb on an index finger, pinching with a thumb and an index finger, etc. Due to the inherent haptic feedback of these gestures, AirPincher eventually supports the eyes-free interaction. To validate AirPincher's feasibility, we implemented two use cases, i.e., controlling a pointing cursor and moving a virtual 3D object on the remote screen.\n",
      "=============================\n",
      "Abracadabra: wireless, high-precision, and unpowered finger input for very small mobile devices\n",
      "We present Abracadabra, a magnetically driven input technique that offers users wireless, unpowered, high fidelity finger input for mobile devices with very small screens. By extending the input area to many times the size of the device's screen, our approach is able to offer a high C-D gain, enabling fine motor control. Additionally, screen occlusion can be reduced by moving interaction off of the display and into unused space around the device. We discuss several example applications as a proof of concept. Finally, results from our user study indicate radial targets as small as 16 degrees can achieve greater than 92% selection accuracy, outperforming comparable radial, touch-based finger input.\n",
      "=============================\n",
      "Considering the direction of cursor movement for efficient traversal of cascading menus\n",
      "Cascading menus are commonly seen in most GUI systems. However, people sometimes choose the wrong items by mistake, or become frustrated when submenus pop up unnecessarily. This paper proposes two methods for improving the usability of cascading menus. The first uses the direction of cursor movement to change the menu behavior: horizontal motion opens/closes submenus, while vertical motion changes the highlight within the current menu. This feature can reduce cursor movement errors. The second causes a submenu to pop up at the position where horizontal motion occurs. This is expected to reduce the length of the movement path for menu traversal. A user study showed that our methods reduce menu selection times, shorten search path lengths, and prevent unexpected submenu appearance and disappearance.\n",
      "=============================\n",
      "Relations, cards, and search templates: user-guided web data integration and layout\n",
      "We present three new interaction techniques for aiding users in collecting and organizing Web content. First, we demonstrate an interface for creating associations between websites, which facilitate the automatic retrieval of related content. Second, we present an authoring interface that allows users to quickly merge content from many different websites into a uniform and personalized representation, which we call a card. Finally, we introduce a novel search paradigm that leverages the relationships in a card to direct search queries to extract relevant content from multiple Web sources and fill a new series of cards instead of just returning a list of webpage URLs. Preliminary feedback from users is positive andvalidates our design.\n",
      "=============================\n",
      "Phidgets: easy development of physical interfaces through physical widgets\n",
      "Physical widgets or phidgets are to physical user interfaces what widgets are to graphical user interfaces. Similar to widgets, phidgets abstract and package input and output devices: they hide implementation and construction details, they expose functionality through a well-defined API, and they have an (optional) on-screen interactive interface for displaying and controlling device state. Unlike widgets, phidgets also require: a connection manager to track how devices appear on-line; a way to link a software phidget with its physical counterpart; and a simulation mode to allow the programmer to develop, debug and test a physical interface even when no physical device is present. Our evaluation shows that everyday programmers using phidgets can rapidly develop physical interfaces.\n",
      "=============================\n",
      "The “growing up” of HyperBraille—an office workspace for blind people\n",
      "Due to of their intuitive usage especially for novice users, graphical user interfaces (GUI) are nowadays a widespread user frontend for almost any kind of application. It is wellknown that the advantages to sighted users hide strong drawbacks for the community of blind people. Their special needs are not very well catered for the common software design. The control over GUI applications with their overlapping windows and buttons are no analog to the way blind people “see” their environment as it is for sighted people. Thus, the competitiveness of these p?ople is drastically reduced. The basic goal of HyperBraille is to enable blind or visually impaired people to participate as fully competitive members in today’s information technology oriented office worlds. We did not aim to create another tool to access graphical user interfaces but rather decided to realize a textscreen-oriented application especially for blind people which integrates tools to retrieve, create and exchange printed as well as electronic documents. Thereby we used the hypertext and formatting features of the Hypertext Markup Language HTML. On the other hand we adapted the GUI concept of the pulldot.vn menus to be customized on a Braille display. As for the sighted user, pull-down menus allow the novice user to immediately operate any application like word-processors or WWW-browsers without knowing the various key bindings. The development of HyperBraille started three years ago with the construction of a World Wide Web client that allowed easy access to all the documents of the Web[l 1]. This article will describe the new features of HyperBraille that are mostly driven by user feedback and by the needs for individual configurations of potential users, Permission to make digital/trard copies of all or part of tbk material for personal or clasaroom use is granted without fee provided that the copies are not made or dkibuted for profit or commercial advantage, the copyright notice, the title of tie publication and its date appear, and notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission and/or fee. UIST ’96 Seattle Washington USA @1996 ACM 0-89791-798-7/96/11 ..$3.50\n",
      "=============================\n",
      "Support for multitasking and background awareness using interactive peripheral displays\n",
      "In this paper, we describe Kimura, an augmented office environment to support common multitasking practices. Previous systems, such as Rooms, limit users by constraining the interaction to the desktop monitor. In Kimura, we leverage interactive projected peripheral displays to support the perusal, manipulation and awareness of background activities. Furthermore, each activity is represented by a montage comprised of images from current and past interaction on the desktop. These montages help remind the user of past actions, and serve as a springboard for ambient context-aware reminders and notifications.\n",
      "=============================\n",
      "Algorithm animation using 3D interactive graphics\n",
      "This report describes a variety of 3D interactive graphics techniques for visualizing programs. The third dimension provides an extra degree of freedom for conveying information, much as color adds to black-and-white images, animation adds to static images, and sound adds to silent animations. The examples in this report illustrate three fundamental uses of 3D: for providing additional information about objects that are intrinsically two-dimensional, for uniting multiple views, and for capturing a history of execution. The application of dynamic three-dimensional graphics to program visualization is largely unexplored. A videotape of these animations is available.\n",
      "=============================\n",
      "Kitty: sketching dynamic and interactive illustrations\n",
      "We present Kitty, a sketch-based tool for authoring dynamic and interactive illustrations. Artists can sketch animated drawings and textures to convey the living phenomena, and specify the functional relationship between its entities to characterize the dynamic behavior of systems and environments. An underlying graph model, customizable through sketching, captures the functional relationships between the visual, spatial, temporal or quantitative parameters of its entities. As the viewer interacts with the resulting dynamic interactive illustration, the parameters of the drawing change accordingly, depicting the dynamics and chain of causal effects within a scene. The generality of this framework makes our tool applicable for a variety of purposes, including technical illustrations, scientific explanation, infographics, medical illustrations, children's e-books, cartoon strips and beyond. A user study demonstrates the ease of usage, variety of applications, artistic expressiveness and creative possibilities of our tool.\n",
      "=============================\n",
      "StickEar: making everyday objects respond to sound\n",
      "This paper presents StickEar, a system consisting of a network of distributed 'Sticker-like' sound-based sensor nodes to propose a means of enabling sound-based interactions on everyday objects. StickEar encapsulates wireless sensor network technology into a form factor that is intuitive to reuse and redeploy. Each StickEar sensor node consists of a miniature sized microphone and speaker to provide sound-based input/output capabilities. We provide a discussion of interaction design space and hardware design space of StickEar that cuts across domains such as remote sound monitoring, remote triggering of sound, autonomous response to sound events, and controlling of digital devices using sound. We implemented three applications to demonstrate the unique interaction capabilities of StickEar.\n",
      "=============================\n",
      "GamiCAD: a gamified tutorial system for first time autocad users\n",
      "We present GamiCAD, a gamified in-product, interactive tutorial system for first time AutoCAD users. We introduce a software event driven finite state machine to model a user's progress through a tutorial, which allows the system to provide real-time feedback and recognize success and failures. GamiCAD provides extensive real-time visual and audio feedback that has not been explored before in the context of software tutorials. We perform an empirical evaluation of GamiCAD, comparing it to an equivalent in-product tutorial system without the gamified components. In an evaluation, users using the gamified system reported higher subjective engagement levels and performed a set of testing tasks faster with a higher completion ratio.\n",
      "=============================\n",
      "Nishanchi: CAD for hand-fabrication\n",
      "We present Nishanchi, a position and orientation aware handheld inkjet printer which can be used to transfer the reference marks from CAD to the workpiece for use in manual fabrication workflows. Nishanchi also has a digitizing tip that can be used to input features about the workpiece to a computer model. By allowing for this two-way exchange of information from CAD to a nonconcormal workpiece, we believe that Nishanchi might help make inclusion of CAD in manual fabrication workflows more seamless.\n",
      "=============================\n",
      "LightRing: always-available 2D input on any surface\n",
      "We present LightRing, a wearable sensor in a ring form factor that senses the 2d location of a fingertip on any surface, independent of orientation or material. The device consists of an infrared proximity sensor for measuring finger flexion and a 1-axis gyroscope for measuring finger rotation. Notably, LightRing tracks subtle fingertip movements from the finger base without requiring instrumentation of other body parts or the environment. This keeps the normal hand function intact and allows for a socially acceptable appearance. We evaluate LightRing in a 2d pointing experiment in two scenarios: on a desk while sitting down, and on the leg while standing. Our results indicate that the device has potential to enable a variety of rich mobile input scenarios.\n",
      "=============================\n",
      "Cirrin: a word-level unistroke keyboard for pen input\n",
      "We present a new system, called Cirrin, for pen input of ASCII characters using word-level unistrokes. Our system addresses the tradeoff between speed and accuracy of penbased text entry by substituting precision on the part of the user for ease of recognitionon the part of the computer. Cirrin supports ease of recognition by the computer combined with natural, script-like input. This paper discusses the design space of word-level, unistroke input, focusing on the choices made in the circular model of Cirrin that is currently in daily use by the first author.\n",
      "=============================\n",
      "Internet scrapbook: automating Web browsing tasks by demonstration\n",
      "This paper describes a programming-by-demonstration system, called Internet Scrapbook, which allows users with little programming skill to automate repetitive browsing tasks. With the system, the user can create a personal page by clipping only the necessary portions from multiple Web pages. Once the personal page is created, the system updates it on behalf of the user by extracting the specified parts from the latest Web pages. The data extraction method in Scrapbook is based on the regularity in modifications of Web pages, i.e. that headings and positions of articles are rarely changed even though the articles themselves are modified. In the experiments to examine the accuracy of the data extraction algorithm, 96 percent of user-specified portions were correctly extracted.\n",
      "=============================\n",
      "WindowScape: a task oriented window manager\n",
      "We propose WindowScape, a window manager that uses a photograph metaphor for lightweight, post hoc task management. This is the first task management windowing model to provide intuitive accessibility while allowing windows to exist simultaneously in multiple tasks. WindowScape exploits users' spatial and visual memories by providing a stable thumbnail layout in which to search for windows. A function is provided to let users search the window space while maintaining a largely consistent screen image to minimize distractions. A novel keyboard interaction technique is also presented.\n",
      "=============================\n",
      "Mime: compact, low power 3D gesture sensing for interaction with head mounted displays\n",
      "We present Mime, a compact, low-power 3D sensor for unencumbered free-form, single-handed gestural interaction with head-mounted displays (HMDs). Mime introduces a real-time signal processing framework that combines a novel three-pixel time-of-flight (TOF) module with a standard RGB camera. The TOF module achieves accurate 3D hand localization and tracking, and it thus enables motion-controlled gestures. The joint processing of 3D information with RGB image data enables finer, shape-based gestural interaction. Our Mime hardware prototype achieves fast and precise 3D gestural control. Compared with state-of-the-art 3D sensors like TOF cameras, the Microsoft Kinect and the Leap Motion Controller, Mime offers several key advantages for mobile applications and HMD use cases: very small size, daylight insensitivity, and low power consumption. Mime is built using standard, low-cost optoelectronic components and promises to be an inexpensive technology that can either be a peripheral component or be embedded within the HMD unit. We demonstrate the utility of the Mime sensor for HMD interaction with a variety of application scenarios, including 3D spatial input using close-range gestures, gaming, on-the-move interaction, and operation in cluttered environments and in broad daylight conditions.\n",
      "=============================\n",
      "Flexkit: a rapid prototyping platform for flexible displays\n",
      "Commercially available development platforms for flexible displays are not designed for rapid prototyping. To create a deformable interface, one that uses a functional flexible display, designers must be familiar with embedded hardware systems and corresponding programming. We introduce Flexkit, a platform that allows designers to rapidly prototype deformable applications. With Flexkit, designers can rapidly prototype using a thin-film electrophoretic display, one that is \"Plug and Play\". To demonstrate Flexkit's ease-of-use, we present its application in PaperTab's design iteration as a case study. We further discuss how dithering can be used to increase the frame rate of electrophoretic displays from 1fps to 5fps.\n",
      "=============================\n",
      "Tools for building asynchronous servers to support speech and audio applications\n",
      "Distributed client/server models are becoming increasingly prevalent in multimedia systems and advanced user interface design. A multimedia application, for example, may play and record audio, use speech recognition input, and use a window system for graphical I/O. The software architecture of such a system can be simplified if the application communicates to multiple servers (e.g., audio servers, recognition servers) that each manage different types of input and output. This paper describes tools for rapidly prototyping distributed asynchronous servers and applications, with an emphasis on supporting highly interactive user interfaces, temporal media, and multi-modal I/O.The Socket Manager handles low-level connection management and device I/O by supporting a callback mechanism for connection initiation, shutdown, and for reading incoming data. The Byte Stream Manager consists of an RPC compiler and run-time library that supports synchronous and asynchronous calls, with both a programmatic interface and a telnet interface that allows the server to act as a command interpreter. This paper details the tools developed for building asynchronous servers, several audio and speech servers built using these tools, and applications that exploit the features provided by the servers.\n",
      "=============================\n",
      "Real-time audio buffering for telephone applications\n",
      "A system that uses an ear proximity sensor to actively manage periods of distraction during telephone conversations is described. We detect when the phone is removed from the ear, record any incoming audio, and play it back when the phone is returned to the ear. By dropping silent intervals and speeding up playback with a pitch-preserving algorithm, we quickly return to real-time without the loss of information. This real-time audio buffering technique also allows us to create a user-activated, lossless instant replay function.\n",
      "=============================\n",
      "iRing: intelligent ring using infrared reflection\n",
      "We present the iRing, an intelligent input ring device developed for measuring finger gestures and external input. iRing recognizes rotation, finger bending, and external force via an infrared (IR) reflection sensor that leverages skin characteristics such as reflectance and softness. Furthermore, iRing allows using a push and stroke input method, which is popular in touch displays. The ring design has potential to be used as a wearable controller because its accessory shape is socially acceptable, easy to install, and safe, and iRing does not require extra devices. We present examples of iRing applications and discuss its validity as an inexpensive wearable interface and as a human sensing device.\n",
      "=============================\n",
      "System lag tests for augmented and virtual environments\n",
      "We describe a simple technique for accurately calibrating the temporal lag in augmented and virtual environments within the Enhanced Virtual Hand Lab (EVHL), a collection of hardware and software to support research on goal-directed human hand motion. Lag is the sum of various delays in the data pipeline associated with sensing, processing, and displaying information from the physical world to produce an augmented or virtual world. Our main calibration technique uses a modified phonograph turntable to provide easily tracked periodic motion, reminiscent of the pendulum-based calibration technique of Liang, Shaw and Green. Measurements show a three-frame (50 ms) lag for the EVHL. A second technique, which uses a specialized analog sensor that is part of the EVHL, provides a “closed loop” calibration capable of sub-frame accuracy. Knowing the lag to sub-frame accuracy enables a predictive tracking scheme to compensate for the end-toend lag in the data pipeline. We describe both techniques and the EVHL environment in which they are used.\n",
      "=============================\n",
      "Simplifying macro definition in programming by demonstration\n",
      "In order to automate repetitive tasks performed in computer applications, users are required to acquire special skills for writing macros or probwams Pro&~amming by demonstration (PBD), a method of converting a user demonstration into an executable code, is one possible solution to this problem, However, many PBD systems require users to spend much time and care in macro definition. This paper describes a PBD system, DemoOffice, which employs two techniques, actzon sllcug and macro auto-d<jinltlon, to simplifi macro definition significantly. The system is able to detect user actions which might be expected to be performed again in the fhture and to automatically convert those actions into a macro, for which no further definition is required,\n",
      "=============================\n",
      "Using the multi-layer model for building interactive graphical applications\n",
      "Most interactive graphical applications that use direct manip- ulation are built with low-level libraries such as Xlib because the graphic and interaction models of higher-level toolkits such as Motif are not extensible. This results in high de- sign, development and maintenance costs and encourages the development of stereotyped applications based on buttons, menus and dialogue boxes instead of direct manipulation of the applications objects. There have been several attempts to provide high level tools for building such applications, including popular toolkits such as Garnet [26], Unidraw [33], Fresco [21, 32] and Open- Inventor [28]. Unfortunately, these tools are not adapted to the development of sophisticated graphical editors because of their lack of extensibility: In this article we argue that these drawbacks come from the fact that high-level toolkits rely on a visualization model to manage interaction. We introduce a model that uses several graphical layers to separate the graphic entities involved in visualization from those involved in feedback and interaction management. We describe the implementation of this Multi- Layer Model and we show how it can take advantage of soft- ware and hardware graphic extensions to provide good per- formance. We also show how it supports multiple input de- vices and simplifies the description of a wide variety of in- teraction styles. Finally, we describe our experience in using this model to implement a set of editors for a professional an- imation system.\n",
      "=============================\n",
      "A rapid prototyping toolkit for touch sensitive objects using active acoustic sensing\n",
      "We present a prototyping toolkit for creating touch sensitive prototypes from everyday objects without needing special skills such as code writing or designing circuits. This toolkit consists of an acoustic based touch sensor module that captures the resonant properties of objects, software modules including one that recognizes how an object is touched by using machine learning, and plugins for visual programming environments such as Scratch and Max/MSP. As a result, our toolkit enables users to easily configure the response of touches using a wide variety of visual or audio responses. We believe that our toolkit expands the creativity of a non-specialist, such as children and media artists.\n",
      "=============================\n",
      "Penumbrae for 3D interactions\n",
      "We propose a new feedback technique for 3D interaction using penumbrae which the objects cast. Rather tlhan generating a real penumbra, which is computationally expensive, a fast, simplified algorithm is employed, which also is better suited for position feedback purposes. User studies show that 1) compared to orthographic shadow projections, 3D spatial recognition and placement tasks are substantially faster with our penumbrae, and 2) the users feel the feedback to be more natural.\n",
      "=============================\n",
      "Situated crowdsourcing using a market model\n",
      "Research is increasingly highlighting the potential for situated crowdsourcing to overcome some crucial limitations of online crowdsourcing. However, it remains unclear whether a situated crowdsourcing market can be sustained, and whether worker supply responds to price-setting in such a market. Our work is the first to systematically investigate workers' behaviour and response to economic incentives in a situated crowdsourcing market. We show that the market-based model is a sustainable approach to recruiting workers and obtaining situated crowdsourcing contributions. We also show that the price mechanism is a very effective tool for adjusting the supply of labour in a situated crowdsourcing market. Our work advances the body of work investigating situated crowdsourcing.\n",
      "=============================\n",
      "Clip, connect, clone: combining application elements to build custom interfaces for information access\n",
      "Many applications provide a form-like interface for requesting information: the user fills in some fields, submits the form, and the application presents corresponding results. Such a procedure becomes burdensome if (1) the user must submit many different requests, for example in pursuing a trial-and-error search, (2) results from one application are to be used as inputs for another, requiring the user to transfer them by hand, or (3) the user wants to compare results, but only the results from one request can be seen at a time. We describe how users can reduce this burden by creating custom interfaces using three mechanisms: clipping of input and result elements from existing applications to form cells on a spreadsheet; connecting these cells using formulas, thus enabling result transfer between applications; and cloning cells so that multiple requests can be handled side by side. We demonstrate a prototype of these mechanisms, initially specialised for handling Web applications, and show how it lets users build new interfaces to suit their individual needs.\n",
      "=============================\n",
      "The missing link: augmenting biology laboratory notebooks\n",
      "Using a participatory design process, we created three prototype augmented laboratory notebooks that provide the missing link between paper, physical artifacts and on-line data. The final a-book combines a graphics tablet and a PDA. The tablet captures writing on the paper notebook and the PDA acts as an \"interaction lens\" or window between physical and electronic documents. Our approach is document-centered, with a software architecture based on layers of physical and electronic information.\n",
      "=============================\n",
      "A new angle on cheap LCDs: making positive use of optical distortion\n",
      "Most LCD screens exhibit color distortions when viewed at oblique angles. Engineers have invested significant time and resources to alleviate this effect. However, the massive manufacturing base, as well as millions of in-the-wild monitors, means this effect will be common for many years to come. We take an opposite stance, embracing these optical peculiarities, and consider how they can be used in productive ways. This paper discusses how a special palette of colors can yield visual elements that are invisible when viewed straight-on, but visible at oblique angles. In essence, this allows conventional, unmodified LCD screens to output two images simultaneously - a feature normally only available in far more complex setups. We enumerate several applications that could take advantage of this ability.\n",
      "=============================\n",
      "The re:search engine: simultaneous support for finding and re-finding\n",
      "Re-finding, a common Web task, is difficult when previously viewed information is modified, moved, or removed. For example, if a person finds a good result using the query \"breast cancer treatments\", she expects to be able to use the same query to locate the same result again. While re-finding could be supported by caching the original list, caching precludes the discovery of new information, such as, in this case, new treatment options. People often use search engines to simultaneously find and re-find information. The Re:Search Engine is designed to support both behaviors in dynamic environments like the Web by preserving only the memorable aspects of a result list. A study of result list memory shows that people forget a lot. The Re:Search Engine takes advantage of these memory lapses to include new results where old results have been forgotten.\n",
      "=============================\n",
      "An event-object recovery model for object-oriented user interfaces\n",
      "An important aspect of interactive systems is the provision of a recovery facility that allows the user to reverse the effects of his interactions with the system. Due to differences between object-oriented and non-objectoriented methodologies, user recovery approaches used for non-object-oriented software are not suitable for object-oriented software. This paper presents an eventobject user recovery model for the construction of recovery facilities in object-oriented user interfaces. Our approach divides traditional history/command lists into per-object lists which fit well with object-oriented structure. Unique features of this framework are the hierarchical structure of the local recovery objects that reflect the application structure, its simple semantics, and its ease of implementation, which greatly reduces the effort required by the interface builder to incorporate it into existing object-oriented user interface structures. We introduce this framework by describing the event-object model, defining the protocol used by the local facilities to perform user recovery, and presenting examples of how the framework is used.\n",
      "=============================\n",
      "Generating user interfaces: principles and use of it style rules\n",
      "As application developers today we all face a problem of great complexity. Because of the diversity of our users, and the variety of their equipment, applications must run in many different configurations. They must support displays of varying size, resolution, and color depth. Different types of input devices are required, from keyboards to touch screens. Applications must run in different countries, being able to reformat messages in varying lengths in each language. Messages should be available in large font sizes for vision impaired users. Interface style should be consistent with other applications running on similar hardware. Style should at the same time conform to guidelines being developed by many organizations for presentation and interaction behaviors.\n",
      "=============================\n",
      "Extending a graphical toolkit for two-handed interaction\n",
      "Multimodal interaction combines input from multiple sensors such as pointing devices or speech recognition systems, in order to achieve more fluid and natural interaction. Two-handed interaction has been used recently to enrich graphical interaction. Building applications that use such combined interaction requires new software techniques and frameworks. Using additional devices means that user interface toolkits must be more flexible with regard to input devices and event types. The possibility of parallel interactions must also be taken into account, with consequences on the structure of toolkits. Finally, frameworks must be provided for the combination of events and status of several devices. This paper reports on the extensions we made to the direct manipulation interface toolkit Whizz in order to experiment two-handed interaction. These extensions range from structural adaptations of the toolkit to new techniques for specifying the time-dependent fusion of events.\n",
      "=============================\n",
      "Enabling social interactions through real-time sketch-based communication\n",
      "We present UbiSketch, a tool for ubiquitous real-time sketch-based communication. We describe the UbiSketch system, which enables people to create doodles, drawings, and notes with digital pens and paper and publish them quickly and easily via their mobile phones to social communication channels, such as Facebook, Twitter, and email. The natural paper-based social interaction enabled by UbiSketch has the potential to enrich current mobile communication practices.\n",
      "=============================\n",
      "Fiberio: a touchscreen that senses fingerprints\n",
      "We present Fiberio, a rear-projected multitouch table that identifies users biometrically based on their fingerprints during each touch interaction. Fiberio accomplishes this using a new type of screen material: a large fiber optic plate. The plate diffuses light on transmission, thereby allowing it to act as projection surface. At the same time, the plate reflects light specularly, which produces the contrast required for fingerprint sensing. In addition to offering all the functionality known from traditional diffused illumination systems, Fiberio is the first interactive tabletop system that authenticates users during touch interaction-unobtrusively and securely using the biometric features of fingerprints, which eliminates the need for users to carry any identification tokens.\n",
      "=============================\n",
      "Hands-on interaction with virtual environments\n",
      "In this paper we describe the evolution of a whole-hand interface to our virtual-environment graphical system. We present a set of abstractions that can be used to implement device-independent interfaces for hand measurement devices. Some of these abstractions correspond to known logical device abstractions, while others take further advantage of the richness of expression in the human hand. We describe these abstractions in the context of their use in our development of virtual environments.\n",
      "=============================\n",
      "Spatial interpretation of domain objects integrated into a freeform electronic whiteboard\n",
      "Our goal is to provide tools to support working meetings on an electronic whiteboard, called Tivoli. This paper describes how we have integrated structured domain objects, which represent the subject matter of meetings, into the freeform whiteboard environment. Domain objects can be tailored to produce meeting tools that are finely tuned to meeting practices. We describe the language for defining domain objects and show examples of meeting tools that have been built with the language. We show that the system can interpret the spatial relationships of domain objects on the whiteboard to encode the meanings of the spatial arrangements, and we describe the computational mechanisms. We discuss some of the design principles for tailoring gestures for domain objects. Finally, we enumerate the techniques we have used to integrate the structured objects into the freeform whiteboard environment.\n",
      "=============================\n",
      "Declarative programming of graphical interfaces by visual examples\n",
      "Graphical user interfaces (GUI) provide intuitive and easy means for users to communicate with computers. However, construction of GUI software requires complex programming that is far from being intuitive. Because of the “semantic gap” between the textual application program and its graphical interface, the programmer himself must conceptually maintain the correspondence between the textual programming and the graphical image of the resulting interface. Instead, we propose a programming environment based on the programming by visual example (PBVE) scheme, which allows the GUI designers to “program” visual interfaces for their applications by “drawing” the example visualization of application data with a direct manipulation interface. Our system, TRIP3, realizes this with (1) the bi-directional translation model between the (abstract) application data and the pictorial data of the GUI, and (2) the ability to generate mapping rules for the translation from example application data and its corresponding example visualization. The latter is made possible by the use of generalization of visual examples, where the system is able to automatically generate generalized mapping rules from a given set of examples.\n",
      "=============================\n",
      "Separating user interface and functionality using a frame based data model\n",
      "The separation between user interface and functionality found in many screen editors is generalized to handle a data model based on frames and binary relations. This paper describes a User Interface Management System (UIMS) based on the data model. The UIMS is capable of maintaining different and simultaneous representations of the same application data objects. The functionality and user interface are implemented on top of a small object oriented programming system. This allows the UIMS to be simple and independent of the graphics software and hardware as well as the data representation used by the application programs.\n",
      "=============================\n",
      "Template-based mapping of application data interactive displays\n",
      "Abstract This paper describes a template-based method for construct-ing interactive displays with the building-blocks (widgets) provided in a user interface toolkit. Templates specify how to break down complex application objects into smaller pieces, specify the graphical components (widgets) to be used for displaying each piece, and specify their layout. Complex interfaces are constructed by recursively applying templates, thus constructing a tree of widgets to display a complex application object. The template-based method is more general than the interactive, WYSIWYG interface builders in that it can specify dynamic displays for applica-tion data that changes at run time. The template-based method also leads to more consistent, extendable and modi-ﬁable interfaces. 1.0 Introduction User interface toolkits such as the X Toolkit [10] and the Macintosh Tool-Box [5] provide abstractions that make the construction of user interfaces signiﬁcantly easier than pro-gramming using graphics primitives. Unfortunately the toolkits do not make the construction of user interfaces easy enough. The tasks of assembling the widgets to construct complex displays, and of tying the widgets to application data structures remains difﬁcult and time consuming.Interactive user interface builder systems such as Prototyper [14] provide interactive what-you-see-is-what-you-get in-terfaces to assemble tool-box widgets into more complex interfaces. These tools are excellent for a restricted class of interfaces, which typically includes only menus and dia-logue boxes. However, these tools do not help with the con-struction of the “main windows” of an application, which display application objects that typically change at run time.This paper describes a template-based method for assem-bling widgets into complex interfaces and tying them to ap-plication objects. Templates specify how to break down complex application objects into smaller pieces, specify the widgets to be used for displaying each piece, and specify their layout. Complex interfaces are constructed by recur-sively applying templates, thus constructing a tree of wid-gets to display a composite application object. The template-based method supports the construction of dynam-ic displays, and also encourages the design of consistent, extendable and modiﬁable interfaces.The paper is organized as follows. Section 2.0 presents an overview of a template-based UIMS named Humanoid, the High-level UIMS for Manufacturing Applications Needing Organized Iterative Development. Section 3.0 compares the template-based method with other methods, sections 4.0 and 5.0 describe the template-based method in detail, and ﬁ-nally section 6.0 discusses our experience with Humanoid.\n",
      "=============================\n",
      "FlexStroke: a jamming brush tip simulating multiple painting tools on digital platform\n",
      "We propose a new system to enable the real painting experience on digital platform and extend it to multi-strokes for different painting needs. In this paper, we describe how the FlexStroke is used as Chinese brush, oil brush and crayon with changes of its jamming tip. This tip has different levels of stiffness based on its jamming structure. Visual simulations on PixelSense jointly enhance the intuitive painting process with highly realistic display results.\n",
      "=============================\n",
      "TopicShop: enhanced support for evaluating and organizing collections of Web sites\n",
      "TopicShop is an interface that helps users evaluate and organize collections of web sites. The main interface components are site profiles, which contain information that helps users select high-quality items, and a work area, which offers thumbnail images, annotation, and lightweight grouping techniques to help users organize selected sites. The two components are linked to allow task integration. Previous work [2] demonstrated that subjects who used TopicShop were able to select significantly more highquality sites, in less time and with less effort. We report here on studies that confirm and extend these results. We also show that TopicShop subjects spent just half the time organizing sites, yet still created more groups and more annotations, and agreed more in how they grouped sites. Finally, TopicShop subjects tightly integrated the tasks of evaluating and organizing sites. INTRODUCTION In previous work [2], we motivated an important task for web users – gathering, evaluating, and organizing information resources for a given topic. Current web tools do not support this task well; specifically, they do not make it easy to evaluate collections of web sites to select the best ones or to organize sites for future reuse and sharing. Users have to browse and view sites one after another until they are satisfied they have a good set, or, more likely, they get tired and give up. Browsing a web site is an expensive operation, both in time and cognitive effort. And bookmarks, the most common form of keeping track of web sites, are a fairly primitive organizational technique. We designed and implemented the TopicShop system to provide comprehensive, integrated support for this task. TopicShop aids users in finding a set of relevant sites, in narrowing down the set into a smaller set of high quality sites, and in organizing sites for future use. TopicShop has evolved through a number of design iterations, driven by extensive user testing. We report here on lessons we learned from a pilot study, how these lessons re-shaped our understanding of the task and led to a significant re-design, and the results of a second, larger user study. RELATED WORK Our research program investigates the major information problems faced by users of the World Wide Web: • finding collections of items relevant to their interests; • identifying high-quality items within a collection; • finding items that contain a certain category of information, e.g., episode guides (for a television show) or song lyrics (for a musician); • organizing personalized subsets of items. We have addressed these problems by developing algorithms, implementing them in web crawling and analysis tools, creating interfaces to support users in exploring, comprehending, and organizing collections of web documents, and performing user studies [2, 3, 4, 15]. The work reported here focuses on understanding the user tasks of evaluating and organizing collections of web sites, as illuminated by the design, evaluation, and re-design of interfaces to support these tasks. Other researchers have investigated these issues. Much recent work has been devoted to algorithms for adding meta-information to collections of web sites to enhance user comprehension, typically by analyzing the structure of links between sites. This approach builds on the intuition that when the author of one site chooses to link to another, this often implies both that the sites have similar content and that the author is endorsing the content of the linked-to site. Pirolli, Pitkow and colleagues [12, 13] experimented with link-based algorithms for clustering and categorizing web pages. Kleinberg’s HITS algorithm [8] defines authoritative and hub pages within a hypertext collection. Authorities and hubs are mutually dependent: a good authority is a page that is linked to by many hubs, and a good hub is one that links to many authorities. After evaluating items and selecting the interesting ones, users must organize the items for future use. Card, Robertson, and Mackinlay [5] introduced the concept of information workspaces to refer to environments in which information items can be stored and manipulated. A departure point for most such systems is the file manager popularized by the Apple Macintosh and then in Microsoft Windows. Such systems typically include a list view, which shows various properties of items, and an icon view, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. UIST ’00. San Diego, CA USA  2000 ACM 1-58113-212-3/00/11... $5.00 CHI Letters vol 2, 2 201 which lets users organize icons representing the items in a 2D space. Mander, Salomon, and Wong [10] enhanced the basic metaphor with the addition of “piles”. Users could create and manipulate piles of items. Interesting interaction techniques for displaying, browsing, and searching piles were designed and tested. Bookmarks are the most popular way to create personal information workspaces of web resources. Bookmarks consist of lists of URLs; typically the title of the web page is used as the label for the URL. Users may organize their bookmarks into a hierarchical category structure. Abrams, Baecker, and Chignell [1] carried out an extensive study of how several hundred web users used bookmarks. They observed a number of strategies for organizing bookmarks, including a flat ordered list, a single level of folders, and hierarchical folders. They also made four design recommendations to help users manage their bookmarks more effectively. First, bookmarks must be easy to organize, e.g., via automatic sorting techniques. Second, visualization techniques are necessary to provide comprehensive overviews of large sets of bookmarks. Third, rich representations of sites are required; many users noted that site titles are not accurate descriptors of site content. Finally, tools for managing bookmarks must be well integrated with web browsers. Many researchers have created experimental information workspace interfaces, often designed expressly for web documents. Card, Robertson, and York [5] describe the WebBook, which uses a book metaphor to group a collection of related web pages for viewing and interaction, and the WebForager, an interface that lets users view and manage multiple WebBooks. Mackinlay, Rao, and Card [9] developed a novel user interface for accessing articles from a citation database. The central UI object is a “Butterfly”, which represents an article, its references, and its citers. The interface makes it easy for users to browse among related articles, group articles, and generate queries to retrieve articles that stand in a particular relationship to the current article. The Data Mountain of Robertson et al [14] represents documents as thumbnail images in a 3D virtual space. Users can move and group the images freely, with various interesting visual and audio cues used to help users arrange the documents. In a study comparing the use of Data Mountain to Internet Explorer Favorites, Data Mountain users retrieved items more quickly, with fewer incorrect or failed retrievals. Our research shares goals with much of the previous work. We focus on designing interfaces that make automatically extracted information about web sites readily accessible to users. We show that this increases users’ ability to select high-quality sites. Through ongoing user studies and redesign, we developed easy-to-use annotation and grouping techniques that let users organize items better and more quickly. Finally, we learned how users interleave work on various tasks and re-designed our interface to support such task interleaving. TOPICSHOPEXPLORER, VERSION 1 The TopicShop Explorer is implemented in C++ and runs on Microsoft Windows platforms. Version 1 was based directly on the Macintosh file manager / MS Windows Explorer metaphor (see [3] for detail of TopicShop Version 1 and the pilot study). Accordingly, users could view collections in either a details (Figure 1) or icons (Figure 2) view. The details view showed site profile information (see below) to help users evaluate sites, and the icons view let users organize sites spatially. Figure 1: TopicShop Explorer (version 1), details view. Each web site is represented by a small thumbnail image, the site title, and profile data including the links to/from other sites in the collection, and the number of pages, images, and audio files on the site. Users can sort by a property by clicking on the appropriate column. Figure 2: TopicShop Explorer (version 1), icons view. Each site is represented by a large thumbnail image and the site title. Users can organize sites by arranging them spatially, a technique especially useful in the early stages of exploration. The collections of sites and site profile data used in TopicShop are obtained by running a webcrawler/analyzer. The crawler takes a user-specified set of seed sites as input, and follows links from the seeds to construct a graph of the seed sites, pages contained on these sites, and, optionally, sites determined to be related based on their textual and hyperlink connections to the seeds. CHI Letters vol 2, 2 202 Site profiles are built by fetching a large number of pages from each site. Profiles contain content data, including the page title, an estimate of the page count, and a roster of audio files, movie files, and images; they also record links between sites in the collection. In addition, a thumbnail image of each site’s root page is constructed. The first goal of TopicShop is to help users evaluate and identify high quality sites. We sought to achieve this goal by providing site profile data and int\n",
      "=============================\n",
      "Transformations on a dialog tree: rule-based maping of content to style\n",
      "Over the past ten years a broad consensus has developed that decomposing applications into separate computational and interface modules is desirable. The motivations for modularity in user interfaces are similar to those elsewhere in computer science, and center around the benefits of hiding internal implementations from other modules in a system. Modularity allows reuse of each component with compatible versions of the other components, and allows separate development processes for each component. In user interfaces, modularity allows the reuse of applications with different interfaces, perhaps for different user classes, window systems, or devices. Modularity also allows application and interface experts, including human factors engineers and graphic designers, to cooperate while minimizing the coordination required among them.\n",
      "=============================\n",
      "Moveable interactive projected displays using projector based tracking\n",
      "Video projectors have typically been used to display images on surfaces whose geometric relationship to the projector remains constant, such as walls or pre-calibrated surfaces. In this paper, we present a technique for projecting content onto moveable surfaces that adapts to the motion and location of the surface to simulate an active display. This is accomplished using a projector based location tracking techinque. We use light sensors embedded into the moveable surface and project low-perceptibility Gray-coded patterns to first discover the sensor locations, and then incrementally track them at interactive rates. We describe how to reduce the perceptibility of tracking patterns, achieve interactive tracking rates, use motion modeling to improve tracking performance, and respond to sensor occlusions. A group of tracked sensors can define quadrangles for simulating moveable displays while single sensors can be used as control inputs. By unifying the tracking and display technology into a single mechanism, we can substantially reduce the cost and complexity of implementing applications that combine motion tracking and projected imagery.\n",
      "=============================\n",
      "A fast, interactive 3D paper-flier metaphor for digital bulletin boards\n",
      "We describe a novel interface for presenting interactive content on public digital bulletin boards. Inspired by paper fliers on physical bulletin boards, posted content is displayed using 3D virtual fliers attached to a virtual corkboard by virtual pushpins. Fliers appear in different orientations, creating an attractive, informal look, and have autonomous behaviors like fluttering in the wind. Passersby can rotate, move and fold fliers; they can also interact with fliers’ live content. Flier content is streamed from a server and represented by the system on large screen displays using a real-time cloth simulation algorithm. We describe our prototype, and offer the results of an initial evaluative user study.\n",
      "=============================\n",
      "TimeSlider: an interface to specify time point\n",
      "This paper introduces TimeSlider, a user interface technique that allows the user to specify time points. TimeSlider is a kind of slider whose time scale is nonlinear and which moves as a user operation. The nonlinearly enables it to display a long time range in a small space, and the movement as a user operation helps the user to specify time points quickly. An example application, in which TimeSlider enabled the user to restore past WWW pages, demonstrated the effectiveness of our technique.\n",
      "=============================\n",
      "AIDE: a step toward metric-based interface development tools\n",
      "Automating any part of the user interface design and evaluation process can help reduce development costs. This paper presents a metric-based tool called AIDE (semi-Automated Interface Designer and Evaluator) which assists designers in creating and evaluating layouts for a given set of interface controls. AIDE is an initial attempt to demonstrate the potential of incorporating metrics into user interface development tools, Analyzing the interfaces produced using AIDE provides encouraging feedback about the potential of this technique.\n",
      "=============================\n",
      "Local tools: an alternative to tool palettes\n",
      "We describe local tools, a general interaction technique that replaces traditional tool palettes. A collection of tools sit on the worksurface along with the data. Each tool can be picked up (where it replaces the cursor), used, and then put down anywhere on the worksurface. There k a toolbox for organizing the tools. These local tools were implemented in Pad++ as part of KidPad, an application for children. INTRODUCTION Sitting at a desk, many people find that they work with several tools simultaneously. Perhaps they have a pencil, a red pen, a stapler, or some paper clips all on their worksurface together. This is a very natural way to work, yet most computer interfaces don’t support this style of interaction. Rather, traditional computer tool palettes allow only a single tool to be active at a time. Inl the real world, this would be equivalent to being forced to put away every tool before another could be used. In the Pad++ research group at the University of New Mexico, we have been experimenting with an alternative style of interaction we call local tools. Motivated by the above scenario, we allow the user to place several tools directly on the work surface, and then pick them up and use them, When a tool is held, it becomes the cursor and can be used just like a regular tool. But these tools can easily be put down anywhere on the work surface, and other tools can be picked up in their place. One of the driving reasons behind this development of local tools has been our work in developing a zooming application for children using Pad++. We tested Ithe standard tool palette interface in Pad++ with computer-novice fourth graders. What we found was that children had a Permission to make digitsi/hard copies of all or part of this material for personal or classroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication and its date appear, and notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, to repubtish, to post on servers or to redkibute to fists, requires specific permission and/or fee. UIST ’96 Seattle Washington USA @ 1996 ACM ()_8979 l_798_7/96/l 1 ..$3.50 Figure 1: Screen snapshot from KidPad showing use of local tools. difficult time with the interface. Despite the fact that the interface used direct manipulation, itwasn’t direct enough. Tool palettes are hard to use. The user must first find the tool palette (sometimes having to access it from a menu). Then she must press the correct button resulting in the cursor changing which is quite confusing to the uninitiated computer user. Finally, she must select various attributes of the tool, such as color and width. A local tool can embody all of these characteristics at once. Local tools remain on the surface and can be picked up and used with all of their attributes, potentially reducing cognitive load. We are building these local tools within Pad++, a zoomable environment [1] [2] [3]. Pad++ provides a huge worksurface where graphical objects can be put on the surface at any position and at any size. The user can navigate through this planar space by panning and zooming. While local tools could be implemented in a non-zooming environment, there are some drawbacks because the local tools sit on the worksurface taking up valuable screen real estate. In a zoomable environment, however, the tools can easily be made as large or small as desired. In addition, they can be pushed off the screen and then easily brought back with a special toolbox (see below). Developments similar to local tools have recently been iescribed by several research groups. Bier et. al. [4] and >ur own research group [1] have recently discussed the\n",
      "=============================\n",
      "The 1line keyboard: a QWERTY layout in a single line\n",
      "Current soft QWERTY keyboards often consume a large portion of the screen space on portable touchscreens. This space consumption can diminish the overall user experi-ence on these devices. In this paper, we present the 1Line keyboard, a soft QWERTY keyboard that is 140 pixels tall (in landscape mode) and 40% of the height of the native iPad QWERTY keyboard. Our keyboard condenses the three rows of keys in the normal QWERTY layout into a single line with eight keys. The sizing of the eight keys is based on users' mental layout of a QWERTY keyboard on an iPad. The system disambiguates the word the user types based on the sequence of keys pressed. The user can use flick gestures to perform backspace and enter, and tap on the bezel below the keyboard to input a space. Through an evaluation, we show that participants are able to quickly learn how to use the 1Line keyboard and type at a rate of over 30 WPM after just five 20-minute typing sessions. Using a keystroke level model, we predict the peak expert text entry rate with the 1Line keyboard to be 66--68 WPM.\n",
      "=============================\n",
      "Extended multitouch: recovering touch posture and differentiating users using a depth camera\n",
      "Multitouch surfaces are becoming prevalent, but most existing technologies are only capable of detecting the user's actual points of contact on the surface and not the identity, posture, and handedness of the user. In this paper, we define the concept of extended multitouch interaction as a richer input modality that includes all of this information. We further present a practical solution to achieve this on tabletop displays based on mounting a single commodity depth camera above a horizontal surface. This will enable us to not only detect when the surface is being touched, but also recover the user's exact finger and hand posture, as well as distinguish between different users and their handedness. We validate our approach using two user studies, and deploy the technique in a scratchpad tool and in a pen + touch sketch tool.\n",
      "=============================\n",
      "Toucheo: multitouch and stereo combined in a seamless workspace\n",
      "We propose a new system that efficiently combines direct multitouch interaction with co-located 3D stereoscopic visualization. In our approach, users benefit from well-known 2D metaphors and widgets displayed on a monoscopic touchscreen, while visualizing occlusion-free 3D objects floating above the surface at an optically correct distance. Technically, a horizontal semi-transparent mirror is used to reflect 3D images produced by a stereoscopic screen, while the user's hand as well as a multitouch screen located below this mirror remain visible. By registering the 3D virtual space and the physical space, we produce a rich and unified workspace where users benefit simultaneously from the advantages of both direct and indirect interaction, and from 2D and 3D visualizations. A pilot usability study shows that this combination of technology provides a good user experience.\n",
      "=============================\n",
      "Sensing and visualizing spatial relations of mobile devices\n",
      "Location information can be used to enhance interaction with mobile devices. While many location systems require instrumentation of the environment, we present a system that allows devices to measure their spatial relations in a true peer-to-peer fashion. The system is based on custom sensor hardware implemented as USB dongle, and computes spatial relations in real-time. In extension of this system we propose a set of spatialized widgets for incorporation of spatial relations in the user interface. The use of these widgets is illustrated in a number of applications, showing how spatial relations can be employed to support and streamline interaction with mobile devices.\n",
      "=============================\n",
      "Ramonamap—an example of graphical groupware\n",
      "Ramonamap is an iterative map for database and communication services within our workgroup. Resources are represented as icons on the map, which preserves their actual (or implied) physical location and capitalizes on a user's understanding of maps. The map is interactive, giving the user control over the level of detail visible, allowing more information and services to appear than could be placed on a static map. The interactivity also allows users to change the map and add icon annotations. Since the map is continuously derived from an on-line database, changes and annotations are immediately shared by all users. As the database contains a wealth of information about the group, it also serves as a source for static maps for other purposes.\n",
      "=============================\n",
      "A direct texture placement and editing interface\n",
      "The creation of most models used in computer animation and computer games requires the assignment of texture coordinates, texture painting, and texture editing. We present a novel approach for texture placement and editing based on direct manipulation of textures on the surface. Compared to conventional tools for surface texturing, our system combines UV-coordinate specification and texture editing into one seamless process, reducing the need for careful initial design of parameterization and providing a natural interface for working with textures directly on 3D surfaces.A combination of efficient techniques for interactive constrained parameterization and advanced input devices makes it possible to realize a set of natural interaction paradigms. The texture is regarded as a piece of stretchable material, which the user can position and deform on the surface, selecting arbitrary sets of constraints and mapping texture points to the surface; in addition, the multi-touch input makes it possible to specify natural handles for texture manipulation using point constraints associated with different fingers. Pressure can be used as a direct interface for texture combination operations. The 3D position of the object and its texture can be manipulated simultaneously using two-hand input.\n",
      "=============================\n",
      "Sawtooth planar waves for haptic feedback\n",
      "Current touchscreen technology does not provide adequate haptic feedback to the user. Mostly haptic feedback solutions for touchscreens involve either a) deforming the surface layers screen itself or b) placing actuators under the screen to vibrate it. This means that we have only limited control over where on the screen the feedback feels like it is coming from, and that we are limited to feedback that feels like movement up and down, orthogonal to the screen. In this work I demonstrate a novel technique for haptic feedback: sawtooth planar waves. In a series of paper Canny & Reznick showed that sawtooth planar waves could be used for object manipulation. Here that technique is applied to haptic feedback. By varying the input waves, from 1 one to 4 actuators, it is possible to provide feelings of motion in any planar direction to a finger at one point on the screen while providing a different sensation, or none at all, to fingers placed at several other points on the screen.\n",
      "=============================\n",
      "A programming model for active documents\n",
      "Traditionally, designers organize software system as active end-points (e.g. applications) linked by passive infrastructures (e.g. networks). Increasingly, however, networks and infrastructures are becoming active components that contribute directly to application behavior. Amongst the various problems that this presents is the question of how such active infrastructures should be programmed. We have been developing an active document management system called Placeless Documents. Its programming model is organized in terms of properties that actively contribute to the functionality and behavior of the documents to which they are attached. This paper discusses active properties and their use as a programming model for active infrastructures. We have found that active properties enable the creation of persistent, autonomous active entities in document systems, independent of specific repositories and applications, but present challenges for managing problems of composition.\n",
      "=============================\n",
      "Orthogonal extensions to the WWW user interface using client-side technologies\n",
      "1 The World-Wide Web as a Universal User Interface Our work is motivated by three trends. First, the ubiquitous migration of services to the World Wide Web is due in part to its simple, consistent, and now universal user interface: navigation by following links and filling out HTML forms are interactions familiar to even novice Internet users. Second, client-side extension technologies such as Java and JavaScript allow sites to extend and “personalize” the behaviors and interfaces of their services, with portable user-interface elements that integrate transparently into the browser’s existing interface. Finally, there has been a recent surge of interest in proxymediated access to the Web, in which proxy agents in the network infrastructure provide caching [ 11, anonymize user requests [2], or accelerate Web access via datatype-specific lossy compression [3,4,5]. Recent results show that these services can be built scalably and cost-effectively, and can shield the user from the limitations of their Internet connections or client platforms. Not surprisingly, the services have become increasingly powerful and therefore parameterizable and customizable by each user, resulting in increased attention on the design and implementation of the user interface by which the service can be controlled [S]\n",
      "=============================\n",
      "Critical zones in desert fog: aids to multiscale navigation\n",
      "In this paper, we introduce the problem of “desert fog,” a condition wherein a view of an information world contains no information on which to base navigational decisions. We present a set of view-based navigational aids that allow navigators to find their way through desert fog in multiscale electronic worlds. Prototypes of these aids have been implemented in the Landmarking and ZTracker systems. We introduce the concept of critical zone analysis, a method of grouping objects according to their visibility in views of the information world rather than their spatial layout. This concept was derived from a formal analysis of desert fog using view-navigation theory. Our analysis informally extends view-navigation theory to accommodate spatial multiscale worlds and is detailed in the paper.\n",
      "=============================\n",
      "Haptic pen: a tactile feedback stylus for touch screens\n",
      "In this paper we present a system for providing tactile feedback for stylus-based touch-screen displays. The Haptic Pen is a simple low-cost device that provides individualized tactile feedback for multiple simultaneous users and can operate on large touch screens as well as ordinary surfaces. A pressure-sensitive stylus is combined with a small solenoid to generate a wide range of tactile sensations. The physical sensations generated by the Haptic pen can be used to enhance our existing interaction with graphical user interfaces as well as to help make modern computing systems more accessible to those with visual or motor impairments.\n",
      "=============================\n",
      "ToolStone: effective use of the physical manipulation vocabularies of input devices\n",
      "The ToolStone is a cordless, multiple degree-of-freedom (MDOF) input device that senses physical manipulation of itself, such as rotating, flipping, or tilting. As an input device for the non-dominant hand when a bimanual interface is used, the ToolStone provides several interaction techniques including a toolpalette selector, and MDOF interactors such as zooming, 3D rotation, and virtual camera control. In this paper, we discuss the design principles of input devices that effectively use a human’s physical manipulation skills, and describe the system architecture and applications of the ToolStone input device.\n",
      "=============================\n",
      "The CSLU toolkit: rapid prototyping of spoken language systems\n",
      "Research and development of spoken language systems is currently limited to relatively few academic and industrial laboratories. This is because building such systems requires multidisciplinary expertise, sophisticated development tools, specialized language resources, substantial computer resources and advanced technologiessuch as speech recognitionand textto-speech synthesis. At the Center for Spoken Language Understanding (CSLU), our mission is to make spoken language systems commonplace. To do so requires that the technology become less exclusive, more affordable and more accessible. An important step towards satisfying this goal is to place the development of spoken language systems in the hands of real domain experts rather than limit it to technical specialists. To address this problem, we have developed the CSLUToolkit, an integrated software environment for research and development of telephone-based spoken language systems (Sutton et al., 1996; Schalkwyk, et al., 1997). It is designed to support a wide range of research and development activities, including data capture and analysis, corpus development, multilingual recognition and understanding, dialogue design, speech synthesis, speaker recognition and language recognition, and systemsevaluationamongothers. Inaddition, theToolkitprovides an excellent environment for learning about spoken language technology, providingopportunitiesfor hands-on leaming, exploration and experimentation. It has been used as a basis for several short courses in which students have produced a wide range of interesting spoken language applicaPermission to nlnke digitnlhrd copies ofnll or parl ofthis mnterinl for personnl or clnssroom use is granted without fee provided that IIE copies nre not made or distributed for profit or commercial ndwmtage. Ihe copyright notice, the title of the publication and its date appear, and notice is given thnt copyright is by permission ofthe ACM, Inc. To copy olherwise, to republish. Lo post oo servers or to redistribute IO lists, requires specific permission nndlor fee UIST 97 Banfl Alberta, Canada Copyright 1997 ACM 0-89791-SSl-9!97/10..$3.50 tions, such as voice mail, airlinereservation and browsing the worldwide web by voice (Colton et al., 1996, Sutton et al., 1997). AkeymoduleoftheToolkitisagraphicalapplication-creation environment called the CSLU Rapid Prototyper (CSLUrp). This integrates state-of-the-art speaker independent and vocabulary independent technology into an easy-to-use graphical interface. It enables spoken language applications to be developed and tested, quickly and easily. Figure 1 shows a prototype application being developed using CSLUrp. The current version of CSLUrp allows for the rapid development of structured dialogues. It is designed to require minimal technical expertise on the author’s part. It provides an intuitive window-like setting, in which applications are built by placing objects onto a canvas (e.g., a telephoneanswering object, a speech recognition object, etc.) and connecting them with simple clicks of the mouse. Specifying words or phrases to be recognized by the system is a matter of simply typing them in. Similarly, specifying what the system will speak is a matter of typing or recording it. Once an application is complete, it can be run at the press of a button and interacted with either over the telephone or in desktop setting via microphone and speaker. The capability to alternate between designing and testing an application allows for incremental development and iterative refinement of systems. CSLUrp provides non-expert and even novice users with the ability to create spoken language systems for themselves. As they become more experienced and familiar with the basic capabilities, they can move beyond the scope of CSLUrp and begin to learn about and take advantage of other modules of the CSLU Toolkit.\n",
      "=============================\n",
      "PointRight: experience with flexible input redirection in interactive workspaces\n",
      "We describe the design of and experience with PointRight, a peer-to-peer pointer and keyboard redirection system that operates in multi-machine, multi-user environments. PointRight employs a geometric model for redirecting input across screens driven by multiple independent machines and operating systems. It was created for interactive workspaces that include large, shared displays and individual laptops, but is a general tool that supports many different configurations and modes of use. Although previous systems have provided for re-routing pointer and keyboard control, in this paper we present a more general and flexible system, along with an analysis of the types of re-binding that must be handled by any pointer redirection system This paper describes the system, the ways in which it has been used, and the lessons that have been learned from its use over the last two years.\n",
      "=============================\n",
      "A shared command line in a virtual space: the working man's MOO\n",
      "The WorkingMan’s MOO extends a text-based virtual environment through a small command server which sits on the user’s workstation. The extended virtual environment can offer the power of a command line, but embedded within a virtual community, enabling the creation of new interface metaphors that connect the virtual/MOO space with the desktop space.\n",
      "=============================\n",
      "Creating interactive techniques by symbolically solving geometric constraints\n",
      "Creating Interactive Techniques by Symbolically Solving Geometric Constraints Dan R. Olsen Jr. Kirk Allan Computer Science Department Brigham Young University Provo, UT 84602 olsen.chi@xerox.com or olsen@bunsen.byu.edu The Geometric Interactive Technique Solver (GITS) is described. New interactive techniques are created by drawing them and then placing constraints on the drawing’s geometry. The semantic interface is defined by parameters of the techniques which are also related to the geometry by constraints. A designer can define interactive methods for a technique, for which GITS will generate symbolic constraint solutions. Code is generated from the constraint solutions to provide an implementation of the interactive technique. The constraint solving and code generation algorithms are discussed.\n",
      "=============================\n",
      "Role-based interfaces for collaborative software development\n",
      "Real-time collaboration between multiple simultaneous contributors to a shared document is full of both opportunities and pitfalls, as evidenced by decades of research and industry work in computer-supported cooperative work. In the domain of software engineering, collaboration is still generally achieved either via shared use of a single computer (e.g. pair programming) or with version control (and manual pushing and pulling of changes). By examining and designing for the different roles collaborating programmers play when working synchronously together, we can build real-time collaborative programming systems that make their collaboration more effective. And beyond simple shared editing, we can provide asymmetric, role-specific interfaces on their shared task. Collabode is a web-based IDE for collaborative programming with simultaneous editors that, along with several novel models for closely-collaborative software development, explores the potential of real-time cooperative programming.\n",
      "=============================\n",
      "Linking and messaging from real paper in the Paper PDA\n",
      "It is well known that paper is a very fluid, natural, and easy to use medium for manipulating some kinds of information. It is familiar, portable, flexible, inexpensive, and offers good readability properties. Paper also has well known limitations when compared with electronic media. Work in hybrid paper electronic interfaces seeks to bring electronic capabilities to real paper in order to obtain the best properties of each. This paper describes a hybrid paper electronic system — the Paper PDA — which is designed to allow electronic capabilities to be employed within a conventional paper notebook, calendar, or organizer. The Paper PDA is based on a simple observation: a paper notebook can be synchronized with a body of electronic information much like an electronic PDA can be synchronized with information hosted on a personal computer. This can be accomplished by scanning, recognizing and processing its contents, then printing a new copy. This paper introduces the Paper PDA concept and considers interaction techniques and applications designed to work within the Paper PDA. The StickerLink technique supports on-paper hyperlinking using removable paper stickers. Two applications are also considered which look at aspects of electronic communications via the Paper PDA.\n",
      "=============================\n",
      "A remote control interface for large displays\n",
      "We describe a new widget and interaction technique, known as a \"Frisbee,\" for interacting with areas of a large display that are difficult or impossible to access directly. A frisbee is simply a portal to another part of the display. It consists of a local \"telescope\" and a remote \"target\". The remote data surrounded by the target is drawn in the telescope and interactions performed within it are applied on the remote data. In this paper we define the behavior of frisbees, show unique affordances of the widget, and discuss design characteristics. We have implemented a test application and report on an experiment that shows the benefit of using the frisbee on a large display. Our results suggest that the frisbee is preferred over walking back and forth to the local and remote spaces at a distance of 4.5 feet.\n",
      "=============================\n",
      "Bringing the field into the lab: supporting capture and replay of contextual data for the design of context-aware applications\n",
      "When designing context-aware applications, it is difficult to for designers in the studio or lab to envision the contextual conditions that will be encountered at runtime. Designers need a tool that can create/re-create naturalistic contextual states and transitions, so that they can evaluate an application under expected contexts. We have designed and developed RePlay: a system for capturing and playing back sensor traces representing scenarios of use. RePlay contributes to research on ubicomp design tools by embodying a structured approach to the capture and playback of contextual data. In particular, RePlay supports: capturing naturalistic data through Capture Probes, encapsulating scenarios of use through Episodes, and supporting exploratory manipulation of scenarios through Transforms. Our experiences using RePlay in internal design projects illustrate its potential benefits for ubicomp design.\n",
      "=============================\n",
      "Cosaliency: where people look when comparing images\n",
      "Image triage is a common task in digital photography. Determining which photos are worth processing for sharing with friends and family and which should be deleted to make room for new ones can be a challenge, especially on a device with a small screen like a mobile phone or camera. In this work we explore the importance of local structure changes?e.g. human pose, appearance changes, object orientation, etc.?to the photographic triage task. We perform a user study in which subjects are asked to mark regions of image pairs most useful in making triage decisions. From this data, we train a model for image saliency in the context of other images that we call cosaliency. This allows us to create collection-aware crops that can augment the information provided by existing thumbnailing techniques for the image triage task.\n",
      "=============================\n",
      "Injured person information management during second triage\n",
      "In a large-scale disaster in which many persons are injured at the same time, triage has been introduced. Triage is a method that temporarily delays the treatment of people with mild to moderate injuries and symptoms and gives priority to those in a critical condition. In the process of multiple triage, more specific information is needed in the second triage compared to the first to accurately prioritize the persons injuries and state. To solve this problem we proposed and constructed a touch-based interface for managing information inserted during second triage. A touch-based tablet interface is introduced to specify wound areas and gestures for wound types. The information is shared wirelessly with all emergency personnel, giving medics shared, data-centric, visibility of overall triage status for the first time. The evaluation experiment shows that this proposed system enables to reduce input errors, speed up injured person care, and facilitate information sharing between medics efficiently. As a result, we believe that many more injured persons can and will be saved.\n",
      "=============================\n",
      "A presentation manager based on application semantics\n",
      "We describe a system for associating the user interface entities of an application with their underlying semantic objects. The associations are classified by arranging the user interface entities in a type lattice in an object-oriented fashion. The interactive behavior of the application is described by defining application operations in terms of methods on the types in the type lattice. This scheme replaces the usual “active region” interaction model, and allows application interfaces to be specified directly in terms of the objects of the application itself. We discuss the benefits of this system and some of the difficulties we encountered.\n",
      "=============================\n",
      "HoloWall: designing a finger, hand, body, and object sensitive wall\n",
      "This TechNote reports on our initial results of realizing a computer augmented wall called the HoloWall. Using an infrared camera located behind the wall, this system allows a user to interact with this computerized wall using ngers, hands, their body, or even a physical object such as a document folder.\n",
      "=============================\n",
      "Leveraging physical human actions in large interaction spaces\n",
      "Large interaction spaces such as wall-size displays allow users to interact not only with their hands, like traditional desktop environment, but also with their whole body by, e.g. walking or moving their head orientation. While this is particularly suitable for tasks where users need to navigate large amounts of data and manipulate them at the same time, we still lack a deep understanding of the advantages of large displays for such tasks. My dissertation begins with a set of studies to understand the benefits and drawbacks of a high-resolution wall-size display vs. a desktop environments. The results show strong benefits of the former due to the flexibility of \"physical navigation\" involving the whole body when compared with mouse input. From whole-body interaction to human-to-human interaction, my current work seeks to leverage natural human actions to collaborative contexts and to design interaction techniques that detects gestural interactions between users to support collaborative data exchange.\n",
      "=============================\n",
      "ARmonica: a collaborative sonic environment\n",
      "ARmonica is a 3D audiovisual augmented reality environment in which players can position and edit virtual bars that play sounds when struck by virtual balls launched under the influence of physics. Players experience ARmonica through head-tracked head-worn displays and tracked hand-held ultramobile personal computers, and interact through tracked Wii remotes and touch-screen taps. The goal is for players to collaborate in the creation and editing of an evolving sonic environment. Research challenges include supporting walk-up usability without sacrificing deeper functionality.\n",
      "=============================\n",
      "WristFlex: low-power gesture input with wrist-worn pressure sensors\n",
      "In this paper we present WristFlex, an always-available on-body gestural interface. Using an array of force sensitive resistors (FSRs) worn around the wrist, the interface can distinguish subtle finger pinch gestures with high accuracy (>80 %) and speed. The system is trained to classify gestures from subtle tendon movements on the wrist. We demonstrate that WristFlex is a complete system that works wirelessly in real-time. The system is simple and light-weight in terms of power consumption and computational overhead. WristFlex's sensor power consumption is 60.7 uW, allowing the prototype to potentially last more then a week on a small lithium polymer battery. Also, WristFlex is small and non-obtrusive, and can be integrated into a wristwatch or a bracelet. We perform user studies to evaluate the accuracy, speed, and repeatability. We demonstrate that the number of gestures can be extended with orientation data from an accelerometer. We conclude by showing example applications.\n",
      "=============================\n",
      "Embedding interface sketches in code\n",
      "This paper presents a user interface (UI) design tool, GUIIO, which uses ASCII text as its medium for rendering interface components. Like other UI design tools, GUIIO allows individuals to create and manipulate UI components as first-class objects. However, GUIIO has the advantage that its UI designs can be embedded directly within the program code itself. We implemented GUIIO as an extension to an existing development environment. As a result, developers can fluidly transition from editing code to editing the UI mock-up, with the text editor automatically switching its mode from code editing to UI editing as a function of the location of the cursor. By rendering UIs as ASCII art, GUIIO fills an important gap in the design, implementation, and revision of UIs by providing a highly portable and immediately accessible visual representation of the UI that embeds with the code itself.\n",
      "=============================\n",
      "Boom chameleon: simultaneous capture of 3D viewpoint, voice and gesture annotations on a spatially-aware display\n",
      "We introduce the Boom Chameleon, a novel input/output device consisting of a flat-panel display mounted on a tracked mechanical boom. The display acts as a physical window into 3D virtual environments, through which a one-to-one mapping between real and virtual space is preserved. The Boom Chameleon is further augmented with a touch-screen and a microphone/speaker combination. We present a 3D annotation application that exploits this unique configuration in order to simultaneously capture viewpoint, voice and gesture information. Design issues are discussed and results of an informal user study on the device and annotation software are presented. The results show that the Boom Chameleon annotation facilities have the potential to be an effective, easy to learn and operate 3D design review system.\n",
      "=============================\n",
      "Execution control for crowdsourcing\n",
      "Crowdsourcing marketplaces enable a wide range of applications, but constructing any new application is challenging - usually requiring a complex, self-managing workflow in order to guarantee quality results. We report on the CLOWDER project, which uses machine learning to continually refine models of worker performance and task difficulty. We present decision-theoretic optimization techniques that can select the best parameters for a range of workflows. Initial experiments show our optimized workflows are significantly more economical than with manually set parameters.\n",
      "=============================\n",
      "VisionWand: interaction techniques for large displays using a passive wand tracked in 3D\n",
      "A passive wand tracked in 3D using computer vision techniques is explored as a new input mechanism for interacting with large displays. We demonstrate a variety of interaction techniques that exploit the affordances of the wand, resulting in an effective interface for large scale interaction. The lack of any buttons or other electronics on the wand presents a challenge that we address by developing a set of postures and gestures to track state and enable command input. We also describe the use of multiple wands, and posit designs for more complex wands in the future.\n",
      "=============================\n",
      "Chisel: a system for creating highly interactive screen layouts\n",
      "The UofA* User Interface Management System (UIMS) generates graphical user interfaces based on a high-level description of semantic commands supported by the application. A main part of the UIMS, called Chisel, generates the presentation component of interfaces. Chisel selects interaction techniques, determines their attributes, and places them on the screen of the display device. While doing so it is capable of considering device properties, end user's preferences, and interface designer's guidelines. The aim of this paper is to discuss in detail the design and implementation of Chisel.\n",
      "=============================\n",
      "DocWizards: a system for authoring follow-me documentation wizards\n",
      "Traditional documentation for computer-based procedures is difficult to use: readers have trouble navigating long complex instructions, have trouble mapping from the text to display widgets, and waste time performing repetitive procedures. We propose a new class of improved documentation that we call follow-me documentation wizards. Follow-me documentation wizards step a user through a script representation of a procedure by highlighting portions of the text, as well application UI elements. This paper presents algorithms for automatically capturing follow-me documentation wizards by demonstration, through observing experts performing the procedure. We also present our DocWizards implementation on the Eclipse platform. We evaluate our system with an initial user study that showing that most users have a marked preference for this form of guidance over traditional documentation.\n",
      "=============================\n",
      "Cliplets: juxtaposing still and dynamic imagery\n",
      "We explore creating \"\"cliplets\"\", a form of visual media that juxtaposes still image and video segments, both spatially and temporally, to expressively abstract a moment. Much as in \"\"cinemagraphs\"\", the tension between static and dynamic elements in a cliplet reinforces both aspects, strongly focusing the viewer's attention. Creating this type of imagery is challenging without professional tools and training. We develop a set of idioms, essentially spatiotemporal mappings, that characterize cliplet elements, and use these idioms in an interactive system to quickly compose a cliplet from ordinary handheld video. One difficulty is to avoid artifacts in the cliplet composition without resorting to extensive manual input. We address this with automatic alignment, looping optimization and feathering, simultaneous matting and compositing, and Laplacian blending. A key user-interface challenge is to provide affordances to define the parameters of the mappings from input time to output time while maintaining a focus on the cliplet being created. We demonstrate the creation of a variety of cliplet types. We also report on informal feedback as well as a more structured survey of users.\n",
      "=============================\n",
      "MUST-D: multi-user see through display\n",
      "In this paper we present MUST-D, a multi-user see-through display that allows users to inspect objects behind a glass panel while projecting view-dependent information on the glass to the user. MUST-D uses liquid crystal panels to implement a multi-view see-through display space in front of physical objects.\n",
      "=============================\n",
      "Gaze-enhanced scrolling techniques\n",
      "Scrolling is an essential part of our everyday computing experience. Contemporary scrolling techniques rely on the explicit initiation of scrolling by the user. The act of scrolling is tightly coupled with the user?s ability to absorb information via the visual channel. The use of eye gaze information is therefore a natural choice for enhancing scrolling techniques. We present several gaze-enhanced scrolling techniques for manual and automatic scrolling which use gaze information as a primary input or as an augmented input. We also introduce the use off-screen gaze-actuated buttons for document navigation and control.\n",
      "=============================\n",
      "FlexSense: a transparent self-sensing deformable surface\n",
      "We present FlexSense, a new thin-film, transparent sensing surface based on printed piezoelectric sensors, which can reconstruct complex deformations without the need for any external sensing, such as cameras. FlexSense provides a fully self-contained setup which improves mobility and is not affected from occlusions. Using only a sparse set of sensors, printed on the periphery of the surface substrate, we devise two new algorithms to fully reconstruct the complex deformations of the sheet, using only these sparse sensor measurements. An evaluation shows that both proposed algorithms are capable of reconstructing complex deformations accurately. We demonstrate how FlexSense can be used for a variety of 2.5D interactions, including as a transparent cover for tablets where bending can be performed alongside touch to enable magic lens style effects, layered input, and mode switching, as well as the ability to use our device as a high degree-of-freedom input controller for gaming and beyond.\n",
      "=============================\n",
      "XJp system: an internationalized language interface for the X Window system\n",
      "This paper discusses the internationalization of the X Window System developed by the MIT X consortium. The main purpose is to enable X Window System Release 4 (X11R4) and earlier versions to support Asian languages, primarily Japanese, Chinese, and Korean. Unlike English and other European-based languages, Asian languages involve idiographic character manipulation. X Window System XIlR4 and earlier versions can output such ideograms when the corresponding fonts are provided, but they have no corresponding input feature. Asian language input thus involves more than one keystroke to input a single ideogram, e.g., Japanese-language input uses romaji-kana-kanji conversion. This paper proposes an ideogram input architecture on the X Window System and discusses the interfaces between conversion systems Window oriented and X application programs. Like the X System, our input-conversion feature is to a distributed network environment.\n",
      "=============================\n",
      "In search for ideal operating system for user interfacing\n",
      "INTRODUCTION interfacing concerns: As the computing environment’s lowest common denominator, the operating system must always play a game of catchup to provide system-wide support for the changing demands and opportunities of its user environment. Never has this been more true than now. Dramatic changes in the economics of computing now make workstation power accessible to tens of millions of ordinary users. Gone are the days of single-purpose computing (e.g., computer as spreadsheet, computer as word processor). The increasing diversity and sophistication of applications software now make interoperability an imperative. l real-time performance. In the stand-alone, single application environment of a personal computer, users have come to expect immediate feedback. How can we preserve this responsiveness in a computing environment that also supports multi-tasking, networking, and distributed processing? Can the operating system help to promote more realistic models of computer response?\n",
      "=============================\n",
      "Advanced interaction with mobile projection interfaces\n",
      "Through the increasing miniaturization of projection units the integration of such units in everyday-life objects is now possible. Even though these so called pico-projectors are already getting integrated into mobile devices like phones or digital cameras, comparably little research has been conducted to empower these devices to their full capabilities. I outline my previous and current work towards an interface design and a privacy framework that will facilitate mobile projection devices to be part in people's everyday-life. In particular my work is divided into two directions, on the one hand the development of a single-user scenario interface and on the other hand a framework to cope with privacy issues. This will allow the deeper exploitation of the capabilities of mobile projection units for a variety of everyday tasks.\n",
      "=============================\n",
      "Conté: multimodal input inspired by an artist's crayon\n",
      "Conté is a small input device inspired by the way artists manipulate a real Conté crayon. By changing which corner, edge, end, or side is contacting the display, the operator can switch interaction modes using a single hand. Conté's rectangular prism shape enables both precise pen-like input and tangible handle interaction. Conté also has a natural compatibility with multi-touch input: it can be tucked in the palm to interleave same-hand touch input, or used to expand the vocabulary of bimanual touch. Inspired by informal interviews with artists, we catalogue Conté's characteristics, and use these to outline a design space. We describe a prototype device using common materials and simple electronics. With this device, we demonstrate interaction techniques in a test-bed drawing application. Finally, we discuss alternate hardware designs and future human factors research to study this new class of input.\n",
      "=============================\n",
      "AIRREAL: tactile interactive experiences in free air\n",
      "AIREAL is a novel haptic technology that delivers effective and expressive tactile sensations in free air, without requiring the user to wear a physical device. Combined with interactive computers graphics, AIREAL enables users to feel virtual 3D objects, experience free air textures and receive haptic feedback on gestures performed in free space. AIREAL relies on air vortex generation directed by an actuated flexible nozzle to provide effective tactile feedback with a 75 degrees field of view, and within an 8.5cm resolution at 1 meter. AIREAL is a scalable, inexpensive and practical free air haptic technology that can be used in a broad range of applications, including gaming, mobile applications, and gesture interaction among many others. This paper reports the details of the AIREAL design and control, experimental evaluations of the device's performance, as well as an exploration of the application space of free air haptic displays. Although we used vortices, we believe that the results reported are generalizable and will inform the design of haptic displays based on alternative principles of free air tactile actuation.\n",
      "=============================\n",
      "Nested user interface components\n",
      "Nested User Interface Components combine the concepts of Zooming User Interfaces (ZUIs) with recursive nesting of active graphical user interface widgets. The resulting system of recursively nesting interface components has a number of desirable properties. The level of detail of the view of any widget component and its children, as well as the responsiveness of that component to the user's actions, can be tuned to the current visible size of that component on the screen.We distinguish between the interaction style of a component, and the semantic result that it produces. Only the latter is used to determine the geographic parameters for that component. In this way, very large and layered control problems can be presented to the user as a cohesive and readily navigable visual surface. It becomes straightforward to layout interaction semantics that are best handled by recursion, such as filters composed of nested expressions.\n",
      "=============================\n",
      "PicoPet: \"Real World\" digital pet on a handheld projector\n",
      "We created PicoPet, a digital pet game based on mobile handheld projectors. The player can project the pet into physical environments, and the pet behaves and evolves differently according to the physical surroundings. PicoPet creates a new form of gaming experience that is directly blended into the physical world, thus could become incorporated into the player's daily life as well as reflecting their lifestyle. Multiple pets projected by multiple players can also interact with each other, potentially triggering social interactions between players. In this paper, we present the design and implementation of PicoPet, as well as directions for future explorations.\n",
      "=============================\n",
      "Fit your hand: personalized user interface considering physical attributes of mobile device users\n",
      "We present a mobile user interface which dynamically reformulates the layout based on the touch input pattern of users. By analyzing the touch input, it infers users' physical characteristics such as handedness, finger length, or usage habits, thereby calculates the optimal touch area for the user. The user interface is gradually adapted to each user by automatically rearranging graphic objects such as application icons to the most easy-to-touch positions. To compute the optimal touch area, we designed software architecture and implemented an Android application which analyzes touch input and determines the touch frequency in specific screen areas, the handedness and hand size of users. As proof of concept, this research prototype shows acceptable performance and accuracy. To decide which items should be placed in the optimal touch area, we plan to integrate our machine learning algorithm which prioritizes applications according to the context of users into the proposed system.\n",
      "=============================\n",
      "Sketching with projective 2D strokes\n",
      "Freehand sketching has long had appeal as an artistic medium for conceptual design because of its immediacy in capturing and communicating design intent and visual experience. We present a sketching paradigm that supports the early stages of design by preserving the fluidity of traditional freehand drawings. In addition, it attempts to fill the gap between 2D drawing programs, which have fixed views, and 3D modeling programs that allow arbitrary views. We implement our application as a two-dimensional drawing program that utilizes a projective representation of points — i.e. points that lie on the surface of a unit sphere centered at the viewpoint. This representation facilitates the production of novel re-projections generated from an initial perspective sketch and gives the user the impression of being immersed in the drawing or space. We describe a method for aligning a sketch drawn outside the system using its vanishing points, allowing the integration of computer sketching and freehand sketching on paper in an iterative manner. The user interface provides a virtual camera, projective grids to guide in the construction of proportionate scenes, and the ability to underlay sketches with other drawings or photographic panoramas.\n",
      "=============================\n",
      "E-conic: a perspective-aware interface for multi-display environments\n",
      "Multi-display environments compose displays that can be at different locations from and different angles to the user; as a result, it can become very difficult to manage windows, read text, and manipulate objects. We investigate the idea of perspective as a way to solve these problems in multi-display environments. We first identify basic display and control factors that are affected by perspective, such as visibility, fracture, and sharing. We then present the design and implementation of E-conic, a multi-display multi-user environment that uses location data about displays and users to dynamically correct perspective. We carried out a controlled experiment to test the benefits of perspective correction in basic interaction tasks like targeting, steering, aligning, pattern-matching and reading. Our results show that perspective correction significantly and substantially improves user performance in all these tasks.\n",
      "=============================\n",
      "Designing for effective end-user interaction with machine learning\n",
      "End-user interactive machine learning is a promising tool for enhancing human capabilities with large data. Recent work has shown that we can create end-user interactive machine learning systems for specific applications. However, we still lack a generalized understanding of how to design effective end-user interaction with interactive machine learning systems. My dissertation work aims to advance our understanding of this question by investigating new techniques that move beyond naïve or ad-hoc approaches and balance the needs of both end-users and machine learning algorithms. Although these explorations are grounded in specific applications, we endeavored to design strategies independent of application or domain specific features. As a result, our findings can inform future end-user interaction with machine learning systems.\n",
      "=============================\n",
      "MobileSurface: interaction in the air for mobile computing\n",
      "We describe a virtual interactive surface technology based on a projector-camera system connected to a mobile device. This system, named mobile surface, can project images on any free surfaces and enable interaction in the air within the projection area. The projector used in the system scans a laser beam very quickly across the projection area to produce a stable image at 60 fps. The camera-projector synchronization is applied to obtain the image of the appointed scanning line. So our system can project what is perceived as a stable image onto the display surface, while simulta neously working as a structured light 3D scanning system.\n",
      "=============================\n",
      "NailSense: fingertip force as a new input modality\n",
      "In this paper, we propose a new interaction technique, called NailSense, which allows users to control a mobile device by hovering and slightly bending/extending fingers behind the device. NailSense provides basic interactions equivalent to that of touchscreen interactions; 2-D locations and binary states (i.e., touch or released) are tracked and used for input, but without any need of touching on the screen. The proposed technique tracks the user's fingertip in real-time and triggers event on color change in the fingernail area. It works with conventional smartphone cameras, which means no additional hardware is needed for its utilization. This novel technique allows users to use mobile devices without occlusion which was a crucial problem in touchscreens, also promising extended interaction space in the air, on desktop, or in everywhere. This new interaction technique is tested with example applications: a drawing app and a web browser.\n",
      "=============================\n",
      "Enjoying virtual handcrafting with ToolDevice\n",
      "ToolDevice is a set of devices developed to help users in spatial work such as layout design and three-dimensional (3D) modeling. It consists of three components: TweezersDevice, Knife/HammerDevice, and BrushDevice, which use hand tool metaphors to help users recognize each device's unique functions. We have developed a mixed reality (MR) 3D modeling system that imitates real-life woodworking using the TweezersDevice and the Knife/HammerDevice. In the system, users can pick up and move virtual objects with the TweezersDevice. Users can also cut and join virtual objects using the Knife/HammerDevice. By repeating these operations, users can build virtual wood models.\n",
      "=============================\n",
      "SHARK2: a large vocabulary shorthand writing system for pen-based computers\n",
      "Zhai and Kristensson (2003) presented a method of speed-writing for pen-based computing which utilizes gesturing on a stylus keyboard for familiar words and tapping for others. In SHARK<sup>2</sup>:, we eliminated the necessity to alternate between the two modes of writing, allowing any word in a large vocabulary (e.g. 10,000-20,000 words) to be entered as a shorthand gesture. This new paradigm supports a gradual and seamless transition from visually guided tracing to recall-based gesturing. Based on the use characteristics and human performance observations, we designed and implemented the architecture, algorithms and interfaces of a high-capacity multi-channel pen-gesture recognition system. The system's key components and performance are also reported.\n",
      "=============================\n",
      "DTLens: multi-user tabletop spatial data exploration\n",
      "Supporting groups of individuals exploring large maps and design diagrams on interactive tabletops is still an open research problem. Today's geospatial, mechanical engineering and CAD design applications are mostly single-user, keyboard and mouse-based desktop applications. In this paper, we present the design of and experience with DTLens, a new zoom-in-context, multi-user, two-handed, multi-lens interaction technique that enables group exploration of spatial data with multiple individual lenses on the same direct-touch interactive tabletop. DTLens provides a set of consistent interactions on lens operations, thus minimizes tool switching by users during spatial data exploration.\n",
      "=============================\n",
      "PadPrints: graphical multiscale Web histories\n",
      "We have implemented a browser companion called PadPrints that dynamically builds a graphical history-map of visited web pages. PadPrints relies on Pad++, a zooming user interface (ZUI) development substrate, to display the history-map. PadPrints functions in conjunction with a traditional web browser but without requiring any browser modifications.\n",
      "=============================\n",
      "FingerPad: private and subtle interaction using fingertips\n",
      "We present FingerPad, a nail-mounted device that turns the tip of the index finger into a touchpad, allowing private and subtle interaction while on the move. FingerPad enables touch input using magnetic tracking, by adding a Hall sensor grid on the index fingernail, and a magnet on the thumbnail. Since it permits input through the pinch gesture, FingerPad is suitable for private use because the movements of the fingers in a pinch are subtle and are naturally hidden by the hand. Functionally, FingerPad resembles a touchpad, and also allows for eyes-free use. Additionally, since the necessary devices are attached to the nails, FingerPad preserves natural haptic feedback without affecting the native function of the fingertips. Through user study, we analyze the three design factors, namely posture, commitment method and target size, to assess the design of the FingerPad. Though the results show some trade-off among the factors, generally participants achieve 93% accuracy for very small targets (1.2mm-width) in the seated condition, and 92% accuracy for 2.5mm-width targets in the walking condition.\n",
      "=============================\n",
      "The reading assistant: eye gaze triggered auditory prompting for reading remediation\n",
      "We have developed a system for remedial reading instruction that uses visually controlled auditory prompting to help the user with recognition and pronunciation of words. Our underlying hypothesis is that the relatively unobtrusive assistance rendered by such a system will be more effective than previous computer aided approaches. We present a description of the design and implementation of our system and discuss a controlled study that we undertook to evaluate the usability of the Reading Assistant.\n",
      "=============================\n",
      "Popup vernier: a tool for sub-pixel-pitch dragging with smooth mode transition\n",
      "Dragging is one of the most useful and popular techniques in direct manipulation graphical user interfaces. However, dragging has inherent restrictions caused by pixel resolution of a display. Although in some situations the restriction could be negligible, certain kinds of applications, e.g., real world applications where the range of adjustable parameters vastly exceed the screen resolution, require sub-pixel-pitch dragging. We propose a sub-pixel-pitch dragging tool, popup vernier, plus a methodology to transfer smoothly into ‘vernier mode’ during dragging. A popup vernier consists of locally zoomed grids and vernier scales displayed around them. Verniers provide intuitive manipulation and feedback of fine grain dragging, in that pixel-pitch movements of the grids represent sub-pixel-pitch movements of a dragged object, and the vernier scales show the object’s position at a sub-pixel accuracy. The effectiveness of our technique is verified with a proposed evaluation measure that captures the smoothness of transition from standard mode to vernier mode, based on the Fitts’ law.\n",
      "=============================\n",
      "ViewPointer: lightweight calibration-free eye tracking for ubiquitous handsfree deixis\n",
      "We introduce ViewPointer, a wearable eye contact sensor that detects deixis towards ubiquitous computers embedded in real world objects. ViewPointer consists of a small wearable camera no more obtrusive than a common Bluetooth headset. ViewPointer allows any real-world object to be augmented with eye contact sensing capabilities, simply by embedding a small infrared (IR) tag. The headset camera detects when a user is looking at an infrared tag by determining whether the reflection of the tag on the cornea of the user's eye appears sufficiently central to the pupil. ViewPointer not only allows any object to become an eye contact sensing appliance, it also allows identification of users and transmission of data to the user through the object. We present a novel encoding scheme used to uniquely identify ViewPointer tags, as well as a method for transmitting URLs over tags. We present a number of scenarios of application as well as an analysis of design principles. We conclude eye contact sensing input is best utilized to provide context to action.\n",
      "=============================\n",
      "Tailor: creating custom user interfaces based on gesture\n",
      "Randy Pausch & Ronald D, Williams University of Virginia Thornton Hall Charlottesville, VA 22903 (pausch@Virginia.edu) 804-982-2211 Physical controls for most devices are either “one size fits all” or require custom hardware for each user. Cost often prohibits custom design, and each user must adapt to the standard device interface, typically with a loss of precision and efficiency. When user abilities vary widely, such as in the disabled community, devices often become unusable. Our goal is to create a system that will track user gestures and interpret them as control signals for devices. Existing gesture recognition research converts continuous body motion into discrete symbols. Our approach is to map continuous motions into a set of analog device control signals. Our system will allow us to quickly tailor a device interface to each user’s best physical range of motion. Our first application domain is a speech synthesizer for disabled users. We expect two major areas of applicability for non-disabled users: in telemanipulator interfaces, and as a design tool for creating biomechanically efficient interfaces. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. Introduction The Augmentative Communications Group at the University of Virginia consists of researchers from the Computer Science and Electrical Engineering departments, and from the Medical and Education schools. Our current effort is to create a speech synthesizer for individuals with cerebral palsy, a disability affecting approximately 700,000 Americans [Il. A significant portion of the cerebral palsy population is communicative but non-verbal although the desire to communicate is present, speech is prohibited by damage to the part of the brain that controls the vocal tract. Most of these individuals do not have enough coordination for handwriting or typing. Al though primitive electronic communication aids exist, most are variations on picture boards, where the user points or looks at a twodimensional array of pictures to convey a thought such as “hungry” or “tired.” The Augmentative Communications Group has developed a speech synthesizer based on two-dimensional analog input. The creation of speech involves the coordination of a large number of muscles in the vocal tract. The synthesizer approximates this by receiving the position of the base and tip of the tongue as input signals and then synthesizes the sound produced by that position of the tongue. We have implemented a prototype @ 1990 ACM 089791410-4/90/0010/0123 $1.50 123 which synthesizes monotone speech from two analogue signals. The original research strategy was to attempt to design and build custom input devices for each user of the system. Biomedical engineers constructed various onedimensional potentiometers to be used in pairs to provide the analog inputs needed to drive the synthesizer. Building custom hardware interfaces is expensive, and cerebral palsy victims often have reduced strength, making control of any physical device cumbersome. As users fatigue, their efficiency with a particular device decreases and several different devices may be needed to accommodate various stages of fatigue. Our new approach is to create an individual gesture interface for each user. Our software maps body motions, reported by magnetic trackers, into continuous control signals for the speech synthesizer. The only physical effort by the user is to move a part of his body. This software tailoring allows us to create interfaces based on each user’s individual abilities, and makes it possible for those interfaces to adapt as the user fatigues. The idea of user tailoring, or customization, has long been understood as a crucial element in the design of traditional computer interfaces. Text editors allow users to rebind keyboards so that commonly used operations are easier to reach [ZI. Mice often allow for alterations in the ratio of device motion to cursor motion, to accommodate variations in user coordination. Some hardware interfaces allow minor customization! such as tilt-lock steering wheels in automobiles. Customization is necessary when there is a high degree of user idiosyncrasy, relative to the dexterity required for the task. While the need for individual tailoring is most apparent in the disabled community, able-bodied users exhibit high idiosyncrasy with respect to tasks where extreme dexterity is required, such as telemanipulation in microsurgery. We expect our techniques to be useful for able-bodied users when they must use interfaces that require high dexterity to complete tasks. Existing gesture research is dominated by a desire to understand or interpret gestures, and is commonly referred to as gesture recognition. Our approach is to map continuous data from one or more sensors to a set of continuous device control signals, rather than transforming gestures into symbols. Our primary goal is to create custom gesture mappings for device control. Our secondary goal is to make our mappings dynamically adjust for fatigue and changes in user ability. A Joystick-Driven Speech Synthesizer Because our synthesizer is such an unusual device, we first describe how it is able to synthesize speech from two analog inputs. Our articulator driven speech synthesizer produces sounds using the positions and motions of implied articulators in a simulated vocal tract. This form of speech synthesis has been discussed previously in the literature [3-51. The problem addressed here differs from previous work because we limit the number of articulator control parameters to those that can be provided by a human user in real-time. Articulator driven. synthesis is unnecessary and constraining in the text-to-speech environment, but this approach is directly analogous to the mechanisms of speech production used for normal human conversational speech. A brief review of human physical speech production will be helpful in understanding the articulator driven synthesis approach. The physical process of speech production can be divided into three parts, First, air is forced through the vocal cords to produce either a voiced or unvoiced glottal excitation. Next, air flow is modified by a series of structures that constitute the vocal tract. Finally, the modified flow is radiated through the lips and nostrils. [61. The articulators used to produce speech are shown in Figure 1.\n",
      "=============================\n",
      "Multimodal system processing in mobile environments\n",
      "One major goal of multimodal system design is to support more robust performance than can be achieved with a unimodal recognition technology, such as a spoken language system. In recent years, the multimodal literatures on speech and pen input and speech and lip movements have begun developing relevant performance criteria and demonstrating a reliability advantage for multimodal architectures. In the present studies, over 2,600 utterances processed by a multimodal pen/voice system were collected during both mobile and stationary use. A new data collection infrastructure was developed, including instrumentation worn by the user while roaming, a researcher field station, and a multimodal data logger and analysis tool tailored for mobile research. Although speech recognition as a stand-alone failed more often during mobile system use, the results confirmed that a more stable multimodal architecture decreased this error rate by 1935%. Furthermore, these findings were replicated across different types of microphone technology. In large part this performance gain was due to significant levels of mutual disambiguation in the multimodal architecture, with higher levels occurring in the noisy mobile environment. Implications of these findings are discussed for expanding computing to support more challenging usage contexts in a robust manner.\n",
      "=============================\n",
      "Specifying composite illustrations with communicative goals\n",
      "IBIS (Intent-Based Illustration System) generates illustrations automatically, guided by communicative goals. Communicative goals specify that particular properties of objects, such as their color, size, or location are to be conveyed in the illustration. IBIS is intended to be part of an interactive multimedia explanation generation system. It has access to a knowledge base that contains a collection of objects, including information about their geometric properties, material, and location. As the goals are interpreted by a rule-based control component, the system generates a precise definition of the final illustration. If IBIS determines that a set of goals cannot be satisfied in a single picture, then it attempts to create a composite illustration that has multiple viewports. For example, a composite illustration may contain a nested inset illustration showing an object in greater detail than is possible in the parent picture. Each component illustration is defined by its placement, size, viewing specification, lighting specification, and list of objects to be displayed and their graphical style.\n",
      "=============================\n",
      "Recognizing currency bills using a mobile phone: an assistive aid for the visually impaired\n",
      "Despite the rapidly increasing use of credit cards and other electronic forms of payment, cash is still widely used for everyday transactions due to its convenience, perceived security and anonymity. However, the visually impaired might have a hard time telling each paper bill apart, since, for example, all dollar bills have the exact same size and, in general, currency bills around the world are not distinguishable by any tactile markings. We propose the use of a broadly available tool, the camera of a smart-phone, and an adaptation of the SIFT algorithm to recognize partial and even distorted images of paper bills. Our algorithm improves memory efficiency and the speed of SIFT key-point classification by using a k-means clustering approach. Our results show that our system can be used in real-world scenarios to recognize unknown bills with a high accuracy.\n",
      "=============================\n",
      "Homework: putting interaction into the infrastructure\n",
      "This paper presents a user driven redesign of the domestic network infrastructure that draws upon a series of ethnographic studies of home networks. We present an infrastructure based around a purpose built access point that has modified the handling of protocols and services to reflect the interactive needs of the home. The developed infrastructure offers a novel measurement framework that allows a broad range of infrastructure information to be easily captured and made available to interactive applications. This is complemented by a diverse set of novel interactive control mechanisms and interfaces for the underlying infrastructure. We also briefly reflect on the technical and user issues arising from deployments.\n",
      "=============================\n",
      "Focus plus context screens: combining display technology with visualization techniques\n",
      "Computer users working with large visual documents, such as large layouts, blueprints, or maps perform tasks that require them to simultaneously access overview information while working on details. To avoid the need for zooming, users currently have to choose between using a sufficiently large screen or applying appropriate visualization techniques. Currently available hi-res \"wall-size\" screens, however, are cost-intensive, space-intensive, or both. Visualization techniques allow the user to more efficiently use the given screen space, but in exchange they either require the user to switch between multiple views or they introduce distortion.In this paper, we present a novel approach to simultaneously display focus and context information. Focus plus context screens consist of a hi-res display and a larger low-res display. Image content is displayed such that the scaling of the display content is preserved, while its resolution may vary according to which display region it is displayed in. Focus plus context screens are applicable to practically all tasks that currently use overviews or fisheye views, but unlike these visualization techniques, focus plus context screens provide a single, non-distorted view. We present a prototype that seamlessly integrates an LCD with a projection screen and demonstrate four applications that we have adapted so far.\n",
      "=============================\n",
      "Jogging over a distance between Europe and Australia\n",
      "Exertion activities, such as jogging, require users to invest intense physical effort and are associated with physical and social health benefits. Despite the benefits, our understanding of exertion activities is limited, especially when it comes to social experiences. In order to begin understanding how to design for technologically augmented social exertion experiences, we present \"Jogging over a Distance\", a system in which spatialized audio based on heart rate allowed runners as far apart as Europe and Australia to run together. Our analysis revealed how certain aspects of the design facilitated a social experience, and consequently we describe a framework for designing augmented exertion activities. We make recommendations as to how designers could use this framework to aid the development of future social systems that aim to utilize the benefits of exertion.\n",
      "=============================\n",
      "Predictive interaction using the delphian desktop\n",
      "This paper details the design and evaluation of the Delphian Desktop, a mechanism for online spatial prediction of cursor movements in a Windows-Icons-Menus-Pointers (WIMP) environment. Interaction with WIMP-based interfaces often becomes a spatially challenging task when the physical interaction mediators are the common mouse and a high resolution, physically large display screen. These spatial challenges are especially evident in overly crowded Windows desktops. The Delphian Desktop integrates simple yet effective predictive spatial tracking and selection paradigms into ordinary WIMP environments in order to simplify and ease pointing tasks. Predictions are calculated by tracking cursor movements and estimating spatial intentions using a computationally inexpensive online algorithm based on estimating the movement direction and peak velocity. In testing the Delphian Desktop effectively shortened pointing time to faraway icons, and reduced the overall physical distance the mouse (and user hand) had to mechanically traverse.\n",
      "=============================\n",
      "An interface agent for non-visual, accessible web automation\n",
      "The Web is far less usable and accessible for the users with visual impairments than it is for the sighted people. Web automation has the potential to bridge the divide between the ways visually impaired people and sighted people access the Web, and enable visually impaired users to breeze through Web browsing tasks that beforehand were slow, hard, or even impossible to achieve. Typical automation interfaces require that the user record a macro, a useful sequence of browsing steps, so that these steps can be re-played in the future. In this paper, I present a high-level overview of an approach that enables users to find quickly relevant information on the webpage, and automate browsing without recording macros. This approach is potentially useful both for visually impaired, and sighted users.\n",
      "=============================\n",
      "Tactile interfaces for small touch screens\n",
      "We present the design, implementation, and informal evaluation of tactile interfaces for small touch screens used in mobile devices. We embedded a tactile apparatus in a Sony PDA touch screen and enhanced its basic GUI elements with tactile feedback. Instead of observing the response of interface controls, users can feel it with their fingers as they press the screen. In informal evaluations, tactile feedback was greeted with enthusiasm. We believe that tactile feedback will become the next step in touch screen interface design and a standard feature of future mobile devices.\n",
      "=============================\n",
      "UIMarks: quick graphical interaction with specific targets\n",
      "This paper reports on the design and evaluation of UIMarks, a system that lets users specify on-screen targets and associated actions by means of a graphical marking language. UIMarks supplements traditional pointing by providing an alternative mode in which users can quickly activate these marks. Associated actions can range from basic pointing facilitation to complex sequences possibly involving user interaction: one can leave a mark on a palette to make it more reachable, but the mark can also be configured to wait for a click and then automatically move the pointer back to its original location, for example. The system has been implemented on two different platforms, Metisse and OS X. We compared it to traditional pointing on a set of elementary and composite tasks in an abstract setting. Although pure pointing was not improved, the programmable automation supported by the system proved very effective.\n",
      "=============================\n",
      "A unidraw-based user interface builder\n",
      "Ibuild is a user interface builder that lets a user manipulate simulations of toolkit objects rather than actual toolkit objects. Ibuild is built with Unidraw, a framework for building graphical editors that is part of the InterViews toolkit. Unidraw makes the simulation-based approach attractive. Simulating toolkit objects in Unidraw makes it easier to support editing facilities that are common in other kinds of graphical editors, and it keeps the builder insulated from a particular toolkit implementation. Ibuild supports direct manipulation analogs of InterViews'' composition mechanisms, which simplify the specification of an interface''s layout and resize semantics. Ibuild also leverages the C++ inheritance mechanism to decouple builder-generated code from the rest of the application. And while current user interface builders stop at the widget level, ibuild incorporates Unidraw abstractions to simplify the implementation of graphical editors.\n",
      "=============================\n",
      "Towards document engineering on pen and touch-operated interactive tabletops\n",
      "Touch interfaces have now become mainstream thanks to modern smartphones and tablets. However, there are still very few \"productivity\" applications, i.e. tools that support mundane but essential work, especially for large interactive surfaces such as digital tabletops. This work aims to partly fill the relative void in the area of document engineering by investigating what kind of intuitive and efficient tools can be provided to support the manipulation of documents on a digital workdesk, in particular the creation and editing of documents. The fundamental interaction model relies on bimanual pen and multitouch input, which was recently introduced to tabletops and enables richer interaction possibilities. The goal is ultimately to provide useful and highly accessible UIs for document-centric applications, whose design principles will hopefully pave the way from DTP towards DTTP (Digital Tabletop Publishing).\n",
      "=============================\n",
      "Multi-touch gesture recognition by single photoreflector\n",
      "A simple technique is proposed that uses a single photoreflector to recognize multi-touch gestures. Touch and multi-finger swipe are robustly discriminated and recognized. Further, swipe direction can be detected by adding a gradient to the sensitivity.\n",
      "=============================\n",
      "Physical telepresence: shape capture and display for embodied, computer-mediated remote collaboration\n",
      "We propose a new approach to Physical Telepresence, based on shared workspaces with the ability to capture and remotely render the shapes of people and objects. In this paper, we describe the concept of shape transmission, and propose interaction techniques to manipulate remote physical objects and physical renderings of shared digital content. We investigate how the representation of user's body parts can be altered to amplify their capabilities for teleoperation. We also describe the details of building and testing prototype Physical Telepresence workspaces based on shape displays. A preliminary evaluation shows how users are able to manipulate remote objects, and we report on our observations of several different manipulation techniques that highlight the expressive nature of our system.\n",
      "=============================\n",
      "Worldlets—3D thumbnails for wayfinding in virtual environments\n",
      "Virtual environment landmarks are essential in wayfinding: they anchor routes through a region and provide memorable destinations to return to later. Current virtual environment browsers provide user interface menus that characterize available travel destinations via landmark textual descriptions or thumbnail images. Such characterizations lack the depth cues and context needed to reliably recognize 3D landmarks. This paper introduces a new user interface affordance that captures a 3D representation of a virtual environment landmark into a 3D thumbnail, called a worldlet. Each worldlet is a miniature virtual world fragment that may be interactively viewed in 3D, enabling a traveler to gain first-person experience with a travel destination. In a pilot study conducted to compare textual, image, and worldlet landmark representations within a wayfinding task, worldlet use significantly reduced the overall travel time and distance traversed, virtually eliminating unnecessary backtracking. Landmarks are the subject of landmark knowledge, but also play a part in survey and procedural knowledge. In survey knowledge, landmarks provide regional anchors with which to calibrate distances and directions. In procedural knowledge, landmarks mark decision points along a route, helping in the recall of procedures to get to and from destinations of interest. Overall, landmarks help to structure an environment and provide directional cues to facilitate wayfinding.\n",
      "=============================\n",
      "Highly deformable interactive 3D surface display\n",
      "In this research, we focused on the flexibility limitation of a display material as one of the main causes for height con-straints in deformable surfaces. We propose a method that does not only utilize the material flexibility but also allows for increased variations of shapes and their corresponding interaction possibilities. Using this method, our proposed display design can then support additional expansion via protrusion of an air-pressure-controlled moldable display surface using a residual cloth-excess method and a fixed airbag mount.\n",
      "=============================\n",
      "Two-handed polygonal surface design\n",
      "This paper describes a Computer Aided Design system for sketching free-form polygonal surfaces such as terrains and other natural objects. The user manipulates two 3D position and orientation trackers with three buttons, one for each hand. Each hand has a distinct role to play, with the dominant hand being responsible for picking and manipulation, and the less-dominant hand being responsible for context setting of various kinds. The less-dominant hand holds the workpiece, sets which refinement level that can be picked by the dominant hand, and generally acts as a counterpoint to the dominant hand. In this paper, the architecture of the system is outlined, and a simple surface is shown.\n",
      "=============================\n",
      "GaussSense: attachable stylus sensing using magnetic sensor grid\n",
      "This work presents GaussSense, which is a back-of-device sensing technique for enabling input on an arbitrary surface using stylus by exploiting magnetism. A 2mm-thick Hall sensor grid is developed to sense magnets that are embedded in the stylus. Our system can sense the magnetic field that is emitted from the stylus when it is within 2cm of any non-ferromagnetic surface. Attaching the sensor behind an arbitrary thin surface enables the stylus input to be recognized by analyzing the distribution of the applied magnetic field. Attaching the sensor grid to the back of a touchscreen device and incorporating magnets into the corresponding stylus enable the system 1) to distinguish touch events that are caused by a finger from those caused by the stylus, 2) to sense the tilt angle of the stylus and the pressure with which it is applied, and 3) to detect where the stylus hovers over the screen. A pilot study reveals that people were satisfied with the novel sketching experiences based on this system.\n",
      "=============================\n",
      "Solving linear arithmetic constraints for user interface applications\n",
      "Linear equality and inequality constraints arise naturally in specifying many aspects of user interfaces, such as requiring that onewindowbe to the left of another, requiring that a pane occupy the leftmost 1/3 of a window, or preferring that an object be contained within a rectangle if possible. Current constraint solvers designed for UI applications cannot efficiently handle simultaneous linear equations and inequalities. This is amajor limitation. We describe incremental algorithms based on the dual simplex and active set methods that can solve such systems of constraints efficiently.\n",
      "=============================\n",
      "Circle & identify: interactivity-augmented object recognition for handheld devices\n",
      "The first requirement of a \"spatial mouse\" is the ability to identify the object that it is aiming at. Among many possible technologies that can be employed for this purpose, possibly the best solution would be object recognition by machine vision. The problem, however, is that object recognition algorithms are not yet reliable enough or light enough for hand-held devices. This paper demonstrates that a simple object recognition algorithm can become a practical solution when augmented by interactivity. The user draw a circle around a target using a spatial mouse, and the mouse captures a series of camera frames. The frames can be easily stitched together to give a target image separated from the background, with which we need only additional steps of feature extraction and object classification. We present here results from two experiments with a few household objects.\n",
      "=============================\n",
      "Fluid sketches: continuous recognition and morphing of simple hand-drawn shapes\n",
      "We describe a new sketching interface in which shape recognition and morphing are tightly coupled. Raw input strokes are continuously morphed into ideal geometric shapes, even before the pen is lifted. By means of smooth and continual shape transformations the user is apprised of recognition progress and the appearance of the final shape, yet always retains a sense of control over the process. At each time t the system uses the trajectory traced out thus far by the pen coupled with the current appearance of the time-varying shape to classify the sketch as one of several pre-defined basic shapes. The recognition operation is performed using shape-specific fits based on least-squares or relaxation, which are continuously updated as the user draws. We describe the time-dependent transformation of the sketch, beginning with the raw pen trajectory, using a family of first-order ordinary differential equations that depend on time and the current shape of the sketch. Using this formalism, we describe several possible behaviors that result by varying the relative significance of new and old portions of a stroke, changing the “viscosity” of the morph, and enforcing different end conditions. A preliminary user study suggests that the new interface is particularly effective for rapidly constructing diagrams consisting of simple shapes.\n",
      "=============================\n",
      "Iterative design and evaluation of an event architecture for pen-and-paper interfaces\n",
      "This paper explores architectural support for interfaces combining pen, paper, and PC. We show how the event-based approach common to GUIs can apply to augmented paper, and describe additions to address paper's distinguishing characteristics. To understand the developer experience of this architecture, we deployed the toolkit to 17 student teams for six weeks. Analysis of the developers' code provided insight into the appropriateness of events for paper UIs. The usage patterns we distilled informed a second iteration of the toolkit, which introduces techniques for integrating interactive and batched input handling, coordinating interactions across devices, and debugging paper applications. The study also revealed that programmers created gesture handlers by composing simple ink measurements. This desire for informal interactions inspired us to include abstractions for recognition. This work has implications beyond paper - designers of graphical tools can examine API usage to inform iterative toolkit development.\n",
      "=============================\n",
      "Imaginary interfaces: spatial interaction with empty hands and without visual feedback\n",
      "Screen-less wearable devices allow for the smallest form factor and thus the maximum mobility. However, current screen-less devices only support buttons and gestures. Pointing is not supported because users have nothing to point at. However, we challenge the notion that spatial interaction requires a screen and propose a method for bringing spatial interaction to screen-less devices. We present Imaginary Interfaces, screen-less devices that allow users to perform spatial interaction with empty hands and without visual feedback. Unlike projection-based solutions, such as Sixth Sense, all visual \"feedback\" takes place in the user's imagination. Users define the origin of an imaginary space by forming an L-shaped coordinate cross with their non-dominant hand. Users then point and draw with their dominant hand in the resulting space. With three user studies we investigate the question: To what extent can users interact spatially with a user interface that exists only in their imagination? Participants created simple drawings, annotated existing drawings, and pointed at locations described in imaginary space. Our findings suggest that users' visual short-term memory can, in part, replace the feedback conventionally displayed on a screen.\n",
      "=============================\n",
      "Backward highlighting: enhancing faceted search\n",
      "Directional faceted browsers, such as the popular column browser iTunes, let a person pick an instance from any column-facet to start their search for music. The expected effect is that any columns to the right are filtered. In keeping with this directional filtering from left to right, however, the unexpected effect is that the columns to the left of the click provide no information about the possible associations to the selected item. In iTunes, this means that any selection in the Album column on the right returns no information about either the Artists (immediate left) or Genres (leftmost) associated with the chosen album. Backward Highlighting (BH) is our solution to this problem, which allows users to see and utilize, during search, associations in columns to the left of a selection in a directional column browser like iTunes. Unlike other possible solutions, this technique allows such browsers to keep direction in their filtering, and so provides users with the best of both directional and non-directional styles. As well as describing BH in detail, this paper presents the results of a formative user study, showing benefits for both information discovery and subsequent retention in memory.\n",
      "=============================\n",
      "Going to the dogs: towards an interactive touchscreen interface for working dogs\n",
      "Computer-mediated interaction for working dogs is an important new domain for interaction research. In domestic settings, touchscreens could provide a way for dogs to communicate critical information to humans. In this paper we explore how a dog might interact with a touchscreen interface. We observe dogs' touchscreen interactions and record difficulties against what is expected of humans' touchscreen interactions. We also solve hardware issues through screen adaptations and projection styles to make a touchscreen usable for a canine's nose touch interactions. We also compare our canine touch data to humans' touch data on the same system. Our goal is to understand the affordances needed to make touchscreen interfaces usable for canines and help the future design of touchscreen interfaces for assistive dogs in the home.\n",
      "=============================\n",
      "Building virtual structures with physical blocks\n",
      "We describe a tangible interface for building virtual structures using physical building blocks. We demonstrate two applications of our system. In one version, the blocks are used to construct geometric models of objects and structures for a popular game, Quake II™. In another version, buildings created with our blocks are rendered in different styles, using intelligent decoration of the building model.\n",
      "=============================\n",
      "Probabilistic state machines: dialog management for inputs with uncertainty\n",
      "Traditional models of input work on the assumption that inputs delivered to a system are fairly certain to have occurred as they are reported. However, a number of new input modalities, such as pen-based inputs, hand and body gesture inputs, and voice input, do not share this property. Inputs under these techniques are normally acquired by a process of recognition. As a result, each of these techniques makes mistakes and provides inputs which are approximate or uncertain. This paper considers some preliminry techniques for dialog management in the presence of this uncertainty. These techniques—including a new input model and a set of extended state machine abstractions—will explicitly model uncertainty and handle it as a normal and expected part of the input process.\n",
      "=============================\n",
      "Jazz: an extensible zoomable user interface graphics toolkit in Java\n",
      "Abstract : In this paper the authors investigate the use of scene graphs as a general approach for implementing two-dimensional (2D) graphical applications, and in particular Zoomable User Interfaces (ZUIs). Scene graphs are typically found in three-dimensional (3D) graphics packages such as Sun's Java3D and SGI's OpenInventor. They have not been widely adopted by 2D graphical user interface toolkits. To explore the effectiveness of scene graph techniques, the authors have developed Jazz, a general-purpose 2D scene graph toolkit. Jazz is implemented in Java using Java2D, and runs on all platforms that support Java 2. This paper describes Jazz and the lessons we learned using Jazz for ZUIs. It also discusses how 2D scene graphs can be applied to other application areas. (5 figures, 27 refs.)\n",
      "=============================\n",
      "Gliimpse: Animating from markup code to rendered documents and vice versa\n",
      "We present a quick preview technique that smoothly transitions between document markup code and its visual rendering. This technique allows users to regularly check the code they are editing in-place, without leaving the text editor. This method can complement classical preview windows by offering rapid overviews of code-to-document mappings and leaving more screen real-estate. We discuss the design and implementation of our technique.\n",
      "=============================\n",
      "MoodMusic: a method for cooperative, generative music playlist creation\n",
      "Music is a major element of social gatherings. However, creating playlists that suit everyone's tastes and the mood of the group can require a large amount of manual effort. In this paper, we present MoodMusic, a method to dynamically generate contextually appropriate music playlists for groups of people. MoodMusic uses speaker pitch and intensity in the conversation to determine the current 'mood'. MoodMusic then queries the online music libraries of the speakers to choose songs appropriate for that mood. This allows groups to listen to music appropriate for their current mood without managing playlists. This work contributes a novel method for dynamically creating music playlists for groups based on their music preferences and current mood.\n",
      "=============================\n",
      "OPA browser: a web browser for cellular phone users\n",
      "Cellular phones are widely used to access the WWW. However, most available Web pages are designed for desktop PCs. Cellular phones only have small screens and poor interfaces, and thus, it is inconvenient to browse such large sized pages. In addition, cellular phone users browse Web pages in various situations, so that appropriate presentation styles for Web pages depend on users' situations. In this paper, we propose a novel Web browsing system for cellular phones that allocates various functions for Web browsing on each numerical key of a cellular phone. Users can browse Web pages comfortably, selecting appropriate functions according to their situations by pushing a single button.\n",
      "=============================\n",
      "A flexible Chinese character input scheme\n",
      "A very flexible and easy-to-use scheme which possesses unique advantages over existing systems is presented in this article. The scheme is based on the partitioning of a character into parts, A character is inputted by specifying the sequence of character parts descriptions, which is then matched against the standard sequences of the characters in the character set. A character part is either described with a unique key or its stroke count. The matching algorithm allows the characters to be partitioned flexibly and inputted in many different ways. An automatic binding mechanism offers very high adaptability to the input style of the user. The user need not remember all the key bindings before he can input Chinese and the scheme is also capable of tolerating many variations in character style and/or errors. CR\n",
      "=============================\n",
      "Intimacy versus privacy\n",
      "When you talk to a person, it's safe to assume that you both share large bodies of \"common sense knowledge.\" But when you converse with a programmed computer, neither of you is likely to know much about what the other one knows. Indeed, in some respects this is desirable - as when we're concerned with our privacy. We don't want strangers to know our most personal goals, or all the resources that we may control. However, when we turn to our computers for help, we'll want that relationship to change - because now it is in our interest for those systems to understand our aims and goals, as well as our fears and phobias. Indeed, the extents to which those processes \"know us as individuals\". Issues like these will always arise whenever we need a new interface - and as one of my teachers wrote long ago, \"The hope is that, in not too many years, human brains and computing machines will be coupled together very tightly, and that the resulting partnership will think as no human brain has ever thought and process data in a way not approached by the information-handling machines we know today.\"1 Indeed, the '60s and '70s saw substantial advancestowars this but it seems to me that then progress slowed down. If so, perhaps this was partly because the AI community moved from semantic and heuristic methods towards more formal (but less flexible) statistical schemes. So nowI'd like to see more researchers remedy this by developing systems that use more commonsense knowledge.\n",
      "=============================\n",
      "Review explorer: an innovative interface for displaying and collecting categorized review information\n",
      "Review Explorer is an interface that utilizes categorized information to help users to explore a huge amount of online reviews more easily. It allows users to sort entities (e.g. restaurants, products) based on their ratings of different aspects (e.g. food for restaurants) and highlight sentences that are related to the selected aspect. Existing interfaces that summarize the aspect information in reviews suffer from the erroneous predictions made by the systems. To solve this problem, Review Explorer performs a real-time aspect sentiment analysis when a reviewer is composing a review and provides an interface for the reviewer to easily correct the errors. This novel design motivates reviewers to provide corrected aspect sentiment labels, which enables our system to provide more accurate information than existing interfaces.\n",
      "=============================\n",
      "An annotated situation-awareness aid for augmented reality\n",
      "We present a situation-awareness aid for augmented reality systems based on an annotated \"world in miniature.\" Our aid is designed to provide users with an overview of their environment that allows them to select and inquire about the objects that it contains. Two key capabilities are discussed that are intended to address the needs of mobile users. The aid's position, scale, and orientation are controlled by a novel approach that allows the user to inspect the aid without the need for manual interaction. As the user alternates their attention between the physical world and virtual aid, popup annotations associated with selected objects can move freely between the objects' representations in the two models.\n",
      "=============================\n",
      "Sex, food, and words: the hidden meanings behind everyday language\n",
      "Language is a subtle and powerful tool for communication. But the words we use also provide a rich mine of information for the social scientist. The history of words like \"ketchup\", \"ceviche\", or \"dessert\" tells us about the relationships between the superpowers who dominated the globe 500 or 1000 years ago. The words on the back of potato chip packages can demonstrate popular attitudes toward social class. And the names we give ice cream flavors may be an evolutionary reflex of the attempt by early mammals to appear larger than their competitors. The language of dating is just as informative as the language of food. In experiments with speed dating, work in our lab shows that we can detect flirtation or other stances in men and women on dates, just by looking at linguistic features like their pitch, their use of negative words like \"can't\" or \"don't\", or how often they use hedges like \"sort of\" or \"kind of\". The language of these two popular topics of conversation, food and dating, can teach us a lot about history, culture, and psychology.\n",
      "=============================\n",
      "The nudging technique: input method without fine-grained pointing by pushing a segment\n",
      "The Nudging Technique is a new manipulation paradigm for GUIs. With traditional techniques, the user sometimes has to perform a fine-grained operation (e.g., pointing at the edge of a window to resize). When the user makes a mistake in the pointing, problems may arise such as an accidental switching of the foreground window. The nudging technique relieves the user from the fine pointing before dragging; the user just moves the cursor to a target then pushes it. Visual and acoustic feedbacks also help the user's operation. We describe two application examples: window resizing and spreadsheet cell resizing systems.\n",
      "=============================\n",
      "TwinSpace: an infrastructure for cross-reality team spaces\n",
      "We introduce TwinSpace, a flexible software infrastructure for combining interactive workspaces and collaborative virtual worlds. Its design is grounded in the need to support deep connectivity and flexible mappings between virtual and real spaces to effectively support collaboration. This is achieved through a robust connectivity layer linking heterogeneous collections of physical and virtual devices and services, and a centralized service to manage and control mappings between physical and virtual. In this paper we motivate and present the architecture of TwinSpace, discuss our experiences and lessons learned in building a generic framework for collaborative cross-reality, and illustrate the architecture using two implemented examples that highlight its flexibility and range, and its support for rapid prototyping.\n",
      "=============================\n",
      "CyberDesk: a framework for providing self-integrating ubiquitous software services\n",
      "Current software suites suffer from problems due to poor integration of their individual tools. They require the designer to think of all possible integrating behaviours and leave little flexibility to the user. In this paper, we discuss CyberDesk, a component software framework that automatically integrates desktop and network services, requiring no integrating decisions to be made by the tool designers and giving total control to the user. We describe CyberDesk’s architecture in detail and show how CyberDesk components can be built. We give examples of extensions to CyberDesk such as chaining, combining, and using higher level context to obtain powerful integrating behaviours.\n",
      "=============================\n",
      "OverCode: visualizing variation in student solutions to programming problems at scale\n",
      "In MOOCs, a single programming exercise may produce thousands of solutions from learners. Understanding solution variation is important for providing appropriate feedback to students at scale. The wide variation among these solutions can be a source of pedagogically valuable examples, and can be used to refine the autograder for the exercise by exposing corner cases. We present OverCode, a system for visualizing and exploring thousands of programming solutions. OverCode uses both static and dynamic analysis to cluster similar solutions, and lets instructors further filter and cluster solutions based on different criteria. We evaluated OverCode against a non-clustering baseline in a within-subjects study with 24 teaching assistants, and found that the OverCode interface allows teachers to more quickly develop a high-level view of students' understanding and misconceptions, and to provide feedback that is relevant to more students.\n",
      "=============================\n",
      "WirePrint: 3D printed previews for fast prototyping\n",
      "Even though considered a rapid prototyping tool, 3D printing is so slow that a reasonably sized object requires printing overnight. This slows designers down to a single iteration per day. In this paper, we propose to instead print low-fidelity wireframe previews in the early stages of the design process. Wireframe previews are 3D prints in which surfaces have been replaced with a wireframe mesh. Since wireframe previews are to scale and represent the overall shape of the 3D object, they allow users to quickly verify key aspects of their 3D design, such as the ergonomic fit. To maximize the speed-up, we instruct 3D printers to extrude filament not layer-by-layer, but directly in 3D-space, allowing them to create the edges of the wireframe model directly one stroke at a time. This allows us to achieve speed-ups of up to a factor of 10 compared to traditional layer-based printing. We demonstrate how to achieve wireframe previews on standard FDM 3D printers, such as the PrintrBot or the Kossel mini. Users only need to install the WirePrint software, making our approach applicable to many 3D printers already in use today. Finally, wireframe previews use only a fraction of material required for a regular print, making it even more affordable to iterate.\n",
      "=============================\n",
      "Rhythm modeling, visualizations and applications\n",
      "People use their awareness of others' temporal patterns to plan work activities and communication. This paper presents algorithms for programatically detecting and modeling temporal patterns from a record of online presence data. We describe analytic and end-user visualizations of rhythmic patterns and the tradeoffs between them. We conducted a design study that explored the accuracy of the derived rhythm models compared to user perceptions, user preference among the visualization alternatives, and users' privacy preferences. We also present a prototype application based on the rhythm model that detects when a person is \"away\" for an extended period and predicts their return. We discuss the implications of this technology on the design of computer-mediated communication.\n",
      "=============================\n",
      "Eyepatch: prototyping camera-based interaction through examples\n",
      "Cameras are a useful source of input for many interactive applications, but computer vision programming is difficult and requires specialized knowledge that is out of reach for many HCI practitioners. In an effort to learn what makes a useful computer vision design tool, we created Eyepatch, a tool for designing camera-based interactions, and evaluated the Eyepatch prototype through deployment to students in an HCI course. This paper describes the lessons we learned about making computer vision more accessible, while retaining enough power and flexibility to be useful in a wide variety of interaction scenarios.\n",
      "=============================\n",
      "Asymmetric cores for low power user interface systems\n",
      "In recent years, advances in hardware design have lead to significant improvements in the battery life of everyday information appliances. In particular, application processors increasingly include low power \"helper\" cores dedicated to simpler tasks. Using a custom board design, Guimbretière et al. [2], demonstrated that such helper cores can also be used to execute simple user interface tasks. We revisit their approach by implementing a similar system on an off-the-shelf application processor (TI OMAP4), and demonstrate that, in many cases, the gains reported by Guimbretière et al. [2], can be achieved by simply having the helper core dispatch input events. This new approach can be implemented by merely changing the toolkit infrastructure, thus greatly simplifying deployment\n",
      "=============================\n",
      "In-air gestures around unmodified mobile devices\n",
      "We present a novel machine learning based algorithm extending the interaction space around mobile devices. The technique uses only the RGB camera now commonplace on off-the-shelf mobile devices. Our algorithm robustly recognizes a wide range of in-air gestures, supporting user variation, and varying lighting conditions. We demonstrate that our algorithm runs in real-time on unmodified mobile devices, including resource-constrained smartphones and smartwatches. Our goal is not to replace the touchscreen as primary input device, but rather to augment and enrich the existing interaction vocabulary using gestures. While touch input works well for many scenarios, we demonstrate numerous interaction tasks such as mode switches, application and task management, menu selection and certain types of navigation, where such input can be either complemented or better served by in-air gestures. This removes screen real-estate issues on small touchscreens, and allows input to be expanded to the 3D space around the device. We present results for recognition accuracy (93% test and 98% train), impact of memory footprint and other model parameters. Finally, we report results from preliminary user evaluations, discuss advantages and limitations and conclude with directions for future work.\n",
      "=============================\n",
      "VUIMS: a visual user interface management system\n",
      "VUIMS is an object-oriented user interface management system that was designed to support reconfigurable components. VUIMS consists of a collection of objects and a semantically rich token language. The objects implement primitive presentation and interaction functions. The token language controls interaction and visual style. High level objects can be created from primitive objects using token templates. The user interface and application are controlled by token streams that are emitted in response to user actions. VUIMS supports a variety of presentation and interaction styles through simple, robust manipulation of a hierarchy of visual panels with a rich set of relationships and constraints. VUIMS has been used to implement two commercial highperformance computer graphics applications and an on-line help system. It has evolved over a three-year period and has proven to be an effective tool in commercial use.\n",
      "=============================\n",
      "Pub - point upon body: exploring eyes-free interaction and methods on an arm\n",
      "This paper presents a novel interaction system, PUB (Point Upon Body), to explore eyes-free interaction in a personal space by allowing users tapping on their own arms to be provided with haptic feedback from their skin. Two user studies determine how users can interact precisely with their forearms and how users behave when operating in their arm space. According to those results, normal users can divide their arm space at most into 6 points between their wrists and elbows with iterative practice. Experimental results also indicate that the divided pattern of each user is unique from that of other ones. Based on the design principles from the observations, an interaction system, PUB, is designed to demonstrate how interaction design benefits from those findings. Two scenarios, remote display control and mobile device control, are demonstrated through the UltraSonic device attached on the users' wrists to detect their tapped positions.\n",
      "=============================\n",
      "Supporting self-expression for informal communication\n",
      "Mobile phones are becoming the central tools for communicating and can help us keep in touch with friends and family on-the-go. However, they can also place high demands on attention and constrain interaction. My research concerns how to design communication mechanisms that mitigate these problems to support self-expression for informal communication on mobile phones. I will study how people communicate with camera-phone photos, paper-based sketches, and projected information and how this communication impacts social practices.\n",
      "=============================\n",
      "Breaking barriers with sound\n",
      "The computer, in its many shapes and sizes, is evolving rapidly and pervading our everyday lives like never before. Mobile computing devices have become much more than simply \"mobile\", increasingly serving as personal and \"natural\" extensions of us. Therein lies immense potential to reshape the way we think and interact, and especially in how we engage one another creatively, expressively, and socially. This talk explores interaction and social design for music through the computer, told through laptop orchestras, mobile phone orchestras, an audio programming language, designing the iPhone's Ocarina, ecosystems for crowd-sourcing musical creation, and an emerging social dimension where computer, music, and people interact.\n",
      "=============================\n",
      "Exploring pen and paper interaction with high-resolution wall displays\n",
      "We introduce HIPerPaper, a novel digital pen and paper interface that enables natural interaction with a 31.8 by 7.5 foot tiled wall display of 268,720,000 pixels. HIPerPaper provides a flexible, portable, and inexpensive medium for interacting with large high-resolution wall displays. While the size and resolution of such displays allow visualization of data sets of a scale not previously possible, mechanisms for interacting with wall displays remain challenging. HIPerPaper enables multiple concurrent users to select, move, scale, and rotate objects on a high-dimension wall display.\n",
      "=============================\n",
      "Humane representation of thought: a trail map for the 21st century\n",
      "New representations of thought -- written language, mathematical notation, information graphics, etc -- have been responsible for some of the most significant leaps in the progress of civilization, by expanding humanity's collectively-thinkable territory. But at debilitating cost. These representations, having been invented for static media such as paper, tap into a small subset of human capabilities and neglect the rest. Knowledge work means sitting at a desk, interpreting and manipulating symbols. The human body is reduced to an eye staring at tiny rectangles and fingers on a pen or keyboard. Like any severely unbalanced way of living, this is crippling to mind and body. But less obviously, and more importantly, it is enormously wasteful of the vast human potential. Human beings naturally have many powerful modes of thinking and understanding. Most are incompatible with static media. In a culture that has contorted itself around the limitations of marks on paper, these modes are undeveloped, unrecognized, or scorned. We are now seeing the start of a dynamic medium. To a large extent, people today are using this medium merely to emulate and extend static representations from the era of paper, and to further constrain the ways in which the human body can interact with external representations of thought. But the dynamic medium offers the opportunity to deliberately invent a humane and empowering form of knowledge work. We can design dynamic representations which draw on the entire range of human capabilities -- all senses, all forms of movement, all forms of understanding -- instead of straining a few and atrophying the rest. This talk suggests how each of the human activities in which thought is externalized (conversing, presenting, reading, writing, etc) can be redesigned around such representations.\n",
      "=============================\n",
      "Making distance matter: leveraging scale and diversity in massive online classes\n",
      "The large scale of online classes and the diversity of the students that participate in them can enable new educational systems. This massive scale and diversity can enable always-available systems that help students share diverse ideas, and inspire and learn from each other. We introduce systems for two core educational processes at scale: discussion and assessment. To date, several thousand students in a dozen online classes have used our discussion system. Controlled experiments suggest that participants in more diverse discussions perform better on tests and that discussion improves engagement. Similarly, more than 100,000 students have reviewed peer work for both summative assessment and feedback. Through these systems, we argue that to create new educational experiences at scale, pedagogical strategies and software that leverage scale and diversity must be co-developed. More broadly, we suggest the key to creating new educational experiences online lies in leveraging massive networks of peers.\n",
      "=============================\n",
      "A three-step interaction pattern for improving discoverability in finger identification techniques\n",
      "Identifying which fingers are in contact with a multi-touch surface provides a very large input space that can be leveraged for command selection. However, the numerous possibilities enabled by such vast space come at the cost of discoverability. To alleviate this problem, we introduce a three-step interaction pattern inspired by hotkeys that also supports feed-forward. We illustrate this interaction with three applications allowing us to explore and adapt it in different contexts.\n",
      "=============================\n",
      "LivingClay: particle actuation to control display volume and stiffness\n",
      "We present a new type of display actuation that is able to control both display geometry and stiffness properties using a filler material and air flow control technique. The display consists of a flat, flexible layer of cells on the surface and chamber filled with particles under it. Display geometries can be changed by transporting an amount of particles between display cells and the particle chamber using pressured air and vacuum to control the air flows. This system also allow for variable stiffness using vacuum technique to harden the particles inside chamber. In this paper, we present the design and control technique of this new type actuator and also possible interaction on a single actuator display. We also propose a low-cost, effective way to control an array of actuators where the air flow line and particle line are arranged in a multiplexed grid configuration.\n",
      "=============================\n",
      "The Cage: efficient construction in 3D using a cubic adaptive grid\n",
      "The Cage is an easy to use 3D grid. Built into a 3D modeler, it provides a visualized reference coordinate system that helps the user to orient himself in 3D space, and that supports efficient alignment and snapping methods. It can be adapted with a single mouse click to any new viewing situation and reference system. The Cage was implemented in C++ under Open Inventor on Silicon Graphics workstations. It was tested as a part of a 3D authoring tool for virtual TV studios.\n",
      "=============================\n",
      "User interfaces for symbolic computation: a case study\n",
      "Designing user interfaces for Symbolic Computation tools like Maple, Mathematical, Reduce, etc, implies solving some challenging difficulties, including the display and manipulation of mathematical formulas, efficient and user-friendly manipulation of possibly large expressions, deep extensibility y of the user interface, etc. This also implies dealing with some complex communication issues in order to efficiently integrate under a common user interface a set of tools from different origins running across a distributed architecture. This includes transparent management of remote comput ations, hiding command language variations, and programmability of the whole environment so that one can solve complex problems by sequentially and/or concurrently applying any needed tools. Very few of these problems are addressed by the user interfaces of commercially available packages. To improve user interfaces and integration of Symbolic Computation tools, a prototyping system named CAS/PI has been implemented. Its main characteristics are to be highly flexible and extensible, and to be based on pre-existing software engineering technologies. Using CAS/PI, each of the previously stated problem may be solved in a generic way, allowing further experiment ations and optimization.\n",
      "=============================\n",
      "Foreign manga reader: learn grammar and pronunciation while reading comics\n",
      "Foreign-language comics are potentially an enjoyable way to learn foreign languages. However, the difficulty of reading authentic material makes them inaccessible to novice learners. We present the Foreign Manga Reader, a system that helps readers comprehend foreign-language written materials and learn multiple aspects of the language. Specifically, it generates a sentence-structure visualization to help learners understand the grammar, pronounces dialogs to improve listening comprehension and pronunciation, and translates dialogs, phrases, and words to teach vocabulary. Learners can use the system to match their experience level, giving novices access to dialog-level translations and pronunciations, and more advanced learners with access to information at the level of phrases and individual words. The annotations are automatically generated, and can be used with arbitrary written materials in several languages. A preliminary study suggests that learners find our system useful for understanding and learning from authentic foreign-language material.\n",
      "=============================\n",
      "Pixel-based reverse engineering of graphical interfaces\n",
      "My dissertation proposes a vision in which anybody can modify any interface of any application. Realizing this vision is difficult because of the rigidity and fragmentation of current interfaces. Specifically, rigidity makes it difficult or impossible for a designer to modify or customize existing interfaces. Fragmentation results from the fact that people generally use many different applications built with a variety of toolkits. Each is implemented differently, so it is difficult to consistently add new functionality. As a result, researchers are often limited to demonstrating new ideas in small testbeds, and practitioners often find it difficult to adopt and deploy ideas from the literature. In my dissertation, I propose transcending the rigidity and fragmentation of modern interfaces by building upon their single largest commonality: that they ultimately consist of pixels painted to a display. Building from this universal representation, I propose pixel-based interpretation to enable modification of interfaces without their source code and independent of their underlying toolkit implementation.\n",
      "=============================\n",
      "Measuring how design changes cognition at work\n",
      "The various fields associated with interactive software systems engage in design activities to enable people who would use the resulting systems to meet goals, coordinate with others, find meaning, and express themselves in myriad ways. Yet many development projects fail, and we all have contact with clumsy software-based systems that force work-arounds and impose substantial attentional, knowledge and workload burdens. On the other hand, field observations reveal people re-shaping the artifacts they encounter and interact with as resources to cope with the demands of the situations they face as they seek to meet their goals. In this process some new devices are quickly seized upon and exploited in ways that transform the nature of human activity, connections, and expression. The software intensive interactive systems and devices under development around us are valuable to the degree that they expand what people in various roles and organizations can achieve. How can we measure this value provided to others? Are current measures of usability adequate? Does creeping complexity wipe out incremental gains as products evolve? Do designers and developers mis-project the impact when systems-to-be-realized are fielded? Which technology changes will trigger waves of expansive adaptations that transform what people do and even why they do it. Sponsors of projects to develop new interactive software systems are asking developers for tangible evidence of the value to be delivered to those people responsible for activities and goals in the world. Traditional measures of usability and human performance seem inadequate. Cycles of inflation in the claims development organizations make (and the legacy of disappointment and surprise) have left sponsors numb and eroded trust. Thus, we need to provide new forms of evidence about the potential of new interactive systems and devices to enhance human capability. Luckily, this need has been accompanied by a period of innovation in ways to measure the impact of new designs on: growth of expertise in roles, synchronizing activities over wider scopes and ranges, expanding adaptive capacities.. This talk reviews a few of the new measures being tested in each of these categories, points to some of the underlying science, and uses these examples to trigger discussion about how design of future interactive software provides will provide value to stakeholders.\n",
      "=============================\n",
      "Specifying behavior and semantic meaning in an unmodified layered drawing package\n",
      "In order to create and use rich custom appearances, designers are often forced to introduce an unnatural gap into the design process. For example, a designer creating a skin for a music player must separately specify the appearance of the elements in the music player skin and the mapping between these visual elements and the functionality provided by the music player. This gap between appearance and semantic meaning creates a number of problems. We present a set of techniques that allows designers to use their preferred drawing tool to specify both appearance and semantic meaning. We demonstrate our techniques in an unmodified version of Adobe Photoshop®, but our techniques are general and adaptable to nearly any layered drawing package.\n",
      "=============================\n",
      "Synchrum: a tangible interface for rhythmic collaboration\n",
      "Synchrum is a tangible interface, inspired by the Tibetan prayer wheel, for audience participation and collaboration during digital performance. It engages audience members in effortful interaction, where they have to rotate the device in accord with a given rotation speed. We used synchrum in a video installation and report our observations.\n",
      "=============================\n",
      "Buttons as first class objects on an X desktop\n",
      "A high-level user interface toolkit, called XButtons, has been developed to support on-screen buttons as first class objects on an X window system desktop, With the toolkit, buttons can be built that connect user interactions with procedures specified as arbitrary Unix Shell scripts. As first class desktop objects, these buttons encapsulate appearance and behavior that is user tailorable. They are persistent objects and may store state relevant to the task they perform. They can also be mailed to other users electronically. In addition to being first class desktop objects, XButtons are gesturebased with multiple actions. They support other interaction styles, like the drag and drop metaphor, in addition to simple button click actions. They also may be concurrently shared among users, with changes reflected to all users of the shared buttons. This paper describes the goals of XButtons and the history of button development that led to XButtons, It also describes XButtons from the user’s point of view. Finally, it discusses some implementation issues encountered in building XButtons on top of the X window system.\n",
      "=============================\n",
      "View management for virtual and augmented reality\n",
      "We describe a view-management component for interactive 3D user interfaces. By view management, we mean maintaining visual constraints on the projections of objects on the view plane, such as locating related objects near each other, or preventing objects from occluding each other. Our view-management component accomplishes this by modifying selected object properties, including position, size, and transparency, which are tagged to indicate their constraints. For example, some objects may have geometric properties that are determined entirely by a physical simulation and which cannot be modified, while other objects may be annotations whose position and size are flexible.We introduce algorithms that use upright rectangular extents to represent on the view plane a dynamic and efficient approximation of the occupied space containing the projections of visible portions of 3D objects, as well as the unoccupied space in which objects can be placed to avoid occlusion. Layout decisions from previous frames are taken into account to reduce visual discontinuities. We present augmented reality and virtual reality examples to which we have applied our approach, including a dynamically labeled and annotated environment.\n",
      "=============================\n",
      "Tactile cue presentation for vocabulary learning with keyboard\n",
      "This paper presents the results of a pilot experiment observ-ing the effect of tactile cues on vocabulary learning. Con-sidering that we generally memorize words by associating them with various cues, we designed a tactile cue presentation device that aids vocabulary learning by applying vibra-tions to the finger that is associated with the next key to press when typing on a keyboard. Experiments comparing tactile and visual cues indicated that tactile cues can signifi-cantly improve long-term retention of vocabulary after one week.\n",
      "=============================\n",
      "Two-finger input with a standard touch screen\n",
      "Most current implementations of multi-touch screens are still too expensive or too bulky for widespread adoption. To improve this situation, this work describes the electronics and software needed to collect more data than one pair of coordinates from a standard 4-wire touch screen. With this system, one can measure the pressure of a single touch and approximately sense the coordinates of two touches occurring simultaneously. Naturally, the system cannot offer the accuracy and versatility of full multi-touch screens. Nonetheless, several example applications ranging from painting to zooming demonstrate a broad spectrum of use.\n",
      "=============================\n",
      "Cross-modal interaction using XWeb\n",
      "The XWeb project addresses the problem of interacting with services by means of a variety of interactive platforms. Interactive clients are provided on a variety of hardware/software platforms that can access and XWeb service. Creators of services need not be concerned with interactive techniques or devices. The cross platform problems of a network model of interaction, adaptation to screen size and supporting both speech and visual interfaces in the same model are addressed.\n",
      "=============================\n",
      "Surfboard: keyboard with microphone as a low-cost interactive surface\n",
      "We introduce a technique to detect simple gestures of \"surfing\" (moving a hand horizontally) on a standard keyboard by analyzing recorded sounds in real-time with a microphone attached close to the keyboard. This technique allows the user to maintain a focus on the screen while surfing on the keyboard. Since this technique uses a standard keyboard without any modification, the user can take full advantage of the input functionality and tactile quality of his favorite keyboard supplemented with our interface.\n",
      "=============================\n",
      "Standardizing the interface between applications and UIM's\n",
      "The user interface building blocks of any User Interface Management System (UIMS) have built-in assumptions about what information about application programs they need, and assumptions about how to get that information. The lack of a standard to represent this information leads to a proliferation of different assumptions by different building blocks, hampering changeability of the user interface and portability of applications to different sets of building blocks. This paper describes a formalism for specifying the information about applications needed by the user interface building blocks (i.e. the UIMS/Application interface) so that all building blocks share a common set of assumptions. The paper also describes a set of user interface building blocks specifically designed for these standard UIMS/Application interfaces. These building blocks can be used to produce a wide variety of user interfaces, and the interfaces can be changed without having to change the application program.\n",
      "=============================\n",
      "An optimization-based approach to dynamic data content selection in intelligent multimedia interfaces\n",
      "We are building a multimedia conversation system to facilitate information seeking in large and complex data spaces. To provide tailored responses to diverse user queries introduced during a conversation, we automate the generation of a system response. Here we focus on the problem of determining the data content of a response. Specifically, we develop an optimization-based approach to content selection. Compared to existing rule-based or plan-based approaches, our work offers three unique contributions. First, our approach provides a general framework that effectively addresses content selection for various interaction situations by balancing a comprehensive set of constraints (e.g., content quality and quantity constraints). Second, our method is easily extensible, since it uses feature-based metrics to systematically model selection constraints. Third, our method improves selection results by incorporating content organization and media allocation effects, which otherwise are treated separately. Preliminary studies show that our method can handle most of the user situations identified in a Wizard-of-Oz study, and achieves results similar to those produced by human designers.\n",
      "=============================\n",
      "Quiet interfaces that help students think\n",
      "As technical as we have become, modern computing has not permeated many important areas of our lives, including mathematics education which still involves pencil and paper. In the present study, twenty high school geometry students varying in ability from low to high participated in a comparative assessment of math problem solving using existing pencil and paper work practice (PP), and three different interfaces: an Anoto-based digital stylus and paper interface (DP), pen tablet interface (PT), and graphical tablet interface (GT). Cognitive Load Theory correctly predicted that as interfaces departed more from familiar work practice (GT > PT > DP), students would experience greater cognitive load such that performance would deteriorate in speed, attentional focus, meta-cognitive control, correctness of problem solutions, and memory. In addition, low-performing students experienced elevated cognitive load, with the more challenging interfaces (GT, PT) disrupting their performance disproportionately more than higher performers. The present results indicate that Cognitive Load Theory provides a coherent and powerful basis for predicting the rank ordering of users' performance by type of interface. In the future, new interfaces for areas like education and mobile computing could benefit from designs that minimize users' load so performance is more adequately supported.\n",
      "=============================\n",
      "The programmable hinge: toward computationally enhanced crafts\n",
      "Traditionally, the practitioners of home crafting and the practitioners of computing tend to occupy distinct, nonoverlapping cultures. Those small, ubiquitous items of the crafting cultureNstring, thumbtacks, screws, nails, and so forthNthus tend to be viewed as inevitably \"lowtech\" objects. This paper describes our initial efforts toward integrating computational and crafting media by creating an instance of a computationally-enhanced craft item: a programmable hinge. We describe several prototype models of the hinge; outline a sample project in which the hinge might be employed; and discuss a variety of fundamental issues that affect the design of computationally-enhanced craft items generally.\n",
      "=============================\n",
      "Stylus input and editing without prior selection of mode\n",
      "This paper offers a solution to the mode problem in computer sketch/notetaking programs. Conventionally, the user must specify the intended \"draw\" or \"command\" mode prior to performing a stroke. This necessity has proven to be a barrier to the usability of pen/stylus systems. We offer a novel Inferred-Mode interaction protocol that avoids the mode hassles of conventional sketch systems. The system infers the user's intent, if possible, from the properties of the pen trajectory and the context of the trajectory. If the intent is ambiguous, the user is offered a choice mediator in the form of a pop-up button. To maximize the fluidity of drawing, the user is entitled to ignore the mediator and continue drawing. We present decision logic for the inferred mode protocol, and discuss subtleties learned in the course of its development. We also present results of initial user trials validating the usability of this interaction design.\n",
      "=============================\n",
      "\"Killer App\" of wearable computing: wireless force sensing body protectors for martial arts\n",
      "Ubiquitous and Wearable Computing both have the goal of pushing the computer into the background, supporting all kinds of human activities. Application areas include areas such as everyday environments (e.g. clothing, home, office), promoting new forms of creative learning via physical/virtual objects, and new tools for interactive design. In this paper, we thrust ubiquitous computing into the extremely hostile environment of the sparring ring of a martial art competition. Our system uses piezoelectric force sensors that transmit signals wirelessly to enable the detection of when a significant impact has been delivered to a competitor's body. The objective is to support the judges in scoring the sparring matches accurately, while preserving the goal of merging and blending into the background of the activity. The system therefore must take into account of the rules of the game, be responsive in real-time asynchronously, and often cope with untrained operators of the system. We present a pilot study of the finished prototype and detail our experience.\n",
      "=============================\n",
      "UIST'007 (panel): where will we be ten years from now?\n",
      "The conference this year is the tenth anniversary of UIST. The keynote talk discusses the history of UIST over the last ten years; this panel looks into the future of the field over the next ten. Each of the panelists will describe a scenario for what life will be like when we meet for UIST’O’I, ten years from now. They will also have a chance to challenge or question each others’ scenarios and to participate in open discussion with the audience.\n",
      "=============================\n",
      "The drawing assistant: automated drawing guidance and feedback from photographs\n",
      "We present an interactive drawing tool that provides automated guidance over model photographs to help people practice traditional drawing-by-observation techniques. The drawing literature describes a number of techniques to %support this task and help people gain consciousness of the shapes in a scene and their relationships. We compile these techniques and derive a set of construction lines that we automatically extract from a model photograph. We then display these lines over the model to guide its manual reproduction by the user on the drawing canvas. Finally, we use shape-matching to register the user's sketch with the model guides. We use this registration to provide corrective feedback to the user. Our user studies show that automatically extracted construction lines can help users draw more accurately. Furthermore, users report that guidance and corrective feedback help them better understand how to draw.\n",
      "=============================\n",
      "Acoustic barcodes: passive, durable and inexpensive notched identification tags\n",
      "We present acoustic barcodes, structured patterns of physical notches that, when swiped with e.g., a fingernail, produce a complex sound that can be resolved to a binary ID. A single, inexpensive contact microphone attached to a surface or object is used to capture the waveform. We present our method for decoding sounds into IDs, which handles variations in swipe velocity and other factors. Acoustic barcodes could be used for information retrieval or to triggering interactive functions. They are passive, durable and inexpensive to produce. Further, they can be applied to a wide range of materials and objects, including plastic, wood, glass and stone. We conclude with several example applications that highlight the utility of our approach, and a user study that explores its feasibility.\n",
      "=============================\n",
      "QuME: a mechanism to support expertise finding in online help-seeking communities\n",
      "Help-seeking communities have been playing an increasingly critical role in the way people seek and share information. However, traditional help-seeking mechanisms of these online communities have some limitations. In this paper, we describe an expertise-finding mechanism that attempts to alleviate the limitations caused by not knowing users' expertise levels. As a result of using social network data from the online community, this mechanism can automatically infer expertise level. This allows, for example, a question list to be personalized to the user's expertise level as well as to keyword similarity. We believe this expertise location mechanism will facilitate the development of next generation help-seeking communities.\n",
      "=============================\n",
      "Summarizing personal web browsing sessions\n",
      "We describe a system, implemented as a browser extension, that enables users to quickly and easily collect, view, and share personal Web content. Our system employs a novel interaction model, which allows a user to specify webpage extraction patterns by interactively selecting webpage elements and applying these patterns to automatically collect similar content. Further, we present a technique for creating visual summaries of the collected information by combining user labeling with predefined layout templates. These summaries are interactive in nature: depending on the behaviors encoded in their templates, they may respond to mouse events, in addition to providing a visual summary. Finally, the summaries can be saved or sent to others to continue the research at another place or time. Informal evaluation shows that our approach works well for popular websites, and that users can quickly learn this interaction model for collecting content from the Web.\n",
      "=============================\n",
      "PortraitSketch: face sketching assistance for novices\n",
      "We present PortraitSketch, an interactive drawing system that helps novices create pleasing, recognizable face sketches without requiring prior artistic training. As the user traces over a source portrait photograph, PortraitSketch automatically adjusts the geometry and stroke parameters (thickness, opacity, etc.) to improve the aesthetic quality of the sketch. We present algorithms for adjusting both outlines and shading strokes based on important features of the underlying source image. In contrast to automatic stylization systems, PortraitSketch is designed to encourage a sense of ownership and accomplishment in the user. To this end, all adjustments are performed in real-time, and the user ends up directly drawing all strokes on the canvas. The findings from our user study suggest that users prefer drawing with some automatic assistance, thereby producing better drawings, and that assistance does not decrease the perceived level of involvement in the creative process.\n",
      "=============================\n",
      "OnObject: gestural play with tagged everyday objects\n",
      "Many Tangible User Interface (TUI) systems employ sensor-equipped physical objects. However they do not easily scale to users' actual environments; most everyday objects lack the necessary hardware, and modification requires hardware and software development by skilled individuals. This limits TUI creation by end users, resulting in inflexible interfaces in which the mapping of sensor input and output events cannot be easily modified reflecting the end user's wishes and circumstances. We introduce OnObject, a small device worn on the hand, which can program physical objects to respond to a set of gestural triggers. Users attach RFID tags to situated objects, grab them by the tag, and program their responses to grab, release, shake, swing, and thrust gestures using a built-in button and a microphone. In this paper, we demonstrate how novice end users including preschool children can instantly create engaging gestural object interfaces with sound feedback from toys, drawings, or clay.\n",
      "=============================\n",
      "The AHI: an audio and haptic interface for contact interactions\n",
      "We have implemented a computer interface that renders synchronized auditory and haptic stimuli with very low (0.5ms) latency. The audio and haptic interface (AHI) includes a Pantograph haptic device that reads position input from a user and renders force output based on this input. We synthesize audio by convolving the force profile generated by user interaction with the impulse response of the virtual surface. Auditory and haptic modes are tightly coupled because we produce both stimuli from the same force profile. We have conducted a user study with the AHI to verify that the 0.5ms system latency lies below the perceptual threshold for detecting separation between auditory and haptic contact events. We discuss future applications of the AHI for further perceptual studies and for synthesizing continuous contact interactions in virtual environments.\n",
      "=============================\n",
      "RubberEdge: reducing clutching by combining position and rate control with elastic feedback\n",
      "Position control devices enable precise selection, but significant clutching degrades performance. Clutching can be reduced with high control-display gain or pointer acceleration, but there are human and device limits. Elastic rate control eliminates clutching completely, but can make precise selection difficult. We show that hybrid position-rate control can outperform position control by 20% when there is significant clutching, even when using pointer acceleration. Unlike previous work, our RubberEdge technique eliminates trajectory and velocity discontinuities. We derive predictive models for position control with clutching and hybrid control, and present a prototype RubberEdge position-rate control device including initial user feedback.\n",
      "=============================\n",
      "Mirage: exploring interaction modalities using off-body static electric field sensing\n",
      "Mirage proposes an effective non body contact technique to infer the amount and type of body motion, gesture, and activity. This approach involves passive measurement of static electric field of the environment flowing through sense electrode. This sensing method leverages electric field distortion by the presence of an intruder (e.g. human body). Mirage sensor has simple analog circuitry and supports ultra-low power operation. It requires no instrumentation to the user, and can be configured as environmental, mobile, and peripheral-attached sensor. We report on a series of experiments with 10 participants showing robust activity and gesture recognition, as well as promising results for robust location classification and multiple user differentiation. To further illustrate the utility of our approach, we demonstrate real-time interactive applications including activity monitoring, and two games which allow the users to interact with a computer using body motion and gestures.\n",
      "=============================\n",
      "Using properties for uniform interaction in the Presto document system\n",
      "Most document or information management systems rely on hierarchies to organise documents (e.g. files, email messages or web bookmarks). However, the rigid structures of hierarchical schemes do not mesh well with the more fluid nature of everyday document practices. This paper describes Presto, a prototype system that allows users to organise their documents entirely in terms of the properties those documents hold for users. Properties provide a uniform mechanism for managing, coding, searching, retrieving and interacting with documents. We concentrate in particular on the challenges that property-based approaches present and the architecture we have developed to tackle them.\n",
      "=============================\n",
      "From desktop to phonetop: a UI for web interaction on very small devices\n",
      "While it is generally accepted that new Internet terminals should leverage the installed base of Web content and services, the differences between desktop computers and very small devices makes this challenging. Indeed, the browser interaction model has evolved on desktop computers having a unique combination of user interface (large display, keyboard, pointing device), hardware, and networking capabilities. In contrast, Internet enabled cell phones, typically with 3-10 lines of text, sacrifice usability as Web terminals in favor of portability and other functions. Based on our earlier experiences building and using a Web browser for small devices we propose a new UI that splits apart the integrated activities of link following and reading into separate modes: navigating to; and acting on web content. This interaction technique for very small devices is both simpler for navigating and allows users to do more than just read. The M-Links system incorporates modal browsing interaction and addresses a number of associated problems. We have built our system with an emphasis on simplicity and user extensibility and describe the design, implementation and evolution of the user interface.\n",
      "=============================\n",
      "Detecting tapping motion on the side of mobile devices by probabilistically combining hand postures\n",
      "We contribute a novel method for detecting finger taps on the different sides of a smartphone, using the built-in motion sensors of the device. In particular, we discuss new features and algorithms that infer side taps by probabilistically combining estimates of tap location and the hand pose--the hand holding the device. Based on a dataset collected from 9 participants, our method achieved 97.3% precision and 98.4% recall on tap event detection against ambient motion. For detecting single-tap locations, our method outperformed an approach that uses inferred hand postures deterministically by 3% and an approach that does not use hand posture inference by 17%. For inferring the location of two consecutive side taps from the same direction, our method outperformed the two baseline approaches by 6% and 17% respectively. We discuss our insights into designing the detection algorithm and the implication on side tap-based interaction behaviors.\n",
      "=============================\n",
      "Comparing the programming demands of single-user and multi-user applications\n",
      "Synchronous multi-user applications are designed to support two or more simultaneous users. The RENDEZVOUSTM1 system is an infrastructure for building such multi-user applications. Several multi-user applications, such as a tic-tac-toe game. a multi-user CardTable application, and a multi-user whiteboard have been or are being constructed with the RENDEZVOUS system. We argue that there are at least three dimensions of programming complexity that are differentially affected by the programming of multi-user applications as compared to the programming of single-user applications. The first, concurrency, addresses the need to cope with parallel activities. The second dimension, abstraction, addresses the need to separate the user-int~rface from an underlying application abstraction, The thmd dimension, roles, addresses the need to differentially characterize users and customize the user-interface appropriately. Certainly, single-user applications often deal with these complexities; we argue that multi-user applications cannot avoid them. [Keyworcls: User Interface Management System, Computer-Supported Cooperative Work, groupware, programming, dialogue separation, concurrency, roles]\n",
      "=============================\n",
      "TouchString: a flexible linear multi-touch sensor for prototyping a freeform multi-touch surface\n",
      "We propose the concept of prototyping a multi-touch surface of an arbitrary form using a flexible linear multi-touch sensor that we call TouchString. We defined the conceptual structure of a TouchString, and implemented an example prototype of a TouchString. We verified the feasibility of the concept by demonstrating a few basic application scenarios using the prototype.\n",
      "=============================\n",
      "Expert crowdsourcing with flash teams\n",
      "We introduce flash teams, a framework for dynamically assembling and managing paid experts from the crowd. Flash teams advance a vision of expert crowd work that accomplishes complex, interdependent goals such as engineering and design. These teams consist of sequences of linked modular tasks and handoffs that can be computationally managed. Interactive systems reason about and manipulate these teams' structures: for example, flash teams can be recombined to form larger organizations and authored automatically in response to a user's request. Flash teams can also hire more people elastically in reaction to task needs, and pipeline intermediate output to accelerate completion times. To enable flash teams, we present Foundry, an end-user authoring platform and runtime manager. Foundry allows users to author modular tasks, then manages teams through handoffs of intermediate work. We demonstrate that Foundry and flash teams enable crowdsourcing of a broad class of goals including design prototyping, course development, and film animation, in half the work time of traditional self-managed teams.\n",
      "=============================\n",
      "MixT: automatic generation of step-by-step mixed media tutorials\n",
      "Users of complex software applications often learn concepts and skills through step-by-step tutorials. Today, these tutorials are published in two dominant forms: static tutorials composed of images and text that are easy to scan, but cannot effectively describe dynamic interactions; and video tutorials that show all manipulations in detail, but are hard to navigate. We hypothesize that a mixed tutorial with static instructions and per-step videos can combine the benefits of both formats. We describe a comparative study of static, video, and mixed image manipulation tutorials with 12 participants and distill design guidelines for mixed tutorials. We present MixT, a system that automatically generates step-by-step mixed media tutorials from user demonstrations. MixT segments screencapture video into steps using logs of application commands and input events, applies video compositing techniques to focus on salient infor-mation, and highlights interactions through mouse trails. Informal evaluation suggests that automatically generated mixed media tutorials were as effective in helping users complete tasks as tutorials that were created manually.\n",
      "=============================\n",
      "Using light emitting diode arrays as touch-sensitive input and output devices\n",
      "Light Emitting Diodes (LEDs) offer long life, low cost, efficiency, brightness, and a full range of colors. Because of these properties, they are widely used for simple displays in electronic devices. A previously characterized, but little known property of LEDs allows them to be used as photo sensors. In this paper, we show how this capability can be used to turn unmodified, off the shelf, LED arrays into touch sensitive input devices (while still remaining capable of producing output). The technique is simple and requires little or no extra hardware - in some cases operating with the same micro-controller based circuitry normally used to produce output, requiring only software changes. We will describe a simple hybrid input/output device prototype implemented with this technique, and discuss the design opportunities that this type of device opens up.\n",
      "=============================\n",
      "Inkantatory paper: dynamically color-changing prints with multiple functional inks\n",
      "We propose an effective combination of multiple functional inks, including conductive silver ink, thermo-chromic ink, and regular inkjet ink, for a novel paper-based interface called Inkantatory Paper that can dynamically change the color of its printed pattern. Constructed with off-the-shelf inkjet printing using silver conductive ink, our system enables users to fabricate thin, flat, flexible, and low-cost interactive paper. We evaluated the characteristics of the conductive silver ink as a heating system for the thermo-chromic ink and created applications demonstrating the usability of the system.\n",
      "=============================\n",
      "A tongue input device for creating conversations\n",
      "We present a new tongue input device, the tongue joystick, for use by an actor inside an articulated-head character costume. Using our device, the actor can maneuver through a dialogue tree, selecting clips of prerecorded audio to hold a conversation in the voice of the character. The device is constructed of silicone sewn with conductive thread, a unique method for creating rugged, soft, low-actuation force devices. This method has application for entertainment and assistive technology. We compare our device against other portable mouth input devices, showing it to be the fastest and most accurate in tasks mimicking our target application. Finally, we show early results of an actor inside an articulated-head costume using the tongue joystick to interact with a child.\n",
      "=============================\n",
      "Scope: automated generation of graphical interfaces\n",
      "We describe the design and prototype implementation of Scope, a system that generates graphical user interfaces for applications programmed in C++. The programmer chooses application data objects and functions that define the capabilities of the interface. At runtime, an interface design component, implemented as a set of production system rules, transforms this semantic specification into an interface built using a window system, an associated user interface toolkit, and the hardware input devices available on the system. The rules match application requirements against a semantic description of the toolkit, selecting virtual devices for input, output, and layout. Thus, Scope uses design rules to create interfaces from high-level programming semantics that are customized both for the application and the run-time environment.\n",
      "=============================\n",
      "Specifying label layout style by example\n",
      "Creating high-quality label layouts in a particular visual style is a time-consuming process. Although automated labeling algorithms can aid the layout process, expert design knowledge is required to tune these algorithms so that they produce layouts which meet the designer's expectations. We propose a system which can learn a labellayout style from a single example layout and then apply this style to new labeling problems. Because designers find it much easier to create example layouts than tune algorithmic parameters, our system provides a more natural workflow for graphic designers. We demonstrate that our system is capable of learning a variety of label layout styles from examples.\n",
      "=============================\n",
      "SenSkin: adapting skin as a soft interface\n",
      "We present a sensing technology and input method that uses skin deformation estimated through a thin band-type device attached to the human body, the appearance of which seems socially acceptable in daily life. An input interface usually requires feedback. SenSkin provides tactile feedback that enables users to know which part of the skin they are touching in order to issue commands. The user, having found an acceptable area before beginning the input operation, can continue to input commands without receiving explicit feedback. We developed an experimental device with two armbands to sense three-dimensional pressure applied to the skin. Sensing tangential force on uncovered skin without haptic obstacles has not previously been achieved. SenSkin is also novel in that quantitative tangential force applied to the skin, such as that of the forearm or fingers, is measured. An infrared (IR) reflective sensor is used since its durability and inexpensiveness make it suitable for everyday human sensing purposes. The multiple sensors located on the two armbands allow the tangential and normal force applied to the skin dimension to be sensed. The input command is learned and recognized using a Support Vector Machine (SVM). Finally, we show an application in which this input method is implemented.\n",
      "=============================\n",
      "From research prototypes to usable, useful systems: lessons learned in the trenches\n",
      "INTRODUCTION A significant amount of innovation from research labs and universities is wastec& it is never applied in systems that the actual users can or want to use. The process of going frc~m a novel concept to a usable, useful system is poorly understood by most researchers. The purpose of this panel is to address how research done in research labs and universities can be converted into systems that the end users would use. Each of the panelists has built at least one substantial system which is currently being used by a large community of real users (other than the team that built the system). Based on their experience, they will make recommendations that, if followed early enough in the project, would make the conversion to usable systems faster and easier.\n",
      "=============================\n",
      "ActiveText: a method for creating dynamic and interactive texts\n",
      "This paper describes ActiveText, a method for creating dynamic and interactive texts. ActiveText uses an object-based hierarchy to represent texts. This hierarchy makes it easy to work with the ASCII component and pixel component of the text at the same time. Static, dynamic and interactive properties of text can be easily intermixed and layered. The user can enter and edit text, adjust static and dynamic layout, apply dynamic and interactive behaviors, and adjust their parameters with a common set of tools and a common interface. Support for continuous editing allows the user to sketch dynamically. A prototype application called It's Alive! has been implemented to explore the ActiveText functionality. The documents produced by It's Alive! can be of use in a wide-range of areas, including chat-spaces, email, web-sites, fiction and poetry writing, and low-end film & video titling.\n",
      "=============================\n",
      "WEST: a Web browser for small terminals\n",
      "We describe WEST, a WEb browser for Small Terminals, that aims to solve some of the problems associated with accessing web pages on hand-held devices. Through a novel combination of text reduction and focus+context visualization, users can access web pages from a very limited display environment, since the system will provide an overview of the contents of a web page even when it is too large to be displayed in its entirety. To make maximum use of the limited resources available on a typical hand-held terminal, much of the most demanding work is done by a proxy server, allowing the terminal to concentrate on the task of providing responsive user interaction. The system makes use of some interaction concepts reminiscent of those defined in the Wireless Application Protocol (WAP), making it possible to utilize the techniques described here for WAP-compliant devices and services that may become available in the near future.\n",
      "=============================\n",
      "Detecting student frustration based on handwriting behavior\n",
      "Detecting states of frustration among students engaged in learning activities is critical to the success of teaching assistance tools. We examine the relationship between a student's pen activity and his/her state of frustration while solving handwritten problems. Based on a user study involving mathematics problems, we found that our detection method was able to detect student frustration with a precision of 87% and a recall of 90%. We also identified several particularly discriminative features, including writing stroke number, erased stroke number, pen activity time, and air stroke speed.\n",
      "=============================\n",
      "GaussStones: shielded magnetic tangibles for multi-token interactions on portable displays\n",
      "This work presents GaussStones, a system of shielded magnetic tangibles design for supporting multi-token interactions on portable displays. Unlike prior works in sensing magnetic tangibles on portable displays, the proposed tangible design applies magnetic shielding by using an inexpensive galvanized steel case, which eliminates interference between magnetic tangibles. An analog Hall-sensor grid can recognize the identity of each shielded magnetic unit since each unit generates a magnetic field with a specific intensity distribution and/or polarization. Combining multiple units as a knob further allows for resolving additional identities and their orientations. Enabling these features improves support for applications involving multiple tokens. Thus, using prevalent portable displays provides generic platforms for tangible interaction design.\n",
      "=============================\n",
      "Stylus user interfaces for manipulating text\n",
      "Abstract This paper is concerned with pen-based (also called stylus-based) computers. Two of the key questions for such computers are how to interface to handwriting recognition algorithms, and whether there are interfaces that can effectively exploit the differences between a stylus and a keyboard/mouse. We describe prototypes that explore each of these questions. Our text entry tool is designed around the idea that handwriting recognition algorithms will always be error prone, and has a different flavor from existing systems. Our prototype editor goes beyond the usual gesture editors used with styli and is based on the idea of leaving the markups visible.\n",
      "=============================\n",
      "A tongue training system for children with down syndrome\n",
      "Children with Down syndrome have a variety of symptoms including speech and swallowing disorders. To improve these symptoms, tongue training is thought to be beneficial. However, inducing children with Down syndrome to do such training is not easy because tongue training can be an unpleasant experience for children. In addition, with no supporting technology for such training, teachers and families around such children must make efforts to induce them to undergo the training. In this research, we develop an interactive tongue training system especially for children with Down syndrome using SITA (Simple Interface for Tongue motion Acquisition) system. In this paper, we describe in detail our preliminary evaluations of SITA, and present the results of user tests.\n",
      "=============================\n",
      "Easily adding animations to interfaces using constraints\n",
      "ABSTRACTAdding animation to interfaces is a very difficult task withtoday’s toolkits, even though there are many situations inwhich it would be useful and effective. The Amulet toolkitcontains a new form of animation constraint that allowsanimations to be added to interfaces extremely easily with-out changing the logic of the application or the graphicalobjects themselves. An animation constraint detectschanges to the value of the slot to which it is attached, andcauses the slot to instead take on a series of values interpo-lated between the original and new values. The advantageover previous approaches is that animation constraintsprovide significantly better modularity and reuse. Theprogrammer has independent control over the graphics tobe animated, the start and end values of the animation, thepath through value space, and the timing of the animation.Animations can be attached to any object, even existingwidgets from the toolkit, and any type of value can beanimated: scalars, coordinates, fonts, colors, line widths,point lists (for polygons), booleans (for visibility), etc. Alibrary of useful animation constraints is provided in thetoolkit, including support for exaggerated, cartoon-styleeffects such as slow-in-slow-out, anticipation, and follow-through. Because animations can be added to an existingapplication with only a single extra line of code, we expectthat this new mechanism will make it easy for researchersand developers to investigate the use of animations in awide variety of applications.Keywords: Animation, Constraints, Toolkits, User Inter-face Development Environments, Amulet.To appear in UIST’96: ACM Symposium onUser Interface Software and Technology, Nov.6-8, 1996. Seattle, WAINTRODUCTIONBy providing better modularity for the software for userinterfaces, the Amulet toolkit [8] achieves increased reuseand decreased code size, and makes it easier for research-ers and developers to create applications. For example, inAmulet, the interactive behavior of objects can be definedentirely independently from their graphical look by attach-ing “Interactor” objects to the graphics. Command obje cts[9] encapsulate the complete information about operations,and can be hierarchically linked so each application layercan be separately defined. We have followed this philoso-phy in our new support for animations and other time-based behaviors. The goal is to make simple animationsextremely easy to add to an interface, and still supportcomplex animations. This is achieved by allowing theanimations to be defined independently from the actionsthat start the animation and the graphics that are ani-mated. Furthermore, the animation itself is modularized:the start and end values of the animation, the path throughthe value space, and the timing of the animation can all beindependently specified.We are able to provide this new level of modularity by us-ing the flexible Amulet constraint system. If an animationconstraint is attached to a slot (instance variable) of anobject, then changes to that slot trigger the animation,causing the slot’s value to change smoothly from its old toits new value. For example, if a slot contains 10 and is setto be 100, an animation constraint might immediatelyoverride the value and set the slot with 11, and then set theslot with a series of values up to 100 over a period of twoseconds. Since there are a wide variety of parameterizedanimation constraints provided in the library, an anima-tion can be added to an existing interface with a single lineof code. All the messy details of timing, redrawing, set-upand clean-up of animations, and interactions with otherparts of the system, are automatically handled by the ani-mation constraint mechanism.We are particularly interested in the use of animations toenhance interaction in regular user interfaces. Althoughanimations are widely used in games and specializedvisualization software ([10] has a survey), their use in con-ventional interfaces is limited to a few predefined cases,\n",
      "=============================\n",
      "Blending structured graphics and layout\n",
      "Conventional windowing environments provide separate classes of objects for user interface components, or “widgets,” and graphical objects. Widgets negotiate layout and can be resized as rectangles, while graphics may be shared, transformed, transparent, and overlaid. This presents a major obstacle to applications like user interface builders and compound document editors where the manipulated objects need to behave both like graphics and widgets.Fresco[1] blends graphics and widgets into a single class of objects. We have an implementation of Fresco and an editor called Fdraw that allows graphical objects to be composed like widgets, and widgets to be transformed and shared like graphics. Performance measurements of Fdraw show that sharing reduces memory usage without slowing down redisplay.\n",
      "=============================\n",
      "Identifying emergent behaviours from longitudinal web use\n",
      "Laboratory studies present difficulties in the understanding of how usage evolves over time. Employed observations are obtrusive and not naturalistic. Our system employs a remote capture tool that provides longitudinal low-level interaction data. It is easily deployable into any Web site allowing deployments in-the-wild and is completely unobtrusive. Web application interfaces are designed assuming users' goals. Requirement specifications contain well defined use cases and scenarios that drive design and subsequent optimisations. Users' interaction patterns outside the expected ones are not considered. This results in an optimisation for a stylised user rather than a real one. A bottom-up analysis from low-level interaction data makes possible the emergence of users' tasks. Similarities among users can be found and solutions that are effective for real users can be designed. Factors such as learnability and how interface changes affect users are difficult to observe in laboratory studies. Our solution makes it possible, adding a longitudinal point of view to traditional laboratory studies. The capture tool is deployed in real world Web applications capturing in-situ data from users. These data serve to explore analysis and visualisation possibilities. We present an example of the exploration results with one Web application.\n",
      "=============================\n",
      "Voice as sound: using non-verbal voice input for interactive control\n",
      "We describe the use of non-verbal features in voice for direct control of interactive applications. Traditional speech recognition interfaces are based on an indirect, conversational model. First the user gives a direction and then the system performs certain operation. Our goal is to achieve more direct, immediate interaction like using a button or joystick by using lower-level features of voice such as pitch and volume. We are developing several prototype interaction techniques based on this idea, such as \"control by continuous voice\", \"rate-based parameter control by pitch,\" and \"discrete parameter control by tonguing.\" We have implemented several prototype systems, and they suggest that voice-as-sound techniques can enhance traditional voice recognition approach.\n",
      "=============================\n",
      "Connectables: dynamic coupling of displays for the flexible creation of shared workspaces\n",
      "We present the ConnecTable, a new mobile, networked and context-aware information appliance that provides affordances for pen-based individual and cooperative work as well as for the seamless transition between the two. In order to dynamically enlarge an interaction area for the purpose of shared use, a flexible coupling of displays has been realized that overcomes the restrictions of display sizes and borders. Two ConnecTable displays dynamically form a homogeneous display area when moved close to each other. The appropriate triggering signal comes from built-in sensors allowing users to temporally combine their individual displays to a larger shared one by a simple physical movement in space. Connected ConnecTables allow their users to work in parallel on an ad-hoc created shared workspace as well as exchanging information by simply shuffling objects from one display to the other. We discuss the user interface and related issues as well as the software architecture. We also present the physical realization of the ConnecTables.\n",
      "=============================\n",
      "An empirical study of constraint usage in graphical applications\n",
      "One-way constraints have been widely incorporated in research toolkits for constructing graphical applications. However, although a number of studies have examined the performance of these toolkits’ constraint satisfaction algorithms, there have not been any empirical studies that have examined how programmers use constraints in actual applications. This paper reports the results of a study intended to address these matters. Seven graphical applications were chosen for their diversity and profiling information was gathered about their use of constraints, The data reveal that constraint networks tend to be modular, that is, divided into a number c~fsmall, independent sets of constraints rather than one mono] ithic set of constraints. This finding suggests that constraint satisfaction algorithms should be able to resatisfy constraints rapidly after a change to one or more variables. It also suggests that debugging constraints should not be unduly burdensome on a programmer since the number of constraints that must be examined to find the source of an error is not terriblly large. Overall, the results of this study should provide a repository of data that will be useful in directing future research on optimizing constraint solvers and developing effective debugging techniques.\n",
      "=============================\n",
      "Ensemble: a graphical user interface development system for the design and use of interactive toolkits\n",
      "User Interface Development Systems (UIDS), as opposed to User Interface Management Systems or UI Toolkits focus on supporting the design and implementation of the user interface. This paper describes Ensemble, an experimental UIDS that begins to explore the electronic creation of interaction techniques as well as the corresponding design processes. Issues related to the impact on the components of the development system are discussed. Finally, problems with the current implementation and future directions are presented.\n",
      "=============================\n",
      "Point and share: from paper to whiteboard\n",
      "Traditional writing instruments have the potential to enable new forms of interactions and collaboration though digital enhancement. This work specifically enables the user to utilize pen and paper as input mechanisms for content to be displayed on a shared interactive whiteboard. We introduce a pen cap with an infrared led, an actuator and a switch. Pointing the pen cap at the whiteboard allows users to select and position a \"canvas\" on the whiteboard to display handwritten text while the actuator enables resizing the canvas and the text. It is conceivable that anything one can write on paper anywhere, could be displayed on an interactive whiteboard.\n",
      "=============================\n",
      "Side views: persistent, on-demand previews for open-ended tasks\n",
      "We introduce Side Views, a user interface mechanism that provides on-demand, persistent, and dynamic previews of commands. Side Views are designed to explicitly support the practices and needs of expert users engaged in openended tasks. In this paper, we summarize results from field studies of expert users that motivated this work, then discuss the design of Side Views in detail. We show how Side Views' design affords their use as tools for clarifying, comparing, and contrasting commands; generating alternative visualizations; experimenting without modifying the original data (i.e., \"what-if\" tools); and as tools that support the serendipitous discovery of viable alternatives. We then convey lessons learned from implementing Side Views in two sample applications, a rich text editor and an image manipulation application. These contributions include a discussion of how to implement Side Views for commands with parameters, for commands that require direct user input (such as mouse strokes for a paint program), and for computationally-intensive commands.\n",
      "=============================\n",
      "3D widgets for exploratory scientific visualization\n",
      "Scientists use a variety of visualization techniques to help understand computational fluid dynamics (CFD) datasets, but the interfaces to these techniques are generally two-dimensional and therefore are separated from the 3D view. Both rapid interactive exploration of datasets and precise control over the parameters and placement of visualization techniques are required to understand complex phenomena contained in these datasets. In this paper, we present work in progress on a 3D user interface for exploratory visualization of these datasets.\n",
      "=============================\n",
      "Interactive graph layout\n",
      "This paper presents a novel methodology for viewing large graphs. The basic concept is to allow the user to interactively navigate through large graphs learning about them in appropriately small and concise pieces. An architecture is present to support graph exploration. It contains methods for building custom layout algorithms hierarchically, interactively decomposing large graphs, and creating interactive parameterized layout algorithms. As a proof of concept, examples are drawn from a working prototype that incorporates this methodology.\n",
      "=============================\n",
      "Interactions in the air: adding further depth to interactive tabletops\n",
      "Although interactive surfaces have many unique and compelling qualities, the interactions they support are by their very nature bound to the display surface. In this paper we present a technique for users to seamlessly switch between interacting on the tabletop surface to above it. Our aim is to leverage the space above the surface in combination with the regular tabletop display to allow more intuitive manipulation of digital content in three-dimensions. Our goal is to design a technique that closely resembles the ways we manipulate physical objects in the real-world; conceptually, allowing virtual objects to be 'picked up' off the tabletop surface in order to manipulate their three dimensional position or orientation. We chart the evolution of this technique, implemented on two rear projection-vision tabletops. Both use special projection screen materials to allow sensing at significant depths beyond the display. Existing and new computer vision techniques are used to sense hand gestures and postures above the tabletop, which can be used alongside more familiar multi-touch interactions. Interacting above the surface in this way opens up many interesting challenges. In particular it breaks the direct interaction metaphor that most tabletops afford. We present a novel shadow-based technique to help alleviate this issue. We discuss the strengths and limitations of our technique based on our own observations and initial user feedback, and provide various insights from comparing, and contrasting, our tabletop implementations\n",
      "=============================\n",
      "A framework for shared applications with a replicated architecture\n",
      "The interaction history of a document can be modelled as a tree of command objects. This model not only supports recovery (undo/redo), but is also suitable for cooperation between distributed users working on a common document. Various coupling modes can be supported. Switching between modes is supported by regarding different versions of a document as different branches of the history. Branches can then be merged using a selective redo mechanism. Synchronous cooperation is supported by replicating the document state and exchanging command objects. Optimistic concurrency control can be applied, because conflicting actions can later be undone automatically.\n",
      "=============================\n",
      "Achieving higher magnification in context\n",
      "The difficulty of accessing information details while preserving context has generated many different focus-in-context techniques. A common limitation of focus-in-context techniques is their ability to work well at high magnification. We present a set of improvements that will make high magnification in context more feasible. We demonstrate new distortion functions that effectively integrate high magnification within its context. Finally, we show how lenses can be used on top of other lenses, effectively multiplying their magnification power in the same manner that a magnifying glass applied on top of another causes multiplicative magnification. The combined effect is to change feasible detail-in-context magnification factors from less than 8 to more than 40.\n",
      "=============================\n",
      "Towards more paper-like input: flexible input devices for foldable interaction styles\n",
      "This paper presents Foldable User Interfaces (FUI), a combination of a 3D GUI with windows imbued with the physics of paper, and Foldable Input Devices (FIDs). FIDs are sheets of paper that allow realistic transformations of graphical sheets in the FUI. Foldable input devices are made out of construction paper augmented with IR reflectors, and tracked by computer vision. Window sheets can be picked up and flexed with simple movements and deformations of the FID. FIDs allow a diverse lexicon of one-handed and two-handed interaction techniques, including folding, bending, flipping and stacking. We show how these can be used to ease the creation of simple 3D models, but also for tasks such as page navigation.\n",
      "=============================\n",
      "Dynamic approximation of complex graphical constraints by linear constraints\n",
      "Current constraint solving techniques for interactive graphical applications cannot satisfactorily handle constraints such as non-overlap, or containment within non-convex shapes or shapes with smooth edges. We present a generic new technique for efficiently handling such kinds of constraints based on trust regions and linear arithmetic constraint solving. Our approach is to model these more complex constraints by a dynamically changing conjunction of linear constraints. At each stage, these give a local approximation to the complex constraints. During direct manipulation, linear constraints in the current local approximation can become active indicating that the current solution is on the boundary of the trust region for the approximation. The associated complex constraint is notified and it may choose to modify the current linear approximation. Empirical evaluation demonstrates that it is possible to (re-)solve systems of linear constraints that are dynamically approximating complex constraints such as non-overlap sufficiently quickly to support direct manipulation in interactive graphical applications.\n",
      "=============================\n",
      "A reconfigurable ferromagnetic input device\n",
      "We present a novel hardware device based on ferromagnetic sensing, capable of detecting the presence, position and deformation of any ferrous object placed on or near its surface. These objects can include ball bearings, magnets, iron filings, and soft malleable bladders filled with ferrofluid. Our technology can be used to build reconfigurable input devices -- where the physical form of the input device can be assembled using combinations of such ferrous objects. This allows users to rapidly construct new forms of input device, such as a trackball-style device based on a single large ball bearing, tangible mixers based on a collection of sliders and buttons with ferrous components, and multi-touch malleable surfaces using a ferrofluid bladder. We discuss the implementation of our technology, its strengths and limitations, and potential application scenarios.\n",
      "=============================\n",
      "EdgeWrite: a stylus-based text entry method designed for high accuracy and stability of motion\n",
      "EdgeWrite is a new unistroke text entry method for handheld devices designed to provide high accuracy and stability of motion for people with motor impairments. It is also effective for able-bodied people. An EdgeWrite user enters text by traversing the edges and diagonals of a square hole imposed over the usual text input area. Gesture recognition is accomplished not through pattern recognition but through the sequence of corners that are hit. This means that the full stroke path is unimportant and recognition is highly deterministic, enabling better accuracy than other gestural alphabets such as Graffiti. A study of able-bodied users showed subjects with no prior experience were 18% more accurate during text entry with Edge Write than with Graffiti (p>.05), with no significant difference in speed. A study of 4 subjects with motor impairments revealed that some of them were unable to do Graffiti, but all of them could do Edge Write. Those who could do both methods had dramatically better accuracy with Edge Write.\n",
      "=============================\n",
      "High-performance pen + touch modality interactions: a real-time strategy game eSports context\n",
      "We used the situated context of real-time strategy (RTS) games to address the design and evaluation of new pen + touch interaction techniques. RTS play is a popular genre of Electronic Sports (eSports), games played and spectated at an extremely high level. Interaction techniques are critical for eSports players, because they so directly impact performance. Through this process, new techniques and implications for pen + touch and bi-manual interaction emerged. We enhance non-dominant hand (NDH) interaction with edge-constrained affordances, anchored to physical features of interactive sur- faces, effectively increasing target width. We develop bi-manual overloading, an approach to reduce the total number of occurrences of NDH retargeting. The novel isosceles lasso select technique facilitates selection of complex object subsets. Pen-in-hand interaction, dominant hand touch interaction performed with the pen stowed in the palm, also emerged as an efficient and expressive interaction paradigm.\n",
      "=============================\n",
      "Two-handed gesture in multi-modal natural dialog\n",
      "Tracking both hands in free-space with accompanying speech input can augment the user's ability to communicate with computers. This paper discusses the kinds of situations which call for two-handed input and not just the single hand, and reports a prototype in which two-handed gestures serve to input concepts, both static and dynamic, manipulate displayed items, and specify actions to be taken. Future directions include enlargement of the vocabulary of two-handed “coverbal” gestures and the modulation by gaze of gestural intent.\n",
      "=============================\n",
      "Building distributed, multi-user applications by direct manipulation\n",
      "This paper describes Visual Obliq, a user interface development environment for constructing distributed, multi-user applications. Applications are created by designing the interface with a GUI-builder and embedding callback code in an interpreted language, in much the same way as one would build a traditional (non-distributed, single-user) application with a modern user interface development environment. The resulting application can be run from within the GUI-builder for rapid turnaround or as a stand-alone executable. The Visual Obliq runtime provides abstractions and support for issues specific to distributed computing, such as replication, sharing, communication, and session management. We believe that the abstractions provided, the simplicity of the programming model, the rapid turnaround time, and the applicability to heterogeneous environments, make Visual Obliq a viable tool for authoring distributed applications and groupware.\n",
      "=============================\n",
      "Programming by manipulation for layout\n",
      "We present Programming by Manipulation, a new programming methodology for specifying the layout of data visualizations, targeted at non-programmers. We address the two central sources of bugs that arise when programming with constraints: ambiguities and conflicts (inconsistencies). We rule out conflicts by design and exploit ambiguity to explore possible layout designs. Our users design layouts by highlighting undesirable aspects of a current design, effectively breaking spurious constraints and introducing ambiguity by giving some elements freedom to move or resize. Subsequently, the tool indicates how the ambiguity can be removed, by computing how the free elements can be fixed with available constraints. To support this workflow, our tool computes the ambiguity and summarizes it visually. We evaluate our work with two user-studies demonstrating that both non-programmers and programmers can effectively use our prototype. Our results suggest that our tool is 5-times more productive than direct programming with constraints.\n",
      "=============================\n",
      "FlickBoard: enabling trackpad interaction with automatic mode switching on a capacitive-sensing keyboard\n",
      "We present FlickBoard, which combines a trackpad and a keyboard into the same interaction area to reduce hand movement between separate keyboards and trackpads. It supports automatic input mode detection and switching (ie. trackpad vs keyboard mode) without explicit user input. We developed a prototype by embedding a 58x20 capacitive sensing grid into a soft keyboard cover, and uses machine learning to distinguish between moving a cursor (trackpad mode) and entering text (keyboard mode). Our prototype has a thin profile and can be placed over existing keyboards.\n",
      "=============================\n",
      "eyeCan: affordable and versatile gaze interaction\n",
      "We present eyeCan, a software system that promises rich, sophisticated, and still usable gaze interactions with low-cost gaze tracking setups. The creation of this practical system was to drastically lower the hurdle of gaze interaction by presenting easy-to-use gaze gestures, and by reducing the cost-of-entry with the utilization of low precision gaze trackers. Our system effectively compensates for the noise from tracking sensors and involuntary eye movements, boosting both the precision and speed in cursor control. Also the possible variety of gaze gestures was explored and defined. By combining eyelid actions and gaze direction cues, our system provides rich set of gaze events and therefore enables the use of sophisticated applications e.g. playing video games or navigating street view.\n",
      "=============================\n",
      "Crowds in two seconds: enabling realtime crowd-powered interfaces\n",
      "Interactive systems must respond to user input within seconds. Therefore, to create realtime crowd-powered interfaces, we need to dramatically lower crowd latency. In this paper, we introduce the use of synchronous crowds for on-demand, realtime crowdsourcing. With synchronous crowds, systems can dynamically adapt tasks by leveraging the fact that workers are present at the same time. We develop techniques that recruit synchronous crowds in two seconds and use them to execute complex search tasks in ten seconds. The first technique, the retainer model, pays workers a small wage to wait and respond quickly when asked. We offer empirically derived guidelines for a retainer system that is low-cost and produces on-demand crowds in two seconds. Our second technique, rapid refinement, observes early signs of agreement in synchronous crowds and dynamically narrows the search space to focus on promising directions. This approach produces results that, on average, are of more reliable quality and arrive faster than the fastest crowd member working alone. To explore benefits and limitations of these techniques for interaction, we present three applications: Adrenaline, a crowd-powered camera where workers quickly filter a short video down to the best single moment for a photo; and Puppeteer and A|B, which examine creative generation tasks, communication with workers, and low-latency voting.\n",
      "=============================\n",
      "Maintaining shared mental models in anesthesia crisis care with nurse tablet input and large-screen displays\n",
      "In an effort to reduce medical errors, doctors are beginning to embrace cognitive aids, such as paper-based checklists. We describe the early stage design process of an interactive cognitive aid for crisis care teams. This process included collaboration with anesthesia professors in the school of medicine and observation of medical students practicing in simulated scenarios. Based on these insights, we identify opportunities to employ large-screen displays and coordinated tablets to support team performance. We also propose a system design for interactive cognitive aids intended to encourage a shared mental model amongst crisis care staff.\n",
      "=============================\n",
      "SeeSS: seeing what i broke -- visualizing change impact of cascading style sheets (css)\n",
      "Cascading Style Sheet (CSS) is a fundamental web language for describing the presentation of web pages. CSS rules are often reused across multiple parts of a page and across multiple pages throughout a site to reduce repetition and to provide a consistent look and feel. When a CSS rule is modified, developers currently have to manually track and visually inspect all possible parts of the site that may be impacted by that change. We present SeeSS, a system that automatically tracks CSS change impact across a site and enables developers to easily visualize all of them. The impacted page fragments are sorted by severity and the differences before and after the change are highlighted using animation.\n",
      "=============================\n",
      "Enabling efficient orienteering behavior in webmail clients\n",
      "Webmail clients provide millions of end users with convenient and ubiquitous access to electronic mail - the most successful collaboration tool ever. Web email clients are also the platform of choice for recent innovations on electronic mail and for integration of related information services into email. In the enterprise, however, webmail applications have been relegated to being a supplemental tool for mail access from home or while on the road. In this paper, we draw on recent research in the area of electronic mail to understand usage models and performance requirements for enterprise email applications. We then present an innovative architecture for a webmail client. By leveraging recent advances in web browser technology, we show that webmail clients can offer performance and responsiveness that rivals a desktop application while still retaining all the advantages of a browser based client.\n",
      "=============================\n",
      "Using information murals in visualization applications\n",
      "Information visualizations must allow users to browse information spaces and focus quickly on items of interest. Navigational techniques which utilize some representation of the entire information space provide context to support more detailed information views. However, the limited number of pixels on the screen makes it difficult to completely display large information spaces. The Zrz.irrnation Mural is a twodimensional, reduced representation of an entire information space that fits entirely within a display window or screen. The mural creates a miniature version of the information space using visual attributes such as grayscale shading, intensity, color, and pixel size, along with anti-aliased compression techniques. Information murals can be used as stand-alone visualizations or in global navigational views.\n",
      "=============================\n",
      "Development of a non-contact tongue-motion acquisition system\n",
      "We present a new tongue detection system called SITA, which comprises only a Kinect device and conventional laptop computer. In contrast with other tongue-based devices, the SITA system does not require the subject to wear a device. This avoids the issue of oral hygiene and removes the risk of swallowing a device inserted in the mouth. In this paper, we introduce the SITA system and an application. To evaluate the system, a user test was conducted. The results indicate that the system could detect the tongue position in real time. Moreover, there are possibilities of training the tongue with this system.\n",
      "=============================\n",
      "Novel interaction techniques for overlapping windows\n",
      "This note presents several techniques to improve window management with overlapping windows: tabbed windows, turning and peeling back windows, and snapping and zipping windows.\n",
      "=============================\n",
      "3D-board: a whole-body remote collaborative whiteboard\n",
      "This paper presents 3D-Board, a digital whiteboard capable of capturing life-sized virtual embodiments of geographically distributed users. When using large-scale screens for remote collaboration, awareness for the distributed users' gestures and actions is of particular importance. Our work adds to the literature on remote collaborative workspaces, it facilitates intuitive remote collaboration on large scale interactive whiteboards by preserving awareness of the full-body pose and gestures of the remote collaborator. By blending the front-facing 3D embodiment of a remote collaborator with the shared workspace, an illusion is created as if the observer was looking through the transparent whiteboard into the remote user's room. The system was tested and verified in a usability assessment, showing that 3D-Board significantly improves the effectiveness of remote collaboration on a large interactive surface.\n",
      "=============================\n",
      "DDMixer2.5D: drag and drop to mix 2.5D video objects\n",
      "We propose a 2.5D video editing system called DDMixer2.5D. 2.5D video contains not only color channels but also a depth channel, which can be recorded easily using recently available depth sensors, such as Microsoft Kinect. Our system employs this depth channel to allow a user to quickly and easily edit video objects by using simple drag-and-drop gestures. For example, a user can copy a video object of a dancing figure from video to video simply by dragging and dropping using finger on the touch screen of a mobile phone handset. In addition, the user can drag to adjust the 3D position in the new video so that contact between foot and floor is preserved and the size of the body is automatically adjusted according to the depth. DDMixer2.5D has other useful functions required for practical use, including object removal, editing 3D camera path, creating of anaglyph 3D video, as well as a timeline interface.\n",
      "=============================\n",
      "Tap control for headphones without sensors\n",
      "A tap control technique for headphones is proposed. A simple circuit is used to detect tapping of the headphone shell by using the speaker unit in the headphone as a tap sensor. No additional devices are required in the headphone shell and cable, so the user can use their favorite headphones as a controller while listening music. A prototype is implemented with several calibration processes to compensate the differences in headphones and users' tapping actions. Tests confirm that the user can control a music player by tapping regular headphones.\n",
      "=============================\n",
      "GripSense: using built-in sensors to detect hand posture and pressure on commodity mobile phones\n",
      "We introduce GripSense, a system that leverages mobile device touchscreens and their built-in inertial sensors and vibration motor to infer hand postures including one- or two-handed interaction, use of thumb or index finger, or use on a table. GripSense also senses the amount of pres-sure a user exerts on the touchscreen despite a lack of direct pressure sensors by inferring from gyroscope readings when the vibration motor is \"pulsed.\" In a controlled study with 10 participants, GripSense accurately differentiated device usage on a table vs. in hand with 99.67% accuracy and when in hand, it inferred hand postures with 84.26% accuracy. In addition, GripSense distinguished three levels of pressure with 95.1% accuracy. A usability analysis of GripSense was conducted in three custom applications and showed that pressure input and hand-posture sensing can be useful in a number of scenarios.\n",
      "=============================\n",
      "Tracking indoor location and motion for navigational assistance\n",
      "Visually impaired people have a harder time remembering their way around complex unfamiliar buildings, whilst obtaining the help of a sighted guide is not always possible or desirable. By sensing the users location and motion, however, mobile phone software can provide navigational assistance in such situations, obviating the need of human guides. We present a simple to operate and highly usable mobile navigational guide that uses Wi-Fi and accelerometer sensors to help the user repeat paths that were already walked once.\n",
      "=============================\n",
      "TelePICTIVE: computer-supported collaborative GUI design for designers with diverse expertise\n",
      "It is generally accepted that it is important to involve the end users of a Graphical User Interface (GUI) in all stages of its design and development. However, traditional GUI development tools typically do not support collaborative design. TelePICTIVE is an experimental software prototype designed to allow computer-naive users to collaborate with experts at possibly remote locations in designing GUIs.TelePICTIVE is based on the PICTIVE participatory design methodology, and has been prototyped using the RENDEZVOUS system. In this paper we describe TelePICTIVE, and show how it is designed to support collaboration among a group of GUI designers with diverse levels of expertise. We also explore some of the issue that have come up during development and initial usability testing, such as how to coordinate simultaneous access to a shared design surface, and how to engage in the participatory design of GUIs using a Computer-Supported Cooperative Work (CSCW) system.\n",
      "=============================\n",
      "An assembly of soft actuators for an organic user interface\n",
      "An organic user interface (OUI) is a kind of interface that is based on natural human-human and human-physical object interaction models. In such situations, hair and fur play important roles in establishing smooth and natural communication. Animals and birds use their hair, fur and feathers to express their emotions, and groom each other when forming closer relationships. Therefore, hair and fur are potential materials for development of the ideal OUI. In this research, we propose the hairlytop interface, which is a collection of hair-like units composed of shape memory alloys, for use as an OUI. The proposed interface is capable of improving its spatial resolution and can be used to develop a hair surface on any electrical device shape.\n",
      "=============================\n",
      "Towards responsive retargeting of existing websites\n",
      "Websites need to be displayed on a panoply of different devices today, but most websites are designed with fixed widths only appropriate to browsers on workstation computers. We propose to programmatically rewrite websites into responsive formats capable of adapting to different device display sizes. To accomplish this goal, we cast retargeting as a cross-compilation problem. We decompose existing HTML pages into boxes (lexing), infer hierarchical structure between these boxes (parsing) and finally generate parameterized layouts from the hierarchical structure (code generation). This document describes preliminary work on ReMorph, a prototype 'retargeting as cross-compilation' system.\n",
      "=============================\n",
      "A pen-based device for sketching with multi-directional traction forces\n",
      "This paper presents a pen-grip-shaped device that assists in sketching using multi-directional traction forces. By using an asymmetric acceleration of the vibration actuator that drive in a linear direction, the system can create a virtual traction force with the proper direction. We augment users' drawing skills with the device that arranged 4 vibration actuators that provides a traction force and a rotary sensation. Therefore the device is portable and does not have any limitation of needing to be in a particular location, this device can be used to guide the direction and assist the user who is sketching on a large piece of paper. Moreover, users can attach it to any writing utensil such as brushes, crayons. In this paper, we describe the details of the design of device, evaluation experiments, and applications.\n",
      "=============================\n",
      "Adding a collaborative agent to graphical user interfaces\n",
      "We have implemented a collaborative agent toolkit called Collagen and used it to build a software agent that collaborates with the user of a direct-manipulation graphical interface by following the rules and conventions of human discourse. One of the main results is an interaction history that is segmented according to the structure of the agent's and user's goals, without requiring the agent to understand natural language.\n",
      "=============================\n",
      "Shape changing device for notification\n",
      "In this paper, we describe a notification method with peripheral cognition technology that uses a human cognitive characteristic. The method achieves notification without interrupting users' primary tasks. We developed a shape changing device that change its shape to notify the arrival of information. Such behavior enables a user to easily find and accept notifications without interruption when their attention on the primary task decreases. The result of an experiment showed that the successful notification rate was 45.5%.\n",
      "=============================\n",
      "Supporting distributed, concurrent, one-way constraints in user interface applications\n",
      "This paper describes Doppler a new, fast algorithm for supporting concurrent, one-way constraints between objects situated in multiple address spaces. Because of their declarative nature, convenience, low amortized cost, and good match to interface tasks, constraints have been used to support a variety of user-interface activities. Unfortunately, nearly all existing constraint maintenance algorithms are sequential in nature, and cannot function effectively in a concurrent or dktributed setting. The Doppler algorithm overcomes these limitations. It is a highly efficient distributed and concurrent algorithm (based on an efficient sequential algorithm for incremental, lazy updates). Doppler relies solely on asynchronous message passing, and does not require shared memory, synchronized clocks, or a global synchronization mechanism. It supports a high degree of concurrency by efficiently tracking potential cause and effect relationships between reads and writes, and allowing all causally independent operations to execute in parallel. This makes it scalable, and optimizes reads and writes by minimizing their blocking time.\n",
      "=============================\n",
      "A finger-mounted, direct pointing device for mobile computing\n",
      "The index (first) finger of the dominant hand seems to be an intuitively natural and efficient means for pointing tasks. This paper presents the design of a device to enable pointing with the index finger as an interaction technique in mobile computers. The device, which uses infrared emission and detection to determine where on a screen the finger is pointing, is inexpensive and can easily be incorporated into a laptop computer. KEY W 0 R DS : Pointing, Interaction devices, Input devices, Infrared detection. INTRODUCTION Pointing with the index finger seems to be a fundamental means of communication among humans. Studies have shown [2] that infants as young as two months will extend the index finger as a means of indicating a desire for attention. By twelve months, infants show facility for both responding to the directional gaze of their mothers by pointing in the direction of the gaze and pointing at an object in order to direct their mothers’ gaze towards it. It is interesting that most pointing devices used with computers (mice, styli, trackballs, joysticks etc.) have little or nothing in common with natural pointing. Notable exceptions are [l] which exploited natural pointing with the entire hand arm system and, more recently the FingerMouse [3] which uses video and image processing to determine both the direction the finger is pointing and the configuration of the hand. Perwission to mnke digiirnl/hnrd cofiirs ofall or pnrl oflhis mlcrinl for perso~nl or clnssroonl use is granted without fee provided thnt the copies are not nlndr or distributed tbr pro12 or conunercinl ndvnnlrgr. IIIC copy. right notice. the title of the publication and its dale appear. and notice is given lhnt copyright is by pcrmissiorl ofthe ACM, hlc. To copy otherwise. . IO republish. lo post 011 wvers or IO redistribute to lists, requires specific permission nnd/or fee UIST 97 I3clnfl.4lbertn, c.hxiff Copyright 1997 ACM 0-89791~881-9/97/lO..S3.50 PHYSIOLOGICAL BASIS Pointing with index finger extended is not only an intuitive behavior, it is also a behavior that humans can perform with a high degree of facility. A relatively large proportion of the primary motor cortex is dedicated to controlling the fingers and the neural connection path from the cortex to the finger muscles is relatively direct [4] yielding a high degree of fine motor control. If you take a moment to point at a few objects in the surrounding environment you will note that your intuitive pointing motion is to extend your index finger in the direction of the target object. If you assume your hand is positioned on the keyboard of a laptop and point only with the index finger you will note that the range of motion is more than sufficient to cover the area of a typical laptop screen. This is an additional advantage of pointing with the finger: since you don’t have to take your hand off the keyboard you avoid the problem of alternating device acquisition. To benefit from these physiological advantages, you must be pointing at the target directly with the finger itself, not using one or more fingers indirectly to control a cursor through a device such as a mouse or trackball. DESIGN FOR MOBILE COMPUTING Mobile computing introduces constraints on input devices that are not present in a workstation environment. Input devices cannot be too large or expensive and they must be contained within the mobile unit (e.g. laptop) or easily attached to it. The input devices most frequently available with laptop computers, trackballs, scratchpads, and isometric joysticks, satisfy these constraints but are used in a relatively difficult indirect control method which most users find less than fully satisfactory. This is why so many users plug a mouse into their laptops whenever possible.\n",
      "=============================\n",
      "Pen-based interaction techniques for organizing material on an electronic whiteboard\n",
      "This paper presents a scheme for extending an informal, penbased whiteboard system (the Tivoli application onthe Xerox LiveBoard) to provide interaction techniques that enable groups of users in informal meetings to easily organize and.rearrange material and to manage the space on the board. The techniques are based on the direct manipulation of boundaries and the implicit recognition of regions. The techniques include operations for shrinking and rearranging, structured borders that tessellate the board, freeform enclosures that can be split, fused, and linked, and collapsible annotations. Experience with using these techniques, the results of a user test, some design trade-offs and lessons, and future directions are discussed.\n",
      "=============================\n",
      "Through the combining glass\n",
      "Reflective optical combiners like beam splitters and two way mirrors are used in AR to overlap digital contents on the users' hands or bodies. Augmentations are usually unidirectional, either reflecting virtual contents on the user's body (Situated Augmented Reality) or augmenting user's reflections with digital contents (AR mirrors). But many other novel possibilities remain unexplored. For example, users' hands, reflected inside a museum AR cabinet, can allow visitors to interact with the artifacts exhibited. Projecting on the user's hands as their reflection cuts through the objects can be used to reveal objects' internals. Augmentations from both sides are blended by the combiner, so they are consistently seen by any number of users, independently of their location or, even, the side of the combiner through which they are looking. This paper explores the potential of optical combiners to merge the space in front and behind them. We present this design space, identify novel augmentations/interaction opportunities and explore the design space using three prototypes.\n",
      "=============================\n",
      "Video text retouch: retouching text in videos with direct manipulation\n",
      "Video Text Retouch is a technique for retouching textual content found in many online videos such as screencasts, recorded presentations and many online e-learning videos. Viewed through our special, HTML5-based player, users can edit in real-time the textual content of the video frames, such as correcting typos or inserting new words between existing characters. Edits are overlaid and tracked at the desired position for as long as the original video content remains similar. We describe the interaction techniques, image processing algorithms and give implementation details of the system.\n",
      "=============================\n",
      "PETALS: a visual interface for landmine detection\n",
      "Post-conflict landmines have serious humanitarian repercussions: landmines cost lives, limbs and land. The primary method used to locate these buried devices relies on the inherently dangerous and difficult task of a human listening to audio feedback from a metal detector. Researchers have previously hypothesized that expert operators respond to these challenges by building mental patterns with metal detectors through the identification of object-dependent spatially distributed metallic fields. This paper presents the preliminary stages of a novel interface - Pattern Enhancement Tool for Assisting Landmine Sensing (PETALS) - that aims to assist with building and visualizing these patterns, rather than relying on memory alone. Simulated demining experiments show that the experimental interface decreases classification error from 23% to 5% and reduces localization error by 54%, demonstrating the potential for PETALS to improve novice deminer safety and efficiency.\n",
      "=============================\n",
      "3-dimensional pliable surfaces: for the effective presentation of visual information\n",
      "A fundamental issue in user interface design is the effective use of available screen space, commonly referred to as the screen real estate problem. This paper presents a new distortion-based viewing tool for exploring large information spaces through the use of a three-dimensional pliable surface. Arbitrarily-shaped regions (foci) on the surface may be selected and pulled towards or pushed away from the viewer thereby increasing or decreasing the level of detail contained within each region. Furthermore, multiple foci are smoothly blended together such that there is no loss of context. The manipulation and blending of foci is accomplished using a fairly simple mathematical model based on gaussian curves. The significance of this approach is that it utilizes precognitive perceptual cues about the three-dimensional surface to make the distortions comprehensible, and allows the user to interactively control the location, shape, and extent of the distortion in very large graphs or maps.\n",
      "=============================\n",
      "PyzoFlex: printed piezoelectric pressure sensing foil\n",
      "Ferroelectric material supports both pyro- and piezoelectric effects that can be used for sensing pressures on large, bended surfaces. We present PyzoFlex, a pressure-sensing input device that is based on a ferroelectric material. It is constructed with a sandwich structure of four layers that can be printed easily on any material. We use this material in combination with a high-resolution Anoto-sensing foil to support both hand and pen input tracking. The foil is bendable, energy-efficient, and it can be produced in a printing process. Even a hovering mode is feasible due to its pyroelectric effect. In this paper, we introduce this novel input technology and discuss its benefits and limitations.\n",
      "=============================\n",
      "Tohme: detecting curb ramps in google street view using crowdsourcing, computer vision, and machine learning\n",
      "Building on recent prior work that combines Google Street View (GSV) and crowdsourcing to remotely collect information on physical world accessibility, we present the first 'smart' system, Tohme, that combines machine learning, computer vision (CV), and custom crowd interfaces to find curb ramps remotely in GSV scenes. Tohme consists of two workflows, a human labeling pipeline and a CV pipeline with human verification, which are scheduled dynamically based on predicted performance. Using 1,086 GSV scenes (street intersections) from four North American cities and data from 403 crowd workers, we show that Tohme performs similarly in detecting curb ramps compared to a manual labeling approach alone (F- measure: 84% vs. 86% baseline) but at a 13% reduction in time cost. Our work contributes the first CV-based curb ramp detection system, a custom machine-learning based workflow controller, a validation of GSV as a viable curb ramp data source, and a detailed examination of why curb ramp detection is a hard problem along with steps forward.\n",
      "=============================\n",
      "Learning design patterns with bayesian grammar induction\n",
      "Design patterns have proven useful in many creative fields, providing content creators with archetypal, reusable guidelines to leverage in projects. Creating such patterns, however, is a time-consuming, manual process, typically relegated to a few experts in any given domain. In this paper, we describe an algorithmic method for learning design patterns directly from data using techniques from natural language processing and structured concept learning. Given a set of labeled, hierarchical designs as input, we induce a probabilistic formal grammar over these exemplars. Once learned, this grammar encodes a set of generative rules for the class of designs, which can be sampled to synthesize novel artifacts. We demonstrate the method on geometric models and Web pages, and discuss how the learned patterns can drive new interaction mechanisms for content creators.\n",
      "=============================\n",
      "Closing the loop between intentions and actions\n",
      "In this document, I propose systems that aim to minimize the gap between intentions and the corresponding actions under different scenarios. The gap exists because of many reasons like subjective mapping between the two, lack of resources to implement the action, or inherent noise in the physical processes. The proposed system observes the action and infers the intention behind it. The system then generates a refined action using the inference. The inferred intention and the refined action are then provided as feedback to the user who can then perform corrective actions or choose the refined action as it is as the desired result. I demonstrate the design and implementation of such systems through five projects - Image Deblurring, Tracking Block Model Assembly, Animating with Physical Proxies, What Affects Handwriting and Spying on the Writer.\n",
      "=============================\n",
      "EmbeddedButtons: documents as user interfaces\n",
      "Recent electronic document editors and hypertext systems allow users to create customized user interfaces by adding user-pressable buttons to on-screen documents. Positioning these buttons is easy because users are already tlamlliar with the use of document editors. Unfortunately, the resulting user interfaces often ex!st only in stand-alone document systems, making it hard to integrate them with other applications. Furthermore, because buttons are usually treated as special document objects, they cannot take advan~age of document editor formatting and layout capabilities to create thenappearance This paper describes the EmbeddedButtons architecture, which makes ~asy to integrate buttons into documents and to use the resulting documents for a variety of user interface types. EmbeddedButtons allows arbitrary document elements to behave as buttons. Documents can be linked to application windows to serve as application control panels. Buttons can store and display application state to serve as mode indicators. New button classes, editors, and apphcations can be added dynamically\n",
      "=============================\n",
      "Ethereal: a toolkit for spatially adaptive augmented reality content\n",
      "In this poster, we describe a framework and toolkit (Ethereal) for creating spatially adaptive content based on complex spatial and visual metrics in augmented reality, and demonstrate our approach with an illustrative example.\n",
      "=============================\n",
      "Separating application code from toolkits: eliminating the spaghetti of call-backs\n",
      "Conventional toolkits today require the programmer to attach call-back procedures to most buttons, scroll bars, menu items, and other widgets in the interface. These procedures are called by the system when the user operates the widget in order to notify the application of the user’s actions. Unfortunately, real interfaces contain hundreds or thousands of widgets, and therefore many call-back procedures, most of which perform trivial tasks, resulting in a maintenance nightmare. This paper describes a system that allows the majority of these procedures to be eliminated. The user interface designer can specify by demonstration many of the desired actions and connections among the widgets, so call-backs are only needed for the most significant application actions. In addition, the callbacks that remain are completely insulated from the widgets, so that the application code is better separated from the user interface.\n",
      "=============================\n",
      "SenseGlass: using google glass to sense daily emotions\n",
      "For over a century, scientists have studied human emotions in laboratory settings. However, these emotions have been largely contrived -- elicited by movies or fake \"lab\" stimuli, which tend not to matter to the participants in the studies, at least not compared with events in their real life. This work explores the utility of Google Glass, a head-mounted wearable device, to enable fundamental advances in the creation of affect-based user interfaces in natural settings.\n",
      "=============================\n",
      "Combining crossing-based and paper-based interaction paradigms for dragging and dropping between overlapping windows\n",
      "Despite novel interaction techniques proposed for virtual desktops, common yet challenging tasks remain to be investigated. Dragging and dropping between overlapping windows is one of them. The fold-and-drop technique presented here offers a natural and efficient way of performing those tasks. We show how this technique successfully builds upon several interaction paradigms previously described, while shedding new light on them.\n",
      "=============================\n",
      "Platform independent user interface builders: where are we headed?\n",
      "Platform independence means that a user interface can be specified and created using a particular combination of hardware, operating system, and windowing environmen~ that single specification can then be recompiled without intervention (ideally) to run on an entirely heterogeneous combination/platform. This is a remarkable feat even considering alone the differences among windowing environments (Motif, Windows, Macintosh), operating systems (various Unix, DOS, Macintosh), or hardware (Sun, HP, DEC, IBM PC/compatible, Macintosh).\n",
      "=============================\n",
      "SemFeel: a user interface with semantic tactile feedback for mobile touch-screen devices\n",
      "One of the challenges with using mobile touch-screen devices is that they do not provide tactile feedback to the user. Thus, the user is required to look at the screen to interact with these devices. In this paper, we present SemFeel, a tactile feedback system which informs the user about the presence of an object where she touches on the screen and can offer additional semantic information about that item. Through multiple vibration motors that we attached to the backside of a mobile touch-screen device, SemFeel can generate different patterns of vibration, such as ones that flow from right to left or from top to bottom, to help the user interact with a mobile device. Through two user studies, we show that users can distinguish ten different patterns, including linear patterns and a circular pattern, at approximately 90% accuracy, and that SemFeel supports accurate eyes-free interactions.\n",
      "=============================\n",
      "Automation and customization of rendered web pages\n",
      "On the desktop, an application can expect to control its user interface down to the last pixel, but on the World Wide Web, a content provider has no control over how the client will view the page, once delivered to the browser. This creates an opportunity for end-users who want to automate and customize their web experiences, but the growing complexity of web pages and standards prevents most users from realizing this opportunity. We describe Chickenfoot, a programming system embedded in the Firefox web browser, which enables end-users to automate, customize, and integrate web applications without examining their source code. One way Chickenfoot addresses this goal is a novel technique for identifying page components by keyword pattern matching. We motivate this technique by studying how users name web page components, and present a heuristic keyword matching algorithm that identifies the desired component from the user's name.\n",
      "=============================\n",
      "An infrastructure for extending applications' user experiences across multiple personal devices\n",
      "Users increasingly interact with a heterogeneous collection of computing devices. The applications that users employ on those devices, however, still largely provide user experiences that assume the use of a single computer. This failure is due in part to the difficulty of creating user experiences that span multiple devices, particularly the need to manage identifying, connecting to, and communicating with other devices. In this paper we present an infrastructure based on instant messaging that simplifies adding that additional functionality to applications. Our infrastructure elevates device ownership to a first class property, allowing developers to provide functionality that spans personal devices without writing code to manage users' devices or establish connections among them. It also provides simple mechanisms for applications to send information, events, or commands between a user's devices. We demonstrate the effectiveness of our infrastructure by presenting a set of sample applications built with it and a user study demonstrating that developers new to the infrastructure can implement all of the cross-device functionality for three applications in, on average, less than two and a half hours.\n",
      "=============================\n",
      "Perceptual interpretation of ink annotations on line charts\n",
      "Asynchronous collaborators often use freeform ink annotations to point to visually salient perceptual features of line charts such as peaks or humps, valleys, rising slopes and declining slopes. We present a set of techniques for interpreting such annotations to algorithmically identify the corresponding perceptual parts. Our approach is to first apply a parts-based segmentation algorithm that identifies the visually salient perceptual parts in the chart. Our system then analyzes the freeform annotations to infer the corresponding peaks, valleys or sloping segments. Once the system has identified the perceptual parts it can highlight them to draw further attention and reduce ambiguity of interpretation in asynchronous collaborative discussions.\n",
      "=============================\n",
      "The go-go interaction technique: non-linear mapping for direct manipulation in VR\n",
      "The Go-Go immersive interaction technique uses the metaphor of interactively growing the user’s arm and non-linear mapping for reaching and manipulating distant objects. Unlike others, our technique allows for seamless direct manipulation of both nearby objects and those at a distance.\n",
      "=============================\n",
      "Medical operating documents: dynamic checklists improve crisis attention\n",
      "The attentional aspects of crisis computing - supporting highly trained teams as they respond to real-life emergencies - have been underexplored in the user interface community. My research investigates the development of interactive software systems that support crisis teams, with an eye towards intelligently managing attention. In this paper, I briefly describe MDOCS, a Medical operating DOCuments System built for time-critical interaction. MDOCS is a multi-user, multi-surface software system that implements dynamic checklists and interactive cognitive aids written to support medical crisis teams. I present the results of a study that evaluates the deployment of MDOCS in a realistic, mannequin-based medical simulator used by anesthesiologists. I propose controlled laboratory experiments that evaluate the feasibility and effectiveness of our design principles and attentional interaction techniques.\n",
      "=============================\n",
      "Artisanship training using wearable egocentric display\n",
      "In recent years, most of traditional artisanship is declining because of aging skilled artisan and fewer successors. Therefore, methods for digital archiving of such traditional artisanship are needed. We have constructed a wearable skill-training interface that displays egocentric visual and audio information and muscle activities of an artisan. We used acceleration data of an instrument associated with the usage of the tools for evaluating the effect of proposed wearable display system. This paper introduces the concept and development of wearable egocentric display. Then briefly reports the application results in Kamisuki, Japanese traditional papermaking.\n",
      "=============================\n",
      "DemoCut: generating concise instructional videos for physical demonstrations\n",
      "Amateur instructional videos often show a single uninterrupted take of a recorded demonstration without any edits. While easy to produce, such videos are often too long as they include unnecessary or repetitive actions as well as mistakes. We introduce DemoCut, a semi-automatic video editing system that improves the quality of amateur instructional videos for physical tasks. DemoCut asks users to mark key moments in a recorded demonstration using a set of marker types derived from our formative study. Based on these markers, the system uses audio and video analysis to automatically organize the video into meaningful segments and apply appropriate video editing effects. To understand the effectiveness of DemoCut, we report a technical evaluation of seven video tutorials created with DemoCut. In a separate user evaluation, all eight participants successfully created a complete tutorial with a variety of video editing effects using our system.\n",
      "=============================\n",
      "Capturing the user's attention: insights from the study of human vision\n",
      "An effective user interface is a cooperative interaction between humans and their technology. For that interaction to work, it needs to recognize the limitations and exploit the strengths of both parties. In this talk, I will concentrate on the human side of the equation. What do we know about human visual perceptual abilities that might have an impact on the design of user interfaces? The world presents us with more information than we can process. Just try to read this abstract and the next piece of prose at the same time. We cope with this problem by using attentional mechanisms to select a subset of the input for further processing. An inter-face might be designed to .capture. attention, in order to induce a human to interact with it. Once the human is using an interface, that interface should .guide. the user.s atten-tion in an intelligent manner. In recent decades, many of the rules of attentional capture and guidance have been worked out in the laboratory. I will illustrate some of the basic principles. For example: Do some colors grab attention better than others? Are faces special? When and why do people fail to .see. things that are right in front of their eyes.\n",
      "=============================\n",
      "Video widgets and video actors\n",
      "Video widgets are user-interface components rendered with video information. The implementation and several usage examples of a family of video widgets, called video actors,, are presented. Video actors rely on two capabilities of digital video: non-linear access, and the layering of video information. Non-linear access allows video frames to be displayed in mbitrary order without loss of continuity, layering allows two or more video streams to be spatially composed. Both capabilities are now becoming available to user-interface designers.\n",
      "=============================\n",
      "Integrating pen operations for composition by example\n",
      "We propose a new pen-based text input method, which is an integration of software keyboards, handwriting recognition, and marking menus. With our method, a user selects a word from a list of candidate words filtered from a dictionary by specifying spelling, pronunciation, or the shape of its characters. Users can easily switch between using a software keyboard and using handwriting recognition systems, by tapping or writing strokes in the same area. Users can also show a menu of candidate words around the pen by stopping pen movement during operation.\n",
      "=============================\n",
      "Visualizing web browsing history with barcode chart\n",
      "Inspired by the DNA art, we introduce a data visualization technique called barcode chart, which uses color-illuminated stripes that resemble barcodes to visualize temporal data. Barcode chart excels at demonstrating high-level patterns in highly segmented temporal data, while retaining details in the data through interaction. We demonstrate Yogurt, a browser extension that implements barcode chart to visualize online browsing history. We conducted a user study and analyzed the effectiveness of using barcode chart for Yogurt in comparison with other applications. We conclude that barcode chart satisfies the need of visualizing the high-density and high-fragmentation nature of temporal data in Yogurt, and helps reveal online distraction and other web browsing patterns.\n",
      "=============================\n",
      "SlickFeel: sliding and clicking haptic feedback on a touchscreen\n",
      "We present SlickFeel, a single haptic display setup that can deliver two distinct types of feedback to a finger on a touchscreen during typical operations of sliding and clicking. Sliding feedback enables the sliding finger to feel interactive objects on a touchscreen through variations in friction. Clicking feedback provides a key-click sensation for confirming a key or button click. Two scenarios have been developed to demonstrate the utility of the two haptic effects. In the first, simple button-click scenario, a user feels the positions of four buttons on a touchscreen by sliding a finger over them and feels a simulated key-click signal by pressing on any of the buttons. In the second scenario, the advantage of haptic feedback is demonstrated in a haptically-enhanced thumb-typing scenario. A user enters text on a touchscreen with two thumbs without having to monitor the thumbs' locations on the screen. By integrating SlickFeel with a Kindle Fire tablet, we show that it can be used with existing mobile touchscreen devices.\n",
      "=============================\n",
      "Using GELO to visualize software systems\n",
      "GELO is a package that supports the interactive graphical display of software systems. Its features include built-in panning and zooming, abstraction of objects too small to see, pick correlation, windowing, and scroll bars. GELO creates a hierarchy of graphical objects that correspond to the components of the structure being displayed. Five flavors of graphical objects are supported, including those for simple structures, tiled layouts, and graph-based layouts. This framework is powerful enough to handle a wide variety of graphical visualizations, and it is general enough that new object flavors can be smoothly integrated in the future.GELO is easy to learn and to use, and is presently employed in two software development environments. Among its current applications are a variety of visual languages, an interactive display of call graphs, an interactive display of data structures, and a graphical representation of module dependencies.\n",
      "=============================\n",
      "A virtual office environment based on a shared room realizing awareness space and transmitting awareness information\n",
      "‘c’ ,. In t&s paper, we describe a system +at provides a “w&k-at-home” environment based on a &tual shared room built ,on”a 3D graphics w&k$ation. We realize ‘chwareness Space” o,n’ the system to avoid a tradeoff bet\\veen’ ‘providing facility of informal comniunication and keeping one’s workSpace from &hers: ,a-ware’ness information. Also, this system provides the feeling of the presence at virtual office by using ‘fAround View? and “Sound Effect”. 1\n",
      "=============================\n",
      "Tag expression: tagging with feeling\n",
      "In this paper we introduce tag expression, a novel form of preference elicitation that combines elements from tagging and rating systems. Tag expression enables users to apply affect to tags to indicate whether the tag describes a reason they like, dislike, or are neutral about a particular item. We present a user interface for applying affect to tags, as well as a technique for visualizing the overall community's affect. By analyzing 27,773 tag expressions from 553 users entered in a 3-month period, we empirically evaluate our design choices. We also present results of a survey of 97 users that explores users' motivations in tagging and measures user satisfaction with tag expression.\n",
      "=============================\n",
      "A toolset for navigation in virtual environments\n",
      "Maintaining knowledge of current position and orienta- tion is frequently a problem for people in virtual environ- ments. In this paper we present a toolset of techniques based on principles of navigation derived from real world analogs. We include a discussion of human and avian navigation behaviors and show how knowledge about them were used to design our tools. We also sum- marize an informal study we performed to determine how our tools influenced the subjects' navigation behav- ior. We conclude that principles extracted from real world navigation aids such as maps can be seen to apply in virtual environments. Our basic approach to this research begins with a charac- terization of the problem domain in terms of its key char- acteristics. This means we must develop a classification of virtual environments, navigational tasks, and orienta- tion. We also want to consider human abilities, both in- nate and artificially enhanced. Based on our best understanding, we build tools to aid users in the naviga- tional tasks we have identified. Empirical studies of the tools are then used to evaluate their effectiveness. In many cases, we hope to begin to understand why certain tools are more effective than others and to feed this knowledge back into our theoretical framework. Even in the physical world, the natural navigational abil- ities of humans and other animals are not completely un- derstood. When the world is a virtual one, the problem is exacerbated by the degradation of sensory cues resulting from poor resolution, device latencies, and other short- comings of current technologies. As virtual spaces be- come larger, more abstract, and more dynamic, the cues and stimuli associated with the physical world may be lacking altogether. This paper focuses specifically on navigation and the ef- fects differing tools and environmental cues have on the way in which people perform a simple set of generic ex- ploration and searching tasks. We will begin with a brief summary of a classification of virtual worlds intended to set the context for the rest of the paper. We will then summarize some hypotheses concerning navigational ca- pabilities of humans and birds. Finally, we will describe a toolset of possible techniques we built based on these hypotheses and summarize an informal empirical study designed to determine how the tools themselves affect their users' behavior.\n",
      "=============================\n",
      "Dialing for documents: an experiment in information theory\n",
      "Standard telephone keypads are labeled with letters of the alphabet, enabling users to enter textual data for a variety of possible applications. However, the overloading of three letters on a single key creates a potential ambiguity as to which character was intended, which must be resolved for unambiguous text entry. Existing systems all use pairs of keypresses to spell out single key letters, but are extremely cumbersome and frustrating to use.Instead, we propose single-stroke text entry on telephone keypads, with the ambiguity resolved by exploiting information-theoretic constraints. We develop algorithms capable of correctly identifying up to 99% of the characters in typical English text, sufficient for such applications as telephones for the hearing impaired, E-mail without a terminal, and advanced voice-response systems.\n",
      "=============================\n",
      "Glance: rapidly coding behavioral video with the crowd\n",
      "Behavioral researchers spend considerable amount of time coding video data to systematically extract meaning from subtle human actions and emotions. In this paper, we present Glance, a tool that allows researchers to rapidly query, sample, and analyze large video datasets for behavioral events that are hard to detect automatically. Glance takes advantage of the parallelism available in paid online crowds to interpret natural language queries and then aggregates responses in a summary view of the video data. Glance provides analysts with rapid responses when initially exploring a dataset, and reliable codings when refining an analysis. Our experiments show that Glance can code nearly 50 minutes of video in 5 minutes by recruiting over 60 workers simultaneously, and can get initial feedback to analysts in under 10 seconds for most clips. We present and compare new methods for accurately aggregating the input of multiple workers marking the spans of events in video data, and for measuring the quality of their coding in real-time before a baseline is established by measuring the variance between workers. Glance's rapid responses to natural language queries, feedback regarding question ambiguity and anomalies in the data, and ability to build on prior context in followup queries allow users to have a conversation-like interaction with their data - opening up new possibilities for naturally exploring video data.\n",
      "=============================\n",
      "Combining multiple depth cameras and projectors for interactions on, above and between surfaces\n",
      "Instrumented with multiple depth cameras and projectors, LightSpace is a small room installation designed to explore a variety of interactions and computational strategies related to interactive displays and the space that they inhabit. LightSpace cameras and projectors are calibrated to 3D real world coordinates, allowing for projection of graphics correctly onto any surface visible by both camera and projector. Selective projection of the depth camera data enables emulation of interactive displays on un-instrumented surfaces (such as a standard table or office desk), as well as facilitates mid-air interactions between and around these displays. For example, after performing multi-touch interactions on a virtual object on the tabletop, the user may transfer the object to another display by simultaneously touching the object and the destination display. Or the user may \"pick up\" the object by sweeping it into their hand, see it sitting in their hand as they walk over to an interactive wall display, and \"drop\" the object onto the wall by touching it with their other hand. We detail the interactions and algorithms unique to LightSpace, discuss some initial observations of use and suggest future directions.\n",
      "=============================\n",
      "Elastic scroll for multi-focus interactions\n",
      "This paper proposes a novel and efficient multi-focus scroll interface that consists of a two-step operation using a con-tents distortion technique. The displayed content can be handled just like an elastic material that can be shrunk and stretched by a user's fingers. In the first operation, the us-er's dragging temporarily shows the results of the viewport transition of the scroll by elastically distorting the content. This operation allows the user to see both the newly obtained and the original focus on the viewport. Then, three types of simple gestures can be used to perform the second operation such as scrolling, restoring and zooming out to get the demanded focus (or foci).\n",
      "=============================\n",
      "Panopticon: a parallel video overview system\n",
      "Panopticon is a video surrogate system that displays multiple sub-sequences in parallel to present a rapid overview of the entire sequence to the user. A novel, precisely animated arrangement slides thumbnails to provide a consistent spatiotemporal layout while allowing any sub-sequence of the original video to be watched without interruption. Furthermore, this output can be generated offline as a highly efficient repeated animation loop, making it suitable for resource-constrained environments, such as web-based interaction. Two versions of Panopticon were evaluated using three different types of video footage with the aim of determining the usability of the proposed system. Results demonstrated an advantage over another surrogate with surveillance footage in terms of search times and this advantage was further improved with Panopticon 2. Eye tracking data suggests that Panopticon's advantage stems from the animated timeline that users heavily rely on.\n",
      "=============================\n",
      "Trampoline: a double-sided elastic touch device for creating reliefs\n",
      "Although reliefs are frequently used to add patterns to product surfaces, there is a lack of interaction techniques to model reliefs on the surface of virtual objects. We adopted the repoussé and chasing artwork techniques in an alternative interaction technique to model relief on virtual surfaces. To support this interaction technique, we developed the double-sided touchpad Trampoline that can detect the position and force of a finger touch on both sides. Additionally, Trampoline provides users with elastic feedback, as its surface consists of a stretchable fabric. We implemented a relief application with this device and the developed interaction technique. An informal user study showed that the proposed system can be a promising solution to create reliefs.\n",
      "=============================\n",
      "Development of the motion-controllable ball\n",
      "In this report, we propose a novel ball type interactive interface device. Balls are one of the most important pieces of equipment used for entertainment and sports. Their motion guides a player's response in terms of, for example, a feint or similar movement. Many kinds of breaking ball throws have been developed for various sports(e.g. baseball). However, acquiring the skill to appropriately react to these breaking balls is often hard to achieve and requires long-term training. Many researchers focus on the ball itself and have developed interactive balls with visual and acoustic feedbacks. However, these balls do not have the ability for motion control. In this paper, we introduce a ball-type motion control interface device. It is composed of a ball and an air-pressure tank to change its vector using gas ejection. We conducted an experiment that measures the ball's flight path while subjected to gas ejection and the results showed that the prototype system had enough power to change the ball's vector while flying\n",
      "=============================\n",
      "Visimu: a game for music color label collection\n",
      "Based on previous studies of the associations between color and music, we introduce a scalable way of using colors to label songs and a visualization of music archives that facilitates music exploration. We present Visimu, an online game that attracted users to generate 926 color labels for 102 songs, with over 75% of the songs having color labels reaching high consensus in the Lab color space. We implemented a music archive visualization using the color labels generated by Visimu, and conducted an experiment to show that labeling music by color is more effective than text tags when the user is looking for songs of a particular mood or use scenario. Our results showed that Visimu is effective to produce meaningful color labels for music mood classification, and such approach enables a wide range of applications for music visualization and discovery.\n",
      "=============================\n",
      "Designing auditory interactions for PDAs\n",
      "This panel addresses issues in designing audio-based user interactions for small, personal computing devices, or PDAs. One issue is the nature of interacting with an auditory PDA and the interplay of affordances and form factors. Another issue is how both new and traditional metaphors and interaction concepts might be applied to auditory PDAs. The utility and design of nonspeech cues are discussed, as are the aesthetic issues of persona and narrative in designing sounds. Also discussed are commercially available sound and speech components and related hardware tradeoffs. Finally, the social implications of auditory interactions are explored, including privacy, fashion and novel social interactions.\n",
      "=============================\n",
      "Scratchpad: mechanisms for better navigation in directed Web searching\n",
      "Current navigation mechanisms for the World Wide Web promote a depth-first search for information on pages in hyperspace. This search strategy frequently results in the unintentional and often undesirable behavior of “web surfing” — a user starts off in search of some information, but is sidetracked by tangential links. We propose a set of mechanisms based on breadth-first traversal that are better suited for directed searching. We have implemented our ideas as a scratchpad by augmenting an existing browser. Such a system makes web navigation both faster and easier.\n",
      "=============================\n",
      "Speeda: adaptive speed-up for lecture videos\n",
      "Increasing the playback speed of lecture videos is a common technique to shorten watching time. This creates challenges when part of the lecture becomes too fast to be discernible, even if the overall playback speed is acceptable. In this paper, we present a speed-up system that preserves lecture clearness in high playback rate. A user test was conducted to evaluate the system. The result indicates that our system significantly improves user's comprehension level.\n",
      "=============================\n",
      "Artists and technologists working together (panel)\n",
      "This panel explores the dialog and interplay between artists and technologists. In the process, the panelists aim to bring considerations of art and the artistic process to the attention of the technology-oriented UIST community. We invite readers to think about how your work relates to art. We encourage the research community to look for ways to integrate art and artists within their own programs, for example, by starting artist-in-residence activities, introducing courses on art and design into CS curricula, or inviting artists to participate in projects.\n",
      "=============================\n",
      "Video lens: rapid playback and exploration of large video collections and associated metadata\n",
      "We present Video Lens, a framework which allows users to visualize and interactively explore large collections of videos and associated metadata. The primary goal of the framework is to let users quickly find relevant sections within the videos and play them back in rapid succession. The individual UI elements are linked and highly interactive, supporting a faceted search paradigm and encouraging exploration of the data set. We demonstrate the capabilities and specific scenarios of Video Lens within the domain of professional baseball videos. A user study with 12 participants indicates that Video Lens efficiently supports a diverse range of powerful yet desirable video query tasks, while a series of interviews with professionals in the field demonstrates the framework's benefits and future potential.\n",
      "=============================\n",
      "Improving structured data entry on mobile devices\n",
      "Structure makes data more useful, but also makes data entry more cumbersome. Studies have found that this is especially true on mobile devices, as mobile users often reject structured personal information management tools because the structure is too restrictive and makes entering data slower. To overcome these problems, we introduce a new data entry technique that lets users create customized structured data in an unstructured manner. We use a novel notepad-like editing interface with built-in data detectors that allow users to specify structured data implicitly and reuse the structures when desired. To minimize the amount of typing, it provides intelligent, context-sensitive autocomplete suggestions using personal and public databases that contain candidate information to be entered. We implemented these mechanisms in an example application called Listpad. Our evaluation shows that people using Listpad create customized structured data 16% faster than using a conventional mobile database tool. The speed further increases to 42% when the fields can be autocompleted.\n",
      "=============================\n",
      "A touchless passive infrared gesture sensor\n",
      "A sensing device for a touchless, hand gesture, user interface based on an inexpensive passive infrared pyroelectric detector array is presented. The 2 x 2 element sensor responds to changing infrared radiation generated by hand movement over the array. The sensing range is from a few millimetres to tens of centimetres. The low power consumption (< 50 μW) enables the sensor's use in mobile devices and in low energy applications. Detection rates of 77% have been demonstrated using a prototype system that differentiates the four main hand motion trajectories -- up, down, left and right. This device allows greater non-contact control capability without an increase in size, cost or power consumption over existing on/off devices.\n",
      "=============================\n",
      "Skillometers: reflective widgets that motivate and help users to improve performance\n",
      "Applications typically provide ways for expert users to increase their performance, such as keyboard shortcuts or customization, but these facilities are frequently ignored. To help address this problem, we introduce skillometers -- lightweight displays that visualize the benefits available through practicing, adopting a better technique, or switching to a faster mode of interaction. We present a general framework for skillometer design, then discuss the design and implementation of a real-world skillometer intended to increase hotkey use. A controlled experiment shows that our skillometer successfully encourages earlier and faster learning of hotkeys. Finally, we discuss general lessons for future development and deployment of skillometers.\n",
      "=============================\n",
      "TapSense: enhancing finger interaction on touch surfaces\n",
      "We present TapSense, an enhancement to touch interaction that allows conventional surfaces to identify the type of object being used for input. This is achieved by segmenting and classifying sounds resulting from an object's impact. For example, the diverse anatomy of a human finger allows different parts to be recognized including the tip, pad, nail and knuckle - without having to instrument the user. This opens several new and powerful interaction opportunities for touch input, especially in mobile devices, where input is extremely constrained. Our system can also identify different sets of passive tools. We conclude with a comprehensive investigation of classification accuracy and training implications. Results show our proof-of-concept system can support sets with four input types at around 95% accuracy. Small, but useful input sets of two (e.g., pen and finger discrimination) can operate in excess of 99% accuracy.\n",
      "=============================\n",
      "Deconstructing and restyling D3 visualizations\n",
      "The D3 JavaScript library has become a ubiquitous tool for developing visualizations on the Web. Yet, once a D3 visualization is published online its visual style is difficult to change. We present a pair of tools for deconstructing and restyling existing D3 visualizations. Our deconstruction tool analyzes a D3 visualization to extract the data, the marks and the mappings between them. Our restyling tool lets users modify the visual attributes of the marks as well as the mappings from the data to these attributes. Together our tools allow users to easily modify D3 visualizations without examining the underlying code and we show how they can be used to deconstruct and restyle a variety of D3 visualizations.\n",
      "=============================\n",
      "Progress in building user interface toolkits: the world according to XIT\n",
      "User interface toolkits and higher-level tools built on top of them play an ever increasing part in developing graphical user interfaces. This paper describes the XIT system, a user interface development tool for the X Window System, based on Common Lisp, comprising user interface toolkits as well as high-level interactive tools organized into a layered architecture. We especially focus on the object-oriented design of the lower-level toolkits and show how advanced features for describing automatic screen layout, visual feedback, application links, complex interaction, and dialog control, usually not included in traditional user interface toolkits, are integrated.\n",
      "=============================\n",
      "You can't force calm: designing and evaluating respiratory regulating interfaces for calming technology\n",
      "Interactive systems are increasingly being used to explicitly support change in the user's psychophysiological state and behavior. One trend in this vein is systems that support calm breathing habits. We designed and evaluated techniques to support respiratory regulation to reduce stress and increase parasympathetic tone. Our study revealed that auditory guidance was more effective than visual at creating self-reported calm. We attribute this to the users' ability to effectively map sound to respiration, thereby reducing cognitive load and mental exertion. Interestingly, we found that visual guidance led to more respiratory change but less subjective calm. Thus, motivating users to exert physical or mental efforts may counter the calming effects of slow breathing. Designers of calming technologies must acknowledge the discrepancy between mechanical slow breathing and experiential calm in designing future systems.\n",
      "=============================\n",
      "Converting an existing user interface to use constraints\n",
      "Constraints have long been championed as a tool for user interface construction. However, while certain constraint systems have established a user community, constraint-based user interfaces have not yet been widely adopted. The pmxnise of this paper is that a major stumbling block to their pervasive use has been the emphasis on designing new interface toolkits rather than augmenting existing ones. The thesis of the work described in this paper is that it is possible, and practical, to convert an existing user interface written in an imperative programming language into a similar user interface implemented with constraints. This thesis is proved by example: the conversion of HotDraw into CoolDraw.\n",
      "=============================\n",
      "TiltType: accelerometer-supported text entry for very small devices\n",
      "TiltType is a novel text entry technique for mobile devices. To enter a character, the user tilts the device and presses one or more buttons. The character chosen depends on the button pressed, the direction of tilt, and the angle of tilt. TiltType consumes minimal power and requires little board space, making it appropriate for wristwatch-sized devices. But because controlled tilting of one's forearm is fatiguing, a wristwatch using this technique must be easily removable from its wriststrap. Applications include two-way paging, text entry for watch computers, web browsing, numeric entry for calculator watches, and existing applications for PDAs.\n",
      "=============================\n",
      "Interacting with massive numbers of student solutions\n",
      "When teaching programming or hardware design, it is pedagogically valuable for students to generate examples of functions, circuits, or system designs. Teachers can be overwhelmed by these types of student submissions when running large residential or recently released massive online courses. The underlying distribution of student solutions submitted in response to a particular assignment may be complex, but the newly available volume of student solutions represents a denser sampling of that distribution. Working with large datasets of students' solutions, I am building systems with user interfaces that allow teachers to explore the variety of their students' correct and incorrect solutions. Forum posts, grading rubrics, and automatic graders can be based on student solution data, and turn massive engineering and computer science classrooms into useful insight and feedback for teachers. In the development process, I hope to describe essential design principles for such systems.\n",
      "=============================\n",
      "BoardLab: PCB as an interface to EDA software\n",
      "The tools used to work with Printed Circuit Boards (PCBs), for example soldering iron, multi-meter and oscilloscope involve working directly with the board and the board components. However, the Electronic Design Automation (EDA) software used to query a PCB's design data requires using a keyboard and a mouse. These different interfaces make it difficult to connect both kinds of operations in a workflow. Further, the measurements made by tools like a multi-meter have to be understood in the context of the schematics of the board manually. We propose a solution to reduce the cognitive load of this disconnect by introducing a handheld probe that allows for direct interactions with the PCB for just-in-time information on board schematics, component datasheets and source code. The probe also doubles up as a voltmeter and annotates the schematics of the board with voltage measurements.\n",
      "=============================\n",
      "The VideoMouse: a camera-based multi-degree-of-freedom input device\n",
      "The VideoMouse is a mouse that uses a camera as its input sensor. A real-time vision algorithm determines the six degree-of-freedom mouse posture, consisting of 2D motion, tilt in the forward/back and left/right axes, rotation of the mouse about its vertical axis, and some limited height sensing. Thus, a familiar 2D device can be extended for three-dimensional manipulation, while remaining suitable for standard 2D GUI tasks. We describe techniques for mouse functionality, 3D manipulation, navigating large 2D spaces, and using the camera for lightweight scanning tasks.\n",
      "=============================\n",
      "Dual touch: a two-handed interface for pen-based PDAs\n",
      "A new interaction technique called Dual Touch has been developed for pen-based PDAs. It enables a user to operate a PDA by tapping and stroking on the screen with a pen and a thumb. The PDA can detect the combined movements of two points on its pressure-based touchscreen without additional hardware. The user can use the thumb to support the task of the pen.\n",
      "=============================\n",
      "dePENd: augmented handwriting system using ferromagnetism of a ballpoint pen\n",
      "This paper presents dePENd, a novel interactive system that assists in sketching using regular pens and paper. Our system utilizes the ferromagnetic feature of the metal tip of a regular ballpoint pen. The computer controlling the X and Y positions of the magnet under the surface of the table provides entirely new drawing experiences. By controlling the movements of a pen and presenting haptic guides, the system allows a user to easily draw diagrams and pictures consisting of lines and circles, which are difficult to create by free-hand drawing. Moreover, the system also allows users to freely edit and arrange prescribed pictures. This is expected to reduce the resistance to drawing and promote users' creativity. In addition, we propose a communication tool using two dePENd systems that is expected to enhance the drawing skills of users. The functions of this system enable users to utilize interactive applications such as copying and redrawing drafted pictures or scaling the pictures using a digital pen. Furthermore, we implement the system and evaluate its technical features. In this paper, we describe the details of the design and implementations of the device, along with applications, technical evaluations, and future prospects.\n",
      "=============================\n",
      "Dasher—a data entry interface using continuous gestures and language models\n",
      "Existing devices for communicating information to computers are bulky, slow to use, or unreliable. Dasher is a new interface incorporating language modelling and driven by continuous two-dimensional gestures, e.g. a mouse, touchscreen, or eye-tracker. Tests have shown that this device can be used to enter text at a rate of up to 34 words per minute, compared with typical ten-finger keyboard typing of 40-60 words per minute. Although the interface is slower than a conventional keyboard, it is small and simple, and could be used on personal data assistants and by motion-impaired computer users.\n",
      "=============================\n",
      "Interactive environment-aware display bubbles\n",
      "We present a novel display metaphor which extends traditional tabletop projections in collaborative environments by introducing freeform, environment-aware display representations and a matching set of interaction schemes. For that purpose, we map personalized widgets or ordinary computer applications that have been designed for a conventional, rectangular layout into space-efficient bubbles whose warping is performed with a potential-based physics approach. With a set of interaction operators based on laser pointer tracking, these freeform displays can be transformed and elastically deformed using focus and context visualization techniques. We also provide operations for intuitive instantiation of bubbles, cloning, cut & pasting, deletion and grouping in an interactive way, and we allow for user-drawn annotations and text entry using a projected keyboard. Additionally, an optional environment-aware adaptivity of the displays is achieved by imperceptible, realtime scanning of the projection geometry. Subsequently, collision-responses of the bubbles with non-optimal surface parts are computed in a rigid body simulation. The extraction of the projection surface properties runs concurrently with the main application of the system. Our approach is entirely based on off the-shelf, low-cost hardware including DLP-projectors and FireWire cameras.\n",
      "=============================\n",
      "Reflection: enabling event prediction as an on-device service for mobile interaction\n",
      "By knowing which upcoming action a user might perform, a mobile application can optimize its user interface for accomplishing the task. However, it is technically challenging for developers to implement event prediction in their own application. We created Reflection, an on-device service that answers queries from a mobile application regarding which actions the user is likely to perform at a given time. Any application can register itself and communicate with Reflection via a simple API. Reflection continuously learns a prediction model for each application based on its evolving event history. It employs a novel method for prediction by 1) combining multiple well-designed predictors with an online learning method, and 2) capturing event patterns not only within but also across registered applications--only possible as an infrastructure solution. We evaluated Reflection with two sets of large-scale, in situ mobile event logs, which showed our infrastructure approach is feasible.\n",
      "=============================\n",
      "Toward more sensitive mobile phones\n",
      "Although cell phones are extremely useful, they can be annoying and distracting to owners and others nearby. We describe sensing techniques intended to help make mobile phones more polite and less distracting. For example, our phone's ringing quiets as soon as the user responds to an incoming call, and the ring mutes if the user glances at the caller ID and decides not to answer. We also eliminate the need to press a TALK button to answer an incoming call by recognizing if the user picks up the phone and listens to it.\n",
      "=============================\n",
      "The role of kinesthetic reference frames in two-handed input performance\n",
      "We present experimental work which explores how the match (or mismatch) between the input space of the hands and the output space of a graphical display influences two-handed input performance. During interaction with computers, a direct correspondence between the input and output spaces is often lacking. Not only are the hands disjoint from the display space, but the reference frames of the hands may in fact be disjoint from one another if two separate input devices (e.g. two mice) are used for two-handed input. In general, we refer to the workspace and origin within which the hands operate as kinesthetic reference frames. Our goal is to better understand how an interface designer's choice of kinesthetic reference frames influences a user's ability to coordinate two-handed movements, and to explore how the answer to this question may depend on the availability of visual feedback. Understanding this issue has implications for the design of two-handed interaction techniques and input devices, as well as for the reference principle of Guiard's Kinematic Chain model of human bimanual action. Our results suggest that the Guiard reference principle is robust with respect to variances in the kinesthetic reference frames as long as appropriate visual feedback is present.\n",
      "=============================\n",
      "Multitoe: high-precision interaction with back-projected floors based on high-resolution multi-touch input\n",
      "Tabletop applications cannot display more than a few dozen on-screen objects. The reason is their limited size: tables cannot become larger than arm's length without giving up direct touch. We propose creating direct touch surfaces that are orders of magnitude larger. We approach this challenge by integrating high-resolution multitouch input into a back-projected floor. As the same time, we maintain the purpose and interaction concepts of tabletop computers, namely direct manipulation. We base our hardware design on frustrated total internal reflection. Its ability to sense per-pixel pressure allows the floor to locate and analyze users' soles. We demonstrate how this allows the floor to recognize foot postures and identify users. These two functions form the basis of our system. They allow the floor to ignore users unless they interact explicitly, identify and track users based on their shoes, enable high-precision interaction, invoke menus, track heads, and allow users to control high-degree of freedom interactions using their feet. While we base our designs on a series of simple user studies, the primary contribution on this paper is in the engineering domain.\n",
      "=============================\n",
      "Dial and see: tackling the voice menu navigation problem with cross-device user experience integration\n",
      "IVR (interactive voice response) menu navigation has long been recognized as a frustrating interaction experience. We propose an IM-based system that sends a coordinated visual IVR menu to the caller's computer screen. The visual menu is updated in real time in response to the caller's actions. With this automatically opened supplementary channel, callers can take advantages of different modalities over different devices and interact with the IVR system with the ease of graphical menu selection. Our approach of utilizing existing network infrastructure to pinpoint the caller's virtual location and coordinating multiple devices and multiple channels based on users' ID registration can also be more generally applied to create integrated user experiences across a group of devices.\n",
      "=============================\n",
      "Improving usability by sharing knowledge\n",
      "There has been great progress in the technology of improving the usability of computer tools. However, the state of the art in the user interface field is far outdistancing the state of affairs at many corporations. One reason is that the knowledge is not effectively communicated, especially in large, complex organizations.At McDonnel Douglas Corporation we have formed the User Interface Share Group to enhance information exchange on user interface technology. This paper discusses the motivation for the group, its formation, its value to the corporation, and some specific lessons learned.\n",
      "=============================\n",
      "CopyCAD: remixing physical objects with copy and paste from the real world\n",
      "This paper introduces a novel technique for integrating geometry from physical objects into computer aided design (CAD) software. We allow users to copy arbitrary real world object geometry into 2D CAD designs at scale through the use of a camera/projector system. This paper also introduces a system, CopyCAD, that uses this technique, and augments a Computer Controlled (CNC) milling machine. CopyCAD gathers input from physical objects, sketches and interactions directly on a milling machine, allowing novice users to copy parts of real world objects, modify them and then create a new physical part.\n",
      "=============================\n",
      "Data-driven interaction techniques for improving navigation of educational videos\n",
      "With an unprecedented scale of learners watching educational videos on online platforms such as MOOCs and YouTube, there is an opportunity to incorporate data generated from their interactions into the design of novel video interaction techniques. Interaction data has the potential to help not only instructors to improve their videos, but also to enrich the learning experience of educational video watchers. This paper explores the design space of data-driven interaction techniques for educational video navigation. We introduce a set of techniques that augment existing video interface widgets, including: a 2D video timeline with an embedded visualization of collective navigation traces; dynamic and non-linear timeline scrubbing; data-enhanced transcript search and keyword summary; automatic display of relevant still frames next to the video; and a visual summary representing points with high learner activity. To evaluate the feasibility of the techniques, we ran a laboratory user study with simulated learning tasks. Participants rated watching lecture videos with interaction data to be efficient and useful in completing the tasks. However, no significant differences were found in task performance, suggesting that interaction data may not always align with moment-by-moment information needs during the tasks.\n",
      "=============================\n",
      "Crowd-powered parameter analysis for visual design exploration\n",
      "Parameter tweaking is one of the fundamental tasks in the editing of visual digital contents, such as correcting photo color or executing blendshape facial expression control. A problem with parameter tweaking is that it often requires much time and effort to explore a high-dimensional parameter space. We present a new technique to analyze such high-dimensional parameter space to obtain a distribution of human preference. Our method uses crowdsourcing to gather pairwise comparisons between various parameter sets. As a result of analysis, the user obtains a goodness function that computes the goodness value of a given parameter set. This goodness function enables two interfaces for exploration: Smart Suggestion, which provides suggestions of preferable parameter sets, and VisOpt Slider, which interactively visualizes the distribution of goodness values on sliders and gently optimizes slider values while the user is editing. We created four applications with different design parameter spaces. As a result, the system could facilitate the user's design exploration.\n",
      "=============================\n",
      "What art can tell us about the brain\n",
      "Artists have been doing experiments on vision longer than neurobiologists. Some major works of art have provided insights as to how we see; some of these insights are so fundamental that they can be understood in terms of the underlying neurobiology. For example, artists have long realized that color and luminance can play independent roles in visual perception. Picasso said, \"Colors are only symbols. Reality is to be found in luminance alone.\" This observation has a parallel in the functional subdivision of our visual systems, where color and luminance are processed by the newer, primate-specific What system, and the older, colorblind, Where (or How) system. Many techniques developed over the centuries by artists can be understood in terms of the parallel organization of our visual systems. I will explore how the segregation of color and luminance processing are the basis for why some Impressionist paintings seem to shimmer, why some op art paintings seem to move, some principles of Matisse's use of color, and how the Impressionists painted \"air\". Central and peripheral vision are distinct, and I will show how the differences in resolution across our visual field make the Mona Lisa's smile elusive, and produce a dynamic illusion in Pointillist paintings, Chuck Close paintings, and photomosaics. I will explore how artists have intuited important features about how our brains extract relevant information about faces and objects, and I will discuss why learning disabilities may be associated with artistic talent.\n",
      "=============================\n",
      "ShowMeHow: translating user interface instructions between applications\n",
      "Many people learn how to use complex authoring applications through tutorials. However, user interfaces for authoring tools differ between versions, platforms, and competing products, limiting the utility of tutorials. Our goal is to make tutorials more useful by enabling users to repurpose tutorials between similar applications. We introduce UI translation interfaces which enable users to locate commands in one application using the interface language of another application. Our end-user tool, ShowMeHow, demonstrates two interaction techniques to accomplish translations: 1) direct manipulation of interface facades and 2) text search for commands using the vocabulary of another application. We discuss tools needed to construct the translation maps that enable these techniques. An initial study (n=12) shows that users can locate unfamiliar commands twice as fast with interface facades. A second study showed that users can work through tutorials written for one application in another application.\n",
      "=============================\n",
      "Issues in combining marking and direct manipulation techniques\n",
      "The direct manipulation paradigm has been effective in helping designers create easy to use mouse and keyboard based interfaces. The development of flat display surfaces and transparent tablets are now making possible interfaces where a user can write directly on the screen using a special stylus. The intention of these types of interfaces is to exploit user’s existing handwriting, mark-up and drawing skills while also providing the benefits of direct manipulation. This paper reports on a test bed program which we are using for exploring hand-marking types of interactions and their integration with direct manipulation interactions.\n",
      "=============================\n",
      "A dynamic grouping technique for ink and audio notes\n",
      "In this paper, we describe a technique for dynamically grouping digital ink and audio to support user interaction in freeform note-taking systems. For ink, groups of strokes might correspond to words, lines, or paragraphs of handwritten text. For audio, groups might be a complete spoken phrase or a speaker turn in a conversation. Ink and audio grouping is important for editing operations such as deleting or moving chunks of ink and audio notes. The grouping technique is based on hierarchical agglomerative clustering. This clustering algorithm yields groups of ink or audio in a range of sizes, depending on the level in the hierarchy, and thus provides structure for simple interactive selection and rapid non-linear expansion of a selection. Ink and audio grouping is also important for marking portions of notes for subsequent browsing and retrieval. Integration of the ink and audio clusters provides a flexible way to browse the notes by selecting the ink cluster and playing the corresponding audio cluster.\n",
      "=============================\n",
      "Enhancing virtual immersion through tactile feedback\n",
      "The lack of tangibility while interacting with virtual objects can be compensated by adding haptic and/or tactile devices or actuators to enhance the user experience. In this demonstration, we present two scenarios that consist of perceiving moving objects on the human body (insects) and feeling physical sensations of virtual thermal objects.\n",
      "=============================\n",
      "The DigitalDesk calculator: tangible manipulation on a desk top display\n",
      "Today’s electronic desktop is quite separate from the physical desk of the user. Electronic documents lack many useful properties of paper, and paper lacks useful properties of electronic documents. Instead of making the electronic desktop more like the physical desk, this work attempts the opposite: to give the physical desk electronic properties and merge the two desktops into one. This paper describes a desk with a computer-controlled camera and projector above it. The camera sees where the user is pointing, rmd it reads portions of documents that are placed on the desk. The projector displays feedback and electronic objects onto the desk surface. This DigitalDesk adds electronic features to physical paper, and it adds physical features to electronic documents. The system allows the user to interact with paper and electronic objects by touching them with a bare finger (digit). Instead of “direct” manipulation with a mouse, this is tangible manipulation with a finger. The DigitalDesk Calculator is a prototype example of a simple application that can benefit from the interaction techniques enabled by this desktop. The paper begins by discussing the motivation behind this work, then describes the Digi talDesk, tangible manipulation, and the calculator prototype. It then discusses implementation details and ends with ideas for the future of tangible manipulation.\n",
      "=============================\n",
      "Ubisonus: spatial freeform interactive speakers\n",
      "We present freeform interactive speakers for creating spatial sound experiences from a variety of surfaces. Traditional surround sound systems are widely used and consist of multiple electromagnetic speakers that create point sound sources within a space. Our proposed system creates directional sound and can be easily embedded into architecture, furniture and many everyday objects. We use electrostatic loudspeaker technology made from thin, flexible, lightweight and low cost materials and can be of different size and shape. In this demonstration we will show various configurations such as single speaker, speaker array and tangible speakers for playful and exciting interactions with spatial sounds. This is an example of new possibilities for the design of various interactive surfaces.\n",
      "=============================\n",
      "Needle user interface: a sewing interface using layered conductive fabrics\n",
      "Embroidery is a creative manual activity practiced by many people for a living. Such a craft demands skill and knowledge, and as it is sometimes complicated and delicate, it can be difficult for beginners to learn. We propose a system, named the Needle User Interface, which enables sewers to record and share their needlework, and receive feedback. In particular, this system can detect the position and orientation of a needle being inserted into and removed from a textile. Moreover, this system can give visual, auditory, and haptic feedback to users in real time for directing their ac-tions appropriately. In this paper, we describe the system design, the input system, and the feedback delivery mechanism.\n",
      "=============================\n",
      "Audio hallway: a virtual acoustic environment for browsing\n",
      "This paper describes the Audio Hallway, a virtual acoustic environment for browsing collections of related audio files. The user travels up and down the Hallway by head motion, passing “rooms” alternately on the left and right sides. Emanating from each room is an auditory collage of “braided audio” which acoustically indicates the contents of the room. Each room represents a broadcast radio news story, and the contents are a collection of individual “sound bites” or actualities related to that story. Upon entering a room, the individual sounds comprising that story are arrayed spatially in front of the listener, with auditory focus controlled by head rotation. The main design challenge for the Audio Hallway is adequately controlling the auditory interface to position sounds so that spatial memory can facilitate navigation and recall in the absence of visual cues.\n",
      "=============================\n",
      "MagGetz: customizable passive tangible controllers on and around conventional mobile devices\n",
      "This paper proposes user-customizable passive control widgets, called MagGetz, which enable tangible interaction on and around mobile devices without requiring power or wireless connections. This is achieved by tracking and ana-lyzing the magnetic field generated by controllers attached on and around the device through a single magnetometer, which is commonly integrated in smartphones today. The proposed method provides users with a broader interaction area, customizable input layouts, richer physical clues, and higher input expressiveness without the need for hardware modifications. We have presented a software toolkit and several applications using MagGetz.\n",
      "=============================\n",
      "Supporting cooperative and personal surfing with a desktop assistant\n",
      "We motivate the use of desktop assistants in the context of web surfing and show how such a too1 may be used to support activities in both cooperative and personal surfing. , By cooperative surfing we mean surfing by a community of users who choose to cooperatively and asynchronously build up knowledge structures relevant to their group. Specifically, we describe the design of an assistant called Vistabar, which lives on the Windows desktop and operates on the currently active web browser. Vistabar instances working for individual users support the authoring of annotations and shared bookmark hierarchies, and work with profiles of community interests to make findings highly available. Thus, they support a form of community memory. Visfabar also serves as a form of personal memory by indexing pages the user sees to assist in recall. We present rationale for the assistant’s design, describe roles it could play to support surfing (including those mentioned above), tid suggest efficient implementation strategies where appropriate.\n",
      "=============================\n",
      "G-raffe: an elevating tangible block supporting 2.5D interaction in a tabletop computing environment\n",
      "We present an elevating tangible block, G-raffe, supporting 2.5-dimensional (2.5-D) interaction in a tabletop computing environment. There is a lack of specialized interface devices for tabletop computing environments. G-raffe overcomes the limitation of conventional 2-D interactions inherited from the vertical desktop computing setting. We adopted a rollable metal tape structure to create up and down movements in a small volume of the block. This also becomes a connecting device for a mobile display to be used with the tabletop computer. We report on our design rationale as well as the results of a preliminary user study.\n",
      "=============================\n",
      "The document lens\n",
      "This paper describes a general visualization technique based on a common strategy for understanding paper documents when their structure is not known, whiclh is to lay the pages of a document in a rectangular array on a large table where the overall structure and distinguishing features can be seen. Given such a presentation, the user wants to quickly view parts of the presentation in detail while remaining in context. A fisheye view or a magnifying lens might be used for this, but they fail to adequately show the global context. The Document Lens is a 3D visualization for large rectangular presentations that allows the user to quickly focus on a part of a presentation while continuously remaining in context. The user grabs a rectangular lens and pulls it around to focus on the desired area at the desired magnification. The presentation outside the lens is stretched to provide a continuous display of the global context. This stretching is efficiently implemented with affine transformations, allowing text documents to be viewed as a whole with an interactive visualization.\n",
      "=============================\n",
      "Viz: a visual analysis suite for explaining local search behavior\n",
      "NP-hard combinatorial optimization problems are common in real life. Due to their intractability, local search algorithms are often used to solve such problems. Since these algorithms are heuristic-based, it is hard to understand how to improve or tune them. We propose an interactive visualization tool, VIZ, meant for understanding the behavior of local search. VIZ uses animation of abstract search trajectories with other visualizations which are also animated in a VCR-like fashion to graphically playback the algorithm behavior. It combines generic visualizations applicable on arbitrary algorithms with algorithm and problem specific visualizations. We use a variety of techniques such as alpha blending to reduce visual clutter and to smooth animation, highlights and shading, automatically generated index points for playback, and visual comparison of two algorithms. The use of multiple viewpoints can be an effective way of understanding search behavior and highlight algorithm behavior which might otherwise be hidden.\n",
      "=============================\n",
      "Frameworks for interactive, extensible, information-intensive applications\n",
      "We describe a set of application frameworks designed especially to support information-intensive applications in complex domains, where the visual organization of an application's information is critical. Our frameworks, called visual formalisms, provide the semantic structures and editing operations, as well as the visual layout algorithms, needed to create a complete application. Examples of visual formalisms include tables, panels, graphs, and outlines. They are designed to be extended both by programmers, through subclassing, and by end users, through an integrated extension language.\n",
      "=============================\n",
      "ReVision: automated classification, analysis and redesign of chart images\n",
      "Poorly designed charts are prevalent in reports, magazines, books and on the Web. Most of these charts are only available as bitmap images; without access to the underlying data it is prohibitively difficult for viewers to create more effective visual representations. In response we present ReVision, a system that automatically redesigns visualizations to improve graphical perception. Given a bitmap image of a chart as input, ReVision applies computer vision and machine learning techniques to identify the chart type (e.g., pie chart, bar chart, scatterplot, etc.). It then extracts the graphical marks and infers the underlying data. Using a corpus of images drawn from the web, ReVision achieves image classification accuracy of 96% across ten chart categories. It also accurately extracts marks from 79% of bar charts and 62% of pie charts, and from these charts it successfully extracts data from 71% of bar charts and 64% of pie charts. ReVision then applies perceptually-based design principles to populate an interactive gallery of redesigned charts. With this interface, users can view alternative chart designs and retarget content to different visual styles.\n",
      "=============================\n",
      "Collapse-to-zoom: viewing web pages on small screen devices by interactively removing irrelevant content\n",
      "Overview visualizations for small-screen web browsers were designed to provide users with visual context and to allow them to rapidly zoom in on tiles of relevant content. Given that content in the overview is reduced, however, users are often unable to tell which tiles hold the relevant material, which can force them to adopt a time-consuming hunt-and-peck strategy. Collapse-to-zoom addresses this issue by offering an alternative exploration strategy. In addition to allowing users to zoom into relevant areas, collapse-to-zoom allows users to collapse areas deemed irrelevant, such as columns containing menus, archive material, or advertising. Collapsing content causes all remaining content to expand in size causing it to reveal more detail, which increases the user's chance of identifying relevant content. Collapse-to-zoom navigation is based on a hybrid between a marquee selection tool and a marking menu, called marquee menu. It offers four commands for collapsing content areas at different granularities and to switch to a full-size reading view of what is left of the page.\n",
      "=============================\n",
      "TactileTape: low-cost touch sensing on curved surfaces\n",
      "TactileTape is a one-dimensional touch sensor that looks and behaves like regular tape. It can be constructed from everyday materials (a pencil, tin foil, and shelf liner) and senses single-touch input on curved and deformable surfaces. It is used as a roll of touch sensitive material from which designers cut pieces to quickly add touch sensitive strips to physical prototypes. TactileTape is low-cost, easy to interface, and, unlike current non-planar touch solutions [2,7,11], it is better adapted for the rapid exploration and iteration in the early design stage.\n",
      "=============================\n",
      "Elastic Windows: a hierarchical multi-window World-Wide Web browser\n",
      "ABSTRACT The World-Wide Web is becoming an invaluable source for the information needs of many users. However, current browsers are still primitive, in that they do not support many of the navigation needs of users, as indicated by user studies. They do not provide an overview and a sense of location in the information structure being browsed. Also they do not facilitate organization and filtering of information nor aid users in accessing already visited pages without high cognitive demands. In this paper, a new browsing interface is proposed with multiple hierarchical windows and efficient multiple window operations. It provides a flexible environment where users can quickly organize, filter, and restructure the information on the screen as they reformulate their goals. Overviews can give the user a sense of location in the browsing history as well as provide fast access to a hierarchy of pages.\n",
      "=============================\n",
      "Quikwriting: continuous stylus-based text entry\n",
      "We present a “heads-up” shorthand for entering text on a stylus-based computer very rapidly. The innovations are that (i) the stylus need never be lifted from the surface, and that (ii) the user need never stop moving the stylus. Continuous multi-word text of arbitrary length can be written fluidly, even as a single continuous gesture if desired.\n",
      "=============================\n",
      "Interactive exploration and selection in volumetric datasets with color tunneling\n",
      "Interactive data exploration and manipulation are often hindered by dataset sizes. For 3D data, this is aggravated by occlusion, important adjacencies, and entangled patterns. Such challenges make visual interaction via common filtering techniques hard. We describe a set of real-time multi-dimensional data deformation techniques that aim to help users to easily select, analyze, and eliminate spatial and data patterns. Our techniques allow animation between view configurations, semantic filtering and view deformation. Any data subset can be selected at any step along the animation. Data can be filtered and deformed to reduce occlusion and ease complex data selections. Our techniques are simple to learn and implement, flexible, and real-time interactive with datasets of tens of millions of data points. We demonstrate our techniques on three domain areas: 2D image segmentation and manipulation, 3D medical volume exploration, and astrophysical exploration.\n",
      "=============================\n",
      "Crowdboard: an augmented whiteboard to support large-scale co-design\n",
      "Co-design efforts attempt to account for many diverse viewpoints. However, design teams lack support for meaningful real-time interaction with a large community of potential stakeholders. We present Crowdboard, a novel whiteboard system that enables many potential stakeholders to provide real-time input during early-stage design activities, such as concept mapping. Local design teams develop ideas on a standard whiteboard, which is augmented with annotations and comments from online participants. The system makes it possible for design teams to solicit real-time opinions and ideas from a community of people intrinsically motivated to shape the product/service.\n",
      "=============================\n",
      "Exposing and understanding scrolling transfer functions\n",
      "Scrolling is controlled through many forms of input devices, such as mouse wheels, trackpad gestures, arrow keys, and joysticks. Performance with these devices can be adjusted by introducing variable transfer functions to alter the range of expressible speed, precision, and sensitivity. However, existing transfer functions are typically \"black boxes\" bundled into proprietary operating systems and drivers. This presents three problems for researchers: (1) a lack of knowledge about the current state of the field; (2) a difficulty in replicating research that uses scrolling devices; and (3) a potential experimental confound when evaluating scrolling devices and techniques. These three problems are caused by gaps in researchers' knowledge about what device and movement factors are important for scrolling transfer functions, and about how existing devices and drivers use these factors. We fill these knowledge gaps with a framework of transfer function factors for scrolling, and a method for analysing proprietary transfer functions---demonstrating how state of the art commercial devices accommodate some of the human control phenomena observed in prior studies.\n",
      "=============================\n",
      "Fluid interaction techniques for the control and annotation of digital video\n",
      "We explore a variety of interaction and visualization techniques for fluid navigation, segmentation, linking, and annotation of digital videos. These techniques are developed within a concept prototype called LEAN that is designed for use with pressure-sensitive digitizer tablets. These techniques include a transient position+velocity widget that allows users not only to move around a point of interest on a video, but also to rewind or fast forward at a controlled variable speed. We also present a new variation of fish-eye views called twist-lens, and incorporate this into a position control slider designed for the effective navigation and viewing of large sequences of video frames. We also explore a new style of widgets that exploit the use of the pen's pressure-sensing capability, increasing the input vocabulary available to the user. Finally, we elaborate on how annotations referring to objects that are temporal in nature, such as video, may be thought of as links, and fluidly constructed, visualized and navigated.\n",
      "=============================\n",
      "Mediated voice communication via mobile IP\n",
      "Impromptu is a mobile audio device which uses wireless Internet Protocol (IP) to access novel computer-mediated voice communication channels. These channels show the richness of IP-based communication as compared to conventional mobile telephony, adding audio processing and storage in the network, and flexible, user-centered call control protocols. These channels may be synchronous, asynchronous, or event-triggered, or even change modes as a function of other user activity. The demands of these modes plus the need to navigate with an entirely non-visual user interface are met with a number of audio-oriented user interaction techniques.\n",
      "=============================\n",
      "Where is information visualization technology going?\n",
      "Over the past few years a lot of different information visualization techniques have been proposed. Being a relatively new and large field, the spectrum of emerging techniques has not clearly been identified. Another major consequence of the youthfulness of the field is that very few evaluation have been conducted so far. The aim of the panel will be to address these two points. First, panelist will characterize the spectrum of information visualization technology depending on tasks, users or data. Panelists will further discuss future trends in visualization technology by determining which are the most important features or challenges that information visualization systems should address. Second, the discussion will focus on how these systems m to be evaluated through controlled experiments, system evaluation, long-time studies, verbal protocols, theoretical evaluations, or else? INTRODUCTION Most information visualization systems that have been proposed over the past few years have been designed for very specific tasks, users or data structures. Characterizing the spectrum of information visualization technology often ends up in a long catalog of diffenmts ystem. Nevertheless, the field is now mature enough to extract from all these systems some of the main characteristics they share. The aim of the panel will be to determine the most important features of these systems and to discuss the future trends and challenges that they will have to face. Permission to make digital/hard copies of all or part of this material for personal or classroom use is granted without fee provided that the copies a~e not made or distributed for profit or commercial advantage, the copyngbt notice, the title of the publication and its date appear, and notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission andlor fee. UIST ’96 Seattle Washington USA @ 1996 ACM 0-89791-798-7/96/11 ..$3.50 Catherine Pk2isant A.V. Williams Building, University of Maryland College Park, MD 20742, U.S.A. Tel: +1 (301) 405-2768, Email: plaisant@cs.umd.edu Matthew Chalmers UBILAB\n",
      "=============================\n",
      "Alice: easy to use interactive 3D graphics\n",
      "Alice is a rapid prototyping system used to create three dimensional graphics simulations like those seen in virtual reality applications. Alice uses an interpreted language called Python as its scripting language to implement user actions. This interactive development environment allows users to explore many more design options than is possible in a compiled language environment. The alpha version of Alice for Windows 95 is available for free over the intemet, with the beta release scheduled for August.\n",
      "=============================\n",
      "An insidious Haptic invasion: adding force feedback to the X desktop\n",
      "This paper describes preliminary work in a project to add force feedback to user interface elements of the X Window System in an attempt to add true “feel” to the window system’s “look and feel”. Additions include adding ridges around icons and menu items to aid interaction, alignment guides for moving windows, and other enhancements to window manipulation. The motivation for this system is the observation that people naturally have many skills for and intuitions about a very rich environment of interaction forces in the non-computer world; however, these skills are largely unused in computer applications. We expect that haptic modifications to conventional graphical user interfaces, such as those we present, can lead to gains in performance, intuition, learnability, and enjoyment of the interf ace. This paper describes details of the implementation of the haptic window system elements, in addition to higher-level haptic design principles and informal observations of users of the system.\n",
      "=============================\n",
      "Content-aware kinetic scrolling for supporting web page navigation\n",
      "Long documents are abundant on the web today, and are accessed in increasing numbers from touchscreen devices such as mobile phones and tablets. Navigating long documents with small screens can be challenging both physically and cognitively because they compel the user to scroll a great deal and to mentally filter for important content. To support navigation of long documents on touchscreen devices, we introduce content-aware kinetic scrolling, a novel scrolling technique that dynamically applies pseudo-haptic feedback in the form of friction around points of high interest within the page. This allows users to quickly find interesting content while exploring without further cluttering the limited visual space. To model degrees of interest (DOI) for a variety of existing web pages, we introduce social wear, a method for capturing DOI based on social signals that indicate collective user interest. Our preliminary evaluation shows that users pay attention to items with kinetic scrolling feedback during search, recognition, and skimming tasks.\n",
      "=============================\n",
      "The \"mighty mouse\" multi-screen collaboration tool\n",
      "Many computer operating systems provide seamless support for multiple display screens, but there are few cross-platform tools for collaborative use of multiple computers in a shared display environment. Mighty Mouse is a novel groupware tool built on the public domain VNC protocol. It is tailored specifically for face-to-face collaboration where multiple heterogeneous computers (usually laptops) are viewed simultaneously (usually via projectors) by people working together on a variety of applications under various operating systems. Mighty Mouse uses only the remote input capability of VNC, but enhances this with various features to support flexible movement between the various platforms, \"floor control\" to facilitate smooth collaboration, and customization features to accommodate different user, platform, and application preferences in a relatively seamless manner. The design rationale arises from specific observations about how people collaborate in meetings, which allows certain simplifying assumptions to be made in the implementation.\n",
      "=============================\n",
      "FatBelt: motivating behavior change through isomorphic feedback\n",
      "The ultimate problem of systems facilitating long-term health and fitness goals is the disconnect between an action and its eventual consequence. As the long-term effects of behavior change are not immediately apparent, it can be hard to motivate the desired behavior over a long period of time. As such, we introduce a system that uses physical feedback through a wearable device that inflates around the stomach as a response to calorie overconsumption, simulating the long-term weight-gain associated with over-eating. We tested a version of this system with 12 users over a period of 2 days, and found a significant decrease in consumption over a baseline period of the same length, suggesting that through physical response, FatBelt moved calorie intake drastically closer to participants' goals. Interviews with participants indicate that isomorphism to the long-term consequences was a large factor in the system's efficacy. In addition, the wearable, physical feedback was perceived as an extension of the user's body, an effect with great emotional consequences.\n",
      "=============================\n",
      "Contact area interaction with sliding widgets\n",
      "We show how to design touchscreen widgets that respond to a finger's contact area. In standard touchscreen systems a finger often appears to touch several screen objects, but the system responds as though only a single pixel is touched. In contact area interaction all objects under the finger respond to the touch. Users activate control widgets by sliding a movable element, as though flipping a switch. These Sliding Widgets resolve selection ambiguity and provide designers with a rich vocabulary of self-disclosing interaction mechanism. We showcase the design of several types of Sliding Widgets, and report study results showing that the simplest of these widgets, the Sliding Button, performs on-par with medium-sized pushbuttons and offers greater accuracy for small-sized buttons.\n",
      "=============================\n",
      "PicturePiper: using a re-configurable pipeline to find images on the Web\n",
      "In this paper, we discuss a re-configurable pipeline architecture that is ideally suited for applications in which a user is interactively managing a stream of data. Currently, document service buses allow stand-alone document services (translation, printing, etc.) to be combined for batch processing. Our architecture allows services to be composed and re-configured on the fly in order to support interactive applications. To motivate the need for such an architecture we address the problem of finding and organizing images on the World Wide Web. The resulting tool, PicturePiper, provides a mechanism for allowing users access to images on the web related to a topic of interest.\n",
      "=============================\n",
      "Accessibility for individuals with color vision deficiency\n",
      "Individuals with Color Vision Deficiency (CVD) are often unable to distinguish between colors that individuals without CVD can distinguish. Recoloring tools exist that modify the colors in an image so they are more easily distinguishable for those with CVD. These tools use models of color differentiation that rely on many assumptions about the environment and user. However, these assumptions rarely hold in real-world use cases, leading to incorrect color modification by recoloring tools. In this doctoral symposium, I will present Situation-Specific Models (SSMs) as a solution to this problem. SSMs are color differentiation models created in-situ via a calibration procedure. This calibration procedure captures the exact color differentiation abilities of the user, allowing a color differentiation model to be created that fits the user and his/her environmental situation. An SSM-based recoloring tool will be able to provide recolored images that most accurately reflect the color differentiation abilities of a particular individual in a particular environment.\n",
      "=============================\n",
      "The architecture and implementation of CPN2000, a post-WIMP graphical application\n",
      "We have developed an interface for editing and simulating Coloured Petri Nets based on toolglasses, marking menus and bi-manual interaction, in order to understand how novel interaction techniques could be supported by a new generation of user interface toolkits. The architecture of CPN2000 is based on three components: the Document Structure stores all the persistent data in the system; the Display Structure represents the contents of the screen and implements rendering and hit detection algorithms; and the Input Structure uses \"instruments\" to manage interaction. The rendering engine is based on OpenGL and a number of techniques have been developed to take advantage of 3D accelerated graphics for a 2D application. Performance data show that high frame rates have been achieved with off-theshelf hardware even with a non-optimized redisplay. This work paves the way towards a post-WIMP UI toolkit.\n",
      "=============================\n",
      "Transparent display interaction without binocular parallax\n",
      "Binocular parallax is a problem for any interaction system that has a transparent display and objects behind it. A proposed quantitative measure called Binocular Selectability Discriminant (BSD) allows UI designers to predict the ability of the user to perform selection task in their transparent display systems, in spite of binocular parallax. A proposed technique called Single-Distance Pseudo Transparency (SDPT) aims to eliminate binocular parallax for on-screen interactions that require precision. A mock-up study shows potentials and directions for future investigation.\n",
      "=============================\n",
      "Visual tracking of bare fingers for interactive surfaces\n",
      "Visual tracking of bare fingers allows more direct manipulation of digital objects, multiple simultaneous users interacting with their two hands, and permits the interaction on large surfaces, using only commodity hardware. After presenting related work, we detail our implementation. Its design is based on our modeling of two classes of algorithms that are key to the tracker: Image Differencing Segmentation (IDS) and Fast Rejection Filters (FRF). We introduce a new chromatic distance for IDS and a FRF that is independent to finger rotation. The system runs at full frame rate (25 Hz) with an average total system latency of 80 ms, independently of the number of tracked fingers. When used in a controlled environment such as a meeting room, its robustness is satisfying for everyday use.\n",
      "=============================\n",
      "Interactive construction: interactive fabrication of functional mechanical devices\n",
      "Personal fabrication tools, such as laser cutters and 3D printers allow users to create precise objects quickly. However, working through a CAD system removes users from the workpiece. Recent interactive fabrication tools reintroduce this directness, but at the expense of precision. In this paper, we introduce constructable, an interactive drafting table that produces precise physical output in every step. Users interact by drafting directly on the workpiece using a hand-held laser pointer. The system tracks the pointer, beautifies its path, and implements its effect by cutting the workpiece using a fast high-powered laser cutter. Constructable achieves precision through tool-specific constraints, user-defined sketch lines, and by using the laser cutter itself for all visual feedback, rather than using a screen or projection. We demonstrate how constructable allows creating simple but functional devices, including a simple gearbox, that cannot be created with traditional interactive fabrication tools.\n",
      "=============================\n",
      "LetterWise: prefix-based disambiguation for mobile text input\n",
      "A new technique to enter text using a mobile phone keypad is described. For text input, the traditional touchtone phone keypad is ambiguous because each key encodes three or four letters. Instead of using a stored dictionary to guess the intended word, our technique uses probabilities of letter sequences --- \"prefixes\" --- to guess the intended letter. Compared to dictionary-based methods, this technique, called LetterWise, takes significantly less memory and allows entry of non-dictionary words without switching to a special input mode. We conducted a longitudinal study to compare LetterWise to Multitap, the conventional text entry method for mobile phones. The experiment included 20 participants (10 LetterWise, 10 Multitap), and each entered phrases of text for 20 sessions of about 30 minutes each. Error rates were similar between the techniques; however, by the end of the experiment the mean entry speed was 36% faster with LetterWise than with Multitap.\n",
      "=============================\n",
      "Time-machine computing: a time-centric approach for the information environment\n",
      "This paper describes the concept of Time-Machine Computing (TMC), a time-centric approach to organizing information on computers. A system based on Time-Machine Computing allows a user to visit the past and the future states of computers. When a user needs to refer to a document that he/she was working on at some other time, he/she can travel in the time dimension and the system restores the computer state at that time. Since the user's activities on the system are automatically archived, the user's daily workspace is seamlessly integrated into the information archive. The combination of spatial information management of the desktop metaphor and time traveling allows a user to organize and archive information without being bothered by folder hierarchies or the file classification problems that are common in today's desktop environments. TMC also provides a mechanism for linking multiple applications and external information sources by exchanging time information. This paper describes the key features of TMC, a time-machine desktop environment called “TimeScape,” and several time-oriented application integration examples.\n",
      "=============================\n",
      "Content-based tools for editing audio stories\n",
      "Audio stories are an engaging form of communication that combine speech and music into compelling narratives. Existing audio editing tools force story producers to manipulate speech and music tracks via tedious, low-level waveform editing. In contrast, we present a set of tools that analyze the audio content of the speech and music and thereby allow producers to work at much higher level. Our tools address several challenges in creating audio stories, including (1) navigating and editing speech, (2) selecting appropriate music for the score, and (3) editing the music to complement the speech. Key features include a transcript-based speech editing tool that automatically propagates edits in the transcript text to the corresponding speech track; a music browser that supports searching based on emotion, tempo, key, or timbral similarity to other songs; and music retargeting tools that make it easy to combine sections of music with the speech. We have used our tools to create audio stories from a variety of raw speech sources, including scripted narratives, interviews and political speeches. Informal feedback from first-time users suggests that our tools are easy to learn and greatly facilitate the process of editing raw footage into a final story.\n",
      "=============================\n",
      "Classroom BRIDGE: using collaborative public and desktop timelines to support activity awareness\n",
      "Classroom BRIDGE supports activity awareness by facilitating planning and goal revision in collaborative, project-based middle school science. It integrates large-screen and desktop views of project times to support incidental creation of awareness information through routine document transactions, integrated presentation of awareness information as part of workspace views, and public access to subgroup activity. It demonstrates and develops an object replication approach to integrating synchronous and asynchronous distributed work for a platform incorporating both desktop and large-screen devices. This paper describes an implementation of these concepts with preliminary evaluation data, using timeline-based user interfaces.\n",
      "=============================\n",
      "Optimizing toolkit-generated graphical interfaces\n",
      "Researchers have developed a variety of toolkits that support the development of highly interactive, graphical, direct manipulation applications such as animations, process monitoring tools, drawing packages, visual programming languages, games, and data and program visualization systems. These toolkits contain many useful features such as 1) structured graphics, 2) automatic display management, 3) constraints, and 4) high-level input-handling models. Despite a number of optimizations that have been described in the literature, most toolkit-generated applications run in a predominantly interpreted mode at runtime: they dynamically determine the set of constraints and objects that must be redisplayed, which requires the use of time consuming algorithms and data structures. The optimizations that do exist rely on semantic information that applies globally to all operations in an application. In this paper we identify a number of optimizations that require local, operation-specific semantic information about an application. For each operation, these optimizations pre-compute update plans that minimize the number of objects that are examined for redisplay, and pre-compute constraint plans that minimize the amount of dynamic scheduling and method dispatching that is performed for constraint satisfaction. We present performance measurements that suggest that these optimizations can significantly improve the performance of an application. We also discuss how a compiler might obtain from a programmer the information required to implement these optimizations.\n",
      "=============================\n",
      "Gamut: demonstrating whole applications\n",
      "Gamut is a new tool for building interactive, graphical software like games, simulations, and educational software. A developer can build entire applications in Gamut’s domain using only programming-by-demonstration (PBD) and never has to look at or modify code to build any behavior. To accomplish this, we have developed a simple, streamlined interaction for demonstrating so that developers can create new examples quickly and can specify negative examples without confusion. Also, Gamut allows the developer to give hints to point out objects in a relationship that would be too time consuming to find by searching. Gamut automatically revises generated code using an efficient algorithm that recursively scans for the differences between a new example and the previous behavior. To correct the discovered differences, Gamut couples heuristic search with a decision tree leamittg algorithm allowing it to build more complicated behaviors than it could using heuristic search alone.\n",
      "=============================\n",
      "Modular and deformable touch-sensitive surfaces based on time domain reflectometry\n",
      "Time domain reflectometry, a technique originally used in diagnosing cable faults, can also locate where a cable is being touched. In this paper, we explore how to extend time domain reflectometry in order to touch-enable thin, modular, and deformable surfaces and devices. We demonstrate how to use this approach to make smart clothing and to rapid prototype touch-sensitive objects of arbitrary shape. To accomplish this, we extend time domain reflectometry in three ways: (1) Thin: We demonstrate how to run time domain reflectometry on a single wire. This allows us to touch-enable thin metal objects, such as guitar strings. (2) Modularity: We present a two-pin connector system that allows users to daisy chain touch-sensitive segments. We illustrate these enhancements with 13 prototypes and a series of performance measurements. (3) Deformability: We create deformable touch devices by mounting stretch-able wire patterns onto elastic tape and meshes. We present selected performance measurements.\n",
      "=============================\n",
      "Nova: low-cost data animation using a radar-sweep metaphor\n",
      "Nova is a simple technique for animating a data sequence whose elements include a primary numeric component and possibly one or more secondary dimensions. We use nova to visualize program behavior such as individual memory allocations, where the number of bytes in each allocation is a natural primary numeric dimension.\n",
      "=============================\n",
      "Powering interactive intelligent systems with the crowd\n",
      "Creating intelligent systems that are able to recognize a user's behavior, understand unrestricted spoken natural language, complete complex tasks, and respond fluently could change the way computers are used in daily life. But fully-automated intelligent systems are a far-off goal -- currently, machines struggle in many real-world settings because problems can be almost entirely unconstrained and can vary greatly between instances. Human computation has been shown to be effective in many of these settings, but is traditionally applied in an offline, batch-processing fashion. My work focuses on a new model of continuous, real-time crowdsourcing that enables interactive crowd-powered systems.\n",
      "=============================\n",
      "Ambient touch: designing tactile interfaces for handheld devices\n",
      "This paper investigates the sense of touch as a channel for communicating with miniature handheld devices. We embedded a PDA with a TouchEngineTM --- a thin, miniature lower-power tactile actuator that we have designed specifically to use in mobile interfaces (Figure 1). Unlike previous tactile actuators, the TouchEngine is a universal tactile display that can produce a wide variety of tactile feelings from simple clicks to complex vibrotactile patterns. Using the TouchEngine, we began exploring the design space of interactive tactile feedback for handheld computers. Here, we investigated only a subset of this space: using touch as the ambient, background channel of interaction. We proposed a general approach to design such tactile interfaces and described several implemented prototypes. Finally, our user studies demonstrated 22% faster task completion when we enhanced handheld tilting interfaces with tactile feedback.\n",
      "=============================\n",
      "Enabling an ecosystem of personal behavioral data\n",
      "Almost every computational system a person interacts with keeps a detailed log of that person's behavior. The possibility of this data promises a breadth of new service opportunities for improving people's lives through deep personalization, tools to manage aspects of their personal wellbeing, and services that support identity construction. However, the way that this data is collected and managed today introduces several challenges that severely limit the utility of this rich data. This thesis maps out a computational ecosystem for personal behavioral data through the design, implementation, and evaluation of Phenom, a web service that factors out common activities in making inferences from personal behavioral data. The primary benefits of Phenom include: a structured process for aggregating and representing user data; support for developing models based on personal behavioral data; and a unified API for accessing inferences made by models within Phenom. To evaluate Phenom for ease of use and versatility, an external set of developers will create example applications with it.\n",
      "=============================\n",
      "Procedural haptic texture\n",
      "We present the Haptic Shading Framework (HSF), a framework for procedurally defining haptic texture. HSF haptic texture shaders are short procedures allowing an application-programmer to easily define interesting haptic surface interaction and the parameters that control the surface properties. These shaders provide the illusion of surface characteristics by altering previously calculated forces from object collision in the haptic pipeline.HSF can be used in an existing haptic application with few modifications. The framework consists of user-programmable modules that are dynamically loaded. This framework and all user-defined procedures are written in C++, with a provided library of useful math and geometry functions. These functions are meant to mimic RenderMan functionality, creating a familiar shading environment. As we demonstrate, many procedural shading methods and algorithms can be directly adopted for haptic shading.\n",
      "=============================\n",
      "Tilting operations for small screen interfaces\n",
      "This TechNote introduces new interaction techniques for small screen devices such as palmtop computers or handheld electric devices, including pagers and cellular phones. Our proposed method uses the tilt of the device itself as input. Using both tilt and buttons, it is possible to build several interaction techniques ranging from menus and scroll bars, to more complicated examples such as a map browsing system and a 3D object viewer. During operation, only one hand is required to both hold and control the device. This feature is especially useful for field workers.\n",
      "=============================\n",
      "E-Block: a tangible programming tool for children\n",
      "E-Block is a tangible programming tool for children aged 5 to 9 which gives children a preliminary understanding of programming. Children can write programs to play a maze game by placing the programming blocks in E-Block. The two stages in a general programming process: programming and running are all embodied in E-Block. We realized E-Block by wireless and infrared technology and gave it feedbacks on both screen and programming blocks. The result of a preliminary user study proved that E-Block is attractive to children and easy to learn and use.\n",
      "=============================\n",
      "Integrated visual representations for programming with real-world input and output\n",
      "As computers become more pervasive, more programs deal with real-world input and output (real-world I/O) such as processing camera images and controlling robots. The real-world I/O usually contains complex data hardly represented by text or symbols, while most of the current integrated development environments (IDEs) are equipped with text-based editors and debuggers. My thesis investigates how visual representations of the real world can be integrated within the text-based development environment to enhance the programming experience. In particular, we have designed and implemented IDEs for three scenarios, all of which make use of photos and videos representing the real world. Based on these experiences, we discuss \"programming with example data,\" a technique where the programmer demonstrates examples to the IDE and writes text-based code with support of the examples.\n",
      "=============================\n",
      "Developing calendar visualizers for the information visualizer\n",
      "The increasing mass of information confronting a business or an individual have created a demand for information management applications. Time-based information, in particular, is an important part of many information access tasks. This paper explores how to use 3D graphics and interactive animation to design and implement visualizers that improve access to large masses of time-based information. Two new visualizers have been developed for the Information Visualizer: 1) the Spiral Calendar was designed for rapid access to an individual's daily schedule, and 2) the Time Lattice was designed for analyzing the time relationships among the schedules of groups of people. The Spiral Calendar embodies a new 3D graphics technique for integrating detail and context by placing objects in a 3D spiral. It demonstrates that advanced graphics techniques can enhance routine office information tasks. The Time Lattice is formed by aligning a collection of 2D calendars. 2D translucent shadows provide views and interactive access to the resulting complex 3D object. The paper focuses on how these visualizations were developed. The Spiral Calendar, in particular, has gone through an entire cycle of development, including design, implementation, evaluation, revision and reuse. Our experience should prove useful to others developing user interfaces based on advanced graphics.\n",
      "=============================\n",
      "Automatic thumbnail cropping and its effectiveness\n",
      "Thumbnail images provide users of image retrieval and browsing systems with a method for quickly scanning large numbers of images. Recognizing the objects in an image is important in many retrieval tasks, but thumbnails generated by shrinking the original image often render objects illegible. We study the ability of computer vision systems to detect key components of images so that automated cropping, prior to shrinking, can render objects more recognizable. We evaluate automatic cropping techniques 1) based on a general method that detects salient portions of images, and 2) based on automatic face detection. Our user study shows that these methods result in small thumbnails that are substantially more recognizable and easier to find in the context of visual search.\n",
      "=============================\n",
      "Glyphs: flyweight objects for user interfaces\n",
      "Current user interface too&its provide components that are complex and expensive. Programmers cannot use these components for many kinds of application data because the resulting implementation would be awkward and inefficient. We have defined a set of small, simple components, called glyphs, that programmers can use in large numbers to build user interfaces. To show that glyphs are simple and efficient, we have implemented a WYSIWYG document editor. The editor’s performance is comparable to that of similar editors built with current tools, but its implementation is much simpler. We used the editor to create and print this paper.\n",
      "=============================\n",
      "Video digests: a browsable, skimmable format for informational lecture videos\n",
      "Increasingly, authors are publishing long informational talks, lectures, and distance-learning videos online. However, it is difficult to browse and skim the content of such videos using current timeline-based video players. Video digests are a new format for informational videos that afford browsing and skimming by segmenting videos into a chapter/section structure and providing short text summaries and thumbnails for each section. Viewers can navigate by reading the summaries and clicking on sections to access the corresponding point in the video. We present a set of tools to help authors create such digests using transcript-based interactions. With our tools, authors can manually create a video digest from scratch, or they can automatically generate a digest by applying a combination of algorithmic and crowdsourcing techniques and then manually refine it as needed. Feedback from first-time users suggests that our transcript-based authoring tools and automated techniques greatly facilitate video digest creation. In an evaluative crowdsourced study we find that given a short viewing time, video digests support browsing and skimming better than timeline-based or transcript-based video players.\n",
      "=============================\n",
      "Powers of ten thousand: navigating in large information spaces\n",
      "How would you interactively browse a <italic>very</italic> large display space, for example, a street map of the entire United States?  The traditional solution is <italic>zoom</italic> and <italic>pan</italic>.  But each time a zoom-in operation takes place, the context from which it came is visually lost.  Sequential applications of the zoom-in and zoom-out operations may become tedious.  This paper proposes an alternative technique, the <italic>macroscope</italic>, based on zooming and planning in multiple translucent layers.  A macroscope display should comfortably permit browsing continuously on a single image, or set of images in multiple resolutions, on a scale of at least 1 to 10,000.\n",
      "=============================\n",
      "Guided gesture support in the paper PDA\n",
      "Ordinary paper offers properties of readability, fluidity, flexibility, cost, and portability that current electronic devices are often hard pressed to match. In fact, a lofty goal for many interactive systems is to be \"as easy to use as pencil and paper\". However, the static nature of paper does not support a number of capabilities, such as search and hyperlinking that an electronic device can provide. The Paper PDA project explores ways in which hybrid paper electronic interfaces can bring some of the capabilities of the electronic medium to interactions occurring on real paper. Key to this effort is the invention of on-paper interaction techniques which retain the flexibility and fluidity of normal pen and paper, but which are structured enough to allow robust interpretation and processing in the digital world. This paper considers the design of a class of simple printed templates that allow users to make common marks in a fluid fashion, and allow additional gestures to be invented by the users to meet their needs, but at the same time encourages marks that are quite easy to recognize.\n",
      "=============================\n",
      "Brainstorm, define, prototype: timing constraints to balance appropriate and novel design\n",
      "We present the results of a human creativity experiment that examined the effect of varying the timing of narrowed constraints. Participants were asked to create a static web ad for Stanford University guided under a timed design process and were introduced to a narrowed constraint either at the beginning, middle, or end of the prototyping process. The narrow constraint addressed goal and task constraints by specifying the target audience and ad size. We find that groups introduced to narrow constraints prior to the brainstorm yielded more appropriate results, while those introduced prior to the final production yielded more novel results. Our results suggest that effective timing of design constraints may further optimize ideation and design methodologies.\n",
      "=============================\n",
      "A proposal for a MMG-based hand gesture recognition method\n",
      "We propose a novel hand-gesture recognition method based on mechanomyograms (MMGs). Skeletal muscles generate sounds specific to their activity. By recording and analyzing these sounds, MMGs provide means to evaluate the activity. Previous research revealed that specific motions produce specific sounds enabling human motion to be classified based on MMGs. In that research, microphones and accelerometers are often used to record muscle sounds. However, environmental conditions such as noise and human motion itself easily overwhelm such sensors. In this paper, we propose to use piezoelectric-based sensing of MMGs to improve robustness from environmental conditions. The preliminary evaluation shows this method is capable of classifying several hand gestures correctly with high accuracy under certain situations.\n",
      "=============================\n",
      "User interface toolkit mechanisms for securing interface elements\n",
      "User interface toolkit research has traditionally assumed that developers have full control of an interface. This assumption is challenged by the mashup nature of many modern interfaces, in which different portions of a single interface are implemented by multiple, potentially mutually distrusting developers (e.g., an Android application embedding a third-party advertisement). We propose considering security as a primary goal for user interface toolkits. We motivate the need for security at this level by examining today's mashup scenarios, in which security and interface flexibility are not simultaneously achieved. We describe a security-aware user interface toolkit architecture that secures interface elements while providing developers with the flexibility and expressivity traditionally desired in a user interface toolkit. By challenging trust assumptions inherent in existing approaches, this architecture effectively addresses important interface-level security concerns.\n",
      "=============================\n",
      "STIMTAC: a tactile input device with programmable friction\n",
      "We present the STIMTAC, a touchpad device that supports friction reduction. Contrary to traditional vibrotactile approaches, the STIMTAC provides information passively, acting as a texture display. It does not transfer energy to the user but modifies how energy is dissipated within the contact area by a user-initiated friction process. We report on the iterative process that led to the current hardware design and briefly describe the software framework that we are developing to illustrate its potential.\n",
      "=============================\n",
      "Providing visually rich resizable images for user interface components\n",
      "User interface components such as buttons, scrollbars, menus, as well as various types of containers and separators, normally need to be resizable so that they can conform to the needs of the contents within them, or the environment in which they are placed. Unfortunately, in the past, providing dynamically resizable component appearances has required writing code to draw the component. As a result, visual designers have often been cut off from the ability to create these appearances. Even when visual designers can be involved, drawing programmatically is comparatively very difficult. Hence, components created this way have not typically contained artistically rich appearances. Because of this need to write drawing code, component appearances have traditionally been quite plain, and have been controlled primarily by a few toolkit writers. This paper presents a suite of very simple techniques, along with a few composition mechanisms, that are designed to overcome this problem. These techniques allow visually rich, dynamically resizable, images to be provided using primarily conventional drawing tools (and with no programming or programming-like activities at all).\n",
      "=============================\n",
      "Loupe: a handheld near-eye display\n",
      "Loupe is a novel interactive device with a near-eye virtual display similar to head-up display glasses that retains a handheld form factor. We present our hardware implementation and discuss our user interface that leverages Loupe's unique combination of properties. In particular, we present our input capabilities, spatial metaphor, opportunities for using the round aspect of Loupe, and our use of focal depth. We demonstrate how those capabilities come together in an example application designed to allow quick access to information feeds.\n",
      "=============================\n",
      "Interacting with live preview frames: in-picture cues for a digital camera interface\n",
      "We present a new interaction paradigm for digital cameras aimed at making interactive imaging algorithms accessible on these devices. In our system, the user creates visual cues in front of the lens during the live preview frames that are continuously processed before the snapshot is taken. These cues are recognized by the camera's image processor to control the lens or other settings. We design and analyze vision-based camera interactions, including focus and zoom controls, and argue that the vision-based paradigm offers a new level of photographer control needed for the next generation of digital cameras.\n",
      "=============================\n",
      "RoboJockey: real-time, simultaneous, and continuous creation of robot actions for everyone\n",
      "We developed a RoboJockey (Robot Jockey) interface for coordinating robot actions, such as dancing - similar to \"Disc jockey\" and \"Video jockey\". The system enables a user to choreograph a dance for a robot to perform by using a simple visual language. Users can coordinate humanoid robot actions with a combination of arm and leg movements. Every action is automatically performed to background music and beat. The RoboJockey will give a new entertainment experience with robots to the end-users.\n",
      "=============================\n",
      "Techniques for low cost spatial audio\n",
      "There are a variety of potential uses for interactive spatial sound in human-computer interfaces, but hardware costs have made most of these applications impractical. Recently, however, single-chip digital signal processors have made real-time spatial audio an affordable possibility for many workstations. This paper describes an efficient spatialization technique and the associated computational requirements. Issues specific to the use of spatial audio in user interfaces are addressed. The paper also describes the design of a network server for spatial audio that can support a number of users at modest cost.\n",
      "=============================\n",
      "Good vibrations: an evaluation of vibrotactile impedance matching for low power wearable applications\n",
      "Vibrotactile devices suffer from poor energy efficiency, arising from a mismatch between the device and the impedance of the human skin. This results in over-sized actuators and excessive power consumption, and prevents development of more sophisticated, miniaturized and low-power mobile tactile devices. In this paper, we present the experimental evaluation of a vibrotactile system designed to match the impedance of the skin to the impedance of the actuator. This system is able to quadruple the motion of the skin without increasing power consumption, and produce sensations equivalent to a standard system while consuming 1/2 of the power. By greatly reducing the size and power constraints of vibrotactile actuators, this technology offers a means to realize more sophisticated, smaller haptic devices for the user interface community.\n",
      "=============================\n",
      "Enhanced area cursors: reducing fine pointing demands for people with motor impairments\n",
      "Computer users with motor impairments face major challenges with conventional mouse pointing. These challenges are mostly due to fine pointing corrections at the final stages of target acquisition. To reduce the need for correction-phase pointing and to lessen the effects of small target size on acquisition difficulty, we introduce four enhanced area cursors, two of which rely on magnification and two of which use goal crossing. In a study with motor-impaired and able-bodied users, we compared the new designs to the point and Bubble cursors, the latter of which had not been evaluated for users with motor impairments. Two enhanced area cursors, the Visual-Motor-Magnifier and Click-and-Cross, were the most successful new designs for users with motor impairments, reducing selection time for small targets by 19%, corrective submovements by 45%, and error rate by up to 82% compared to the point cursor. Although the Bubble cursor also improved performance, participants with motor impairments unanimously preferred the enhanced area cursors.\n",
      "=============================\n",
      "Reflective physical prototyping through integrated design, test, and analysis\n",
      "Prototyping is the pivotal activity that structures innovation, collaboration, and creativity in design. Prototypes embody design hypotheses and enable designers to test them. Framin design as a thinking-by-doing activity foregrounds iteration as a central concern. This paper presents d.tools, a toolkit that embodies an iterative-design-centered approach to prototyping information appliances. This work offers contributions in three areas. First, d.tools introduces a statechart-based visual design tool that provides a low threshold for early-stage prototyping, extensible through code for higher-fidelity prototypes. Second, our research introduces three important types of hardware extensibility - at the hardware-to-PC interface, the intra-hardware communication level, and the circuit level. Third, d.tools integrates design, test, and analysis of information appliances. We have evaluated d.tools through three studies: a laboratory study with thirteen participants; rebuilding prototypes of existing and emerging devices; and by observing seven student teams who built prototypes with d.tools.\n",
      "=============================\n",
      "Metisse is not a 3D desktop!\n",
      "Twenty years after the general adoption of overlapping windows and the desktop metaphor, modern window systems differ mainly in minor details such as window decorations or mouse and keyboard bindings. While a number of innovative window management techniques have been proposed, few of them have been evaluated and fewer have made their way into real systems. We believe that one reason for this is that most of the proposed techniques have been designed using a low fidelity approach and were never made properly available. In this paper, we present Metisse, a fully functional window system specifically created to facilitate the design, the implementation and the evaluation of innovative window management techniques. We describe the architecture of the system, some of its implementation details and present several examples that illustrate its potential.\n",
      "=============================\n",
      "No more bricolage!: methods and tools to characterize, replicate and compare pointing transfer functions\n",
      "Transfer functions are the only pointing facilitation technique actually used in modern graphical interfaces involving the indirect control of an on-screen cursor. But despite their general use, very little is known about them. We present EchoMouse, a device we created to characterize the transfer functions of any system, and libpointing, a toolkit that we developed to replicate and compare the ones used by Windows, OS X and Xorg. We describe these functions and report on an experiment that compared the default one of the three systems. Our results show that these default functions improve performance up to 24% compared to a unitless constant CD gain. We also found significant differences between them, with the one from OS X improving performance for small target widths but reducing its performance up to 9% for larger ones compared to Windows and Xorg. These results notably suggest replacing the constant CD gain function commonly used by HCI researchers by the default function of the considered systems.\n",
      "=============================\n",
      "Real-world interaction using the FieldMouse\n",
      "We introduce an inexpensive position input device called the FieldMouse, with which a computer can tell the position of the device on paper or any flat surface without using special input tablets or position detection devices. A FieldMouse is a combination of an ID recognizer like a barcode reader and a mouse which detects relative movement of the device. Using a FieldMouse, a user first detects an ID on paper by using the barcode reader, and then drags it from the ID using the mouse. If the location of the ID is known, the location of the dragged FieldMouse can also be calculated by adding the amount of movement from the ID to the position of the FieldMouse. Using a FieldMouse in this way, any flat surface can work as a pointing device that supports absolute position input, just by putting an ID tag somewhere on the surface. A FieldMouse can also be used for enabling a graphical user interface (GUI) on paper or on any flat surface by analyzing the direction and the amount of mouse movement after detecting an ID. In this paper, we introduce how a FieldMouse can be used in various situations to enable computing in real-world environments.\n",
      "=============================\n",
      "Teegi: tangible EEG interface\n",
      "We introduce Teegi, a Tangible ElectroEncephaloGraphy (EEG) Interface that enables novice users to get to know more about something as complex as brain signals, in an easy, engaging and informative way. To this end, we have designed a new system based on a unique combination of spatial augmented reality, tangible interaction and real-time neurotechnologies. With Teegi, a user can visualize and analyze his or her own brain activity in real-time, on a tangible character that can be easily manipulated, and with which it is possible to interact. An exploration study has shown that interacting with Teegi seems to be easy, motivating, reliable and informative. Overall, this suggests that Teegi is a promising and relevant training and mediation tool for the general public.\n",
      "=============================\n",
      "A temporal model for multi-level undo and redo\n",
      "All of these systems rely on an explicit model of history, which can be scanned to support search or “navigation” over a timeline, and all allow their timelines to be “traversed” to move the application’s state to other points in its history. However, as powerful as these applications are, their timeline representations are for the most part exceedingly simple. They typically support only linear, not branching timelines (GINA and Timewarp are exceptions, however); the “nodes” in a timeline must represent atomic operations with side effects that are well understood at the time the application is created; and, typically, the timeline of the entire application must be navigated or traversed as a whole—it is impossible to have a portion of the timeline exist in a “bubble” that can be manipulated separately.\n",
      "=============================\n",
      "D-Macs: building multi-device user interfaces by demonstrating, sharing and replaying design actions\n",
      "Multi-device user interface design mostly implies creating suitable interface for each targeted device, using a diverse set of design tools and toolkits. This is a time consuming activity, concerning a lot of repetitive design actions without support for reusing this effort in later designs. In this paper, we propose D-Macs: a design tool that allows designers to record their design actions across devices, to share these actions with other designers and to replay their own design actions and those of others. D-Macs lowers the burden in multi-device user interface design and can reduce the necessity for manually repeating design actions.\n",
      "=============================\n",
      "Tracs: transparency-control for see-through displays\n",
      "We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n",
      "=============================\n",
      "Quick: a user-interface design kit for non-programmers\n",
      "Interface design toolkits have proven useful, both for exploring conceptual issues in user interface design, and for constructing product quality interfaces for commercial applications. However, most such toolkits focus on a relatively low-level of abstraction, are oriented towards design of a limited set of “standard” interface types, and are intended for expert users. Our QUICK system explores the opposite pole. QUICK is a toolkit for the design of highly interactive direct manipulation interfaces oriented specifically towards nonprogrammers. The challenge we face in QUICK lies in maximizing power and flexibility in an extremely simple environment. We explore the utility of direct manipulation, the object oriented paradigm and a structure editor in this context.\n",
      "=============================\n",
      "Concept clustering ina query interface to an image database\n",
      "The VIMSYS project provides environmental scientists with the ability to perform content-based querying over a database of satellite images. This paper descriks the enduser query interface which facilitates identification of multiple object types, reduces emphasis on numerical data, and simplifies the use of numerous sets of parameters. To address these issues in a way that can be applied to similar databases, the end-user query interface utilizes abstract query structures called clusters, in addition to frames and links. We describe the requirements of the system, review commonly available query methods, discuss how the VIMSYS interface meets the needs of the audience, present user reaction to the prototype, and summarize other relevant details of VIMSYS.\n",
      "=============================\n",
      "Proximal sensing: supporting context sensitive\n",
      "This talk addresses the issue of increasing complexity for the user that accompanies new functionality. Briefly, we discuss how complexity can, through appropriate design, be offloaded to the system -at least for secondary commands. Consider photography, for example. The 35 mm SLR of a decade ago was analogous to MS-DOS. You could do everything in theory, but in practice, were unlikely to do anything without making an error. When we think of photography, however, we see that there are only two primary decisions: “what” and “when”, which correspond to the two primary actions: “point” and “click”. By embedding domain-specific knowledge, modem cameras off-load all other decisions to the computer (a.k.a. camera) with the option of overriding the defaults. The net result is that the needs of the novice and expert are met with a single apparatus device. What we do in this presentation is talk about how this type of off-loading can be supported, and why this should be done. We do this by example, drawing mainly on the experiences of the Ontario Telepresence Project. November 14-17, 1995 UIST ’95 169\n",
      "=============================\n",
      "Ambient surface: enhancing interface capabilities of mobile objects aided by ambient environment\n",
      "We introduce Ambient Surface, an interactive surrounded equipment for enhancing interface capabilities of mobile devices placed on an ordinary surface. Object information and a user's interaction are captured by 2D/3D cameras, and appropriate feedback images are projected on the surface. By the help of the ambient system, we may not only provide a wider screen for mobile devices with a limited screen size, but also allow analog objects to dynamically interact with users. We believe that this demo will help interaction designers to draw new inspiration of utilizing mobile objects with ambient environment.\n",
      "=============================\n",
      "Graphical specification of flexible user interface displays\n",
      "This paper describes the implementation concepts behind the user interface editor of the Apogee UIMS. This editor allows many aspects of a user interface to be specified graphically without a conventional textual specification. The system supports the specification of flexible user interfaces — ones that can adapt automatically to changes in the size of objects they present and that can adapt to specific user needs in a dynamic and responsive fashion. To serve as an implementation base for this editor, the Apogee UIMS supports an active data model based on one-way constraints. This model is implemented by a small object-oriented programming language embedded within the system.\n",
      "=============================\n",
      "A procedure for evaluating human-computer interface development tools\n",
      "Human-computer interface development tools are interactive systems that support production and execution of the human-computer interface. With their recent proliferation, evaluations and comparisons are constantly done, but without a formal, structured approach. Addressing these problems is difficult, largely because of the relative newness of such tools, because of the many different kinds of systems that are called UIMS, and because of their inherent complexity. These tools are complex because human-computer interfaces, which produce tools, are complex.\n",
      "=============================\n",
      "Comparing and managing multiple versions of slide presentations\n",
      "Despite the ubiquity of slide presentations, managing multiple presentations remains a challenge. Understanding how multiple versions of a presentation are related to one another, assembling new presentations from existing presentations, and collaborating to create and edit presentations are difficult tasks. In this paper, we explore techniques for comparing and managing multiple slide presentations. We propose a general comparison framework for computing similarities and differences between slides. Based on this framework we develop an interactive tool for visually comparing multiple presentations. The interactive visualization facilitates understanding how presentations have evolved over time. We show how the interactive tool can be used to assemble new presentations from a collection of older ones and to merge changes from multiple presentation authors.\n",
      "=============================\n",
      "d.tour: style-based exploration of design example galleries\n",
      "In design, people often seek examples for inspiration. However, current example-finding practices suffer many drawbacks: templates present designs without a usage context; search engines can only examine the text on a page. This paper introduces exploratory techniques for finding relevant and inspiring design examples. These novel techniques include searching by stylistic similarity to a known example design and searching by stylistic keyword. These interactions are manifest in d.tour, a style-based design exploration tool. d.tour presents a curated database of Web pages as an explorable design gallery. It extracts and analyzes design features of these pages, allowing it to process style-based queries and recommend designs to the user. d.tour's gallery interface decreases the gulfs of execution and evaluation for design example-finding.\n",
      "=============================\n",
      "Projector-guided painting\n",
      "This paper presents a novel interactive system for guiding artists to paint using traditional media and tools. The enabling technology is a multi-projector display capable of controlling the appearance of an artist's canvas. This display-on-canvas guides the artist to construct the painting as a series of layers. Our process model for painting is based on classical techniques and was designed to address three main issues which are challenging to novices: (1) positioning and sizing elements on the canvas, (2) executing the brushstrokes to achieve a desired texture and (3) mixing pigments to make a target color. These challenges are addressed through a set of interaction modes. Preview and color selection modes enable the artist to focus on the current target layer by highlighting the areas of the canvas to be painted. Orientation mode displays brushstroke guidelines for the creation of desired brush texture. Color mixing mode guides the artist through the color mixing process with a user interface similar to a color wheel. These interaction modes allow a novice artist to focus on a series of manageable subtasks in executing a complex painting. Our system covers the gamut of the painting process from overall composition down to detailed brushwork. We present the results from a user study which quantify the benefit that our system can provide to a novice painter.\n",
      "=============================\n",
      "The virtual tricorder: a uniform interface for virtual reality\n",
      "We describe a new user-interface metaphor for immersive virtual reality — the virtual tricorder. The virtual tricorder visually duplicates a six-degrees-of-freedom input device in the virtual environment. Since we map the input device to the tricorder one-to-one at all times, the user identifies the two. Thus, the resulting interface is visual as well as tactile, multipurpose, and based on a tool metaphor. It unifies many existing interaction techniques for immersive virtual reality.\n",
      "=============================\n",
      "JellyLens: content-aware adaptive lenses\n",
      "Focus+context lens-based techniques smoothly integrate two levels of detail using spatial distortion to connect the magnified region and the context. Distortion guarantees visual continuity, but causes problems of interpretation and focus targeting, partly due to the fact that most techniques are based on statically-defined, regular lens shapes, that result in far-from-optimal magnification and distortion. JellyLenses dynamically adapt to the shape of the objects of interest, providing detail-in-context visualizations of higher relevance by optimizing what regions fall into the focus, context and spatially-distorted transition regions. This both improves the visibility of content in the focus region and preserves a larger part of the context region. We describe the approach and its implementation, and report on a controlled experiment that evaluates the usability of JellyLenses compared to regular fisheye lenses, showing clear performance improvements with the new technique for a multi-scale visual search task.\n",
      "=============================\n",
      "AttachMate: highlight extraction from email attachments\n",
      "While email is a major conduit for information sharing in enterprise, there has been little work on exploring the files sent along with these messages -- attachments. These accompanying documents can be large (multiple megabytes), lengthy (multiple pages), and not optimized for the smaller screen sizes, limited reading time, and expensive bandwidth of mobile users. Thus, attachments can increase data storage costs (for both end users and email servers), drain users' time when irrelevant, cause important information to be missed when ignored, and pose a serious access issue for mobile users. To address these problems we created AttachMate, a novel email attachment summarization system. AttachMate can summarize the content of email attachments and automatically insert the summary into the text of the email. AttachMate also stores all files in the cloud, reducing file storage costs and bandwidth consumption. In this paper, the primary contribution is the AttachMate client/server architecture. To ground, support and validate the AttachMate system we present two upfront studies (813 participants) to understand the state and limitations of attachments, a novel algorithm to extract representative concept sentences (tested through two validation studies), and a user study of AttachMate within an enterprise.\n",
      "=============================\n",
      "SUIT: the Pascal of user interface toolkits\n",
      "User interface support software, such as UI toolkits, UIMSs, and interface builders, are currently too complex for undergraduates. Tools typically require a learning period of several weeks, which is impractical in a semester course. Most tools are also limited to a specific platform, usually either Macintosh, DOS, or UNIX/X. This is problematic for students who switch from DOS or Macintosh machines to UNIX machines as they move through the curriculum. The situation is similar to programming languages before the introduction of Pascal, which provided an easily ported, easily learned language for undergraduate instruction. SUIT (the Simple User Interface Toolkit), is a C subroutine library which provides an external control UIMS, an interactive layout editor, and a set of standard screen objects. SUIT applications run transparently across Macintosh, DOS, UNIX/X, and Silicon Graphics platforms. Through careful design and extensive user testing of the system and its documentation, we have been able to reduce learning time. We have formally measured that new users are productive with SUIT in less than three hours. SUIT currently has over one hundred students using it for undergraduate and graduate course work and for research projects.\n",
      "=============================\n",
      "Audio aura: light-weight audio augmented reality\n",
      "/ The physical world can be augmented with auditory cues allowing passive interaction by the user. By combining active badges, distributed systems, and wireless headphones, the movements of users through their workplace can trigger the transmission of auditory cues. These cues can summarize information about the activity of colleagues, notify the status of email or the start of a meeting, and remind of tasks such as retrieving a book at opportune times. We are currently experimenting with a prototype audio augmented reality system, Audio Aura, at Xerox PARC. The goal of this work is to create an aura of auditory information that mimics existing background, auditory awareness cues. We are prototyping sound designs for Audio Aurain VRML --ln L.“.\n",
      "=============================\n",
      "The world through the computer: computer augmented interaction with real world environments\n",
      "Current user interface techniques such as WIMP or the desktop metaphor do not support real world tasks, because the focus of these user interfaces is only on human–computer interactions, not on human–real world interactions. In this paper, we propose a method of building computer augmented environments using a situation-aware portable device. This device, calledNaviCam, has the ability to recognize the user’s situation by detecting color-code IDs in real world environments. It displays situation sensitive information by superimposing messages on its video see-through screen. Combination of ID-awareness and portable video-see-through display solves several problems with current ubiquitous computers systems and augmented reality systems.\n",
      "=============================\n",
      "A guidance technique for motion tracking with a handheld camera using auditory feedback\n",
      "We introduce a novel guidance technique based on auditory feedback for a handheld video camera. Tracking a moving object with a handheld camera is a difficult task, especially when the camera operator follows the target, because it is difficult to see through the viewfinder at the same time as following the target. The proposed technique provides auditory feedback via a headphone, which assists the operator to keep the target in sight. Two feedback sounds are introduced: three-dimensional (3D) audio and amplitude modulation (AM)-based sonification.\n",
      "=============================\n",
      "Illusions of infinity: feedback for infinite worlds\n",
      "Sensory feedback for user actions in arbitrarily large information worlds can exhaust the limited dynamic range of human sensation. Two well-known illusions, one optical and one auditory, can be used to give arbitrarily large ranges of feedback.\n",
      "=============================\n",
      "TSI (teething ring sound instrument): a design of the sound instrument for the baby\n",
      "In this paper, we will describe the TSI (Teething ring Sound Instrument), a new sound instrument given to babies, which consists of a teething ring, a knob, an I-CubeX Digitizer [1] and a computer which processes MIDI messages. The TSI is designed to bring music experience to baby with the movement of the babies reflex sucking motion. We provided the TSI to a baby and observed her action to the TSI and her reaction to the generated sound. This experiment showed the high potential of the TSI.\n",
      "=============================\n",
      "Brain-computer interaction\n",
      "The promise of Brain-Computer Interfaces (BCI) technology is to augment human capabilities by enabling people to interact with a computer through a conscious and spontaneous modulation of their brainwaves after a short training period. Indeed, by analyzing brain electrical activity online, several groups have designed brain-actuated systems that provide alternative channels for communication, entertainment and control. Thus, a person can write messages using a virtual keyboard on a computer screen and also browse the internet. Alternatively, subjects can operate simple computer games, or brain games, and interact with educational software. Researchers have also been able to train monkeys to move a computer cursor to desired targets and also to control a robot arm. Work with humans has shown that it is possible for them to move a cursor and even to drive a mobile robot between rooms in a house model. In this talk I will review the field of BCI, with a focus on non-invasive systems based on electroencephalogram (EEG) signals. I will also describe three brain-actuated applications we have developed: a virtual keyboard, a brain game, and a mobile robot (emulating a motorized wheelchair). Finally, we discuss current research directions we are pursuing in order to improve the performance and robustness of our BCI system, especially for real-time control of brain-actuated robots.\n",
      "=============================\n",
      "Integrated manipulation: context-aware manipulation of 2D diagrams\n",
      "Diagram manipulation in conventional CAD systems requires frequent mode switching and explicit placement of the pivot for rotation and scaling. In order to simplify this process, we propose an interaction technique called integrated manipulation, where the user can move, rotate, and scale without mode switching. In addition, the pivot for rotation and scaling automatically snaps to a contact point during moving operation. We performed a user study is performed using our prototype system and a commercial CAD system. The results showed that users could perform a diagram manipulation task much more rapidly using our technique.\n",
      "=============================\n",
      "Declarative interaction design for data visualization\n",
      "Declarative visualization grammars can accelerate development, facilitate retargeting across platforms, and allow language-level optimizations. However, existing declarative visualization languages are primarily concerned with visual encoding, and rely on imperative event handlers for interactive behaviors. In response, we introduce a model of declarative interaction design for data visualizations. Adopting methods from reactive programming, we model low-level events as composable data streams from which we form higher-level semantic signals. Signals feed predicates and scale inversions, which allow us to generalize interactive selections at the level of item geometry (pixels) into interactive queries over the data domain. Production rules then use these queries to manipulate the visualization's appearance. To facilitate reuse and sharing, these constructs can be encapsulated as named interactors: standalone, purely declarative specifications of interaction techniques. We assess our model's feasibility and expressivity by instantiating it with extensions to the Vega visualization grammar. Through a diverse range of examples, we demonstrate coverage over an established taxonomy of visualization interaction techniques.\n",
      "=============================\n",
      "Some virtues and limitations of action inferring interfaces\n",
      "An action inferring facility for a multimodal interface called Edward is described. Based on the actions the user performs, Edward anticipates future actions and offers to perform them automatically. The system uses inductive inference to anticipate actions. It generalizes over arguments and results, and detects patterns on the basis of a small sequence of user actions, e.g. “copy a lisp file; change extension of original file into .org; put the copy in the backup folder”. Multimodality (particularly the combination of natural language and simulated pointing gestures) and the reuse of patterns are important new features. Some possibilities and problems of action inferring interfaces in general are addressed. Action inferring interfaces are particularly useful for professional users of general-purpose applications. Such users are unable to program repetitive patterns because either the applications do not provide the facilities or the users lack the capabilities.\n",
      "=============================\n",
      "Exploring back-of-device interaction\n",
      "Back of device interaction is gaining popularity as an alternative input modality in mobile devices, however it is still unclear how the back of device is related to other interactions. My research explores the relationship between hand grip from the back of the device and other interactions. In order to investigate this relationship, I will use touch target application to study hand grip patterns, then analyse the correlation that exists between touch target and hand grip. Finally I will explore the possibilities offered when the relationship between the touch target and hand grip is established in a quantifiable way.\n",
      "=============================\n",
      "The Lego interface toolkit\n",
      "This paper describes a rapid prototyping system for physical interaction devices in immersive virtual environments. Because of the increased complexity of 3D interactive environments and the lack of standard interactive tools, designers are unable to use traditional 2D hardware in 3D virtual environments, As a result, designers must create entirely new interaction devices, a both slow and expensive process. We propose a system which allows hardware designers to experiment with the construction of new 3D interaction devices both quickly and inexpensively.\n",
      "=============================\n",
      "Data mountain: using spatial memory for document management\n",
      "Effective management of documents on computers has been a central user interface problem for many years. One common approach involves using 2D spatial layouts of icons representing the documents, particularly for information workspace tasks. This approach takes advantage of human 2D spatial cognition. More recently, several 3D spatial layouts have engaged 3D spatial cognition capabilities. Some have attempted to use spatial memory in 3D virtual environments. However, there has been no proof to date that spatial memory works the same way in 3D virtual environments as it does in the real world. We describe a new technique for document management called the Data Mountain, which allows users to place documents at arbitrary positions on an inclined plane in a 3D desktop virtual environment using a simple 2D interaction technique. We discuss how the design evolved in response to user feedback. We also describe a user study that shows that the Data Mountain does take advantage of spatial memory. Our study shows that the Data Mountain has statistically reliable advantages over the Microsoft Internet Explorer Favorites mechanism for managing documents of interest in an information workspace.\n",
      "=============================\n",
      "A support to multi-devices web application\n",
      "Programming an application which uses interactive devices located on different terminals is not easy. Programming such applications with standard Web technologies (HTTP, Javascript, Web browser) is even more difficult. However, Web applications have interesting properties like running on very different terminals, the lack of a specific installation step, the ability to evolve the application code at runtime. Our demonstration presents a support for designing multi-devices Web applications. After introducing the context of this work, we briefly describe some problems related to the design of multi-devices web application. Then, we present the toolkit we have implemented to help the development of applications based upon distant interactive devices.\n",
      "=============================\n",
      "A demonstrational technique for developing interfaces with dynamically created objects\n",
      "The development of user inteijaces is often facilitated by the use of a drawing editor. The user interface specialist draws pictures of the different “states” of the inte~ace and passes these specifications on to the programmer. The user interface specialist might also use the drawing editor to demonstrate to the programmer the interactive behavior that the interface should exhibit; that is, he might demonstrate to the programmer the actions that an end-user can pe~orm, and the graphical manner by which the application should respond to the end-user’s stimuli. From the specljications, and the in-person demonstrations, the programmer implements a protoppe of the interface. DEMO is a User Interface Development System (UIDS) that eliminates the programmer from the above process. Using an enhanced drawing editor, the user interface specialist demonstrates the actions of the end-user and the system, just as he would if the programmer were watching. However no programmer is necessary: DEMO recorak these demonstrations, makes generalizations from them, and automatically generates a prototype of the inte~ace. Key Phrases: User Interface Development System\n",
      "=============================\n",
      "Learning from TV programs: application of TV presentation to a videoconferencing system\n",
      "In this paper, we propose to direct the visual image of a videoconferencing system. Pictures of current videoconferencing systems are often boring. We thought presentation of pictures on TV and in movies should be studied to improve videoconferencing. For this purpose, we investigated several debate programs on TV. We classified all the shots into eight classes, and then determined the duration of each shot and the transition probabilities among the classes in order to describe the structure of TV programs. From this, rules to control pictures have been obtained. After that, we made a twopoint multi-party videoconferencing system that utilizes the rules. The system includes automated control of changes in camera focus.\n",
      "=============================\n",
      "Peripheral paced respiration: influencing user physiology during information work\n",
      "We present the design and evaluation of a technique for influencing user respiration by integrating respiration-pacing methods into the desktop operating system in a peripheral manner. Peripheral paced respiration differs from prior techniques in that it does not require the user's full attention. We conducted a within-subjects study to evaluate the efficacy of peripheral paced respiration, as compared to no feedback, in an ecologically valid environment. Participant respiration decreased significantly in the pacing condition. Upon further analysis, we attribute this difference to a significant decrease in breath rate while the intermittent pacing feedback is active, rather than a persistent change in respiratory pattern. The results have implications for researchers in physiological computing, biofeedback designers, and human-computer interaction researchers concerned with user stress and affect.\n",
      "=============================\n",
      "XXL: a dual approach for building user interfaces\n",
      "This paper presents XXL, a new interactive development system for building user interfaces which is based on the concept of textual and visual equivalence. XXL includes an interactive builder and a “small” C compatible special-purpose language that is both interpretable and compilable. The visual builder is able to establish the reverse correspondence between the dynamic objects that it manipulates and their textual descriptions in the original source code. Interactive modifications performed by using the builder result in incremental modifications of the original text. Lastly, XXL not only allows users to specify the widget part of the interface but can also be used to manage various behaviors and to create distributed interfaces.\n",
      "=============================\n",
      "Videoink: a pen-based approach for video editing\n",
      "Due the growth of video sharing, its manipulation is important, however still a hard task. In order to improve it, this work proposes a pen-based approach, called VideoInk. The concept exploits the painting metaphor, replacing digital ink with video frames. The method allows the user to paint video content in a canvas, which works as a two dimensional timeline. This approach includes transition effects and zoom features based on pen pressure. A Tablet PC prototype implementing the concept was also developed.\n",
      "=============================\n",
      "SUAVE: sensor-based user-aware viewing enhancement for mobile device displays\n",
      "As mobile devices are used in various environments, ambient light and wide viewing direction impair a display's perceived display quality. To combat these effects, we introduce SUAVE, our Sensor-based User-Aware Viewing Enhancement system. SUAVE senses the ambient light and viewing direction and applies corresponding image enhancements to the display content, increasing its usability. SUAVE employs a parameter calibration process to help users select suitable image enhancements for particular viewing contexts. We report implementations of SUAVE on a Motorola Xoom Tablet and an Apple iPhone 4.\n",
      "=============================\n",
      "Trainer: a motion-based interactive game for balance rehabilitation training\n",
      "In physiotherapy, the traditional approach of using fixed aids to train patients to keep their balance is often ineffective, due to the tendency of people to lose interest in the training or to lose confidence in their ability to finish the training. A Trainer system is proposed on traditional physiotherapy treatment methods to allow patients to play qualified and immersive games with a mobile aid. Using RF localization and self-balancing technology, the system allows patients to control a vehicle with their sense of balance. This platform provides a series of game feedback interface which involves part-body motion in sitting manipulation therapy to make the rehabilitation more flexible and more effective. This paper reports the designing and the control of the Trainer, the experimental evaluations of the performance of system, as well as an exploration of the future work in detail. Our work is intended to improve the patient experience of the physiotherapy rehabilitation using games with instinctive ways of controlling mobile instruments.\n",
      "=============================\n",
      "FlexAura: a flexible near-surface range sensor\n",
      "The availability of flexible capacitive sensors that can be fitted around mice, smartphones, and pens carries great potential in leveraging grasp as a new interaction modality. Unfortunately, most capacitive sensors only track interaction directly on the surface, making it harder to differentiate among grips and constraining user movements. We present a new optical range sensor design based on high power infrared LEDs and photo-transistors, which can be fabricat-ed on a flexible PCB and wrapped around a wide variety of graspable objects including pens, mice, smartphones, and slates. Our sensor offers a native resolution of 10 dpi with a sensing range of up to 30mm (1.2\"\") and sampling speed of 50Hz. Based on our prototype wrapped around the barrel of a pen, we present a summary of the characteristics of the sensor and describe the sensor output in several typical pen grips. Our design is versatile enough to apply not only to pens but to a wide variety of graspable objects including smartphones and slates.\n",
      "=============================\n",
      "Magic finger: always-available input through finger instrumentation\n",
      "We present Magic Finger, a small device worn on the fingertip, which supports always-available input. Magic Finger inverts the typical relationship between the finger and an interactive surface: with Magic Finger, we instrument the user's finger itself, rather than the surface it is touching. Magic Finger senses touch through an optical mouse sensor, enabling any surface to act as a touch screen. Magic Finger also senses texture through a micro RGB camera, allowing contextual actions to be carried out based on the particular surface being touched. A technical evaluation shows that Magic Finger can accurately sense 22 textures with an accuracy of 98.9%. We explore the interaction design space enabled by Magic Finger, and implement a number of novel interaction techniques that leverage its unique capabilities.\n",
      "=============================\n",
      "PiVOT: personalized view-overlays for tabletops\n",
      "We present PiVOT, a tabletop system aimed at supporting mixed-focus collaborative tasks. Through two view-zones, PiVOT provides personalized views to individual users while presenting an unaffected and unobstructed shared view to all users. The system supports multiple personalized views which can be present at the same spatial location and yet be only visible to the users it belongs to. The system also allows the creation of personal views that can be either 2D or (auto-stereoscopic) 3D images. We first discuss the motivation and the different implementation principles required for realizing such a system, before exploring different designs able to address the seemingly opposing challenges of shared and personalized views. We then implement and evaluate a sample prototype to validate our design ideas and present a set of sample applications to demonstrate the utility of the system.\n",
      "=============================\n",
      "TOPS: television object promoting system\n",
      "In this short paper, we propose the Television Object Promoting system (TOPS), a What You See Is What You Get (WYSIWYG) user interface and user experience designed for users to interact with objects in TV programs. Using TOPS while watching TV, consumers can acquire informa-tion about objects appearing in TV programs, such as mer-chandise, people, and scenic spots. Moreover, consumers can purchase merchandise directly, or can obtain services or items related to those objects. Besides, vendors are able to provide detail and selling information about the objects. The Television Object Promoting system (TOPS) offers not only convenience to consumers, but also new marketing methods to vendors. The paper also discusses the features, design, and implementation of TOPS.\n",
      "=============================\n",
      "A history-based macro by example system\n",
      "Many tasks performed using computer interfaces are very repetitive. While programmers can write macros or procedures to automate these repetitive tasks, this requires special skills. Demonstrational systems make macro building accessible to all users, but most provide either no visual representation of the macro or only a textual representation. We have developed a history-based visual representation of commands in a graphical user interface. This representation supports the definition of macros by example in several novel ways. At any time, a user can open a history window, review the commands executed in a session, select operations to encapsulate into a macro, and choose objects and their attributes as arguments. The system has facilities to generalize the macro automatically, save it for future use, and edit it.\n",
      "=============================\n",
      "Design as exploration: creating interface alternatives through parallel authoring and runtime tuning\n",
      "Creating multiple prototypes facilitates comparative reasoning, grounds team discussion, and enables situated exploration. However, current interface design tools focus on creating single artifacts. This paper introduces the Juxtapose code editor and runtime environment for designing multiple alternatives of both application logic and interface parameters. For rapidly comparing code alternatives, Juxtapose introduces selectively parallel source editing and execution. To explore parameter variations, Juxtapose automatically creates control interfaces for \"tuning\" application variables at runtime. This paper describes techniques to support design exploration for desktop, mobile, and physical interfaces, and situates this work in a larger design space of tools for explorative programming. A summative study of Juxtapose with 18 participants demonstrated that parallel editing and execution are accessible to interaction designers and that designers can leverage these techniques to survey more options, faster.\n",
      "=============================\n",
      "Capturing indoor scenes with smartphones\n",
      "In this paper, we present a novel smartphone application designed to easily capture, visualize and reconstruct homes, offices and other indoor scenes. Our application leverages data from smartphone sensors such as the camera, accelerometer, gyroscope and magnetometer to help model the indoor scene. The output of the system is two-fold; first, an interactive visual tour of the scene is generated in real time that allows the user to explore each room and transition between connected rooms. Second, with some basic interactive photogrammetric modeling the system generates a 2D floor plan and accompanying 3D model of the scene, under a Manhattan-world assumption. The approach does not require any specialized equipment or training and is able to produce accurate floor plans.\n",
      "=============================\n",
      "Augmenting the input space of portable displays using add-on hall-sensor grid\n",
      "Since handheld and wearable displays are highly mobile, various applications are enabled to enrich our daily life. In addition to displaying high-fidelity information, these devices also support natural and effective user interactions by exploiting the capability of various embedded sensors. Nonetheless, the set of built-in sensors has limitations. Add-on sensor technologies, therefore, are needed. This work chooses to exploit magnetism as an additional channel of user input. The author first explains the reasons of developing the add-on magnetic field sensing technology based on neodymium magnets and the analog Hall-sensor grid. Then, the augmented input space is showcased through two instances. 1) For handheld displays, the sensor extends the object tracking capability to the near-surface 3D space by simply attaching it to the back of devices. 2) For wearable displays, the sensor enables private and rich-haptic 2D input by wearing it on user's fingernails. Limitations and possible research directions of this approach are highlighted in the end of paper.\n",
      "=============================\n",
      "Tweeting halo: clothing that tweets\n",
      "People often like to express their unique personalities, interests, and opinions. This poster explores new ways that allow a user to express her feelings in both physical and virtual settings. With our Tweeting Halo, we demonstrate how a wearable lightweight projector can be used for self-expression very much like a hairstyle, makeup or a T-shirt imprint. Our current prototype allows a user to post a message physically above their head and virtually on Twitter at the same time. We also explore simple ways that will allow physical followers of the Tweeting Halo user to easily become virtual followers by simply taking a snapshot of her projected tweet with a mobile device such as a camera phone. In this extended abstract we present our current prototype, and the results of a design critique we performed using it.\n",
      "=============================\n",
      "BackTap: robust four-point tapping on the back of an off-the-shelf smartphone\n",
      "We present BackTap, an interaction technique that extends the input modality of a smartphone to add four distinct tap locations on the back case of a smartphone. The BackTap interaction can be used eyes-free with the phone in a user's pocket, purse, or armband while walking, or while holding the phone with two hands so as not to occlude the screen with the fingers. We employ three common built-in sensors on the smartphone (microphone, gyroscope, and accelerometer) and feature a lightweight heuristic implementation. In an evaluation with eleven participants and three usage conditions, users were able to tap four distinct points with 92% to 96% accuracy.\n",
      "=============================\n",
      "Requirements for an extensible object-oriented tree/graph editor\n",
      "Software engineers use graphs to represent many types of information. This paper describes a tool which is used to rapidly extend base classes to create graph editors as a user-interface to these information domains. This paper also presents requirements for extensible graph editors. These requirements establish a basis of comparison for extensible graph editors. An object-oriented programming language and an object-oriented user interface toolkit provide a great degree of flexibility for creating graph editors. Users create instances of a graph editor by specifying global and local functionality. Global functionality takes the form of graph layout algorithms, user interaction, and interaction with other tools. Local functionality is the description of the meaning and pictorial representation of nodes and arcs. As such, this paper describes a number of example graph editors that have been developed with these mechanisms which satisfy the requirements.\n",
      "=============================\n",
      "A user-specific machine learning approach for improving touch accuracy on mobile devices\n",
      "We present a flexible Machine Learning approach for learning user-specific touch input models to increase touch accuracy on mobile devices. The model is based on flexible, non-parametric Gaussian Process regression and is learned using recorded touch inputs. We demonstrate that significant touch accuracy improvements can be obtained when either raw sensor data is used as an input or when the device's reported touch location is used as an input, with the latter marginally outperforming the former. We show that learned offset functions are highly nonlinear and user-specific and that user-specific models outperform models trained on data pooled from several users. Crucially, significant performance improvements can be obtained with a small (≈200) number of training examples, easily obtained for a particular user through a calibration game or from keyboard entry data.\n",
      "=============================\n",
      "Tag system with low-powered tag and depth sensing camera\n",
      "A tag system is proposed that offers a practical approach to ubiquitous computing. It provides small and low-power tags that are easy to distribute; does not need a special device to read the tags (in the future), thus enabling their use anytime, anywhere; and has a wide reading range in angle and distance that extends the design space of tag-based applications. The tag consists of a kind of liquid crystal (LC) and a retroreflector, and it sends its ID by switching the LC. A depth sensing camera that emits infrared (IR) is used as the tag reader; we assume that it will be part of the user's everyday devices, such as a smartphone. Experiments were conducted to confirm its potential, and a regular IR camera was also tested for comparison. The results show that the tag system has a wide readable range in terms of both distance (up to 8m) and viewing angle offset. Several applications were also developed to explore the design space. Finally, limitations of the current setup and possible improvements are discussed.\n",
      "=============================\n",
      "Recognizing multistroke geometric shapes: an experimental evaluation\n",
      "A fast, simple algorithm for recognizing hand drawn geometric shapes is presented and evaluated. The algorithm recognizes (without regard to size) rectangles, ellipses, circles, diamonds, triangles, and lines. Each shape may consist of multiple strokes as long as they are entered without pauses. A pen-based graphic diagram editor employing this algorithm was developed on GO’s PenPoint operating system. The editor will be part of a pen-based notebook system for teaching math to school children. The recognition algorithm was evaluated by ten subjects drawing a total of 200 shapes. It achieved a recognition rate of up to 98Y0.\n",
      "=============================\n",
      "Disappearing mobile devices\n",
      "In this paper, we extrapolate the evolution of mobile devices in one specific direction, namely miniaturization. While we maintain the concept of a device that people are aware of and interact with intentionally, we envision that this concept can become small enough to allow invisible integration into arbitrary surfaces or human skin, and thus truly ubiquitous use. This outcome assumed, we investigate what technology would be most likely to provide the basis for these devices, what abilities such devices can be expected to have, and whether or not devices that size can still allow for meaningful interaction. We survey candidate technologies, drill down on gesture-based interaction, and demonstrate how it can be adapted to the desired form factors. While the resulting devices offer only the bare minimum in feedback and only the most basic interactions, we demonstrate that simple applications remain possible. We complete our exploration with two studies in which we investigate the affordance of these devices more concretely, namely marking and text entry using a gesture alphabet.\n",
      "=============================\n",
      "Pop through mouse button interactions\n",
      "We present a range of novel interactions enabled by a simple modification in the design of a computer mouse. By converting each mouse button to pop through tactile push-buttons, similar to the focus/shutter-release buttons used in many cameras, users can feel, and the computer can sense, two distinct \"clicks\" corresponding to pressing lightly and pressing firmly to pop through. Despite the prototypical status of our hardware and software implementations, our current pop through mouse interactions are compelling and warrant further investigation. In particular, we demonstrate that pop through buttons not only yield an additional button activation state that is composable with, or even preferable to, techniques such as double-clicking, but also can endow a qualitatively novel user experience when meaningfully and consistently applied. We propose a number of software guidelines that may provide a consistent, systemic benefit; for example, light pressure may invoke default interaction (short menu), and firm pressure may supply more detail (long menu).\n",
      "=============================\n",
      "Unidraw: a framework for building domain-specific\n",
      "Unidraw is a framework for creating object-oriented graphical editors in domains such as technical and artistic drawing, music composition, and CAD. The Unidraw architecture simplifies the construction of these editors by providing programming abstractions that are common across domains. Unidraw defines four basic abstractions: components encapsulate the appearance and behavior of objects, tools support direct manipulation of components, commands define operations on components, and external representations define the mapping between components and a file or database. Unidraw also supports multiple views, graphical connectivity, and dataflow between components. This paper presents Unidraw and three prototype domain-specific editors we have developed with it: a schematic capture system, a user interface builder, and a drawing editor. Experience indicates a substantial reduction in implementation time and effort compared with existing tools.\n",
      "=============================\n",
      "BodyAvatar: creating freeform 3D avatars using first-person body gestures\n",
      "BodyAvatar is a Kinect-based interactive system that allows users without professional skills to create freeform 3D avatars using body gestures. Unlike existing gesture-based 3D modeling tools, BodyAvatar centers around a first-person \"you're the avatar\" metaphor, where the user treats their own body as a physical proxy of the virtual avatar. Based on an intuitive body-centric mapping, the user performs gestures to their own body as if wanting to modify it, which in turn results in corresponding modifications to the avatar. BodyAvatar provides an intuitive, immersive, and playful creation experience for the user. We present a formative study that leads to the design of BodyAvatar, the system's interactions and underlying algorithms, and results from initial user trials.\n",
      "=============================\n",
      "Content-aware scrolling\n",
      "Scrolling is used to navigate large information spaces on small screens, but is often too restrictive or cumbersome to use for particular types of content, such as multi-page, multi-column documents. To address this problem, we introduce content-aware scrolling (CAS), an approach that takes into account various characteristics of document content to determine scrolling direction, speed, and zoom. We also present the CAS widget, which supports scrolling through a content-aware path using traditional scrolling methods, demonstrating the advantages of making a traditional technique content-aware.\n",
      "=============================\n",
      "Beyond: collapsible input device for direct 3D manipulation beyond the screen\n",
      "What would it be like to reach into a screen and manipulate or design virtual objects as in real world? We present Beyond, a collapsible input device for direct 3D manipulation. When pressed against a screen, Beyond collapses in the physical world and extends into the digital space of the screen, such that users can perceive that they are inserting the tool into the virtual space. Beyond allows users to directly interact with 3D media, avoiding separation between the users' input and the displayed 3D graphics without requiring special glasses or wearables, thereby enabling users to select, draw, and sculpt in 3D virtual space unfettered. We describe detailed interaction techniques, implementation and application scenarios focused on 3D geometric design and prototyping.\n",
      "=============================\n",
      "Push-push: a two-point touchscreen operation utilizing the pressed state and the hover state\n",
      "A drag operation is used for many two-point functions in mouse-based graphical user interfaces (GUIs), but its usage in touchscreen GUIs is limited because it is mainly used for scrolling. We propose Push-Push as a second two-point touchscreen operation that is not in conflict with a drag operation. We implemented three application scenarios and showed how Push-Push can be used effectively for other two-point functions while overlapping drag operations are used for scrolling.\n",
      "=============================\n",
      "Creating the invisible interface: (invited talk)\n",
      "For thirty years, most interface design, and most computer design, has been headed down the path of the “dramatic” machine. Its highest ideal is to make a computer so exciting, so wonderful so interesting, that we never want to be without it. A less-traveled path I call the “invisible”; its highest ideal is to make a computer so imbedded, so fitting, so natural, that we use it without even thinking about it. (I have also called this notion “Ubiquitous Computing.”) I believe that in the next twenty years the second path will come to dominate. But this will not be easy; very little of our current systems infrastructure will survive. We have been building versions of the infrastructure-to-come at PARC for the past four years, in the form of inch-, foot-, and yard-sized computers we call Tabs, Pads, and Boards. In this talk I will describe the humanistic origins of the “invisible” ideal in post-modernist thought. I will then describe some of our prototypes, how they succeed and fail to be invisible, and what we have learned. I will illustrate new systems issues that user interface designers will face when creating invisibility. And I will indicate some new directions we are now exploring, including the famous “dangling string” display.\n",
      "=============================\n",
      "Data-driven interactions for web design\n",
      "This thesis describes how data-driven approaches to Web design problems can enable useful interactions for designers. It presents three machine learning applications which enable new interaction mechanisms for Web design: rapid retargeting between page designs, scalable design search, and generative probabilistic model induction to support design interactions cast as probabilistic inference. It also presents a scalable architecture for efficient data-mining on Web designs, which supports these three applications.\n",
      "=============================\n",
      "Lightweight material detection for placement-aware mobile computing\n",
      "Numerous methods have been proposed that allow mobile devices to determine where they are located (e.g., home or office) and in some cases, predict what activity the user is currently engaged in (e.g., walking, sitting, or driving). While useful, this sensing currently only tells part of a much richer story. To allow devices to act most appropriately to the situation they are in, it would also be very helpful to know about their placement - for example whether they are sitting on a desk, hidden in a drawer, placed in a pocket, or held in one's hand - as different device behaviors may be called for in each of these situations. In this paper, we describe a simple, small, and inexpensive multispectral optical sensor for identifying materials in proximity to a device. This information can be used in concert with e.g., location information, to estimate, for example, that the device is \"sitting on the desk at home\", or \"in the pocket at work\". This paper discusses several potential uses of this technology, as well as results from a two-part study, which indicates that this technique can detect placement at 94.4% accuracy with real-world placement sets.\n",
      "=============================\n",
      "An architecture for expert user interface and design management\n",
      "From a user interface point of view, expert systems are different from applications in general in that the reasoning process of the system often defines the dialogue structure. This has several advantages, but there may also be problems due to the lack of separation between functionality and user interface. This paper investigates the possibility of treating an expert system user interface as separate from the reasoning process of the system, and the consequences thereof.We propose that an expert system user interface can be seen as a combination of two different structures; the surface dialogue, comprising mainly lexical and syntactical aspects, and the session discourse which represents the interaction between user and system on a discourse level. A proposed architecture for a software tool managing these two structures is presented and discussed, with particular emphasis on the session discourse manager.\n",
      "=============================\n",
      "Automatic image retargeting with fisheye-view warping\n",
      "Image retargeting is the problem of adapting images for display on devices different than originally intended. This paper presents a method for adapting large images, such as those taken with a digital camera, for a small display, such as a cellular telephone. The method uses a non-linear fisheye-view warp that emphasizes parts of an image while shrinking others. Like previous methods, fisheye-view warping uses image information, such as low-level salience and high-level object recognition to find important regions of the source image. However, unlike prior approaches, a non-linear image warping function emphasizes the important aspects of the image while retaining the surrounding context. The method has advantages in preserving information content, alerting the viewer to missing information and providing robustness.\n",
      "=============================\n",
      "3D puppetry: a kinect-based interface for 3D animation\n",
      "We present a system for producing 3D animations using physical objects (i.e., puppets) as input. Puppeteers can load 3D models of familiar rigid objects, including toys, into our system and use them as puppets for an animation. During a performance, the puppeteer physically manipulates these puppets in front of a Kinect depth sensor. Our system uses a combination of image-feature matching and 3D shape matching to identify and track the physical puppets. It then renders the corresponding 3D models into a virtual set. Our system operates in real time so that the puppeteer can immediately see the resulting animation and make adjustments on the fly. It also provides 6D virtual camera \\\\rev{and lighting} controls, which the puppeteer can adjust before, during, or after a performance. Finally our system supports layered animations to help puppeteers produce animations in which several characters move at the same time. We demonstrate the accessibility of our system with a variety of animations created by puppeteers with no prior animation experience.\n",
      "=============================\n",
      "Riding the plane: bimanual, desktop 3D manipulation\n",
      "A bimanual 7 Degree of Freedom (DOF) manipulation technique based on a hybrid 3D cursor driven by the combination of mouse and trackball is presented. This technique allows the user to move the cursor to the target location in 3D scene by following a conceived straight or curved path. In the pilot study, participants could learn the technique in a short time and perform the docking task steadily without physical fatigue.\n",
      "=============================\n",
      "Speed-dependent automatic zooming for browsing large documents\n",
      "We propose a navigation technique for browsing large documents that integrates rate-based scrolling with automatic zooming. The view automatically zooms out when the user scrolls rapidly so that the perceptual scrolling speed in screen space remains constant. As a result, the user can efficiently and smoothly navigate through a large document without becoming disoriented by extremely fast visual flow. By incorporating semantic zooming techniques, the user can smoothly access a global overview of the document during rate-based scrolling. We implemented several prototype systems, including a web browser, map viewer, image browser, and dictionary viewer. An informal usability study suggests that for a document browsing task, most subjects prefer automatic zooming and the technique exhibits approximately equal performance time to scroll bars , suggesting that automatic zooming is a helpful alternative to traditional scrolling when the zoomed out view provides appropriate visual cues.\n",
      "=============================\n",
      "Reaching targets on discomfort region using tilting gesture\n",
      "We present three novel methods to facilitate one hand targeting at discomfort regions on a mobile touch screen using tilting gestures; TiltSlide, TiltReduction, and TiltCursor. We conducted a controlled user study to evaluate them in terms of their performance and user preferences by comparing them with other related methods, i.e. ThumbSpace, Edge Triggered with Extendible Cursor (ETEC), and Direct Touch (directly touching with a thumb). All three methods showed better performance than ThumbSpace in terms of speed and accuracy. Moreover, TiltReduction led users to require less thumb/grip movement than Direct Touch while showing comparable performance in speed and accuracy.\n",
      "=============================\n",
      "Multiple-view approach for smooth information retrieval\n",
      "Although various visualization techniques have been proposed for information retrieval tasks, most of them are based on a single strategy for viewing and navigating through the information space, and vague knowledge such as a fragment of the name of the object is not effective for the search. In contrast, people usually look for things using various vague clues simultaneously. For example, in a library, people can not only walk through the shelves to find a book they have in mind, but also they can be reminded of the author’s name by viewing the books on the shelf and check the index cards to get more information. To enable such realistic search strategies, we developed a multiple-view information retrieval system where data visualization, keyword search, and category search are integrated with the same smooth zooming interface, and any vague knowledge about the data can be utilized to narrow the search space. Users can navigate through the information space at will, by modifying the search area in each view.\n",
      "=============================\n",
      "The web page as a WYSIWYG end-user customizable database-backed information management application\n",
      "Dido is an application (and application development environment) in a web page. It is a single web page containing rich structured data, an AJAXy interactive visualizer/editor for that data, and a \"metaeditor\" for WYSIWYG editing of the visualizer/editor. Historically, users have been limited to the data schemas, visualizations, and interactions offered by a small number of heavyweight applications. In contrast, Dido encourages and enables the end user to edit (not code) in his or her web browser a distinct ephemeral interaction \"wrapper\" for each data collection that is specifically suited to its intended use. Dido's active document metaphor has been explored before but we show how, given today's web infrastructure, it can be deployed in a small self-contained HTML document without touching a web client or server.\n",
      "=============================\n",
      "Open project: a lightweight framework for remote sharing of mobile applications\n",
      "The form factor of mobile devices remains small while their computing power grows at an accelerated rate. Prior work has explored expanding the output space by leveraging free displays in the environment. However, existing solutions often do not scale. In this paper we discuss Open Project, an end-to-end framework that allows a user to \"project\" a native mobile application onto a display using a phone camera, leveraging interaction spaces ranging from a PC monitor to a public wall-sized display. Any display becomes projectable instantaneously by simply accessing the lightweight Open Project server via a web browser. By distributing computation load onto each projecting mobile device, our framework easily scales for hosting many projection sessions and devices simultaneously. Our performance experiments and user studies indicated that Open Project supported a variety of useful collaborative, sharing scenarios and performed reliably in diverse settings.\n",
      "=============================\n",
      "iRotate grasp: automatic screen rotation based on grasp of mobile devices\n",
      "Automatic screen rotation improves viewing experience and usability of mobile devices, but current gravity-based approaches do not support postures such as lying on one side, and manual rotation switches require explicit user input. iRotate Grasp automatically rotates screens of mobile devices to match users' viewing orientations based on how users are grasping the devices. Our insight is that users' grasps are consistent for each orientation, but significantly differ between different orientations. Our prototype embeds a total of 32 light sensors along the four sides and the back of an iPod Touch, and uses support vector machine (SVM) to recognize grasps at 25Hz. We collected 6-users' usage under 54 different conditions: 1) grasping the device using left, right, and both hands, 2) scrolling, zooming and typing, 3) in portrait, landscape-left, and landscape-right orientations, and while 4) sitting and lying down on one side. Results show that our grasp-based approach is promising, and our iRotate Grasp prototype could correctly rotate the screen 90.5% of the time when training and testing on different users.\n",
      "=============================\n",
      "Defining the presentation of application data by a graphical language\n",
      "On the basis of a graphical language for defining a dynamic picture and the control actions applied to it, a system is built for developing the presentation of application data for user interfaces. This system provides user interface developers a friendly and high efficient programming environment.\n",
      "=============================\n",
      "Collabio: a game for annotating people within social networks\n",
      "We present Collabio, a social tagging game within an online social network that encourages friends to tag one another. Collabio's approach of incentivizing members of the social network to generate information about each other produces personalizing information about its users. We report usage log analysis, survey data, and a rating exercise demonstrating that Collabio tags are accurate and augment information that could have been scraped online.\n",
      "=============================\n",
      "Worlds within worlds: metaphors for exploring n-dimensional virtual worlds\n",
      "n-Vision is a testbed for exploring n-dimensional worlds containing functions of an arbitrary number of variables. Although our interaction devices and display hardware are inherently 3D, we demonstrate how they can be used to support interaction with these higher-dimensional objects. We introduce a new interaction metaphor developed for the system, which we call “worlds within worlds”: nested heterogeneous coordinate systems that allow the user to view and manipulate functions. Objects in our world may be explored with a set of tools. We describe an example n-Vision application in “financial visualization,” where the functions are models of financial instruments. n-Vision’s software architecture supports a hierarchy of arbitrarily transformed, nested boxes that defines an interactive space within which information is displayed and input obtained. Our design, modeled in part after the hierarchical 2D windows of the X Window System, is intended to provide an environment that is well suited to the use of true 3D input and stereo display devices. Boxes are associated with event handlers that support 3D motion, enter, and leave events, and provide recognition of finger gestures. CR\n",
      "=============================\n",
      "Visual separation in mobile multi-display environments\n",
      "Projector phones, handheld game consoles and many other mobile devices increasingly include more than one display, and therefore present a new breed of mobile Multi-Display Environments (MDEs) to users. Existing studies illustrate the effects of visual separation between displays in MDEs and suggest interaction techniques that mitigate these effects. Currently, mobile devices with heterogeneous displays such as projector phones are often designed without reference to visual separation issues; therefore it is critical to establish whether concerns and opportunities raised in the existing MDE literature apply to the emerging category of Mobile MDEs (MMDEs). This paper investigates the effects of visual separation in the context of MMDEs and contrasts these with fixed MDE results, and explores design factors for Mobile MDEs. Our study uses a novel eye-tracking methodology for measuring switches in visual context between displays and identifies that MMDEs offer increased design flexibility over traditional MDEs in terms of visual separation. We discuss these results and identify several design implications.\n",
      "=============================\n",
      "Supporting dynamic downloadable appearances in an extensible user interface toolkit\n",
      "Most consumer products, from automobiles t,o breakfast cereals, pay significant attention to the visual appearance they present to the consumer. Designers of these .products normally create custom appearances that reflect things such as the functionality or purpose of the product, the market they are trying to reach, and the image, that the company creating the product is trying to create. As graphical user interfaces begin to fully penetrate the consumer market, we expect that similar customization of appearance will and should become part of every day practice in user interface design as well. This paper describes new user interface toolkit techniques designed to support dynamic, even downloadable, appearance* changes for graphical user interfaces. The long teF.goal of this work is to create a system of styles which is analogous to current systems of fonts. That is, to provide a system for applying a style of visual appearance to an interface independent of the content of the interface, and for allowing such styles to be developed at least partially independent of specific user interface components; even in many cases supporting custom interactive components that did not exist when a style was created.\n",
      "=============================\n",
      "Zero-latency tapping: using hover information to predict touch locations and eliminate touchdown latency\n",
      "A method of reducing the perceived latency of touch input by employing a model to predict touch events before the finger reaches the touch surface is proposed. A corpus of 3D finger movement data was collected, and used to develop a model capable of three granularities at different phases of movement: initial direction, final touch location, time of touchdown. The model is validated for target distances >= 25.5cm, and demonstrated to have a mean accuracy of 1.05cm 128ms before the user touches the screen. Preference study of different levels of latency reveals a strong preference for unperceived latency touchdown feedback. A form of 'soft' feedback, as well as other uses for this prediction to improve performance, is proposed.\n",
      "=============================\n",
      "An event language for building user interface frameworks\n",
      "Languages based on the event model are widely regarded as expressive and flexible notations for the specification of interactive graphical user interfaces. However, until now, they have only been used to specify and implement the dialogue control component of user interfaces.This paper presents an extension of the event model. A computable notation, the event language, based on this is used to construct a complete user interface framework. The framework forms the runtime component of a UIMS.The event language allows the modular construction of complex event systems. This is supported by the addition of a tagged addressing mode. Furthermore, the control structure of event handlers is extended with exception management, permitting unspecified events and thereby facilitating the use of predefined building blocks.A general purpose run-time framework for user interfaces has been constructed using the event language. We present the architecture of the presentation component of this framework including the window manager and the I/O model.\n",
      "=============================\n",
      "Don't click, paint! Using toggle maps to manipulate sets of toggle switches\n",
      "A toggle map is a set of toggle switches that allows the manipulation of several switches with a single mouse drag interaction. Because toggle switches are functionally equivalent to black and white pixels interaction techniques from paint programs can be adopted for this task. A controlled experiment shows that toggle maps can speed up interfaces containing many toggle switches such as the interactive definition of user profiles. To maximize time savings toggle maps have to be laid-out according to co-occurrences between toggles. Efficiency gains resulting from the paint method open up new application areas such as segmented interval sliders. As an example an efficient timer dialog is presented.\n",
      "=============================\n",
      "FlatFitFab: interactive modeling with planar sections\n",
      "We present a comprehensive system to author planar section structures, common in art and engineering. A study on how planar section assemblies are imagined and drawn guide our design principles: planar sections are best drawn in-situ, with little foreshortening, orthogonal to intersecting planar sections, exhibiting regularities between planes and contours. We capture these principles with a novel drawing workflow where a single fluid user stroke specifies a 3D plane and its contour in relation to existing planar sections. Regularity is supported by defining a vocabulary of procedural operations for intersecting planar sections. We exploit planar structure properties to provide real-time visual feedback on physically simulated stresses, and geometric verification that the structure is stable, connected and can be assembled. This feedback is validated by real-world fabrication and testing. As evaluation, we report on over 50 subjects who all used our system with minimal instruction to create unique models.\n",
      "=============================\n",
      "Cursive: a novel interaction technique for controlling expressive avatar gesture\n",
      "We are developing an interaction technique for rich nonverbal communication through an avatar. By writing a single letter on a pen tablet device, a user can express their ideas or intentions, non-verbally, using their avatar body. Our system solves the difficult problem of controlling the movements of a highly articulated, 3D avatar model using a common input device within the context of an office environment. We believe that writing is a richly expressive and natural means for controlling expressive avatar gesture.\n",
      "=============================\n",
      "LuminAR: portable robotic augmented reality interface design and prototype\n",
      "In this paper we introduce LuminAR: a prototype for a new portable and compact projector-camera system designed to use the traditional incandescent bulb interface as a power source, and a robotic desk lamp that carries it, enabling it with dynamic motion capabilities. We are exploring how the LuminAR system embodied in a familiar form factor of a classic Angle Poise lamp may evolve into a new class of robotic, digital information devices.\n",
      "=============================\n",
      "What can internet search engines \"suggest\" about the usage and usability of popular desktop applications?\n",
      "In this paper, we show how Internet search query logs can yield rich, ecologically valid data sets describing the common tasks and issues that people encounter when using software on a day-to-day basis. These data sets can feed directly into standard usability practices. We address challenges in collecting, filtering, and summarizing queries, and show how data can be collected at very low cost, even without direct access to raw query logs.\n",
      "=============================\n",
      "QWIC: performance heuristics for large scale exploratory user interfaces\n",
      "Faceted browsers offer an effective way to explore relationships and build new knowledge across data sets. So far, web-based faceted browsers have been hampered by limited feature performance and scale. QWIC, Quick Web Interface Control, describes a set of design heuristics to address performance speed both at the interface and the backend to operate on large-scale sources.\n",
      "=============================\n",
      "User created tangible controls using ForceForm: a dynamically deformable interactive surface\n",
      "Touch surfaces are common devices but they are often uniformly flat and provide little flexibility beyond changing the visual information communicated to the user via software. Furthermore, controls for interaction are not tangible and are usually specified and placed by the user interface designer. Using ForceForm, a dynamically deformable interactive surface, the user is able to directly sculpt the surface to create tangible controls with force feedback properties. These controls can be made according to the user's specifications, and can then be relinquished when no longer needed. We describe this method of interaction, provide an implementation of a slider, and ideas for further controls.\n",
      "=============================\n",
      "TouchCast: an on-line platform for creation and sharing of tactile content based on tactile copy & paste\n",
      "We propose TouchCast, which is an on-line platform for the creating and sharing of tactile content based on Tactile Copy & Paste. User-Generated Tactile Content refers to tactile content that is created, shared and appreciated by general Internet users. TouchCast enables users to create tactile content by applying tactile textures to existing on- line content (e.g., illustrations) and to share the created content over the network. Applied textures are scanned from real objects as audio signals and we call this technique Tactile Copy & Paste. In this study, we implement the system as a web browser add-on and to create User Generated Tactile Content.\n",
      "=============================\n",
      "A spreadsheet based on constraints\n",
      "Constraints allow the user to declare relationships among objects and let the system maintain and satisfy these relationships. This paper is concerned with the design of a spreadsheet based on constraints. Instead of formulas, we let the user enter numerical constraints, such as >, c and = over the real values in the cells of the spreadsheet. Recalculating the spreadsheet then means (1) checking whether the given cellvalues satisfy all constraints and (2) finding values for cells that satisfy the constraints.\n",
      "=============================\n",
      "Making the web easier to see with opportunistic accessibility improvement\n",
      "Many people would find the Web easier to use if content was a little bigger, even those who already find the Web possible to use now. This paper introduces the idea of opportunistic accessibility improvement in which improvements intended to make a web page easier to access, such as magnification, are automatically applied to the extent that they can be without causing negative side effects. We explore this idea with oppaccess.js, an easily-deployed system for magnifying web pages that iteratively increases magnification until it notices negative side effects, such as horizontal scrolling or overlapping text. We validate this approach by magnifying existing web pages 1.6x on average without introducing negative side effects. We believe this concept applies generally across a wide range of accessibility improvements designed to help people with diverse abilities.\n",
      "=============================\n",
      "PreSense: interaction techniques for finger sensing input devices\n",
      "Although graphical user interfaces started as imitations of the physical world, many interaction techniques have since been invented that are not available in the real world. This paper focuses on one of these \"previewing\", and how a sensory enhanced input device called \"PreSense Keypad\" can provide a preview for users before they actually execute the commands. Preview important in the real world because it is often not possible to undo an action. This previewable feature helps users to see what will occur next. It is also helpful when the command assignment of the keypad dynamically changes, such as for universal commanders. We present several interaction techniques based on this input device, including menu and map browsing systems and a text input system. We also discuss finger gesture recognition for the PreSense Keypad.\n",
      "=============================\n",
      "A 3D tracking experiment on latency and its compensation methods in virtual environments\n",
      "In this paper, we conducted an experiment on the latency and its compensation methods in a virtual reality application using an HMD and a 3D head tracker. Our purpose is to make a comparison both in the simulation and in the real task among four tracker prediction methods: the Grey system theory based prediction proposed in 1994. the Kalman filtering which is well-known and widespreading since 1991, a simple linear extrapolation. and the basic method without prediction. In our 3D target tracing task that involved eight subjects, who used their head motion to trace a flying target in random motion, we have found that when the system latency is 120ms. two prediction methods, Kalman filtering (not inertial-based) and Grey system prediction, are significantly better than the one without prediction, and the former two methods are equally well in performance. Typical motion trajectories of four methods in simulation are plotted, and jittering effects are examined. In terms of jittering at 120ms prediction length, Kalman filtering was evaluated to have the largest.\n",
      "=============================\n",
      "Gaze-touch: combining gaze with multi-touch for interaction on the same surface\n",
      "Gaze has the potential to complement multi-touch for interaction on the same surface. We present gaze-touch, a technique that combines the two modalities based on the principle of 'gaze selects, touch manipulates'. Gaze is used to select a target, and coupled with multi-touch gestures that the user can perform anywhere on the surface. Gaze-touch enables users to manipulate any target from the same touch position, for whole-surface reachability and rapid context switching. Conversely, gaze-touch enables manipulation of the same target from any touch position on the surface, for example to avoid occlusion. Gaze-touch is designed to complement direct-touch as the default interaction on multi-touch surfaces. We provide a design space analysis of the properties of gaze-touch versus direct-touch, and present four applications that explore how gaze-touch can be used alongside direct-touch. The applications demonstrate use cases for interchangeable, complementary and alternative use of the two modes of interaction, and introduce novel techniques arising from the combination of gaze-touch and conventional multi-touch.\n",
      "=============================\n",
      "Flexible conflict detection and management in collaborative applications\n",
      "This paper presents a comprehensive model for dealing with semantic conflicts in applications, and the implementation of this model in a toolkit for collaborative systems. Conflicts are defined purely through application semantics—the set of behaviors supported by the applications—and yet can be detected and managed by the infrastructure with minimal application code. This work describes a number of novel techniques for managing conflicts, both in the area of resolution policies and user interfaces for presenting standing conflicts in application data.\n",
      "=============================\n",
      "Smoothly integrating rule-based techniques into a direct manipulation interface builder\n",
      "Work in automating the production of user interf%ce software has recently concentrated on two distinct approaches: systems that provide a direct manipulation editor for specifying user interfaces and systems that attempt to automatically generate much or all of the interface. This paper considers how a middle ground between these approaches can be constructed. It presents a technique whereby the rule-base inference methods used in many automatic generation systems can be smoothly integrated into a direct manipulation interface builder. This integration is achieved by explicitly representing the results of inference rules in the direct manipulation framework and by using semantic snapping techniques to give the user direct feedback and interactive control over the application of rules.\n",
      "=============================\n",
      "Graphical query specification and dynamic result previews for a digital library\n",
      "Textual query languages based on Boolean logic are common amongst the search facilities of on-line information repositories. However, there is evidence to suggest that the syntactic and semantic demands of such languages lead to user errors and adversely affect the time that it takes users to form queries. Additionally, users are faced with user interfaces to these repositories which are unresponsive and uninformative, and consequently fail to support effective query refinement. We suggest that graphical query languages, particularly Venn-like diagrams, provide a natural medium for Boolean query specification which overcomes the problems of textual query languages. Also, dynamic result previews can be seamlessly integrated with graphical query specification to increase the effectiveness of query refinements. We describe VQuery, a query interface to the New Zealand Digital Library which exploits querying by Venn diagrams and integrated query result previews.\n",
      "=============================\n",
      "Sensory triptych: here, near, out there\n",
      "Sensory Triptych is a set of exploratory, interactive sensors designed for children that invite \"new ways of seeing\" our world from the perspective of the here (the earth, air, and water around us), near (things just out of sight), and out there (orbiting satellites and space junk) using familiar and novel interfaces, affordances, and narratives. We present a series of novel physical design prototypes that reframe sensing technologies for children that foster an early adoption of technology usage for exploring, understanding, communicating, sharing, and changing our world. Finally, we discuss how such designs expand the potential opportunities and landscapes for our future interactive systems and experiences within the UIST community.\n",
      "=============================\n",
      "BitWear: a platform for small, connected, interactive devices\n",
      "We describe BitWear, a platform for prototyping small, wireless, interactive devices. BitWear incorporates hardware, wireless connectivity, and a cloud component to enable collections of connected devices. We are using this platform to create, explore, and experiment with a multitude of wearable and deployable physical forms and interactions.\n",
      "=============================\n",
      "uTrack: 3D input using two magnetic sensors\n",
      "While much progress has been made in wearable computing in recent years, input techniques remain a key challenge. In this paper, we introduce uTrack, a technique to convert the thumb and fingers into a 3D input system using magnetic field (MF) sensing. A user wears a pair of magnetometers on the back of their fingers and a permanent magnet affixed to the back of the thumb. By moving the thumb across the fingers, we obtain a continuous input stream that can be used for 3D pointing. Specifically, our novel algorithm calculates the magnet's 3D position and tilt angle directly from the sensor readings. We evaluated uTrack as an input device, showing an average tracking accuracy of 4.84 mm in 3D space - sufficient for subtle interaction. We also demonstrate a real-time prototype and example applications allowing users to interact with the computer using 3D finger input.\n",
      "=============================\n",
      "Tangible and modular input device for character articulation\n",
      "We present a modular, novel mechanical device for animation authoring. The pose of the device is sensed at interactive rates, enabling quick posing of characters rigged with a skeleton of arbitrary topology. The mapping between the physical device and virtual skeleton is computed semi-automatically guided by sparse user correspondences. Our demonstration allows visitors to experiment with our device and software, choosing from a variety of characters to control.\n",
      "=============================\n",
      "CodeGraffiti: communication by sketching for pair programmers\n",
      "In pair programming, two software developers work on their code together in front of a single workstation, one typing, the other commenting. This frequently involves pointing to code on the screen, annotating it verbally, or sketching on paper or a nearby whiteboard, little of which is captured in the source code for later reference. CodeGraffiti lets pair programmers simultaneously write their code, and annotate it with ephemeral and persistent sketches on screen using touch or pen input. We integrated CodeGraffiti into the Xcode software development environment, to study how these techniques may improve the pair programming workflow.\n",
      "=============================\n",
      "Multi-layer interaction for digital tables\n",
      "Interaction on digital tables has been restricted to a single layer on the table's active work-surface. We extend the design space of digital tables to include multiple layers of interaction. We leverage 3D position information of a pointing device to support interaction in the space above the active work-surface by creating multiple layers with drift-correction in which the user can interact with an application. We also illustrate through a point-design that designers can use multiple-layers to create a rich and clutter free application. A subjective evaluation showed that users liked the interaction techniques and found that, because of the drift correction we use, they could control the pointer when working in any layer.\n",
      "=============================\n",
      "Multimodal agent interface based on dynamical dialogue model: MAICO: multimodal agent interface for communication\n",
      "In this paper, we describe a multimodal interface prototype system based on Dynamical Dialogue Model. This system not only integrates information of speech and gestures, but also controls the response timing in order to realize a smooth interaction between user and computer. Our approach consists of human-human dialogue analysis, and computational modeling of dialogue.\n",
      "=============================\n",
      "DuploTrack: a real-time system for authoring and guiding duplo block assembly\n",
      "We demonstrate a realtime system which infers and tracks the assembly process of a snap-together block model using a Kinect® sensor. The inference enables us to build a virtual replica of the model at every step. Tracking enables us to provide context specific visual feedback on a screen by augmenting the rendered virtual model aligned with the physical model. The system allows users to author a new model and uses the inferred assembly process to guide its recreation by others. We propose a novel way of assembly guidance where the next block to be added is rendered in blinking mode with the tracked virtual model on screen. The system is also able to detect any mistakes made and helps correct them by providing appropriate feedback. We focus on assemblies of Duplo® blocks. We discuss the shortcomings of existing methods of guidance - static figures or recorded videos - and demonstrate how our method avoids those shortcomings. We also report on a user study to compare our system with standard figure-based guidance methods found in user manuals. The results of the user study suggest that our method is able to aid users' structural perception of the model better, leads to fewer assembly errors, and reduces model construction time.\n",
      "=============================\n",
      "QOOK: a new physical-virtual coupling experience for active reading\n",
      "We present QOOK, an interactive reading system that incorporates the benefits of both physical and digital books to facilitate active reading. QOOK uses a top-projector to create digital contents on a blank paper book. By detecting markers attached to each page, QOOK allows users to flip pages just like they would with a real book. Electronic functions such as keyword searching, highlighting and bookmarking are included to provide users with additional digital assistance. With a Kinect sensor that recognizes touch gestures, QOOK enables people to use these electronic functions directly with their fingers. The combination of the electronic functions of the virtual interface and free-form interaction with the physical book creates a natural reading experience, providing an opportunity for faster navigation between pages and better understanding of the book contents.\n",
      "=============================\n",
      "Eugenie: gestural and tangible interaction with active tokens for bio-design\n",
      "We present a case study of a tangible user interface that implements novel interaction techniques for the construction of complex queries in large data sets. Our interface, Eugenie, utilizes gestural interaction with active physical tokens and a multi-touch interactive surface to aid in the collaborative design process of synthetic biological circuits. We developed new interaction techniques for navigating large hierarchical data sets and for exploring a combinatorial design space. The goal of this research is to study the effect of gestural and tangible interaction with active tokens on sense-making throughout the bio-design process.\n",
      "=============================\n",
      "Continuum: designing timelines for hierarchies, relationships and scale\n",
      "Temporal events, while often discrete, also have interesting relationships within and across times: larger events are often collections of smaller more discrete events (battles within wars; artists' works within a form); events at one point also have correlations with events at other points (a play written in one period is related to its performance over a period of time). Most temporal visualisations, however, only represent discrete data points or single data types along a single timeline: this event started here and ended there; this work was published at this time; this tag was popular for this period. In order to represent richer, faceted attributes of temporal events, we present Continuum. Continuum enables hierarchical relationships in temporal data to be represented and explored; it enables relationships between events across periods to be expressed, and in particular it enables user-determined control over the level of detail of any facet of interest so that the person using the system can determine a focus point, no matter the level of zoom over the temporal space. We present the factors motivating our approach, our evaluation and implementation of this new visualisation which makes it easy for anyone to apply this interface to rich, large-scale datasets with temporal data.\n",
      "=============================\n",
      "A general framework for Bi-directional translation between abstract and pictorial data\n",
      "The merits of direct manipulation are now widely recognized. However, direct manipulation interfaces incur high cost in their creation. To cope with this problem, we present a model of bidirectional translation between pictures and abstract application data, and a prototype system, TRIP2, based on this model. Using this model, general mapping from abstract data to pictures and from pictures to abstract data is realized merely by giving declarative mapping rules, allowing fast and easy creation of direct manipulation interfaces. We apply the prototype system to the generation of the interfaces for kinship diagrams, Graph Editors, E-R diagrams, and an Othello game.\n",
      "=============================\n",
      "RoomAlive: magical experiences enabled by scalable, adaptive projector-camera units\n",
      "RoomAlive is a proof-of-concept prototype that transforms any room into an immersive, augmented entertainment experience. Our system enables new interactive projection mapping experiences that dynamically adapts content to any room. Users can touch, shoot, stomp, dodge and steer projected content that seamlessly co-exists with their existing physical environment. The basic building blocks of RoomAlive are projector-depth camera units, which can be combined through a scalable, distributed framework. The projector-depth camera units are individually auto-calibrating, self-localizing, and create a unified model of the room with no user intervention. We investigate the design space of gaming experiences that are possible with RoomAlive and explore methods for dynamically mapping content based on room layout and user position. Finally we showcase four experience prototypes that demonstrate the novel interactive experiences that are possible with RoomAlive and discuss the design challenges of adapting any game to any room.\n",
      "=============================\n",
      "A cluster information navigate method by gaze tracking\n",
      "According to the rapid growth of data volume, it's increasingly complicated to present and navigate large amount of data in a convenient method on mobile devices with a small screen. To address this challenge, we present a new method which displays cluster information in a hierarchy pattern and interact with them by eyes' movement captured by the front camera of mobile devices. The key of this system is providing users a new interacting method to navigate and select data quickly by eyes without any additional equipment.\n",
      "=============================\n",
      "The music notepad\n",
      "We present a system for entering common music notation based on 2D gestural input. The key feature of the system is the look-and-feel of the interface which approximates sketching music with paper and pencil. A probability-based interpreter integrates sequences of gestural input to perform the most common notation and editing operations. In this paper, we present the user’s model of the system, the components of the high-level recognition system, and a discussion of the evolution of the system including user feedback.\n",
      "=============================\n",
      "Integrating optical waveguides for display and sensing on pneumatic soft shape changing interfaces\n",
      "We introduce the design and fabrication process of integrating optical fiber into pneumatically driven soft composite shape changing interfaces. Embedded optical waveguides can provide both sensing and illumination, and add one more building block to the design of designing soft pneumatic shape changing interfaces.\n",
      "=============================\n",
      "Cracking the cocoa nut: user interface programming at runtime\n",
      "This article introduces runtime toolkit overloading, a novel approach to help third-party developers modify the interaction and behavior of existing software applications without access to their underlying source code. We describe the abstractions provided by this approach as well as the mechanisms for implementing them in existing environments. We describe Scotty, a prototype implementation for Mac OS X Cocoa that enables developers to modify existing applications at runtime, and we demonstrate a collection of interaction and functional transformations on existing off-the-shelf applications. We show how Scotty helps a developer make sense of unfamiliar software, even without access to its source code. We further discuss what features of future environments would facilitate this kind of runtime software development.\n",
      "=============================\n",
      "Interacting with dynamically defined information spaces using a handheld projector and a pen\n",
      "The recent trend towards miniaturization of projection technology indicates that handheld devices will soon have the ability to project information onto any surface, thus enabling interfaces that are not possible with current handhelds. We explore the design space of dynamically defining and interacting with multiple virtual information spaces embedded in a physical environment using a handheld projector and a passive pen tracked in 3D. We develop techniques for defining and interacting with these spaces, and explore usage scenarios.\n",
      "=============================\n",
      "Bridging the gap from theory to practice: the path toward innovation in human-computer interaction\n",
      "How do we break away from existing tools and techniques in HCI and truly innovate in a way that benefits the next generation of computer users? Today, too many of our technological designs and inventions are \"one off\" point designs, not building on or contributing to a theoretical foundation of understanding around human perception, cognition, social behavior and physical movement. Of course, these point designs can be successful in and of themselves, so why bother with theory and models? In order to mature as a field in a way that benefits users, it can be argued that we need to work more closely together and with an awareness of multiple disciplines, including not just the computer science and engineering arenas, but also psychology, sociology, and any field of human behavior. Of course, this could be a daunting task-how do we know that important improvements in user interface design can be obtained? I will present a series of examples of what I consider to be significant contributions to the field of HCI, each based on a multidisciplinarian, theory-driven approach. I hope to challenge the audience to creatively consider ways that their own work could be more theoretically motivated, and what it might take for more of us to move forward in that direction.\n",
      "=============================\n",
      "Implementing phicons: combining computer vision with infrared technology for interactive physical icons\n",
      "This paper describes a novel physical icon [3] (“phicon”) based system that can be programmed to issue a range of commands about what the user wishes to do with handdrawn whiteboard content. Through the phicon's UI, a command to process whiteboard context is issued using infrared signaling in combination with image processing and a ceiling-mounted camera system. We leverage camera systems that are already used for capturing whiteboard content [4] by further augmenting these systems to detect the presence and location of IR beacons within an image. An HDLC-based protocol and a built-in IR transmitter are used to send these signals.\n",
      "=============================\n",
      "Virtual shelves: interactions with orientation aware devices\n",
      "Triggering shortcuts or actions on a mobile device often requires a long sequence of key presses. Because the functions of buttons are highly dependent on the current application's context, users are required to look at the display during interaction, even in many mobile situations when eyes-free interactions may be preferable. We present Virtual Shelves, a technique to trigger programmable shortcuts that leverages the user's spatial awareness and kinesthetic memory. With Virtual Shelves, the user triggers shortcuts by orienting a spatially-aware mobile device within the circular hemisphere in front of her. This space is segmented into definable and selectable regions along the phi and theta planes. We show that users can accurately point to 7 regions on the theta and 4 regions on the phi plane using only their kinesthetic memory. Building upon these results, we then evaluate a proof-of-concept prototype of the Virtual Shelves using a Nokia N93. The results show that Virtual Shelves is faster than the N93's native interface for common mobile phone tasks.\n",
      "=============================\n",
      "For novices playing music together, adding structural constraints leads to better music and may improve user experience\n",
      "We investigate the effects of adding structure to musical interactions for novices. A simple instrument allows control of three musical parameters: pitch, timbre, and note density. Two users can play at once, and their actions are visible on a public display. We asked pairs of users to perform duets under two interaction conditions: unstructured, where users are free to play what they like, and structured, where users are directed to different areas of the musical parameter space by time-varying constraints indicated on the display. A control group played two duets without structure, while an experimental group played one duet with structure and a second without. By crowd-sourcing the ranking of recorded duets we find that structure leads to musically better results. A post experiment survey showed that the experimental group had a better experience during the second unstructured duet than during the structured.\n",
      "=============================\n",
      "C-blink: a hue-difference-based light signal marker for large screen interaction via any mobile terminal\n",
      "To enable common mobile terminals to interact with contents shown on large screens, we propose \"C-Blink\", a new light signal marker method that uses the color liquid-crystal display of a mobile terminal as a visible light source. We overcome the performance limitations of such displays by developing a hue-difference-blink technique. In combination with a screen-side sensor, we describe a system that detects and receives light signal markers sent by cell phone displays. Evaluations of a prototype system confirm that C-Blink performs well under common indoor lighting. The C-Blink program can be installed in any mobile terminal that has a color display, and the installation costs are small. C-Blink is a very useful way of enabling ubiquitous large screens to become interfaces for mobile terminals.\n",
      "=============================\n",
      "FlowMenu: combining command, text, and data entry\n",
      "We present a new kind of marking menu that was developed for use with a pen device on display surfaces such as large, high resolution, wall-mounted displays. It integrates capabilities of previously separate mechanisms such as marking menus and Quikwriting, and facilitates the entry of multiple commands. While using this menu, the pen never has to leave the active surface so that consecutive menu selections, data entry (text and parameters) and direct manipulation tasks can be integrated fluidly.\n",
      "=============================\n",
      "Toto: a tool for selecting interaction techniques\n",
      "The construction and maintenance of interactive user interfaces have been simplified by the development of a generation of software tools. The tools range from window managers, toolkits, and widget sets to user interface management systems and knowledge-based design assistants. However, only a small number of the tools attempt to incorporate principles of good design. They offer no help with decisions regarding the variety of input devices and methods available. In this paper we briefly describe a methodology for interaction technique selection based on natural physical analogs of the application tasks. Special emphasis is given to the physical characteristics of input devices and the pragmatics of their use. The methodology is incorporated in a software environment named Toto which includes knowledge acquired from a variety of disciplines such as: semiotics, ergonomics, and industrial design. Toto also incorporates a set of interactive tools for modifying the knowledge and for supporting the selection of natural interaction techniques. A two phased design process (matching followed by sequencing) is embedded in the Toto rule base. Examples of the use of Toto tools are provided to illustrate the design process.\n",
      "=============================\n",
      "Informative things: how to attach information to the real world\n",
      "We describe a new method and implementation for managing information through the use of physical objects. In today’s networked world, the trend is toward working in a global virtual environment. To transfer information, users are responsible for finding an appropriate storage location, naming the information, selecting the transport mechanism, and setting the access permissions. Much of the time, these burdens are needless and, in fact, stand in the way of productive use of the networked environment. In many circumstances, a physical floppy disk is the ideal medium for transferring information, as it eliminates these complications. Our Informative Things approach provides a “floppy-like” user interface that gives the impression of storing information on physical objects. In reality, our system stores information in the network, associating pointers to information with objects in the physical world. By hiding these details, we simplify information management. By linking the physical and virtual worlds, we leverage users’ highly-developed ability to work in the real world.\n",
      "=============================\n",
      "Animated paper: a moving prototyping platform\n",
      "We have developed a novel prototyping method that utilizes animated paper, a versatile platform created from paper and shape memory alloy (SMA), which is easy to control using a range of different energy sources from sunlight to lasers. We have further designed a laser point tracking system to improve the precision of the wireless control system by embedding retro-reflective material on the paper to act as light markers. It is possible to change the movement of paper prototypes by varying where to mount the SMA or how to heat it, creating a wide range of applications.\n",
      "=============================\n",
      "Building implicit interfaces for wearable computers with physiological inputs: zero shutter camera and phylter\n",
      "We propose implicit interfaces that use passive physiological input as additional communication channels between wearable devices and wearers. A defining characteristic of physiological input is that it is implicit and continuous, distinguishing it from conventional event-driven action on a keyboard, for example, which is explicit and discrete. By considering the fundamental differences between the two types of inputs, we introduce a core framework to support building implicit interface, such that the framework follows the three key principles: Subscription, Accumulation, and Interpretation of implicit inputs. Unlike a conventional event driven system, our framework subscribes to continuous streams of input data, accumulates the data in a buffer, and subsequently attempts to recognize patterns in the accumulated data -- upon request from the application, rather than directly in response to the input events. Finally, in order to embody the impacts of implicit interfaces in the real world, we introduce two prototype applications for Google Glass, Zero Shutter Camera triggering a camera snapshot and Phylter filtering notifications the both leverage the wearer's physiological state information.\n",
      "=============================\n",
      "Role-based control of shared application views\n",
      "Collaboration often relies on all group members having a shared view of a single-user application. A common situation is a single active presenter sharing a live view of her workstation screen with a passive audience, using simple hardware-based video signal projection onto a large screen or simple bitmap-based sharing protocols. This offers simplicity and some advantages over more sophisticated software-based replication solutions, but everyone has the exact same view of the application. This conflicts with the presenter's need to keep some information and interaction details private. It also fails to recognize the needs of the passive audience, who may struggle to follow the presentation because of verbosity, display clutter or insufficient familiarity with the application.Views that cater to the different roles of the presenter and the audience can be provided by custom solutions, but these tend to be bound to a particular application. In this paper we describe a general technique and implementation details of a prototype system that allows standardized role-specific views of existing single-user applications and permits additional customization that is application-specific with no change to the application source code. Role-based policies control manipulation and display of shared windows and image buffers produced by the application, providing semi-automated privacy protection and relaxed verbosity to meet both presenter and audience needs.\n",
      "=============================\n",
      "Glassified: an augmented ruler based on a transparent display for real-time interactions with paper\n",
      "We introduce Glassified, a modified ruler with a transparent display to supplement physical strokes made on paper with virtual graphics. Because the display is transparent, both the physical strokes and the virtual graphics are visible in the same plane. A digitizer captures the pen strokes in order to update the graphical overlay, fusing the traditional function of a ruler with the added advantages of a digital, display-based system. We describe use-cases of Glassified in the areas of math and physics and discuss its advantages over traditional systems.\n",
      "=============================\n",
      "Using brain-computer interfaces for implicit input\n",
      "Passive brain-computer interfaces, in which implicit input is derived from a user's changing brain activity without conscious effort from the user, may be one of the most promising applications of brain-computer interfaces because they can improve user performance without additional effort on the user's part. I seek to use physiological signals that correlate to particular brain states in order to adapt an interface while the user behaves normally. My research aims to develop strategies to adapt the interface to the user and the user's cognitive state using functional near-infrared spectroscopy (fNIRS), a non-invasive, lightweight brain-sensing technique. While passive brain-computer interfaces are currently being developed and researchers have shown their utility, there has been little effort to develop a framework or hierarchy for adaptation strategies.\n",
      "=============================\n",
      "Vibkinesis: notification by direct tap and 'dying message' using vibronic movement controllable smartphones\n",
      "We propose Vibkinesis, a smartphone that can control its angle and directions of movement and rotation. By separately controlling the vibration motors attached to it, the smartphone can move on a table in the direction it chooses. Vibkinesis can inform a user of a message received when the user is away from the smartphone by changing its orientation, e.g., the smartphone has rotated 90° to the left before the user returns to the smartphone. With this capability, Vibkinesis can notify the user of a message even if the battery is discharged. We also extend the sensing area of Vibkinesis by using an omni-directional lens so that the smartphone tracks the surrounding objects. This allows Vibkinesis to tap the user's hand. These novel interactions expand the mobile device's movement area, notification channels, and notification time span.\n",
      "=============================\n",
      "Browsing large HTML tables on small screens\n",
      "We propose new interaction techniques that support better browsing of large HTML tables on small screen devices, such as mobile phones. We propose three modes for browsing tables: normal mode, record mode, and cell mode. Normal mode renders tables in the ordinary way, but provides various useful functions for browsing large tables, such as hiding unnecessary rows and columns. Record mode regards each row (or column) as the basic information unit and displays it in a record-like format with column (or row) headers, while cell mode regards each cell as the basic unit and displays each cell together with its corresponding row and column headers. For these table presentations, we need to identify row and column headers that explain the meaning of rows and columns. To provide users with both row and column headers even when the tables have attributes for only one of them, we introduce the concept of keys and develop a method of automatically discovering attributes and keys in tables. Another issue in these presentations is how to handle composite cells spanning multiple rows or columns. We determine the semantics of such composite cells and render them in appropriate ways in accordance with their semantics.\n",
      "=============================\n",
      "Usability analysis of 3D rotation techniques\n",
      "We report results from a formal user study of interactive 3D rotation using the mouse-driven Virtual Sphere and Arcball techniques, as well as multidimensional input techniques based on magnetic orientation sensors. MultidimensionaI input is often assumed to allow users to work quickly, but at the cost of precision, due to the instability of the hand moving in the open air. We show that, at least for the orientation matching task used in this experiment, users can take advantage of the integrated degrees of freedom provided by multidimensional input without necessarily sacrificing precision: using multidimensional input, users completed the experimental task up to 36% faster without any statistically detectable loss of accuracy. We also report detailed observations of common usability problems when first encountering the techniques. Our observations suggest some design issues for 3D input devices. For example, the physical form-factors of the 3D input device significantly influenced user acceptance of otherwise identical input sensors. The device should afford some tactile cues, so the user can feel its orientation without looking at it. In the absence of such cues, some test users were unsure of how to use the device.\n",
      "=============================\n",
      "A molecular architecture for creating advanced GUIs\n",
      "This paper presents a new GUI architecture for creating advanced interfaces. This model is based on a limited set of general principles that improve flexibility and provide capabilities for implementing information visualization techniques such as magic lenses, transparent tools or semantic zooming. This architecture also makes it possible to create multiple views and application-sharing systems (by sharing views on multiple computer screens) in a simple and uniform way and to handle bimanual interaction and multiple pointers. An experimental toolkit called Ubit was implemented to test the feasibility of this approach. It is based on a pseudo-declarative C++ API that tries to simplify GUI programming by providing a higher level of abstraction.\n",
      "=============================\n",
      "Creating interactive web data applications with spreadsheets\n",
      "While more and more data are available through web services, it remains difficult for end-users to create web applications that make use of these data without having to write complex code. We present Gneiss, a live programming environment that extends the spreadsheet metaphor to support creating interactive web applications that dynamically use local or web data from multiple sources. Gneiss closely integrates a spreadsheet editor with a web interface builder to let users demonstrate bindings between properties of web GUI elements and cells in the spreadsheet while working with real web service data. The spreadsheet editor provides two-way connections to web services, to both visualize and retrieve different data based on the user input in the web interface. Gneiss achieves rich interactivity without the need for event-based programming by extending the 'pull model' of formulas that is familiar to the spreadsheet users. We use a series of examples to demonstrate Gneiss's ability to create a variety of interactive web data applications.\n",
      "=============================\n",
      "Path drawing for 3D walkthrough\n",
      "This paper presents an interaction technique for walkthrough in virtual 3D spaces, where the user draws the intended path directly on the scene, and the avatar automatically moves along the path. The system calculates the path by projecting the stroke drawn on the screen to the walking surface in the 3D world. Using this technique, the user can specify not only the goal position, but also the route to take and the camera direction at the goal with a single stroke. A prototype system is tested using a displayintegrated tablet, and experimental results suggest that the technique can enhance existing walkthrough techniques.\n",
      "=============================\n",
      "Directed social queries with transparent user models\n",
      "The friend list of many social network users can be very large. This creates challenges when users seek to direct their social interactions to friends that share a particular interest. We present a self-organizing online tool that by incorporating ideas from user modeling and data visualization allows a person to quickly identify which friends best match a social query, enabling precise and efficient directed social interactions. To cover the different modalities in which our tool might be used, we introduce two different interactive visualizations. One view enables a human-in-the-loop approach for result analysis and verification, and, in a second view, location, social affiliations and \"personality\" data is incorporated, allowing the user to quickly consider different social and spatial factors when directing social queries. We report on a qualitative analysis, which indicates that transparency leads to an increased effectiveness of the system. This work contributes a novel method for exploring online friends.\n",
      "=============================\n",
      "A tool for creating predictive performance models from user interface demonstrations\n",
      "A central goal of many user interface development tools has been to make the construction of high quality interfaces easy enough that iterative design approaches could be a practical reality. In the last 15 years significant advances in this regard have been achieved. However, the evaluation portion of the iterative design process has received relatively little support from tools. Even though advances have also been made in usability evaluation methods, nearly all evaluation is still done “by hand,” making it more expensive and difficult than it might be. This paper considers a partial implementation of the CRITIQUE usability evaluation tool that is being developed to help remedy this situation by automating a number of evaluation tasks. This paper will consider techniques used by the system to produce predictive models (keystroke level models and simplified GOMS models) from demonstrations of sample tasks in a fraction of the time needed by conventional handcrafting methods. A preliminary comparison of automatically generated models with models created by an expert modeler show them to produce very similar predictions (within 2%). Further, because they are automated, these models promise to be less subject to human error and less affected by the skill of the modeler.\n",
      "=============================\n",
      "The FreeD: a handheld digital milling device for craft and fabrication\n",
      "We present an approach to combine digital fabrication and craft that is focused on a new fabrication experience. The FreeD is a hand-held, digitally controlled, milling device. It is guided and monitored by a computer while still preserving gestural freedom. The computer intervenes only when the milling bit approaches the 3D model, which was designed beforehand, either by slowing down the spindle's speed or by drawing back the shaft. The rest of the time it allows complete freedom, allowing the user to manipulate and shape the work in any creative way. We believe The FreeD will enable a designer to move in between the straight boundaries of established CAD systems and the free expression of handcraft.\n",
      "=============================\n",
      "Supporting interspecies social awareness: using peripheral displays for distributed pack awareness\n",
      "In interspecies households, it is common for the non homo sapien members to be isolated and ignored for many hours each day when humans are out of the house or working. For pack animals, such as canines, information about a pack member's extended pack interactions (outside of the nuclear household) could help to mitigate this social isolation. We have developed a Pack Activity Watch System: Allowing Broad Interspecies Love In Telecommunication with Internet-Enabled Sociability (PAWSABILITIES) for helping to support remote awareness of social activities. Our work focuses on canine companions, and includes, pawticipatory design, labradory tests, and canid camera monitoring.\n",
      "=============================\n",
      "Gesture keyboard requiring only one camera\n",
      "In this paper, we propose a novel gesture-based virtual keyboard (Gesture Keyboard) of QWERTY key layout requiring only one camera. Gesture Keyboard tracks the user's fingers and recognizes gestures as the input, and each virtual key of it follows a corresponding finger. Therefore, it is possible to input characters at the user's preferred hand position even if displacing hands during inputting. Because Gesture Keyboard requires only one camera to obtain sensor information, keyboard-less devices can feature it easily.\n",
      "=============================\n",
      "HaptoMime: mid-air haptic interaction with a floating virtual screen\n",
      "We present HaptoMime, a mid-air interaction system that allows users to touch a floating virtual screen with hands-free tactile feedback. Floating images formed by tailored light beams are inherently lacking in tactile feedback. Here we propose a method to superpose hands-free tactile feedback on such a floating image using ultrasound. By tracking a fingertip with an electronically steerable ultrasonic beam, the fingertip encounters a mechanical force consistent with the floating image. We demonstrate and characterize the proposed transmission scheme and discuss promising applications with an emphasis that it helps us 'pantomime' in mid-air.\n",
      "=============================\n",
      "Translating keyword commands into executable code\n",
      "Modern applications provide interfaces for scripting, but many users do not know how to write script commands. However, many users are familiar with the idea of entering keywords into a web search engine. Hence, if a user is familiar with the vocabulary of an application domain, we anticipate that they could write a set of keywords expressing a command in that domain. For instance, in the web browsing domain, a user might enter <B>click search button</B>. We call expressions of this form keyword commands, and we present a novel approach for translating keyword commands directly into executable code. Our prototype of this system in the web browsing domain translates <B>click search button</B> into the Chickenfoot code <B>click(findButton(\"search\"))</B>. This code is then executed in the context of a web browser to carry out the effect. We also present an implementation of this system in the domain of Microsoft Word. A user study revealed that subjects could use keyword commands to successfully complete 90% of the web browsing tasks in our study without instructions or training. Conversely, we would expect users to complete close to 0% of the tasks if they had to guess the underlying JavaScript commands with no instructions or training.\n",
      "=============================\n",
      "Machine learning models for uncertain interaction\n",
      "As interaction methods beyond the static mouse and keyboard setup of the desktop era - such as touch, gesture sensing, and visual tracking - become more common, existing interaction paradigms are no longer good enough. These new modalities have high uncertainty, and conventional interfaces are not designed to reflect this. Research has shown that modelling uncertainty can improve the quality of interaction with these systems. Machine learning offers a rich set of tools to make probabilistic inferences in uncertain systems - this is the focus of my thesis work. In particular, I'm interested in making inferences at the sensor level and propagating uncertainty forward appropriately to applications. In this paper I describe a probabilistic model for touch interaction, and discuss how I intend to use the uncertainty in this model to improve typing accuracy on a soft keyboard. The model described here lays the groundwork for a rich framework for interaction in the presence of uncertainty, incorporating data from multiple sensors to make more accurate inferences about the goals of users, and allowing systems to adapt smoothly and appropriately to their context of use.\n",
      "=============================\n",
      "Automatic construction of intelligent diagram editors\n",
      "The intelligent diagram is a recent metaphor for diagramming in which the underlying graphic editor parses the diagram as it is being constructed, performing error correction and collecting geometric constraints which capture the relationships between diagram components. During diagram manipulation a constraint solver uses these geometric constraints to maintain the diagram’s semantics. We describe the Penguins system. This automates the development of graphical editors that support the intelligent diagram metaphor. It takes a grammatical specification of a particular diagram language and generates an editor that supports the creation, manipulation and parsing of diagrams in that visual language. Our empirical results show that the system can be used to generate specialized editors for a wide variety of diagram languages, ranging from state transition diagrams to mathematical equations, with real-time incremental parsing, error correction and direct manipulation.\n",
      "=============================\n",
      "Interactive generation of graphical user interfaces by multiple visual examples\n",
      "The construction of application-specific Graphical User Interfaces (GUI) still needs considerable programming partly because the mapping between application data and its visual representation is complicated. This study proposes a system which generates GUIs by generalizing multiple sets of application data and its visualization examples. The most notable characteristic of the system is that programmers can interactively modify the mapping by “correcting” the system-generated visualization examples that represent the system's current notion of programmer's intentions. Conflicting mappings are automatically resolved via the use of constraint hierarchies.\n",
      "=============================\n",
      "Indigo: a local propagation algorithm for inequality constraints\n",
      "Inequality constraints are useful for specifying various aspects of user interfaces, such as constraints that clne window is to the left of another, or that an object is contained within a rectangle. However, current local propagation constraint solvers can’t handle inequality constraints. We present Indigo, an efficient local propagation algorithm for satisfying acyclic constraint hierarchies, including inequality constraints.\n",
      "=============================\n",
      "Extension sticker: a method for transferring external touch input using a striped pattern sticker\n",
      "A method for transferring external touch input is proposed by partially attaching a sticker to a touch-panel display. The touch input area can be extended by printing striped patterns using a conductive ink and attaching them to overlap with a portion of a touch-panel display. Even if the user does not touch the touch panel directly, a touch event can be generated by touching the stripes at an arbitrary point corresponding to the touched area. Thus, continuous touch input can be generated, such as a scrolling operation without interruption. This method can be applied to a variety of devices including PCs, smartphones, and wearable devices. In this paper, we present several different examples of applications, including a method for extending control areas outside of the touch panel, such as the side or back of a smartphone.\n",
      "=============================\n",
      "M-gesture: geometric gesture authoring framework for multi-device gestures using wearable devices\n",
      "Wearable devices and mobile devices have great potential to detect various body motions as they are attached to different body parts. We present M-Gesture, a geometric gesture authoring framework using multiple wearable devices. We implemented physical metaphor, geometric gesture language, and continuity in spatial layout for easy and clear gesture authoring. M-Gesture demonstrates the use of geometric notation as an intuitive gesture language.\n",
      "=============================\n",
      "Camera phone based motion sensing: interaction techniques, applications and performance study\n",
      "This paper presents TinyMotion, a pure software approach for detecting a mobile phone user's hand movement in real time by analyzing image sequences captured by the built-in camera. We present the design and implementation of TinyMotion and several interactive applications based on TinyMotion. Through both an informal evaluation and a formal 17-participant user study, we found that 1. TinyMotion can detect camera movement reliably under most background and illumination conditions. 2. Target acquisition tasks based on TinyMotion follow Fitts' law and Fitts law parameters can be used for TinyMotion based pointing performance measurement. 3. The users can use Vision TiltText, a TinyMotion enabled input method, to enter sentences faster than MultiTap with a few minutes of practicing. 4. Using camera phone as a handwriting capture device and performing large vocabulary, multilingual real time handwriting recognition on the cell phone are feasible. 5. TinyMotion based gaming is enjoyable and immediately available for the current generation camera phones. We also report user experiences and problems with TinyMotion based interaction as resources for future design and development of mobile interfaces.\n",
      "=============================\n",
      "Programming time in multimedia user interfaces\n",
      "The new media types used in advance user interfaces and interactive systems introduce time as a significant variable. This paper addresses the architectural support and programming tools that should be provided to the programmer to manage the time dependencies. The approach considers that the basic models and programming paradigms adopted in the manipulation and management of time should be isomorphic with the spatial models used in existing graphical user interfaces.The paper describes the architectural principles of a toolkit designed to support the construction of user interfaces with temporal characteristics. The Ttoolkit is an extension of an existing graphical user interface toolkit, the Xt toolkit. Its design is presented and a sample application is described.\n",
      "=============================\n",
      "PeopleGarden: creating data portraits for users\n",
      "Many on-line interaction environments have a large number of users. It is difficult for the participants, especially new ones, to form a clear mental image about those with whom they are interacting. How can we compactly convey information about these participants to each other? We propose the data portrait, a novel graphical representation of users based on their past interactions. Data portraits can inform users about each other and the overall social environment. We use a flower metaphor for creating individual data portraits, and a garden metaphor for combining these portraits to represent an on-line environment. We will review previous work in visualizing both individuals and groups. We will then describe our visualizations, explain how to create them, and show how they can be used to address user questions.\n",
      "=============================\n",
      "A multiple timeline editor for developing multi-threaded animated interfaces\n",
      "This paper describes a new approach for the static viewing and editing of an animated interface that has been created with a Programming By Demonstration system. The approach is based on an event-based, multipletimeline view that extends the traditional single timeline view found in systems such as Director. The result is a reduction in the amount of frame branching required to build an interface, and an ability to display some multithreaded interfaces not in the range of the single timeline approach.\n",
      "=============================\n",
      "Yelling in the hall: using sidetone to address a problem with mobile remote presence systems\n",
      "In our field deployments of mobile remote presence (MRP) systems in offices, we observed that remote operators of MRPs often unintentionally spoke too loudly. This disrupted their local co-workers, who happened to be within earshot of the MRP system. To address this issue, we prototyped and empirically evaluated the effect of sidetone to help operators self regulate their speaking loudness. Sidetone is the intentional, attenuated feedback of speakers' voices to their ears while they are using a telecommunication device. In a 3-level (no sidetone vs. low sidetone vs. high sidetone) within- participants pair of experiments, people interacted with a confederate through an MRP system. The first experiment involved MRP operators using headsets with boom microphones (N=20). The second experiment involved MRP operators using loudspeakers and desktop microphones (N=14). While we detected the effects of the sidetone manipulation in our audio-visual context, the effect was attenuated in comparison to earlier audio-only studies. We hypothesize that the strong visual component of our MRP system interferes with the sidetone effect. We also found that engaging in more social tasks (e.g., a getting-to-know-you activity) and more intellectually demanding tasks (e.g., a creativity exercise) influenced how loudly people spoke. This suggests that testing such sidetone effects in the typical read-aloud setting is insufficient for generalizing to more interactive, communication tasks. We conclude that MRP application support must reach beyond the time honored audio-only technologies to solve the problem of excessive speaker loudness.\n",
      "=============================\n",
      "Primitives for programming multi-user interfaces\n",
      "We have designed a set of primitives for programming multi-user interfaces by extending a set of existing highlevel primitives for programming single-user interfaces. These primitives support both collaboration-transparent and collaboration-aware multi-user programs and allow existing single-user programs to be incrementally changed to corresponding multi-user programs, The collaborationaware primitives include primitives for tailoring the input and output to a user, authenticating users, executing code in a user’s environment and querying and setting properties of it, and tailoring the user interface coupling. We have identified several application-independent user groups that arise in a collaborative setting and allow the original single-user calls to be targeted at these groups. In addition, we provide primitives for defining application-specific groups. Our preliminary experience with these primitives shows that they can be used to easily implement collaborative tasks of a wide range of applications including message systems, multi-user editors, computer conferencing systems, and coordination systems. In this paper, we motivate, describe, and illustrate these primitives, discuss how primitives similar to them can be offered by a variety of user interface tools, and point out future directions for work.\n",
      "=============================\n",
      "Hybrid user interfaces: breeding virtually bigger interfaces for physically smaller computers\n",
      "While virtual worlds offer a compelling alternative to conventional interfaces, the technologies these systems currently use do not provide sufficient resolution and accuracy to support detailed work such as text editing. We describe a pragmatic approach to interface design that provides users with a large virtual world in which such high-resolution work can be performed. Our approach is based on combining heterogeneous display and interaction device technologies to produce a hybrid user interface. Display and interaction technologies that have relatively low resolution, but which cover a wide (visual and interactive) field are used to form an information surround. Display and interaction technologies that have relatively high resolution over a limited visual and interaction range are used to present concentrated information in one or more selected portions of the surround. These highresolution fields are embedded within the low-resolution surround by choosing and coordinating complementary devices that permit the user to see and interact with both simultaneously. This allows each embedded high-resolution interface to serve as a “sweet spot” within which intonation may be preferentially processed, We have developed a preliminary implementation, described in this paper, that uses a Reflection Technology Private Eye display and a Polhemus sensor to provide the secondary lowresohttion surround, and a flat-panel display and mouse to provide the primary high-resolution interface. CR\n",
      "=============================\n",
      "Aperture based selection for immersive virtual environments\n",
      "We present two novel techniques for effectively selecting objects in immersive virtual environments using a single 6 DOF magnetic tracker. These techniques advance the state of the art in that they exploit the participant’s visual frame of reference and fully utilize the position and orientation data from the tracker to improve accuracy of the selection task. Preliminary results from pilot usability studies validate our designs. Finally, the two techniques combine to compensate for each other’s weaknesses.\n",
      "=============================\n",
      "VizWiz: nearly real-time answers to visual questions\n",
      "The lack of access to visual information like text labels, icons, and colors can cause frustration and decrease independence for blind people. Current access technology uses automatic approaches to address some problems in this space, but the technology is error-prone, limited in scope, and quite expensive. In this paper, we introduce VizWiz, a talking application for mobile phones that offers a new alternative to answering visual questions in nearly real-time - asking multiple people on the web. To support answering questions quickly, we introduce a general approach for intelligently recruiting human workers in advance called quikTurkit so that workers are available when new questions arrive. A field deployment with 11 blind participants illustrates that blind people can effectively use VizWiz to cheaply answer questions in their everyday lives, highlighting issues that automatic approaches will need to address to be useful. Finally, we illustrate the potential of using VizWiz as part of the participatory design of advanced tools by using it to build and evaluate VizWiz::LocateIt, an interactive mobile tool that helps blind people solve general visual search problems.\n",
      "=============================\n",
      "Humans and the coming machine revolution\n",
      "The key components of feedback control systems -- sensors, actuators, computation, power, and communication -- are continually becoming smaller, lighter, more robust, higher performance, and less expensive. By using appropriate algorithms and system architectures, it is thus becoming possible to \"close the loop\" on almost any machine, and to create new capabilities that fully exploit their dynamic potential. In this talk I will discuss various projects -- involving mobile robots, flying machines, an autonomous table, and actuated wingsuits -- where these new machine competencies are interfaced with the ultimate dynamic entities: human beings.\n",
      "=============================\n",
      "User interface continuations\n",
      "Dialog boxes that collect parameters for commands often create ephemeral, unnatural interruptions of a program's normal execution flow, encouraging the user to complete the dialog box as quickly as possible in order for the program to process that command. In this paper we examine the idea of turning the act of collecting parameters from a user into a first class object called a user interface continuation. Programs can create user interface continuations by specifying what information is to be collected from the user and supplying a callback (i.e., a continuation) to be notified with the collected information. A partially completed user interface continuation can be saved as a new command, much as currying and partially evaluating a function with a set of parameters produces a new function. Furthermore, user interface continuations, like other continuation-passing paradigms, can be used to allow program execution to continue uninterrupted while the user determines a command's parameters at his or her leisure.\n",
      "=============================\n",
      "MMM: a user interface architecture for shared editors on a single screen\n",
      "There is a growing interest in software applications that allow several users to simultaneously interact with computer applications either in the same room or at a distance. Much early work focused on sharing exnting single-user applications across a network. The Multi-Device Multi-User Multi-Editor (MMM) project is developing a user interface and software architecture to support a new generation of editors specifically designed to be used by groups, including groups who share a single screen. Each user has his or her own modes, style settings, msertlon points, and feedback. Screen space is conserved by reducing the size and number of on-screen tools. The editors use per-user data structures to respond to multiuser input.\n",
      "=============================\n",
      "Lowering the barrier to applying machine learning\n",
      "Machine learning algorithms are key components in many cutting edge applications of computation. However, the full potential of machine learning has not been realized because using machine learning is hard, even for otherwise tech-savvy developers. This is because developing with machine learning is different than normal programming. My thesis is that developers applying machine learning need new general-purpose tools that provide structure for common processes and common pipelines while remaining flexible to account for variability in problems. In this paper, I describe my efforts to understanding the difficulties that developers face when applying machine learning. I then describe Gestalt, a general-purpose integrated development environment designed the application of machine learning. Finally, I describe work on developing a pattern language for building machine learning systems and creating new techniques that help developers understand the interaction between their data and learning algorithms.\n",
      "=============================\n",
      "Reconnaissance support for juggling multiple processing options\n",
      "A large proportion of computer-supported tasks—such as design exploration, decision analysis, data presentation, and many kinds of retrieval—can be characterised as user-driven processing of a body of data in search of an outcome that satisfies the user. Clearly such tasks can never be automated fully, but few existing tools offer support for mechanising more than the simplest repetitive aspects of the search. Reconnaissance facilities, in which the computer produces summary reports from exploration in directions suggested by the user, can save the user time and effort by revealing which areas are the most deserving of detailed investigation. The time users are prepared to spend on searching will be more effectively used, improving the likelihood of finding solutions that really meet their needs rather than merely being the first to appear satisfactory. This note describes an implemented example of reconnaissance, based on the parallel coordinates presentation technique.\n",
      "=============================\n",
      "Simplicial families of drawings\n",
      "In this paper we present a method for helping artists make artwork more accessible to casual users. We focus on the specific case of drawings, showing how a small number of drawings can be transformed into a richer object containing an entire family of similar drawings. This object is represented as a simplicial complex approximating a set of valid interpolations in configuration space. The artist does not interact directly with the simplicial complex. Instead, she guides its construction by answering a specially chosen set of yes/no questions. By combining the flexibility of a simplicial complex with direct human guidance, we are able to represent very general constraints on membership in a family. The constructed simplicial complex supports a variety of algorithms useful to an end user, including random sampling of the space of drawings, constrained interpolation between drawings, projection of another drawing into the family, and interactive exploration of the family.\n",
      "=============================\n",
      "The Rendezvous constraint maintenance system\n",
      "The Rendezvousml language and architecture are designed to build computer-supported cooperative work applications that support simultaneous work at a distance. The language includes a feature-rich constraint maintenance system. The features in the constraint system were selected based, on feedback from user-interface developers using earlier versions of the Rendezvous language. Three important features in the constraint system are: indirectly referenced source and target variables, tolerance of side-effects, and special handling of uninitialized variables to simplify initial-ization. Application requirements that motivate these features and implementation techniques are described. The Rendezvous constraint system is faster, on some benchmarks , than other constraint systems currently being used for interface construction.\n",
      "=============================\n",
      "Re-framing the desktop interface around the activities of knowledge work\n",
      "The venerable desktop metaphor is beginning to show signs of strain in supporting modern knowledge work. In this paper, we examine how the desktop metaphor can be re-framed, shifting the focus away from a low-level (and increasingly obsolete) focus on documents and applications to an interface based upon the creation of and interaction with manually declared, semantically meaningful activities. We begin by unpacking some of the foundational assumptions of desktop interface design, describe an activity-based model for organizing the desktop interface based on theories of cognition and observations of real-world practice, and identify a series of high-level system requirements for interfaces that use activity as their primary organizing principle. Based on these requirements, we present the novel interface design of the Giornata system, a prototype activity-based desktop interface, and share initial findings from a longitudinal deployment of the Giornata system in a real-world setting.\n",
      "=============================\n",
      "Structured handoffs in expert crowdsourcing improve communication and work output\n",
      "Expert crowdsourcing allows specialized, remote teams to complete projects, often large and involving multiple stages. Its execution is complicated due to communication difficulties between remote workers. This paper investigates whether structured handoff methods, from one worker to the next, improve final product quality by helping the workers understand the input of their tasks and reduce overall integration cost. We investigate this question through 1) a \"live\" handoff method where the next worker shadows the former via screen sharing technology and 2) a \"recorded\" handoff, where workers summarize work done for the next, via a screen capture and narration. We confirm the need for a handoff process. We conclude that structured handoffs result in higher quality work, improved satisfaction (especially for workers with creative tasks), improved communication of non-obvious instructions, and increased adherence to the original intent of the project.\n",
      "=============================\n",
      "WebNexter: dynamic guided tours for screen readers\n",
      "Recent research has shown that screen-reader users can find information on a website almost twice as fast if they bypass indexes and just navigate the content pages of a collection linearly (in a guided-tour fashion). Yet manually building a guided tour for each existing index requires significant resources from web developers, especially for very large web applications. To address this problem, we introduce WebNexter, a web browser extension that automatically generates guided tours from the indexes present in the page a screen-reader user is currently visiting. WebNexter is manifest in a Google Chrome extension that implements screen-reader accessible, dynamic construction of guided tours from a very large, eCommerce website prototype. Our goal is to develop WebNexter extensions for multiple browsers that will work on any website; this will relieve developers from the burden of designing guided tours while greatly accelerating the screen-reader navigation experience during fact-finding.\n",
      "=============================\n",
      "Scopemate: a tracking inspection microscope\n",
      "We propose a new interaction mechanism for inspection microscopy. The novel input device combines an optically augmented web-cam with a head tracker. A head tracker controls the inspection angle of a webcam fitted with ap-propriate microscope optics. This allows an operator the full use of their hands while intuitively looking at the work area from different perspectives.\n",
      "=============================\n",
      "An explanatory and “argumentative” interface for a model-based diagnostic system\n",
      "That intelligent systems need an explanatory capability if they are to aid or support human users has long been understood. A system which can justify its decisions generally obtains improved user trust, greater accuracy in use and offers embedded training potential. Extensive work has been done to provide rule-based systems with explanatory interfaces, but little has been done to provide the same benefits for model-based systems. We develop an approach to organizing the presentation of large amounts of model-based data in an interactive format patterned after a model of human-human explanatory and argumentative discourse. Portions of this interface were implemented for Honeywell's model-based Flight Control Maintenance and Diagnostic System (FCMDS). We conclude that sufficient information exists in a model-based system to provide a wide range of explanation types, and that, the discourse approach is a convenient, powerful and broadly applicable method of organizing and controlling information exchange involving this data.\n",
      "=============================\n",
      "Cloudtop: a workspace for the cloud\n",
      "Even as users rely more on the web for their computing needs, they continue to depend on a desktop-like area for quick access to in-use resources. The traditional desktop is file-centric and prone to clutter, making it suboptimal for use in a web-dominated world. This paper introduces Cloudtop, a browser plugin that offers a lightweight workplace for temporary items, optimized around the idea that its contents originate from and will ultimately return to the web. Cloudtop improves upon the desktop by 1) implementing a simple, time-based notebook metaphor for managing clutter, 2) capturing and bundling extensible metadata for web resources, and 3) providing a platform for greater interface uniformity across sites.\n",
      "=============================\n",
      "Collision avoidance interface for safe piloting of unmanned vehicles using a mobile device\n",
      "Autonomous robots and vehicles can perform tasks that are unsafe or undesirable for humans to do themselves, such as investigate safety in nuclear reactors or assess structural damage to a building or bridge after an earthquake. In addition, improvements in autonomous modes of such vehicles are making it easier for minimally-trained individuals to operate the vehicles. As the autonomous capabilities advance, the user's role shifts from a direct teleoperator to a supervisory control role. Since the human operator is often better suited to make decisions in uncertain situations, it is important for the human operator to have awareness of the environment in which the vehicle is operating in order to prevent collisions and damage to the vehicle as well as the structures and people in the vicinity. In this paper, we present the Collision and Obstacle Detection and Alerting (CODA) display, a novel interface to enable safe piloting of a Micro Aerial Vehicle with a mobile device in real-world settings.\n",
      "=============================\n",
      "A new direct manipulation technique for aligning objects in drawing programs\n",
      "Current drawing programs provide mainly three ways for carrying out object alignment: either by issuing an alignment command, or by using direct positioning with the help of gravity active points, or by making use of constraints. The first technique has limited functionality, and the other two may be mysterious for a novice. We describe here a new direct manipulation tool for alignment. We show that while direct manipulation helps to make the tool intuitive, it has through iterative design evolved into a tool that also offers functionality not found in current commercial products.\n",
      "=============================\n",
      "A gesture based user interface prototyping system\n",
      "GID, for Gestural Interface Designer, is an experimental system for prototyping gesture-based user interfaces. GID structures an interface as a collection of “controls”: objects that maintain an image on the display and respond to input from pointing and gesture-sensing devices. GID includes an editor for arranging controls on the screen and saving screen layouts to a file. Once an interface is created, GID provides mechanisms for routing input to the appropriate destination objects even when input arrives in parallel from several devices. GID also provides low level feature extraction and gesture representation primitives to assist in parsing gestures.\n",
      "=============================\n",
      "mashpoint: browsing the web along structured lines\n",
      "Large numbers of Web sites support rich data-centric features to explore and interact with data. In this paper we present mashpoint, a framework that allows distributed data-powered Web applications to linked based on similarities of the entities in their data. By linking applications in this way we allow browsing with selections of data from one application to another application. This sort of browsing allows complex queries and exploration of data to be done by average Web users using multiple applications. We additionally use this concept to surface structured information to users in Web pages. In this paper we present this concept and our initial prototype.\n",
      "=============================\n",
      "Hover Pad: interacting with autonomous and self-actuated displays in space\n",
      "Handheld displays enable flexible spatial exploration of information spaces -- users can physically navigate through three-dimensional space to access information at specific locations. Having users constantly hold the display, however, has several limitations: (1) inaccuracies due to natural hand tremors; (2) fatigue over time; and (3) limited exploration within arm's reach. We investigate autonomous, self-actuated displays that can freely move and hold their position and orientation in space without users having to hold them at all times. We illustrate various stages of such a display's autonomy ranging from manual to fully autonomous, which -- depending on the tasks -- facilitate the interaction. Further, we discuss possible motion control mechanisms for these displays and present several interaction techniques enabled by such displays. Our Hover Pad toolkit enables exploring five degrees of freedom of self-actuated and autonomous displays and the developed control and interaction techniques. We illustrate the utility of our toolkit with five prototype applications, such as a volumetric medical data explorer.\n",
      "=============================\n",
      "Animating user interfaces using animation servers\n",
      "Animated demonstration systems such as MacroMind Director [6] are becoming popular. Our notion of animation is more restricted. We define user interface animation as the process of emulating the interaction of a user with the interface. The interaction should be real, in the sense that it should engage the actual application. Such systems are powerful because of their ability to invoke actions, and expressive because they can be used to demonstrate interaction techniques. The effect of the presentation may be enhanced by displaying the behavior of input devices audio-visually.\n",
      "=============================\n",
      "A1: end-user programming for web-based system administration\n",
      "System administrators work with many different tools to manage and fix complex hardware and software infrastructure in a rapidly paced work environment. Through extensive field studies, we observed that they often build and share custom tools for specific tasks that are not supported by vendor tools. Recent trends toward web-based management consoles offer many advantages but put an extra burden on system administrators, as customization requires web programming, which is beyond the skills of many system administrators. To meet their needs, we developed A1, a spreadsheet-based environment with a task-specific system-administration language for quickly creating small tools or migrating existing scripts to run as web portlets. Using A1, system administrators can build spreadsheets to access remote and heterogeneous systems, gather and integrate status data, and orchestrate control of disparate systems in a uniform way. A preliminary user study showed that in just a few hours, system administrators can learn to use A1 to build relatively complex tools from scratch.\n",
      "=============================\n",
      "ZeroN: mid-air tangible interaction enabled by computer controlled magnetic levitation\n",
      "This paper presents ZeroN, a new tangible interface element that can be levitated and moved freely by computer in a three dimensional space. ZeroN serves as a tangible rep-resentation of a 3D coordinate of the virtual world through which users can see, feel, and control computation. To ac-complish this, we developed a magnetic control system that can levitate and actuate a permanent magnet in a pre-defined 3D volume. This is combined with an optical tracking and display system that projects images on the levitating object. We present applications that explore this new interaction modality. Users are invited to place or move the ZeroN object just as they can place objects on surfaces. For example, users can place the sun above physical objects to cast digital shadows, or place a planet that will start revolving based on simulated physical conditions. We describe the technology and interaction scenarios, discuss initial observations, and outline future development.\n",
      "=============================\n",
      "The designers' outpost: a tangible interface for collaborative web site\n",
      "In our previous studies into web design, we found that pens, paper, walls, and tables were often used for explaining, developing, and communicating ideas during the early phases of design. These wall-scale paper-based design practices inspired The Designers' Outpost, a tangible user interface that combines the affordances of paper and large physical workspaces with the advantages of electronic media to support information design. With Outpost, users collaboratively author web site information architectures on an electronic whiteboard using physical media (Post-it notes and images), structuring and annotating that information with electronic pens. This interaction is enabled by a touch-sensitive SMART Board augmented with a robust computer vision system, employing a rear-mounted video camera for capturing movement and a front-mounted high-resolution camera for capturing ink. We conducted a participatory design study with fifteen professional web designers. The study validated that Outpost supports information architecture work practice, and led to our adding support for fluid transitions to other tools.\n",
      "=============================\n",
      "Visual interfaces for solids modeling\n",
      "This paper explores the use of visual operators for solids modeling. We focus on designing interfaces for free-form operators such as blends, sweeps, and deformations, because these operators have a large number of interacting parameters whose effects are often determined by an underlying parameterization. In this type of interactive modeling good solutions to the design problem have aesthetic as well as engineering components. Traditionally, interaction with the parameters of these operators has been through text editors, curve editors, or trial-and-error with a slider bar. Parametric values have been estimated from data, but not interactively. These parametersare usually oneor two-dimensional,but the operators themselves are intrinsically three-dimensional in that they are used to model surfaces visualized in 3D. The traditional textual style of interaction is tedious and interposes a level of abstraction between the parameters and the resulting surface. A 3D visual interface has the potential to reduce or eliminate these problems by combining parameters and representing them with a higherlevel visual tool. The visual tools we present not only speed up the process of determining good parameter values but also provide visual interactions that are either independent of the particular parameterizations or make explicit the effect of the parameterizations. Additionally, these tools can be manipulated in the same 3D space as the surfaces producedby the operators, supporting quick, interactive exploration of the large design space of these free-form operators. This paper discusses the difficulties in creating a coherent user interface for interactive modeling. To this end we present four principles for designing visual operators, using several free-form visual operators as concrete examples. CR Categories: I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling, Curve, surface, solid, and object representations, Splines; Additional\n",
      "=============================\n",
      "Reflective haptics: haptic augmentation of GUIs through frictional actuation of stylus-based interactions\n",
      "In this paper, we present a novel system for stylus-based GUI interactions: Simulated physics through actuated frictional properties of a touch screen stylus. We present a prototype that implements a series of principles which we propose for the design of frictionally augmented GUIs. It is discussed how such actuation could be a potential addition of value for stylus-controlled GUIs, through enabling prioritized content, allowing for inherent confirmation, and leveraging on manual dexterity.\n",
      "=============================\n",
      "Join and capture: a model for nomadic interaction\n",
      "The XWeb architecture delivers interfaces to a wide variety of interactive platforms. XWeb's SUBSCRIBE mechanism allows multiple interactive clients to synchronize with each other. We define the concept of Join as the mechanism for acquiring access to a service's interface. Join also allows the formation of spontaneous collaborations with other people. We define the concept of Capture as the means for users to assemble suites of interactive resources to apply to a particular problem. These mechanisms allow users to access devices that they encounter in their environment rather than carrying all their devices with them. We describe two prototype implementations of Join and Capture. One uses a Java ring to carry a user's identification and to make connections. The other uses a set of cameras to watch where users are and what they touch. Lastly we present algorithms for resolving conflicts generated when independent interactive clients manipulate the same information.\n",
      "=============================\n",
      "Translucent patches—dissolving windows\n",
      "This paper presents motivation, design, and algorithms for using and implementing translucent, non-rectangular patches as a substitute for rectangular opaque windows. The underlying metaphor is closer to a mix between the architects yellow paper and the usage of white boards, than to rectangular opaque paper in piles and folders on a desktop.Translucent patches lead to a unified view of windows, sub-windows and selections, and provide a base from which the tight connection between windows, their content, and applications can be dissolved. It forms one aspect of on-going work to support design activities that involve “marking” media, like paper and white boards, with computers. The central idea of that research is to allow the user to associate structure and meaning dynamically and smoothly to marks on a display surface.\n",
      "=============================\n",
      "Lost in the dark: emotion adaption\n",
      "Having environments that are able to adjust accordingly with the user has been sought in the last years particularly in the area of Human Computer Interfaces. Environments able to recognize the user emotions and react in consequence have been of interest on the area of Affective Computing. This work presents a project -- an adaptable 3D video game, Lost in the Dark: Emotion Adaption, which uses user's emotions as input to alter and adjust the gaming environment. To achieve this, an interface that is capable of reading brain waves, facial expressions, and head motion was used, an Emotiv® EPOC headset. For our purposes we read emotions such as meditation, excitement, and engagement into the game, altering the lighting, music, gates, colors, and other elements that would appeal to the user emotional state. With this, we achieve closing the loop of using the emotions as inputs, adjusting a system accordingly as a result, and elicit emotions.\n",
      "=============================\n",
      "Stretching the rubber sheet: a metaphor for viewing large layouts on small screens\n",
      "We propose the metaphor of rubber sheet stretching for viewing large and complex layouts within small display areas. Imagine the original 2D layout on a rubber sheet. Users can select and enlarge different areas of the sheet by holding and stretching it with a set of special tools called handles. As the user stretches an area, a greater level of detail is displayed there. The technique has some additional desirable features such as areas specified as arbitrary closed polygons, multiple regions of interest, and uniform scaling inside the stretched regions.\n",
      "=============================\n",
      "Third surface: an augmented world wide web for the physical world\n",
      "The ubiquitous use of Augmented Reality (AR) applications is dependent on an easy way of authoring and using content. Present systems depend on specific authoring tools or content delivery systems that provide a limited amount of freedom and content ownership to the author compared to the possibilities of the World Wide Web (WWW). Third Surface is a system that allows the user to publish and use WWW content saved on personal HTTP servers for augmented reality applications in the physical environment. The contribution of this work is a system that allows a web developer to post location-based augmented reality content and AR marker on one's own HTTP server. A Global Location Service (GLS) provides a browsing application with location-based URLs that link the browsing application to content, AR markers, and data for the right positioning of content in the augmented reality interface. The Third Surface has three advantages compared to other concepts. It is globally scalable able to millions of users. The interactive possibilities for developers and users are the same as for the WWW. The developers are in charge of their own content distribution.\n",
      "=============================\n",
      "Follow-me!: conducting a virtual concert\n",
      "In this paper, we present a real-time continuous gesture recognition system for conducting a virtual concert. Our systems allow the user control over beat, by conducting four different beat-pattern gestures; tempo, by making faster or slower gestures; volume, by making larger or smaller gestures; and instrument emphasis, by directing the gestures towards specific areas of the orchestra on a large display. A recognition accuracy of up to 95% could be achieved for the conducting gestures (beat, tempo, and volume).\n",
      "=============================\n",
      "ShrinkyCircuits: sketching, shrinking, and formgiving for electronic circuits\n",
      "In this paper we describe the development of ShrinkyCircuits, a novel electronic prototyping technique that captures the flexibility of sketching and leverages properties of a common everyday plastic polymer to enable low-cost, miniature, planar, and curved, multi-layer circuit designs in minutes. ShrinkyCircuits take advantage of inexpensive prestressed polymer film that shrinks to its original size when exposed to heat. This enables improved electrical characteristics though sintering of the conductive electrical layer, partial self-assembly of the circuit and components, and mechanically robust custom shapes Including curves and non-planar form factors. We demonstrate the range and adaptability of ShrinkyCircuits designs from simple hand drawn circuits with through-hole components to complex multilayer, printed circuit boards (PCB), with curved and irregular shaped electronic layouts and surface mount components. Our approach enables users to create extremely customized circuit boards with dense circuit layouts while avoiding messy chemical etching, expensive board milling machines, or time consuming delays in using outside PCB production houses.\n",
      "=============================\n",
      "NaviRadar: a novel tactile information display for pedestrian navigation\n",
      "We introduce NaviRadar: an interaction technique for mobile phones that uses a radar metaphor in order to communicate the user's correct direction for crossings along a desired route. A radar sweep rotates clockwise and tactile feedback is provided where each sweep distinctly conveys the user's current direction and the direction in which the user must travel. In a first study, we evaluated the overall concept and tested five different tactile patterns to communicate the two different directions via a single tactor. The results show that people are able to easily understand the NaviRadar concept and can identify the correct direction with a mean deviation of 37° out of the full 360° provided. A second study shows that NaviRadar achieves similar results in terms of perceived usability and navigation performance when compared with spoken instructions. By using only tactile feedback, NaviRadar provides distinct advantages over current systems. In particular, no visual attention is required to navigate; thus, it can be spent on providing greater awareness of one's surroundings. Moreover, the lack of audio attention enables it to be used in noisy environments or this attention can be better spent on talking with others during navigation.\n",
      "=============================\n",
      "Soap: a pointing device that works in mid-air\n",
      "Soap is a pointing device based on hardware found in a mouse, yet works in mid-air. Soap consists of an optical sensor device moving freely inside a hull made of fabric. As the user applies pressure from the outside, the optical sensor moves independent from the hull. The optical sensor perceives this relative motion and reports it as position input. Soap offers many of the benefits of optical mice, such as high-accuracy sensing. We describe the design of a soap prototype and report our experiences with four application scenarios, including a wall display, Windows Media Center, slide presentation, and interactive video games.\n",
      "=============================\n",
      "Generalized and stationary scrolling\n",
      "We present a generalized definition of scrolling that unifies a wide range of existing interaction techniques, from conventional scrolling through pan and zoom systems and fish-eye views. Furthermore it suggests a useful class of new scrolling techniques in which objects do not move across the display. These “stationary scrolling” techniques do not exhibit either of two problems that plague spatial scrolling system: discontinuity in salience and the undermining of the user's spatial memory.\n",
      "=============================\n",
      "Creating map-based storyboards for browsing tour videos\n",
      "Watching a long unedited video is usually a boring experience. In this paper we examine a particular subset of videos, tour videos, in which the video is captured by walking about with a running camera with the goal of conveying the essence of some place. We present a system that makes the process of sharing and watching a long tour video easier, less boring, and more informative. To achieve this, we augment the tour video with a map-based storyboard, where the tour path is reconstructed, and coherent shots at different locations are directly visualized on the map. This allows the viewer to navigate the video in the joint location-time space. To create such a storyboard we employ an automatic pre-processing component to parse the video into coherent shots, and an authoring tool to enable the user to tie the shots with landmarks on the map. The browser-based viewing tool allows users to navigate the video in a variety of creative modes with a rich set of controls, giving each viewer a unique, personal viewing experience. Informal evaluation shows that our approach works well for tour videos compared with conventional media players.\n",
      "=============================\n",
      "Gui --- phooey!: the case for text input\n",
      "Information cannot be found if it is not recorded. Existing rich graphical application approaches interfere with user input in many ways, forcing complex interactions to enter simple information, requiring complex cognition to decide where the data should be stored, and limiting the kind of information that can be entered to what can fit into specific applications' data models. Freeform text entry suffers from none of these limitations but produces data that is hard to retrieve or visualize. We describe the design and implementation of Jourknow, a system that aims to bridge these two modalities, supporting lightweight text entry and weightless context capture that produces enough structure to support rich interactive presentation and retrieval of the arbitrary information entered.\n",
      "=============================\n",
      "WebThumb: interaction techniques for small-screen browsers\n",
      "The proliferation of wireless handheld devices is placing the World Wide Web in the palms of users, but this convenience comes at a high interactive cost. The Web that came of age on the desktop is ill-suited for use on the small displays of handhelds. Today, handheld browsing often feels like browsing on a PC with a shrunken desktop. Overreliance on scrolling is a big problem in current handheld browsing. Users confined to viewing a small portion of each page often lack a sense of the overall context --- they may feel lost in a large page and be forced to remember the locations of items as those items scroll out of view. In this paper, we present a synthesis of interaction techniques to address these problems. We implemented these techniques in a prototype, WebThumb, that can browse the live Web.\n",
      "=============================\n",
      "Speaking with the crowd\n",
      "Automated systems are not yet able to engage in a robust dialogue with users due the complexity and ambiguity of natural language. However, humans can easily converse with one another and maintain a shared history of past interactions. In this paper, we introduce Chorus, a system that enables real-time, two-way natural language conversation between an end user and a crowd acting as a single agent. Chorus is capable of maintaining a consistent, on-topic conversation with end users across multiple sessions, despite constituent individuals perpetually joining and leaving the crowd. This is enabled by using a curated shared dialogue history. Even though crowd members are constantly providing input, we present users with a stream of dialogue that appears to be from a single conversational partner. Experiments demonstrate that dialogue with Chorus displays elements of conversational memory and interaction consistency. Workers were able to answer 84.6% of user queries correctly, demonstrating that crowd-powered communication interfaces can serve as a robust means of interacting with software systems.\n",
      "=============================\n",
      "A mixed-initiative tool for designing level progressions in games\n",
      "Creating game content requires balancing design considerations at multiple scales: each level requires effort and iteration to produce, and broad-scale constraints such as the order in which game concepts are introduced must be respected. Game designers currently create informal plans for how the game's levels will fit together, but they rarely keep these plans up-to-date when levels change during iteration and testing. This leads to violations of constraints and makes changing the high-level plans expensive. To address these problems, we explore the creation of mixed-initiative game progression authoring tools which explicitly model broad-scale design considerations. These tools let the designer specify constraints on progressions, and keep the plan synchronized when levels are edited. This enables the designer to move between broad and narrow-scale editing and allows for automatic detection of problems caused by edits to levels. We further leverage advances in procedural content generation to help the designer rapidly explore and test game progressions. We present a prototype implementation of such a tool for our actively-developed educational game, Refraction. We also describe how this system could be extended for use in other games and domains, specifically for the domains of math problem sets and interactive programming tutorials.\n",
      "=============================\n",
      "Facet: a multi-segment wrist worn system\n",
      "We present Facet, a multi-display wrist worn system consisting of multiple independent touch-sensitive segments joined into a bracelet. Facet automatically determines the pose of the system as a whole and of each segment individually. It further supports multi-segment touch, yielding a rich set of touch input techniques. Our work builds on these two primitives to allow the user to control how applications use segments alone and in coordination. Applications can expand to use more segments, collapses to encompass fewer, and be swapped with other segments. We also explore how the concepts from Facet could apply to other devices in this design space.\n",
      "=============================\n",
      "Gestures without libraries, toolkits or training: a $1 recognizer for user interface prototypes\n",
      "Although mobile, tablet, large display, and tabletop computers increasingly present opportunities for using pen, finger, and wand gestures in user interfaces, implementing gesture recognition largely has been the privilege of pattern matching experts, not user interface prototypers. Although some user interface libraries and toolkits offer gesture recognizers, such infrastructure is often unavailable in design-oriented environments like Flash, scripting environments like JavaScript, or brand new off-desktop prototyping environments. To enable novice programmers to incorporate gestures into their UI prototypes, we present a \"$1 recognizer\" that is easy, cheap, and usable almost anywhere in about 100 lines of code. In a study comparing our $1 recognizer, Dynamic Time Warping, and the Rubine classifier on user-supplied gestures, we found that $1 obtains over 97% accuracy with only 1 loaded template and 99% accuracy with 3+ loaded templates. These results were nearly identical to DTW and superior to Rubine. In addition, we found that medium-speed gestures, in which users balanced speed and accuracy, were recognized better than slow or fast gestures for all three recognizers. We also discuss the effect that the number of templates or training examples has on recognition, the score falloff along recognizers' N-best lists, and results for individual gestures. We include detailed pseudocode of the $1 recognizer to aid development, inspection, extension, and testing.\n",
      "=============================\n",
      "The IR ring: authenticating users' touches on a multi-touch display\n",
      "Multi-touch displays are particularly attractive for collaborative work because multiple users can interact with applications simultaneously. However, unfettered access can lead to loss of data confidentiality and integrity. For example, one user can open or alter files of a second user, or impersonate the second user, while the second user is absent or not looking. Towards preventing these attacks, we explore means to associate the touches of a user with the user's identity in a fashion that is cryptographically sound as well as easy to use. We describe our current solution, which relies on a ring-like device that transmits a continuous pseudorandom bit sequence in the form of infrared light pulses. The multi-touch display receives and localizes the sequence, and verifies its authenticity. Each sequence is bound to a particular user, and all touches in the direct vicinity of the location of the sequence on the display are associated with that user.\n",
      "=============================\n",
      "An exploration of pen rolling for pen-based interaction\n",
      "Current pen input mainly utilizes the position of the pen tip, and occasionally, a button press. Other possible device parameters, such as rolling the pen around its longitudinal axis, are rarely used. We explore pen rolling as a supporting input modality for pen-based interaction. Through two studies, we are able to determine 1) the parameters that separate intentional pen rolling for the purpose of interaction from incidental pen rolling caused by regular writing and drawing, and 2) the parameter range within which accurate and timely intentional pen rolling interactions can occur. Building on our experimental results, we present an exploration of the design space of rolling-based interaction techniques, which showcase three scenarios where pen rolling interactions can be useful: enhanced stimulus-response compatibility in rotation tasks [7], multi-parameter input, and simplified mode selection.\n",
      "=============================\n",
      "User interface models for the cloud\n",
      "The current desktop metaphor is unsuitable for the coming age of cloud-based applications. The desktop was developed in an era that was focused on local resources, and consequently its gestures, semantics, and security model reflect heavy reliance on hierarchy and physical locations. This paper proposes a new user interface model that accounts for cloud applications, incorporating representations of people and new gestures for sharing and access, while minimizing the prominence of location. The model's key feature is a lightweight mechanism to group objects for resource organization, sharing, and access control, towards the goal of providing simple semantics for a wide range of tasks, while also achieving security through greater usability.\n",
      "=============================\n",
      "Constraint cascading style sheets for the Web\n",
      "Cascading Style Sheets have been introduced by the W3C as a mechanism for controlling the appearance of HTML documents. In this paper, we demonstrate how constraints provide a powerful unifying formalism for declaratively understanding and specifying style sheets for web documents. With constraints we can naturally and declaratively specify complex behavior such as inheritance of properties and cascading of conflicting style rules. We give a detailed description of a constraint-based style sheet model, CCSS, which is compatible with virtually all of the CSS 2.0 specification. It allows more flexible specification of layout, and also allows the designer to provide multiple layouts that better meet the desires of the user and environmental restrictions. We also describe a prototype extension of the Amaya browser that demonstrates the feasibility of CCSS.\n",
      "=============================\n",
      "DataPlay: interactive tweaking and example-driven correction of graphical database queries\n",
      "Writing complex queries in SQL is a challenge for users. Prior work has developed several techniques to ease query specification but none of these techniques are applicable to a particularly difficult class of queries: quantified queries. Our hypothesis is that users prefer to specify quantified queries interactively by trial-and-error. We identify two impediments to this form of interactive trial-and-error query specification in SQL: (i) changing quantifiers often requires global syntactical query restructuring, and (ii) the absence of non-answers from SQL's results makes verifying query correctness difficult. We remedy these issues with DataPlay, a query tool with an underlying graphical query language, a unique data model and a graphical interface. DataPlay provides two interaction features that support trial-and-error query specification. First, DataPlay allows users to directly manipulate a graphical query by changing quantifiers and modifying dependencies between constraints. Users receive real-time feedback in the form of updated answers and non-answers. Second, DataPlay can auto-correct a user's query, based on user feedback about which tuples to keep or drop from the answers and non-answers. We evaluated the effectiveness of each interaction feature with a user study and we found that direct query manipulation is more effective than auto-correction for simple queries but auto-correction is more effective than direct query manipulation for more complex queries.\n",
      "=============================\n",
      "IODisk: disk-type i/o interface for browsing digital contents\n",
      "We propose a disk-type I/O interface, IODisk, which helps users browse various digital contents intuitively in their living environment. IODisk mainly consists of a forcefeedback mechanism integrated in the rotation axis of a disk. Users can control the playing speed/direction contents (e.g., videos or picture slideshows) in proportion to the rotational speed/direction of the disk. We developed a prototype system and some applications.\n",
      "=============================\n",
      "Is the sky pure today? AwkChecker: an assistive tool for detecting and correcting collocation errors\n",
      "Collocation preferences represent the commonly used expressions, idioms, and word pairings of a language. Because collocation preferences arise from consensus usage, rather than a set of well-defined rules, they must be learned on a case-by-case basis, making them particularly challenging for non-native speakers of a language. To assist non-native speakers with these parts of a language, we developed AwkChecker, the first end-user tool geared toward helping non-native speakers detect and correct collocation errors in their writing. As a user writes, AwkChecker automatically flags collocation errors and suggests replacement expressions that correspond more closely to consensus usage. These suggestions include example usage to help users choose the best candidate. We describe AwkChecker's interface, its novel methods for detecting collocation errors and suggesting alternatives, and an early study of its use by non-native English speakers at our institution. Collectively, these contributions advance the state of the art in writing aids for non-native speakers.\n",
      "=============================\n",
      "Designing and implementing asynchronous collaborative applications with Bayou\n",
      "Asynchronous collaboration is characterized by the degree of independence collaborators have from one another. In particular, collaborators working asynchronously typically have little need for frequent and fine-grained coordination with one another, and typically do not need to be notified immediately of changes made by others to any shared artifacts they are working with. We present an infrastructure, called Bayou, designed to support the construction of asynchronous collaborative applications. Bayou provides a replicated, weakly-consistent, data storage engine to application writers. The system supports a number of mechanisms for leveraging application semantics; using these mechanisms, applications can implement complex conflict detection and resolution policies, and choose the level of consistency and stability they will see in their databases. We present a number of applications we have built or are building using the Bayou system, and examine how these take advantage of the Bayou architecture.\n",
      "=============================\n",
      "Dynamic tactile guidance for visual search tasks\n",
      "Visual search in large real-world scenes is both time consuming and frustrating, because the search becomes serial when items are visually similar. Tactile guidance techniques can facilitate search by allowing visual attention to focus on a subregion of the scene. We present a technique for dynamic tactile cueing that couples hand position with a scene position and uses tactile feedback to guide the hand actively toward the target. We demonstrate substantial improvements in task performance over a baseline of visual search only, when the scene's complexity increases. Analyzing task performance, we demonstrate that the effect of visual complexity can be practically eliminated through improved spatial precision of the guidance.\n",
      "=============================\n",
      "SketchWizard: Wizard of Oz prototyping of pen-based user interfaces\n",
      "SketchWizard allows designers to create Wizard of Oz prototypes of pen-based user interfaces in the early stages of design. In the past, designers have been inhibited from participating in the design of pen-based interfaces because of the inadequacy of paper prototypes and the difficulty of developing functional prototypes. In SketchWizard, designers and end users share a drawing canvas between two computers, allowing the designer to simulate the behavior of recognition or other technologies. Special editing features are provided to help designers respond quickly to end-user input. This paper describes the SketchWizard system and presents two evaluations of our approach. The first is an early feasibility study in which Wizard of Oz was used to prototype a pen-based user interface. The second is a laboratory study in which designers used SketchWizard to simulate existing pen-based interfaces. Both showed that end users gave valuable feedback in spite of delays between end-user actions and wizard updates.\n",
      "=============================\n",
      "Towards personalized surface computing\n",
      "With recent progress in the field of surface computing it becomes foreseeable that interactive surfaces will turn into a commodity in the future, ubiquitously integrated into our everyday environments. At the same time, we can observe a trend towards personal data and whole applications being accessible over the Internet, anytime from anywhere. We envision a future where interactive surfaces surrounding us serve as powerful portals to access these kinds of data and services. In this paper, we contribute two novel interaction techniques supporting parts of this vision: First, HandsDown, a biometric user identification approach based on hand contours and, second, PhoneTouch, a novel technique for using mobile phones in conjunction with interactive surfaces.\n",
      "=============================\n",
      "PocketTouch: through-fabric capacitive touch input\n",
      "PocketTouch is a capacitive sensing prototype that enables eyes-free multitouch input on a handheld device without having to remove the device from the pocket of one's pants, shirt, bag, or purse. PocketTouch enables a rich set of gesture interactions, ranging from simple touch strokes to full alphanumeric text entry. Our prototype device consists of a custom multitouch capacitive sensor mounted on the back of a smartphone. Similar capabilities could be enabled on most existing capacitive touchscreens through low-level access to the capacitive sensor. We demonstrate how touch strokes can be used to initialize the device for interaction and how strokes can be processed to enable text recognition of characters written over the same physical area. We also contribute a comparative study that empirically measures how different fabrics attenuate touch inputs, providing insight for future investigations. Our results suggest that PocketTouch will work reliably with a wide variety of fabrics used in today's garments, and is a viable input method for quick eyes-free operation of devices in pockets.\n",
      "=============================\n",
      "onNote: playing printed music scores as a musical instrument\n",
      "This paper presents a novel musical performance system named onNote that directly utilizes printed music scores as a musical instrument. This system can make users believe that sound is indeed embedded on the music notes in the scores. The users can play music simply by placing, moving and touching the scores under a desk lamp equipped with a camera and a small projector. By varying the movement, the users can control the playing sound and the tempo of the music. To develop this system, we propose an image processing based framework for retrieving music from a music database by capturing printed music scores. From a captured image, we identify the scores by matching them with the reference music scores, and compute the position and pose of the scores with respect to the camera. By using this framework, we can develop novel types of musical interactions.\n",
      "=============================\n",
      "Digits: freehand 3D interactions anywhere using a wrist-worn gloveless sensor\n",
      "Digits is a wrist-worn sensor that recovers the full 3D pose of the user's hand. This enables a variety of freehand interactions on the move. The system targets mobile settings, and is specifically designed to be low-power and easily reproducible using only off-the-shelf hardware. The electronics are self-contained on the user's wrist, but optically image the entirety of the user's hand. This data is processed using a new pipeline that robustly samples key parts of the hand, such as the tips and lower regions of each finger. These sparse samples are fed into new kinematic models that leverage the biomechanical constraints of the hand to recover the 3D pose of the user's hand. The proposed system works without the need for full instrumentation of the hand (for example using data gloves), additional sensors in the environment, or depth cameras which are currently prohibitive for mobile scenarios due to power and form-factor considerations. We demonstrate the utility of Digits for a variety of application scenarios, including 3D spatial interaction with mobile devices, eyes-free interaction on-the-move, and gaming. We conclude with a quantitative and qualitative evaluation of our system, and discussion of strengths, limitations and future work.\n",
      "=============================\n",
      "Attribit: content creation with semantic attributes\n",
      "We present AttribIt, an approach for people to create visual content using relative semantic attributes expressed in linguistic terms. During an off-line processing step, AttribIt learns semantic attributes for design components that reflect the high-level intent people may have for creating content in a domain (e.g. adjectives such as \"dangerous\", \"scary\" or \"strong\") and ranks them according to the strength of each learned attribute. Then, during an interactive design session, a person can explore different combinations of visual components using commands based on relative attributes (e.g. \"make this part more dangerous\"). Novel designs are assembled in real-time as the strengths of selected attributes are varied, enabling rapid, in-situ exploration of candidate designs. We applied this approach to 3D modeling and web design. Experiments suggest this interface is an effective alternative for novices performing tasks with high-level design goals.\n",
      "=============================\n",
      "Ripples: utilizing per-contact visualizations to improve user interaction with touch displays\n",
      "We present Ripples, a system which enables visualizations around each contact point on a touch display and, through these visualizations, provides feedback to the user about successes and errors of their touch interactions. Our visualization system is engineered to be overlaid on top of existing applications without requiring the applications to be modified in any way, and functions independently of the application's responses to user input. Ripples reduces the fundamental problem of ambiguity of feedback when an action results in an unexpected behaviour. This ambiguity can be caused by a wide variety of sources. We describe the ambiguity problem, and identify those sources. We then define a set of visual states and transitions needed to resolve this ambiguity, of use to anyone designing touch applications or systems. We then present the Ripples implementation of visualizations for those states, and the results of a user study demonstrating user preference for the system, and demonstrating its utility in reducing errors.\n",
      "=============================\n",
      "PAPILLON: designing curved display surfaces with printed optics\n",
      "We present a technology for designing curved display surfaces that can both display information and sense two dimensions of human touch. It is based on 3D printed optics, where the surface of the display is constructed as a bundle of printed light pipes, that direct images from an arbitrary planar image source to the surface of the display. This effectively decouples the display surface and image source, allowing to iterate the design of displays without requiring changes to the complex electronics and optics of the device. In addition, the same optical elements also direct light from the surface of the display back to the image sensor allowing for touch input and proximity detection of a hand relative to the display surface. The resulting technology is effective in designing compact, efficient displays of a small size; this has been applied in the design of interactive animated eyes.\n",
      "=============================\n",
      "KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera\n",
      "KinectFusion enables a user holding and moving a standard Kinect camera to rapidly create detailed 3D reconstructions of an indoor scene. Only the depth data from Kinect is used to track the 3D pose of the sensor and reconstruct, geometrically precise, 3D models of the physical scene in real-time. The capabilities of KinectFusion, as well as the novel GPU-based pipeline are described in full. Uses of the core system for low-cost handheld scanning, and geometry-aware augmented reality and physics-based interactions are shown. Novel extensions to the core GPU pipeline demonstrate object segmentation and user interaction directly in front of the sensor, without degrading camera tracking or reconstruction. These extensions are used to enable real-time multi-touch interactions anywhere, allowing any planar or non-planar reconstructed physical surface to be appropriated for touch.\n",
      "=============================\n",
      "Giving a hand to the eyes: leveraging input accuracy for subpixel interaction\n",
      "We argue that the current practice of using integer positions for pointing events artificially constrains human precision capabilities. The high sensitivity of current input devices can be harnessed to enable precise direct manipulation \"\"in between\"\" pixels, called subpixel interaction. We provide detailed analysis of subpixel theory and implementation, including the critical component of revised control-display gain transfer functions. A prototype implementation is described with several illustrative examples. Guidelines for subpixel domain applicability are provided and an overview of required changes to operating systems and graphical user interface frameworks are discussed.\n",
      "=============================\n",
      "Painting with Bob: assisted creativity for novices\n",
      "Current digital painting tools are primarily targeted at professionals and are often overwhelmingly complex for use by novices. At the same time, simpler tools may not invoke the user creatively, or are limited to plain styles that lack visual sophistication. There are many people who are not art professionals, yet would like to partake in digital creative expression. Challenges and rewards for novices differ greatly from those for professionals. In this paper, we leverage existing works in Creativity and Creativity Support Tools (CST) to formulate design goals specifically for digital art creation tools for novices. We implemented these goals within a digital painting system, called Painting with Bob. We evaluate the efficacy of the design and our prototype with a user study, and we find that users are highly satisfied with the user experience, as well as the paintings created with our system.\n",
      "=============================\n",
      "The auckland layout editor: an improved GUI layout specification process\n",
      "Layout managers are used to control the placement of widgets in graphical user interfaces (GUIs). Constraint-based layout managers are among the most powerful. However, they are also more complex and their layouts are prone to problems such as over-constrained specifications and widget overlap. This poses challenges for GUI builder tools, which ideally should address these issues automatically. We present a new GUI builderthe Auckland Layout Editor (ALE)that addresses these challenges by enabling GUI designers to specify constraint-based layouts using simple, mouse-based operations. We give a detailed description of ALE's edit operations, which do not require direct constraint editing. ALE guarantees that all edit operations lead to sound specifications, ensuring solvable and non-overlapping layouts. To achieve that, we present a new algorithm that automatically generates the constraints necessary to keep a layout non-overlapping. Furthermore, we discuss how our innovations can be combined with manual constraint editing in a sound way. Finally, to aid designers in creating layouts with good resize behavior, we propose a novel automatic layout preview. This displays the layout at its minimum and in an enlarged size, which allows visualizing potential resize issues directly. All these features permit GUI developers to focus more on the overall UI design.\n",
      "=============================\n",
      "Medusa: a proximity-aware multi-touch tabletop\n",
      "We present Medusa, a proximity-aware multi-touch tabletop. Medusa uses 138 inexpensive proximity sensors to: detect a user's presence and location, determine body and arm locations, distinguish between the right and left arms, and map touch point to specific users and specific hands. Our tracking algorithms and hardware designs are described. Exploring this unique design, we develop and report on a collection of interactions enabled by Medusa in support of multi-user collaborative design, specifically within the context of Proxi-Sketch, a multi-user UI prototyping tool. We discuss design issues, system implementation, limitations, and generalizable concepts throughout the paper.\n",
      "=============================\n",
      "Highlight: a system for creating and deploying mobile web applications\n",
      "We present a new server-side architecture that enables rapid prototyping and deployment of mobile web applications created from existing web sites. Key to this architecture is a remote control metaphor in which the mobile device controls a fully functional browser that is embedded within a proxy server. Content is clipped from the proxy browser, transformed if necessary, and then sent to the mobile device as a typical web page. Users' interactions with that content on the mobile device control the next steps of the proxy browser. We have found this approach to work well for creating mobile sites from a variety of existing sites, including those that use dynamic HTML and AJAX technologies. We have conducted a small user study to evaluate our model and API with experienced web programmers.\n",
      "=============================\n",
      "MudPad: localized tactile feedback on touch surfaces\n",
      "We present MudPad, a system that is capable of localized active haptic feedback on multitouch surfaces. An array of electromagnets locally actuates a tablet-sized overlay containing magnetorheological (MR) fluid. The reaction time of the fluid is fast enough for realtime feedback ranging from static levels of surface softness to a broad set of dynamically changeable textures. As each area can be addressed individually, the entire visual interface can be enriched with a multi-touch haptic layer that conveys semantic information as the appropriate counterpart to multi-touch input.\n",
      "=============================\n",
      "Phosphor: explaining transitions in the user interface using afterglow effects\n",
      "Sometimes users fail to notice a change that just took place on their display. For example, the user may have accidentally deleted an icon or a remote collaborator may have changed settings in a control panel. Animated transitions can help, but they force users to wait for the animation to complete. This can be cumbersome, especially in situations where users did not need an explanation. We propose a different approach. Phosphor objects show the outcome of their transition instantly; at the same time they explain their change in retrospect. Manipulating a phosphor slider, for example, leaves an afterglow that illustrates how the knob moved. The parallelism of instant outcome and explanation supports both types of users. Users who already understood the transition can continue interacting without delay, while those who are inexperienced or may have been distracted can take time to view the effects at their own pace. We present a framework of transition designs for widgets, icons, and objects in drawing programs. We evaluate phosphor objects in two user studies and report significant performance benefits for phosphor objects.\n",
      "=============================\n",
      "Multi-user interaction using handheld projectors\n",
      "Recent research on handheld projector interaction has expanded the display and interaction space of handheld devices by projecting information onto the physical environment around the user, but has mainly focused on single-user scenarios. We extend this prior single-user research to co-located multi-user interaction using multiple handheld projectors. We present a set of interaction techniques for supporting co-located collaboration with multiple handheld projectors, and discuss application scenarios enabled by them.\n",
      "=============================\n",
      "DigiTaps: eyes-free number entry on touchscreens with minimal audio feedback\n",
      "Eyes-free input usually relies on audio feedback that can be difficult to hear in noisy environments. We present DigiTaps, an eyes-free number entry method for touchscreen devices that requires little auditory attention. To enter a digit, users tap or swipe anywhere on the screen with one, two, or three fingers. The 10 digits are encoded by combinations of these gestures that relate to the digits' semantics. For example, the digit 2 is input with a 2-finger tap. We conducted a longitudinal evaluation with 16 people and found that DigiTaps with no audio feedback was faster but less accurate than with audio feedback after every input. Throughout the study, participants entered numbers with no audio feedback at an average rate of 0.87 characters per second, with an uncorrected error rate of 5.63%.\n",
      "=============================\n",
      "Haptic props: semi-actuated tangible props for haptic interaction on the surface\n",
      "While multiple methods to extend the expressiveness of tangible interaction have been proposed, e. g., self-motion, stacking and transparency, providing haptic feedback to the tangible prop itself has rarely been considered. In this poster we present a semi-actuated, nano-powered, tangible prop, which is able to provide programmable friction for interaction with a tabletop setup. We have conducted a preliminary user study evaluating the users' acceptance for the device and their ability to detect changes in the programmed level of friction and received some promising results.\n",
      "=============================\n",
      "Assieme: finding and leveraging implicit references in a web search interface for programmers\n",
      "Programmers regularly use search as part of the development process, attempting to identify an appropriate API for a problem, seeking more information about an API, and seeking samples that show how to use an API. However, neither general-purpose search engines nor existing code search engines currently fit their needs, in large part because the information programmers need is distributed across many pages. We present Assieme, a Web search interface that effectively supports common programming search tasks by combining information from Web-accessible Java Archive (JAR) files, API documentation, and pages that include explanatory text and sample code. Assieme uses a novel approach to finding and resolving implicit references to Java packages, types, and members within sample code on the Web. In a study of programmers performing searches related to common programming tasks, we show that programmers obtain better solutions, using fewer queries, in the same amount of time spent using a general Web search interface.\n",
      "=============================\n",
      "Composition for conductor and audience: new uses for mobile devices in the concert hall\n",
      "Composition for Conductor and Audience is an audience interaction piece first performed for an audience of over seventy-five people in June of 2011. The audience becomes the orchestra in this composition as they control different musical variables using the touchscreen surfaces on their personal mobile devices. To the authors' knowledge this is the first concert piece for bi-directional networked interactivity on audience-owned mobile devices to ever be performed. Audience members participated using the iOS / Android application 'Control', a generic solution for creating touchscreen interfaces written by the first author. Over twenty members of the audience participated in the performance, using their personal devices to match gestures made by the conductor with corresponding gestures on their mobile devices.\n",
      "=============================\n",
      "Efficient distributed implementation of semi-replicated synchronous groupware\n",
      "The Model View Controller (MVC) architecture has proven to be an effective way of organizing synchronous groupware applications. Distributed implementations of MVC, however, can suffer from poor performance. This paper demonstrates how optimized semi-replication of MVC architectures can lead to good performance over both local and wide area networks. We present a series of optimizations to network communication based on specific communication properties of groupware. These optimizations have been implemented in the Clock groupware development toolkit, allowing programmers to develop applications directly in the high-level MVC style, with Clock automatically providing optimized performance. Timings of an application developed in Clock show that usable speed was obtained in a highly interactive groupware application running between Toronto and Calgary, with a typical latency of 190 ms per round trip message. The paper discusses the tradeoffs involved in the algorithms, and presents timings to demonstrate the effectiveness of the different approaches. The timings show that when running over a wide area network, the best optimization can achieve a factor 60 speedup over the naive implementation of distributed MVC.\n",
      "=============================\n",
      "InterState: a language and environment for expressing interface behavior\n",
      "InterState is a new programming language and environment that addresses the challenges of writing and reusing user interface code. InterState represents interactive behaviors clearly and concisely using a combination of novel forms of state machines and constraints. It also introduces new language features that allow programmers to easily modularize and reuse behaviors. InterState uses a new visual notation that allows programmers to better understand and navigate their code. InterState also includes a live editor that immediately updates the running application in response to changes in the editor and vice versa to help programmers understand the state of their program. Finally, InterState can interface with code and widgets written in other languages, for example to create a user interface in InterState that communicates with a database. We evaluated the understandability of InterState's programming primitives in a comparative laboratory study. We found that participants were twice as fast at understanding and modifying GUI components when they were implemented with InterState than when they were implemented in a conventional textual event-callback style. We evaluated InterState's scalability with a series of benchmarks and example applications and found that it can scale to implement complex behaviors involving thousands of objects and constraints.\n",
      "=============================\n",
      "Content-aware dynamic timeline for video browsing\n",
      "When browsing a long video using a traditional timeline slider control, its effectiveness and precision degrade as a video's length grows. When browsing videos with more frames than pixels in the slider, aside from some frames being inaccessible, scrolling actions cause sudden jumps in a video's continuity as well as video frames to flash by too fast for one to assess the content. We propose a content-aware dynamic timeline control that is designed to overcome these limitations. Our timeline control decouples video speed and playback speed, and leverages video content analysis to allow salient shots to be presented at an intelligible speed. Our control also takes advantage of previous work on elastic sliders, which allows us to produce an accurate navigation control.\n",
      "=============================\n",
      "Alternative interfaces for chat\n",
      "We describe some common problems experienced by users of computer-based text chat, and show how many of these problems relate to the loss of timing-specific information. We suggest that thinking of chat as a real-time streaming media data type, with status and channel indicators, might solve some of these problems. We then present a number of alternative chat interfaces along with results from user studies comparing and contrasting them both with each other and with the standard chat interface. These studies show some potential, but indicate that more work needs to be done.\n",
      "=============================\n",
      "SearchTogether: an interface for collaborative web search\n",
      "Studies of search habits reveal that people engage in many search tasks involving collaboration with others, such as travel planning, organizing social events, or working on a homework assignment. However, current Web search tools are designed for a single user, working alone. We introduce SearchTogether, a prototype that enables groups of remote users to synchronously or asynchronously collaborate when searching the Web. We describe an example usage scenario, and discuss the ways SearchTogether facilitates collaboration by supporting awareness, division of labor, and persistence. We then discuss the findings of our evaluation of SearchTogether, analyzing which aspects of its design enabled successful collaboration among study participants.\n",
      "=============================\n",
      "SpeechSkimmer: interactively skimming recorded speech\n",
      "Skimming or browsing audio recordings is much more difficult than visually scanning a document because of the temporal nature of audio. By exploiting properties of spontaneous speech it is possible to automatically select and present salient audio segments in a time-efficient manner. Techniques for segmenting recordings and a prototype user interface for skimming speech are described. The system developed incorporates time-compressed speech and pause removal to reduce the time needed to listen to speech recordings. This paper presents a multi-level approach to auditory skimming, along with user interface techniques for interacting with the audio and providing feedback. Several time compression algorithms ami an adaptive speech detection technique are also stuntnarized.\n",
      "=============================\n",
      "Midas: fabricating custom capacitive touch sensors to prototype interactive objects\n",
      "An increasing number of consumer products include user interfaces that rely on touch input. While digital fabrication techniques such as 3D printing make it easier to prototype the shape of custom devices, adding interactivity to such prototypes remains a challenge for many designers. We introduce Midas, a software and hardware toolkit to support the design, fabrication, and programming of flexible capacitive touch sensors for interactive objects. With Midas, designers first define the desired shape, layout, and type of touch sensitive areas, as well as routing obstacles, in a sensor editor. From this high-level specification, Midas automatically generates layout files with appropriate sensor pads and routed connections. These files are then used to fabricate sensors using digital fabrication processes, e.g., vinyl cutters and conductive ink printers. Using step-by-step assembly instructions generated by Midas, designers connect these sensors to the Midas microcontroller, which detects touch events. Once the prototype is assembled, designers can define interactivity for their sensors: Midas supports both record-and-replay actions for controlling existing local applications and WebSocket-based event output for controlling novel or remote applications. In a first-use study with three participants, users successfully prototyped media players. We also demonstrate how Midas can be used to create a number of touch-sensitive interfaces.\n",
      "=============================\n",
      "Towards a unified framework for modeling, dispatching, and interpreting uncertain input\n",
      "Many new input technologies (such as touch and voice) hold the promise of more natural user interfaces. However, many of these technologies create inputs with some uncertainty. Unfortunately, conventional infrastructure lacks a method for easily handling uncertainty, and as a result input produced by these technologies is often converted to conventional events as quickly as possible, leading to a stunted interactive experience. Our ongoing work aims to design a unified framework for modeling uncertain input and dispatching it to interactors. This should allow developers to easily create interactors which can interpret uncertain input, give the user appropriate feedback, and accurately resolve any ambiguity. This abstract presents an overview of the design of a framework for handling input with uncertainty and describes topics we hope to pursue in future work. We also give an example of how we built highly accurate touch buttons using our framework. For examples of what interactors can be built and a more detailed description of our framework we refer the reader to [8].\n",
      "=============================\n",
      "The information grid: a framework for information retrieval and retrieval-centered applications\n",
      "The Information Grid (InfoGrid) is a framework for building information access applications that provides a user interface design and an interaction model. It focuses on retrieval of application objects as its top level mechanism for accessing user information, documents, or services. We have embodied the InfoGrid design in an object-oriented application framework that supports rapid construction of applications. This application framework has been used to build a number of applications, some that are classically characterized as information retrieval applications, others that are more typically viewed as personal work tools.\n",
      "=============================\n",
      "Mining web interactions to automatically create mash-ups\n",
      "The deep web contains an order of magnitude more information than the surface web, but that information is hidden behind the web forms of a large number of web sites. Metasearch engines can help users explore this information by aggregating results from multiple resources, but previously these could only be created and maintained by programmers. In this paper, we explore the automatic creation of metasearch mash-ups by mining the web interactions of multiple web users to find relations between query forms on different web sites. We also present an implemented system called TX2 that uses those connections to search multiple deep web resources simultaneously and integrate the results in context in a single results page. TX2 illustrates the promise of constructing mash-ups automatically and the potential of mining web interactions to explore deep web resources.\n",
      "=============================\n",
      "The MaggLite post-WIMP toolkit: draw it, connect it and run it\n",
      "This article presents MaggLite, a toolkit and sketch-based interface builder allowing fast and interactive design of post-WIMP user interfaces. MaggLite improves design of advanced UIs thanks to its novel <i>mixed-graph</i> architecture that dynamically combines scene-graphs with interaction-graphs. <i>Scene-graphs</i> provide mechanisms to describe and produce rich graphical effects, whereas <i>interaction-graphs</i> allow expressive and fine-grained description of advanced interaction techniques and behaviors such as multiple pointers management, toolglasses, bimanual interaction, gesture, and speech recognition. Both graphs can be built interactively by sketching the UI and specifying the interaction using a dataflow visual language. Communication between the two graphs is managed at runtime by components we call <i>Interaction Access Points</i>. While developers can extend the toolkit by refining built-in generic mechanisms, UI designers can quickly and interactively design, prototype and test advanced user interfaces by applying the MaggLite principle: \"draw it, connect it and run it\".\n",
      "=============================\n",
      "YourGloves, hothands and hotmits: devices to hold hands at a distance\n",
      "There is a growing body of work in HCI on the design of communication technologies to help support lovers in long distance relationships. We build upon this work by presenting an exploratory study of hand-holding prototypes. Our work distinguishes itself by basing distance communication metaphors on elements of familiar, simple co-located behaviours. We argue that the combined evocative power of unique co-created physical representations of the absent other can be used by separated lovers to generate powerful and positive experiences, in turn sustaining romantic connections at a distance.\n",
      "=============================\n",
      "Access overlays: improving non-visual access to large touch screens for blind users\n",
      "Many touch screens remain inaccessible to blind users, and those approaches to providing access that do exist offer minimal support for interacting with large touch screens or spatial data. In this paper, we introduce a set of three software-based access overlays intended to improve the accessibility of large touch screen interfaces, specifically interactive tabletops. Our access overlays are called edge projection, neighborhood browsing, and touch-and-speak. In a user study, 14 blind users compared access overlays to an implementation of Apple's VoiceOver screen reader. Our results show that two of our techniques were faster than VoiceOver, that participants correctly answered more questions about the screen's layout using our techniques, and that participants overwhelmingly preferred our techniques. We developed several applications demonstrating the use of access overlays, including an accessible map kiosk and an accessible board game.\n",
      "=============================\n",
      "Poke: emotional touch delivery through an inflatable surface over interpersonal mobile communications\n",
      "In this paper we present Poke - a soft and human-like re-mote touch technique through an inflatable surface. We aimed to design it for delivering more emotional and plea-sant touches over interpersonal mobile communications. Poke enables to touch human's skin with an inflatable sur-face according to the other user's finger pressures and hand gestures during a phone call. It delivers different kinds of pokes and other affective touches with its inflating patterns (strengths and repetitions) and vibrations from the top of the inflatable surface. The paper also suggests affective touches such as weak/hard poke, poke and then shake, poke back and pat which can be exchanged during typical phone calls.\n",
      "=============================\n",
      "Lineogrammer: creating diagrams by drawing\n",
      "We present the design of Lineogrammer, a diagram-drawing system motivated by the immediacy and fluidity of pencil-drawing. We attempted for Lineogrammer to feel like a modeless diagramming \"medium\" in which stylus input is immediately interpreted as a command, text label or a drawing element, and drawing elements snap to or sculpt from existing elements. An inferred dual representation allows geometric diagram elements, no matter how they were entered, to be manipulated at granularities ranging from vertices to lines to shapes. We also integrate lightweight tools, based on rulers and construction lines, for controlling higher-level diagram attributes, such as symmetry and alignment. We include preliminary usability observations to help identify areas of strength and weakness with this approach.\n",
      "=============================\n",
      "Imaginary reality gaming: ball games without a ball\n",
      "We present imaginary reality games, i.e., games that mimic the respective real world sport, such as basketball or soccer, except that there is no visible ball. The ball is virtual and players learn about its position only from watching each other act and a small amount of occasional auditory feed-back, e.g., when a person is receiving the ball. Imaginary reality games maintain many of the properties of physical sports, such as unencumbered play, physical exertion, and immediate social interaction between players. At the same time, they allow introducing game elements from video games, such as power-ups, non-realistic physics, and player balancing. Most importantly, they create a new game dynamic around the notion of the invisible ball. To allow players to successfully interact with the invisible ball, we have created a physics engine that evaluates all plausible ball trajectories in parallel, allowing the game engine to select the trajectory that leads to the most enjoyable game play while still favoring skillful play.\n",
      "=============================\n",
      "A tool to support speech and non-speech audio feedback generation in audio interfaces\n",
      "Development of new auditory interfaces requires the integration of text-to-speech synthesis, digitized audio, and non-speech audio output. This paper describes a tool for specifying speech and non-speech audio feedback and its use in the development of a speech interface, Conversational VoiceNotes. Auditory feedback is specified as a context-free grammar, where the basic elements in the grammar can be either words or non-speech sounds. The feedback specification method described here provides the ability to vary the feedback based on the current state of the system, and is flexible enough to allow different feedback for different input modalities (e.g., speech, mouse, buttons). The declarative specification is easily modifiable, supporting an iterative design process.\n",
      "=============================\n",
      "Gilded gait: reshaping the urban experience with augmented footsteps\n",
      "In this paper we describe Gilded Gait, a system that changes the perceived physical texture of the ground, as felt through the soles of users' feet. Ground texture, in spite of its potential as an effective channel of peripheral information display, has so far been paid little attention in HCI research. The system is designed as a pair of insoles with embedded actuators, and utilizes vibrotactile feedback to simulate the perceptions of a range of different ground textures. The discreet, low-key nature of the interface makes it particularly suited for outdoor use, and its capacity to alter how people experience the built environment may open new possibilities in urban design.\n",
      "=============================\n",
      "Annotating gigapixel images\n",
      "Panning and zooming interfaces for exploring very large images containing billions of pixels (gigapixel images) have recently appeared on the internet. This paper addresses issues that arise when creating and rendering auditory and textual annotations for such images. In particular, we define a distance metric between each annotation and any view resulting from panning and zooming on the image. The distance then informs the rendering of audio annotations and text labels. We demonstrate the annotation system on a number of panoramic images.\n",
      "=============================\n",
      "The IBar: a perspective-based camera widget\n",
      "We present a new screen space widget, the IBar, for effective camera control in 3D graphics environments. The IBar provides a compelling interface for controlling scene perspective based on the artistic concept of vanishing points. Various handles on the widget manipulate multiple camera parameters simultaneously to create a single perceived projection change. For example, changing just the perspective distortion is accomplished by simultaneously decreasing the camera's distance to the scene while increasing focal length. We demonstrate that the IBar is easier to learn for novice users and improves their understanding of camera perspective.\n",
      "=============================\n",
      "Some design refinements and principles on the appearance and behavior of marking menus\n",
      "This paper describes some design refinements on marking menus and shows how these refinements embody interest ing and relevant design principles for HCI. These refinements are based on the design principles OR (1) maintaining visual context, (2) hiding unnecessmy information, and (3) supporting skill development by graphical feedback. The result is a new graphiczit representation and a more effective form of visual feedback and behavior for marking menus.\n",
      "=============================\n",
      "Computing versus human thinking\n",
      "Dr. Naur was editor in 1960 of the hugely influential “Report on the Algorithmic Language Algol 60.” He is recognized for the report's elegance, uniformity and coherence, and credited as an important contributor to the language's power and simplicity. The report made pioneering use of what later became known as Backus-Naur Form (BNF) to define the syntax of programs. BNF is now the standard way to define a computer language. Naur is also cited for his contribution to compiler design and to the art and practice of computer programming.\n",
      "=============================\n",
      "StyleCam: interactive stylized 3D navigation using integrated spatial & temporal controls\n",
      "This paper describes StyleCam, an approach for authoring 3D viewing experiences that incorporate stylistic elements that are not available in typical 3D viewers. A key aspect of StyleCam is that it allows the author to significantly tailor what the user sees and when they see it. The resulting viewing experience can approach the visual richness and pacing of highly authored visual content such as television commercials or feature films. At the same time, StyleCam allows for a satisfying level of interactivity while avoiding the problems inherent in using unconstrained camera models. The main components of StyleCam are camera surfaces which spatially constrain the viewing camera; animation clips that allow for visually appealing transitions between different camera surfaces; and a simple, unified, interaction technique that permits the user to seamlessly and continuously move between spatial-control of the camera and temporal-control of the animated transitions. Further, the user's focus of attention is always kept on the content, and not on extraneous interface widgets. In addition to describing the conceptual model of StyleCam, its current implementation, and an example authored experience, we also present the results of an evaluation involving real users.\n",
      "=============================\n",
      "Automatic generation of task-oriented help\n",
      "This work presents an approach to the design of the software component of an Interactive System, which supports the generation of automatic task-oriented help. Help can easily be generated from the abstract formal specification of the associated system without any further effort. The architectural description is obtained in a taskclriven way, where tasks are specified by indicating temporal ordering constraints using operators of a concurrent formal notation. The association of user tasks with software interaction objects, which inherit constraints of related tasks, gives the information to structure task-oriented help in an immediate way. The help given is thus more expressive with a consequent improvement in the usability of an Interactive System.\n",
      "=============================\n",
      "Interactive public ambient displays: transitioning from implicit to explicit, public to personal, interaction with multiple users\n",
      "We develop design principles and an interaction framework for sharable, interactive public ambient displays that support the transition from implicit to explicit interaction with both public and personal information. A prototype system implementation that embodies these design principles is described. We use novel display and interaction techniques such as simple hand gestures and touch screen input for explicit interaction and contextual body orientation and position cues for implicit interaction. Techniques are presented for subtle notification, self-revealing help, privacy controls, and shared use by multiple people each in their own context. Initial user feedback is also presented, and future directions discussed.\n",
      "=============================\n",
      "Laying out and visualizing large trees using a hyperbolic space\n",
      "We present a new focus+context (fisheye) scheme for visualizing and manipulating large hierarchies. The essence of our approach is to lay out the hierarchy uniformly on the hyperbolic plane and map this plane onto a circular display region. The projection onto the disk provides a natural mechanism for assigning more space to a portion of the hierarchy while still embedding it in a much larger context. Change of focus is accomplished by translating the structure on the hyperbolic plane, which allows a smooth transition without compromising the presentation of the context.\n",
      "=============================\n",
      "Augmenting conversations using dual-purpose speech\n",
      "In this paper, we explore the concept of dual-purpose speech: speech that is socially appropriate in the context of a human-to-human conversation which also provides meaningful input to a computer. We motivate the use of dual-purpose speech and explore issues of privacy and technological challenges related to mobile speech recognition. We present three applications that utilize dual-purpose speech to assist a user in conversational tasks: the Calendar Navigator Agent, DialogTabs, and Speech Courier. The Calendar Navigator Agent navigates a user's calendar based on socially appropriate speech used while scheduling appointments. DialogTabs allows a user to postpone cognitive processing of conversational material by proving short-term capture of transient information. Finally, Speech Courier allows asynchronous delivery of relevant conversational information to a third party.\n",
      "=============================\n",
      "Sensing techniques for tablet+stylus interaction\n",
      "We explore grip and motion sensing to afford new techniques that leverage how users naturally manipulate tablet and stylus devices during pen + touch interaction. We can detect whether the user holds the pen in a writing grip or tucked between his fingers. We can distinguish bare-handed inputs, such as drag and pinch gestures produced by the nonpreferred hand, from touch gestures produced by the hand holding the pen, which necessarily impart a detectable motion signal to the stylus. We can sense which hand grips the tablet, and determine the screen's relative orientation to the pen. By selectively combining these signals and using them to complement one another, we can tailor interaction to the context, such as by ignoring unintentional touch inputs while writing, or supporting contextually-appropriate tools such as a magnifier for detailed stroke work that appears when the user pinches with the pen tucked between his fingers. These and other techniques can be used to impart new, previously unanticipated subtleties to pen + touch interaction on tablets.\n",
      "=============================\n",
      "Shadow reaching: a new perspective on interaction for large displays\n",
      "We introduce Shadow Reaching, an interaction technique that makes use of a perspective projection applied to a shadow representation of a user. The technique was designed to facilitate manipulation over large distances and enhance understanding in collaborative settings. We describe three prototype implementations that illustrate the technique, examining the advantages of using shadows as an interaction metaphor to support single users and groups of collaborating users. Using these prototypes as a design probe, we discuss how the three components of the technique (sensing, modeling, and rendering) can be accomplished with real (physical) or computed (virtual) shadows, and the benefits and drawbacks of each approach.\n",
      "=============================\n",
      "FlowBlocks: a multi-touch ui for crowd interaction\n",
      "Multi-touch technology lends itself to collaborative crowd interaction (CI). However, common tap-operated widgets are impractical for CI, since they are susceptible to accidental touches and interference from other users. We present a novel multi-touch interface called FlowBlocks in which every UI action is invoked through a small sequence of user actions: dragging parametric UI-Blocks, and dropping them over operational UI-Docks. The FlowBlocks approach is advantageous for CI because it a) makes accidental touches inconsequential; and b) introduces design parameters for mutual awareness, concurrent input, and conflict management. FlowBlocks was successfully used on the floor of a busy natural history museum. We present the complete design space and describe a year-long iterative design and evaluation process which employed the Rapid Iterative Test and Evaluation (RITE) method in a museum setting.\n",
      "=============================\n",
      "The design and evaluation of selection techniques for 3D volumetric displays\n",
      "Volumetric displays, which display imagery in true 3D space, are a promising platform for the display and manipulation of 3D data. To fully leverage their capabilities, appropriate user interfaces and interaction techniques must be designed. In this paper, we explore 3D selection techniques for volumetric displays. In a first experiment, we find a ray cursor to be superior to a 3D point cursor in a single target environment. To address the difficulties associated with dense target environments we design four new ray cursor techniques which provide disambiguation mechanisms for multiple intersected targets. Our techniques showed varied success in a second, dense target experiment. One of the new techniques, the depth ray, performed particularly well, significantly reducing movement time, error rate, and input device footprint in comparison to the 3D point cursor.\n",
      "=============================\n",
      "PICOntrol: using a handheld projector for direct control of physical devices through visible light\n",
      "Today's environments are populated with a growing number of electric devices which come in diverse form factors and provide a plethora of functions. However, rich interaction with these devices can become challenging if they need be controlled from a distance, or are too small to accommodate user interfaces on their own. In this work, we explore PICOntrol, a new approach using an off-the-shelf handheld pico projector for direct control of physical devices through visible light. The projected image serves a dual purpose by simultaneously presenting a visible interface to the user, and transmitting embedded control information to inexpensive sensor units integrated with the devices. To use PICOntrol, the user points the handheld projector at a target device, overlays a projected user interface on its sensor unit, and performs various GUI-style or gestural interactions. PICOntrol enables direct, visible, and rich interactions with various physical devices without requiring central infrastructure. We present our prototype implementation as well as explorations of its interaction space through various application examples.\n",
      "=============================\n",
      "Optically sensing tongue gestures for computer input\n",
      "Many patients with paralyzing injuries or medical conditions retain the use of their cranial nerves, which control the eyes, jaw, and tongue. While researchers have explored eye-tracking and speech technologies for these patients, we believe there is potential for directly sensing explicit tongue movement for controlling computers. In this paper, we describe a novel approach of using infrared optical sensors embedded within a dental retainer to sense tongue gestures. We describe an experiment showing our system effectively discriminating between four simple gestures with over 90% accuracy. In this experiment, users were also able to play the popular game Tetris with their tongues. Finally, we present lessons learned and opportunities for future work.\n",
      "=============================\n",
      "Simplifying component development in an integrated groupware environment\n",
      "This paper describes our experiences implementing a component architecture for TeamWave Workplace, an integrated groupware environment using a rooms metaphor. The problem we faced was how to design the architecture to support rapid development of new embedded components. Our solution, based on Tcl/Tk and GroupKit, uses multiple interpreters and a shared window hierarchy. This proved effective in easing development complexity in TeamWave. We discuss some of the strategies we used, and identify the types of interactions between system components. The lessons learned in developing this component model should be generally applicable to future integrated groupware systems in different environments.\n",
      "=============================\n",
      "A semi-automatic approach to home video editing\n",
      "Hitchcock is a system that allows users to easily create custom videos from raw video shot with a standard video camera. In contrast to other video editing systems, Hitchcock uses automatic analysis to determine the suitability of portions of the raw video. Unsuitable video typically has fast or erratic camera motion. Hitchcock first analyzes video to identify the type and amount of camera motion: fast pan, slow zoom, etc. Based on this analysis, a numerical \" unsuit-ability \" score is computed for each frame of the video. Combined with standard editing rules, this score is used to identify clips for inclusion in the final video and to select their start and end points. To create a custom video, the user drags keyframes corresponding to the desired clips into a storyboard. Users can lengthen or shorten the clip without specifying the start and end frames explicitly. Clip lengths are balanced automatically using a spring-based algorithm.\n",
      "=============================\n",
      "Cobi: a community-informed conference scheduling tool\n",
      "Effectively planning a large multi-track conference requires an understanding of the preferences and constraints of organizers, authors, and attendees. Traditionally, the onus of scheduling the program falls on a few dedicated organizers. Resolving conflicts becomes difficult due to the size and complexity of the schedule and the lack of insight into community members' needs and desires. Cobi presents an alternative approach to conference scheduling that engages the entire community in the planning process. Cobi comprises (a) communitysourcing applications that collect preferences, constraints, and affinity data from community members, and (b) a visual scheduling interface that combines communitysourced data and constraint-solving to enable organizers to make informed improvements to the schedule. This paper describes Cobi's scheduling tool and reports on a live deployment for planning CHI 2013, where organizers considered input from 645 authors and resolved 168 scheduling conflicts. Results show the value of integrating community input with an intelligent user interface to solve complex planning tasks.\n",
      "=============================\n",
      "Obake: interactions on a 2.5D elastic display\n",
      "In this poster we present an interaction language for the manipulation of an elastic deformable 2.5D display. We discuss a range of gestures to interact and directly deform the surface. To demonstrate these affordances and the associated interactions, we present a scenario of a topographic data viewer using this prototype system.\n",
      "=============================\n",
      "Creating collections with automatic suggestions and example-based refinement\n",
      "To create collections, like music playlists from personal media libraries, users today typically do one of two things. They either manually select items one-by-one, which can be time consuming, or they use an example-based recommendation system to automatically generate a collection. While such automatic engines are convenient, they offer the user limited control over how items are selected. Based on prior research and our own observations of existing practices, we propose a semi-automatic interface for creating collections that combines automatic suggestions with manual refinement tools. Our system includes a keyword query interface for specifying high-level collection preferences (e.g., \"some rock, no Madonna, lots of U2,\") as well as three example-based collection refinement techniques: 1) a suggestion widget for adding new items in-place in the context of the collection; 2) a mechanism for exploring alternatives for one or more collection items; and 3) a two-pane linked interface that helps users browse their libraries based on any selected collection item. We demonstrate our approach with two applications. SongSelect helps users create music playlists, and PhotoSelect helps users select photos for sharing. Initial user feedback is positive and confirms the need for semi-automated tools that give users control over automatically created collections.\n",
      "=============================\n",
      "Pick-and-drop: a direct manipulation technique for multiple computer environments\n",
      "This paper proposes a new field of user interfaces called multi-computer direct manipulation and presents a penbased direct manipulation technique that can be used for data transfer between different computers as well as within the same computer. The proposed Pick-andDrop allows a user to pick up an object on a display and drop it on another display as if he/she were manipulating a physical object. Even though the pen itself does not have storage capabilities, a combination of Pen-ID and the pen manager on the network provides the illusion that the pen can physically pick up and move a computer object. Based on this concept, we have built several experimental applications using palm-sized, desk-top, and wall-sized pen computers. We also considered the importance of physical artifacts in designing user interfaces in a future computing environment.\n",
      "=============================\n",
      "Touch & activate: adding interactivity to existing objects using active acoustic sensing\n",
      "In this paper, we present a novel acoustic touch sensing technique called Touch & Activate. It recognizes a rich context of touches including grasp on existing objects by attaching only a vibration speaker and a piezo-electric microphone paired as a sensor. It provides easy hardware configuration for prototyping interactive objects that have touch input capability. We conducted a controlled experiment to measure the accuracy and trade-off between the accuracy and number of training rounds for our technique. From its results, per-user recognition accuracies with five touch gestures for a plastic toy as a simple example and six hand postures for the posture recognition as a complex example were 99.6% and 86.3%, respectively. Walk up user recognition accuracies for the two applications were 97.8% and 71.2%, respectively. Since the results of our experiment showed a promising accuracy for the recognition of touch gestures and hand postures, Touch & Activate should be feasible for prototype interactive objects that have touch input capability.\n",
      "=============================\n",
      "Enabling web browsers to augment web sites' filtering and sorting functionalities\n",
      "Existing augmentations of web pages are mostly small cosmetic changes (e.g., removing ads) and minor addition of third-party content (e.g., product prices from competing sites). None leverages the structured data presented in web pages. This paper describes Sifter, a web browser extension that can augment a well-structured web site with advanced filtering and sorting functionality. These added features work inside the site's own pages, preserving the site's presentational style and the user's context. Sifter contains an algorithm that scrapes structured data out of well-structured web pages while usually requiring no user intervention. We tested Sifter on real web sites and real users and found that people could use Sifter to perform sophisticated queries and high-level analyses on sizable data collections on the Web. We propose that web sites can be similarly augmented with other sophisticated data-centric functionality, giving users new benefits over the existing Web.\n",
      "=============================\n",
      "Relaxed selection techniques for querying time-series graphs\n",
      "Time-series graphs are often used to visualize phenomena that change over time. Common tasks include comparing values at different points in time and searching for specified patterns, either exact or approximate. However, tools that support time-series graphs typically separate query specification from the actual search process, allowing users to adapt the level of similarity only after specifying the pattern. We introduce relaxed selection techniques, in which users implicitly define a level of similarity that can vary across the search pattern, while creating a search query with a single-gesture interaction. Users sketch over part of the graph, establishing the level of similarity through either spatial deviations from the graph, or the speed at which they sketch (temporal deviations). In a user study, participants were significantly faster when using our temporally relaxed selection technique than when using traditional techniques. In addition, they achieved significantly higher precision and recall with our spatially relaxed selection technique compared to traditional techniques.\n",
      "=============================\n",
      "Swipeboard: a text entry technique for ultra-small interfaces that supports novice to expert transitions\n",
      "Ultra-small smart devices, such as smart watches, have become increasingly popular in recent years. Most of these devices rely on touch as the primary input modality, which makes tasks such as text entry increasingly difficult as the devices continue to shrink. In the sole pursuit of entry speed, the ultimate solution is a shorthand technique (e.g., Morse code) that sequences tokens of input (e.g., key, tap, swipe) into unique representations of each character. However, learning such techniques is hard, as it often resorts to rote memory. Our technique, Swipeboard, leverages our spatial memory of a QWERTY keyboard to learn, and eventually master a shorthand, eyes-free text entry method designed for ultra-small interfaces. Characters are entered with two swipes; the first swipe specifies the region where the character is located, and the second swipe specifies the character within that region. Our study showed that with less than two hours' training, Tested on a reduced word set, Swipeboard users achieved 19.58 words per minute (WPM), 15% faster than an existing baseline technique.\n",
      "=============================\n",
      "Physink: sketching physical behavior\n",
      "Describing device behavior is a common task that is currently not well supported by general animation or CAD software. We present PhysInk, a system that enables users to demonstrate 2D behavior by sketching and directly manipulating objects on a physics-enabled stage. Unlike previous tools that simply capture the user's animation, PhysInk captures an understanding of the behavior in a timeline. This enables useful capabilities such as causality-aware editing and finding physically-correct equivalent behavior. We envision PhysInk being used as a physics teacher's sketchpad or a WYSIWYG tool for game designers.\n",
      "=============================\n",
      "Direct and gestural interaction with relief: a 2.5D shape display\n",
      "Actuated shape output provides novel opportunities for experiencing, creating and manipulating 3D content in the physical world. While various shape displays have been proposed, a common approach utilizes an array of linear actuators to form 2.5D surfaces. Through identifying a set of common interactions for viewing and manipulating content on shape displays, we argue why input modalities beyond direct touch are required. The combination of freehand gestures and direct touch provides additional degrees of freedom and resolves input ambiguities, while keeping the locus of interaction on the shape output. To demonstrate the proposed combination of input modalities and explore applications for 2.5D shape displays, two example scenarios are implemented on a prototype system.\n",
      "=============================\n",
      "Low-cost audience polling using computer vision\n",
      "Electronic response systems known as \"clickers\" have demonstrated educational benefits in well-resourced classrooms, but remain out-of-reach for most schools due to their prohibitive cost. We propose a new, low-cost technique that utilizes computer vision for real-time polling of a classroom. Our approach allows teachers to ask a multiple-choice question. Students respond by holding up a qCard: a sheet of paper that contains a printed code, similar to a QR code, encoding their student IDs. Students indicate their answers (A, B, C or D) by holding the card in one of four orientations. Using a laptop and an off-the-shelf webcam, our software automatically recognizes and aggregates the students' responses and displays them to the teacher. We built this system and performed initial trials in secondary schools in Bangalore, India. In a 25-student classroom, our system offers 99.8% recognition accuracy, captures 97% of responses within 10 seconds, and costs 15 times less than existing electronic solutions.\n",
      "=============================\n",
      "Adding imageability features to information displays\n",
      "Techniques for improving the imageability of an existing data visualisation are described. The aim is to make the visualisation more easily explored, navigated and remembered. Starting from a relatively sparse landscape‐ like representation of a set of objects, we selectively add to the visualisation static features such as clusters, and dynamic features such as view‐specific sampling of object detail. Information on past usage is used in this process, making manifest an aspect of interaction which is often neglected. Issues arising from the use of such features in a shared virtual environment are discussed.\n",
      "=============================\n",
      "Tangible NURBS-curve manipulation techniques using graspable handles on a large display\n",
      "This paper presents tangible interaction techniques for fine-tuning one-to-one scale NURBS curves on a large display for automotive design. We developed a new graspable handle with a transparent groove that allows designers to manipulate virtual curves on a display screen directly. The use of the proposed handle leads naturally to a rich vocabulary of terms describing interaction techniques that reflect existing shape styling methods. A user test raised various issues related to the graspable user interface, two-handed input, and large-display interaction.\n",
      "=============================\n",
      "An architecture for transforming graphical interfaces\n",
      "While graphical user interfaces have gained much popularity in recent years, there are situations when the need to use existing applications in a nonvisual modality is clear. Examples of such situations include the use of applications on hand-held devices with limited screen space (or even no screen space, as in the case of telephones), or users with visual impairments.We have developed an architecture capable of transforming the graphical interfaces of existing applications into powerful intuitive nonvisual interfaces. Our system, called Mercator, provides new input and output techniques for working in the nonvisual domain. Navigation is accomplished by traversing a hierarchical tree representation of the interface structure. Output is primarily auditory, although other output modalities (such as tactile) can be used as well. The mouse, an inherently visually-oriented device, is replaced by keyboard and voice interaction.Our system is currently in its third major revision. We have gained insight into both the nonvisual interfaces presented by our system and the architecture necessary to construct such interfaces. This architecture uses several novel techniques to efficiently and flexibly map graphical interfaces into new modalities.\n",
      "=============================\n",
      "TwitApp: in-product micro-blogging for design sharing\n",
      "We describe TwitApp, an enhanced micro-blogging system integrated within AutoCAD for design sharing. TwitApp integrates rich content and still keeps the sharing transaction cost low. In TwitApp, tweets are organized by their project, and users can follow or unfollow each individual project. We introduce the concept of automatic tweet drafting and other novel features such as enhanced real-time search and integrated live video streaming. The TwitApp system leverages the existing Twitter micro-blogging system. We also contribute a study which provides insights on these concepts and associated designs, and demonstrates potential user excitement of such tools.\n",
      "=============================\n",
      "The satellite cursor: achieving MAGIC pointing without gaze tracking using multiple cursors\n",
      "We present the satellite cursor - a novel technique that uses multiple cursors to improve pointing performance by reducing input movement. The satellite cursor associates every target with a separate cursor in its vicinity for pointing, which realizes the MAGIC (manual and gaze input cascade) pointing method without gaze tracking. We discuss the problem of visual clutter caused by multiple cursors and propose several designs to mitigate it. Two controlled experiments were conducted to evaluate satellite cursor performance in a simple reciprocal pointing task and a complex task with multiple targets of varying layout densities. Results show the satellite cursor can save significant mouse movement and consequently pointing time, especially for sparse target layouts, and that satellite cursor performance can be accurately modeled by Fitts' Law.\n",
      "=============================\n",
      "A framework for sharing handwritten notes\n",
      "NotePals is an ink-based, collaborative note taking application that runs on personal digital assistants (PDAs). Meeting participants write notes in their own handwriting on a PDA. These notes are shared with other participants by synchronizing later with a shared note repository that can be viewed using a desktop-based web browser. NotePals is distinguished by its lightweight process, interface, and hardware. This demonstration illustrates the design of two different NotePals clients and our web-based note browser.\n",
      "=============================\n",
      "Smart bookmarks: automatic retroactive macro recording on the web\n",
      "We present a new web automation system that allows users to create a smart bookmark, consisting of a starting URL plus a script of commands that returns to a particular web page or state of a web application. A smart bookmark can be requested for any page, and the necessary commands are automatically extracted from the user's interaction history. Unlike other web macro recorders, which require the user to start recording before navigating to the desired page, smart bookmarks are generated retroactively, after the user has already reached a page, and the starting point of the macro is found automatically. Smart bookmarks have a rich graphical visualization that combines textual commands, web page screenshots, and animations to explain what the bookmark does. A bookmark's script consists of keyword commands, interpreted without strict reliance on syntax, allowing bookmarks to be easily edited and shared.\n",
      "=============================\n",
      "Navigating documents with the virtual scroll ring\n",
      "We present a technique for scrolling through documents that is simple to implement and requires no special hardware. This is accomplished by simulating a hardware scroll ring--a device that maps circular finger motion into vertical scrolling. The technique performs at least as well as a mouse wheel for medium and long distances, and is preferred by users. It can be particularly useful in portable devices where screen-space and space for peripherals is at a premium.\n",
      "=============================\n",
      "Gesture search: a tool for fast mobile data access\n",
      "Modern mobile phones can store a large amount of data, such as contacts, applications and music. However, it is difficult to access specific data items via existing mobile user interfaces. In this paper, we present Gesture Search, a tool that allows a user to quickly access various data items on a mobile phone by drawing gestures on its touch screen. Gesture Search contributes a unique way of combining gesture-based interaction and search for fast mobile data access. It also demonstrates a novel approach for coupling gestures with standard GUI interaction. A real world deployment with mobile phone users showed that Gesture Search enabled fast, easy access to mobile data in their day-to-day lives. Gesture Search has been released to public and is currently in use by hundreds of thousands of mobile users. It was rated positively by users, with a mean of 4.5 out of 5 for over 5000 ratings.\n",
      "=============================\n",
      "SmartMusicKIOSK: music listening station with chorus-search function\n",
      "This paper describes a new music-playback interface for trial listening, SmartMusicKIOSK. In music stores, short trial listening of CD music is not usually a passive experience -- customers often search out the chorus or \"hook\" of a song using the fast-forward button. Listening of this type, however, has not been traditionally supported. This research achieves a function for jumping to the chorus section and other key parts of a song plus a function for visualizing song structure. These functions make it easier for a listener to find desired parts of a song and thereby facilitate an active listening experience. The proposed functions are achieved by an automatic chorus-section detecting method, and the results of implementing them as a listening station have demonstrated their usefulness.\n",
      "=============================\n",
      "MARBLS: a visual environment for building clinical alert rules\n",
      "Physicians and nurses usually rely on hospital information systems (HIS) for detecting a variety of adverse clinical conditions and reminding repetitive treatments. However, the acquisition of alert rules expected by HIS from experts remains a challenging, error-prone, and time-consuming process. In this work, we present MARBLS (Medical Alert Rule BuiLding System) - a visual environment to facilitate the design and definition of clinical alert rules. MARBLS enables a two-way, synchronized visual rule workspace and visual query explorer. Monitoring rules can be built by manipulating block components in the rule workspace, by querying and generalizing region of interests in the visual query explorer via direct manipulations, or a combination of both. Informal testing with doctors has shown positive feedback.\n",
      "=============================\n",
      "6D hands: markerless hand-tracking for computer aided design\n",
      "Computer Aided Design (CAD) typically involves tasks such as adjusting the camera perspective and assembling pieces in free space that require specifying 6 degrees of freedom (DOF). The standard approach is to factor these DOFs into 2D subspaces that are mapped to the x and y axes of a mouse. This metaphor is inherently modal because one needs to switch between subspaces, and disconnects the input space from the modeling space. In this paper, we propose a bimanual hand tracking system that provides physically-motivated 6-DOF control for 3D assembly. First, we discuss a set of principles that guide the design of our precise, easy-to-use, and comfortable-to-use system. Based on these guidelines, we describe a 3D input metaphor that supports constraint specification classically used in CAD software, is based on only a few simple gestures, lets users rest their elbows on their desk, and works alongside the keyboard and mouse. Our approach uses two consumer-grade webcams to observe the user's hands. We solve the pose estimation problem with efficient queries of a precomputed database that relates hand silhouettes to their 3D configuration. We demonstrate efficient 3D mechanical assembly of several CAD models using our hand-tracking system.\n",
      "=============================\n",
      "Extending 2D object arrangement with pressure-sensitive layering cues\n",
      "We demonstrate a pressure-sensitive depth sorting technique that extends standard two-dimensional (2D) manipulation techniques, particularly those used with multi-touch or multi-point controls. We combine this layering operation with a page-folding metaphor for more fluid interaction in applications requiring 2D sorting and layout.\n",
      "=============================\n",
      "TiltText: using tilt for text input to mobile phones\n",
      "TiltText, a new technique for entering text into a mobile phone is described. The standard 12-button text entry keypad of a mobile phone forces ambiguity when the 26- letter Roman alphabet is mapped in the traditional manner onto keys 2-9. The TiltText technique uses the orientation of the phone to resolve this ambiguity, by tilting the phone in one of four directions to choose which character on a particular key to enter. We first discuss implementation strategies, and then present the results of a controlled experiment comparing TiltText to MultiTap, the most common text entry technique. The experiment included 10 participants who each entered a total of 640 phrases of text chosen from a standard corpus, over a period of about five hours. The results show that text entry speed including correction for errors using TiltText was 23% faster than MultiTap by the end of the experiment, despite a higher error rate for TiltText. TiltText is thus amongst the fastest known language-independent techniques for entering text into mobile phones.\n",
      "=============================\n",
      "Crowd-powered interfaces\n",
      "We investigate crowd-powered interfaces: interfaces that embed human activity to support high-level conceptual activities such as writing, editing and question-answering. For example, a crowd-ppowered interface using paid crowd workers can compute a series of textual cuts and edits to a paragraph, then provide the user with an interface to condense his or her writing. We map out the design space of interfaces that depend on outsourced, friendsourced, and data mined resources, and report on designs for each of these. We discuss technical and motivational challenges inherent in human-powered interfaces.\n",
      "=============================\n",
      "Harpoon selection: efficient selections for ungrouped content on large pen-based surfaces\n",
      "In this paper, we present the Harpoon selection tool, a novel selection technique specifically designed for interactive whiteboards. The tool combines area cursors and crossing to perform complex selections amongst a large number of unsorted, ungrouped items. It is optimized for large-scale pen-based surfaces and works well in both dense and sparse surroundings. We describe a list of key features relevant to the design of the tool and provide a detailed description of both the mechanics as well as the feedback of the tool. The results of a user study are described and analyzed to confirm our design. The study shows that the Harpoon tool performs significantly faster than Tapping and Lassoing.\n",
      "=============================\n",
      "Platemate: crowdsourcing nutritional analysis from food photographs\n",
      "We introduce PlateMate, a system that allows users to take photos of their meals and receive estimates of food intake and composition. Accurate awareness of this information can help people monitor their progress towards dieting goals, but current methods for food logging via self-reporting, expert observation, or algorithmic analysis are time-consuming, expensive, or inaccurate. PlateMate crowdsources nutritional analysis from photographs using Amazon Mechanical Turk, automatically coordinating untrained workers to estimate a meal's calories, fat, carbohydrates, and protein. We present the Management framework for crowdsourcing complex tasks, which supports PlateMate's nutrition analysis workflow. Results of our evaluations show that PlateMate is nearly as accurate as a trained dietitian and easier to use for most users than traditional self-reporting.\n",
      "=============================\n",
      "DejaVu: integrated support for developing interactive camera-based programs\n",
      "The increasing popularity of interactive camera-based programs highlights the inadequacies of conventional IDEs in developing these programs given their distinctive attributes and workflows. We present DejaVu, an IDE enhancement that eases the development of these programs by enabling programmers to visually and continuously monitor program data in consistency with the frame-based pipeline of computer-vision programs; and to easily record, review, and reprocess temporal data to iteratively improve the processing of non-reproducible camera input. DejaVu was positively received by three experienced programmers of interactive camera-based programs in our preliminary user trial.\n",
      "=============================\n",
      "TalkBack: a conversational answering machine\n",
      "Current asynchronous voice messaging interfaces, like voicemail, fail to take advantage of our conversational skills. TalkBack restores conversational turn-taking to voicemail retrieval by dividing voice messages into smaller sections based on the most significant silent and filled pauses and pausing after each to record a response. The responses are composed into a reply, alternating with snippets of the original message for context. TalkBack is built into a digital picture frame; the recipient touches a picture of the caller to hear each segment of the message in turn. The minimal interface models synchronous interaction and facilitates asynchronous voice messaging. TalkBack can also present a voice-annotated slide show which it receives over the Internet.\n",
      "=============================\n",
      "Microtask programming: building software with a crowd\n",
      "Microtask crowdsourcing organizes complex work into workflows, decomposing large tasks into small, relatively independent microtasks. Applied to software development, this model might increase participation in open source software development by lowering the barriers to contribu-tion and dramatically decrease time to market by increasing the parallelism in development work. To explore this idea, we have developed an approach to decomposing programming work into microtasks. Work is coordinated through tracking changes to a graph of artifacts, generating appropriate microtasks and propagating change notifications to artifacts with dependencies. We have implemented our approach in CrowdCode, a cloud IDE for crowd development. To evaluate the feasibility of microtask programming, we performed a small study and found that a small crowd of 12 workers was able to successfully write 480 lines of code and 61 unit tests in 14.25 person-hours of time.\n",
      "=============================\n",
      "A colorful approach to text processing by example\n",
      "Text processing, tedious and error-prone even for programmers, remains one of the most alluring targets of Programming by Example. An examination of real-world text processing tasks found on help forums reveals that many such tasks, beyond simple string manipulation, involve latent hierarchical structures. We present STEPS, a programming system for processing structured and semi-structured text by example. STEPS users create and manipulate hierarchical structure by example. In a between-subject user study on fourteen computer scientists, STEPS compares favorably to traditional programming.\n",
      "=============================\n",
      "Direct manipulation of programming: How should we design interfaces?\n",
      "ion. There is a strong tendency to apply these notions to any problem area that we tackle, even if they are not appropriate. Our programming notations tend to be linear in nature, there is one thread of control through the entire program. In user interfaces there are typically multiple threads of control. This is very hard to describe in a textual notation. Many claims have been made for the textual description of user interfaces. Textual notations are more powerful than the existing graphical notations (a general programming language can do anything that is computable). There are user interfaces that can be described by programs that cannot be produced by existing graphical techniques. But, do we need this generality, and will it be effectively used? The problem with using programming languages is that vast amounts of code must be written in order to produce the user interface. Thousands of lines of code may be required to produce a user interface that could be constructed in 30 minutes with a graphical tool. Recently rule based techniques have been proposed as a way of designing user interfaces. These techniques can be characterized as a textual description with a high level of abstraction. This is a very nice approach (otherwise 1 wouldn’t be using it), since it allows us to capture some of the expertise of good designers. We can develop rules for the selection of interaction techniques, details of their graphicalion. This is a very nice approach (otherwise 1 wouldn’t be using it), since it allows us to capture some of the expertise of good designers. We can develop rules for the selection of interaction techniques, details of their graphical presentation (fonts and line styles, for example), and their placement on the screen. This removes control from the user interface designer, since he or she has no control over the rules that fire in the design of the user interface. It could be argued that the designer could edit the rule base in order to have some control over the design process, but this is not a viable solution. A realistic rule base will have a large number of rules (thousands?) and there will be subtle interactions between these rules. As a result., it may be very difficult to modify the rule base without introducing undesirable side\n",
      "=============================\n",
      "Sensing Tablet Grasp + Micro-mobility for Active Reading\n",
      "The orientation and repositioning of physical artefacts (such as paper documents) to afford shared viewing of content, or to steer the attention of others to specific details, is known as micro-mobility. But the role of grasp in micro-mobility has rarely been considered, much less sensed by devices. We therefore employ capacitive grip sensing and inertial motion to explore the design space of combined grasp + micro-mobility by considering three classes of technique in the context of active reading. Single user, single device techniques support grip-influenced behaviors such as bookmarking a page with a finger, but combine this with physical embodiment to allow flipping back to a previous location. Multiple user, single device techniques, such as passing a tablet to another user or working side-by-side on a single device, add fresh nuances of expression to co-located collaboration. And single user, multiple device techniques afford facile cross-referencing of content across devices. Founded on observations of grasp and micro-mobility, these techniques open up new possibilities for both individual and collaborative interaction with electronic documents.\n",
      "=============================\n",
      "Pebbles: an interactive configuration tool for indoor robot navigation\n",
      "This study presents an interactive configuration tool that assists non-expert users to design specific navigation route for mobile robot in an indoor environment. The user places small active markers, called pebbles, on the floor along the desired route in order to guide the robot to the destination. The active markers establish a navigation network by communicating each other with IR beacon and the robot follows the markers to reach the designated goal. During the installation, a user can get effective feedback from LED indicators and voice prompts, so that the user can immediately understand if the navigation route is appropriately configured as expected. With this tool a novice user may easily customize a mobile robot for various indoor tasks.\n",
      "=============================\n",
      "Jamming user interfaces: programmable particle stiffness and sensing for malleable and shape-changing devices\n",
      "Malleable and organic user interfaces have the potential to enable radically new forms of interactions and expressiveness through flexible, free-form and computationally controlled shapes and displays. This work, specifically focuses on particle jamming as a simple, effective method for flexible, shape-changing user interfaces where programmatic control of material stiffness enables haptic feedback, deformation, tunable affordances and control gain. We introduce a compact, low-power pneumatic jamming system suitable for mobile devices, and a new hydraulic-based technique with fast, silent actuation and optical shape sensing. We enable jamming structures to sense input and function as interaction devices through two contributed methods for high-resolution shape sensing using: 1) index-matched particles and fluids, and 2) capacitive and electric field sensing. We explore the design space of malleable and organic user interfaces enabled by jamming through four motivational prototypes that highlight jamming's potential in HCI, including applications for tabletops, tablets and for portable shape-changing mobile devices.\n",
      "=============================\n",
      "Reducing the storage requirements of constraint dataflow graphs\n",
      "Most one-way constraint solvers use directed dataflow graphs to represent the dependencies among variables in a constraint. Unfortunately, dataflow graphs require a great deal of storage. These storage costs can help push a large application into virtual memory, thus significantly degrading interactive performance. Reducing the storage costs of dataflow graphs is therefore an important goal in constraint research. This paper describes a study that makes two contributions to solving this problem:\n",
      "=============================\n",
      "Immersion in desktop virtual reality\n",
      "This paper explores techniques for evaluating and improving immersion in Desktop Virtual Reality (VR). Three experiments are reported which extend findings on immersion in VR reported by Pausch et al. [9]. In the current experiments, a visual search paradigm was used to examine navigation in Desktop VR both with and without navigational aids. Pausch et al. found that non-head tracked users took significantly longer than predicted when the search target was absent, which was interpreted as indicative of a loss of sense of immersion. Our first experiment extended the Pausch et al. experiment to a desktop display. Our findings differ in that search times matched prediction when the target was absent, indicating that the Pausch et al. study does not transfer to Desktop VR. In the second and third experiments, our visual search task was performed while navigating a set of 3D hallways. We introduce a new navigation aid called Peripheral L.enses, intended to provide simulated peripheral vision. Informal studies suggested that Peripheral Lenses decrease search time, indicating an enhanced sense of immersion in Desktop VR. However, formal studies contradict that, demonstrating the importance of formal usability studies in the development of user interface software. We also gained evidence that visual attention findings transfer to Desktop VR.\n",
      "=============================\n",
      "Zliding: fluid zooming and sliding for high precision parameter manipulation\n",
      "High precision parameter manipulation tasks typically require adjustment of the scale of manipulation in addition to the parameter itself. This paper introduces the notion of Zoom Sliding, or Zliding, for fluid integrated manipulation of scale (zooming) via pressure input while parameter manipulation within that scale is achieved via x-y cursor movement (sliding). We also present the Zlider (Figure 1), a widget that instantiates the Zliding concept. We experimentally evaluate three different input techniques for use with the Zlider in conjunction with a stylus for x-y cursor positioning, in a high accuracy zoom and select task. Our results marginally favor the stylus with integrated isometric pressure sensing tip over bimanual techniques which separate zooming and sliding controls over the two hands. We discuss the implications of our results and present further designs that make use of Zliding.\n",
      "=============================\n",
      "Zoetrope: interacting with the ephemeral web\n",
      "The Web is ephemeral. Pages change frequently, and it is nearly impossible to find data or follow a link after the underlying page evolves. We present Zoetrope, a system that enables interaction with the historicalWeb (pages, links, and embedded data) that would otherwise be lost to time. Using a number of novel interactions, the temporal Web can be manipulated, queried, and analyzed from the context of familar pages. Zoetrope is based on a set of operators for manipulating content streams. We describe these primitives and the associated indexing strategies for handling temporal Web data. They form the basis of Zoetrope and enable our construction of new temporal interactions and visualizations.\n",
      "=============================\n",
      "Projectron mapping: the exercise and extension of augmented workspaces for learning electronic modeling through projection mapping\n",
      "There has been research using software simulations to support the learning of electronic modeling by beginners. There have also been systems to extend workspaces and support electronic modeling on tabletop interfaces. However, in the case of software-based circuit operation, as it is not possible to operate the actual elements, the feeling of actually moving the elements is lacking. For this reason, we are proposing a system that extends the sense of reality in software simulators through the use of projection mapping. This will make it possible to actually give the impression of moving the elements by using a software simulator, and to achieve both high speed and a sense of reality through trial and error.\n",
      "=============================\n",
      "Detecting shape deformation of soft objects using directional photoreflectivity measurement\n",
      "We present the FuwaFuwa sensor module, a round, hand-size, wireless device for measuring the shape deformations of soft objects such as cushions and plush toys. It can be embedded in typical soft objects in the household without complex installation procedures and without spoiling the softness of the object because it requires no physical connection. Six LEDs in the module emit IR light in six orthogonal directions, and six corresponding photosensors measure the reflected light energy. One can easily convert almost any soft object into a touch-input device that can detect both touch position and surface displacement by embedding multiple FuwaFuwa sensor modules in the object. A variety of example applications illustrate the utility of the FuwaFuwa sensor module. An evaluation of the proposed deformation measurement technique confirms its effectiveness.\n",
      "=============================\n",
      "Digital flavor interface\n",
      "This demo presents a unique technology to enable digital simulation of flavors. The Digital Flavor Interface, a digital control system, is developed to stimulate taste (using electrical and thermal stimulation methodologies on the human tongue) and smell (using a controlled scent emitting mechanism) senses simultaneously, thus simulating different virtual flavors. A preliminary user experiment was conducted to investigate the effectiveness of this approach with five distinct flavor stimuli. The experimental results suggested that the users' were effectively able to identify different flavors such as minty, spicy, and lemon flavor. In summary, our work demonstrates a novel controllable digital flavor instrument, which may be utilized in interactive computer systems for rendering virtual flavors.\n",
      "=============================\n",
      "CueTIP: a mixed-initiative interface for correcting handwriting errors\n",
      "With advances in pen-based computing devices, handwriting has become an increasingly popular input modality. Researchers have put considerable effort into building intelligent recognition systems that can translate handwriting to text with increasing accuracy. However, handwritten input is inherently ambiguous, and these systems will always make errors. Unfortunately, work on error recovery mechanisms has mainly focused on interface innovations that allow users to manually transform the erroneous recognition result into the intended one. In our work, we propose a mixed-initiative approach to error correction. We describe CueTIP, a novel correction interface that takes advantage of the recognizer to continually evolve its results using the additional information from user corrections. This significantly reduces the number of actions required to reach the intended result. We present a user study showing that CueTIP is more efficient and better preferred for correcting handwriting recognition errors. Grounded in the discussion of CueTIP, we also present design principles that may be applied to mixed-initiative correction interfaces in other domains.\n",
      "=============================\n",
      "Air+touch: interweaving touch & in-air gestures\n",
      "We present Air+Touch, a new class of interactions that interweave touch events with in-air gestures, offering a unified input modality with expressiveness greater than each input modality alone. We demonstrate how air and touch are highly complementary: touch is used to designate targets and segment in-air gestures, while in-air gestures add expressivity to touch events. For example, a user can draw a circle in the air and tap to trigger a context menu, do a finger 'high jump' between two touches to select a region of text, or drag and in-air 'pigtail' to copy text to the clipboard. Through an observational study, we devised a basic taxonomy of Air+Touch interactions, based on whether the in-air component occurs before, between or after touches. To illustrate the potential of our approach, we built four applications that showcase seven exemplar Air+Touch interactions we created.\n",
      "=============================\n",
      "Redprint: integrating API specific \"instant example\" and \"instant documentation\" display interface in IDEs\n",
      "Software libraries for most of the modern programming languages are numerous, large and complex. Remembering the syntax and usage of APIs is a difficult task for not just novices but also expert programmers. IDEs (Integrated Development Environment) provide capabilities like autocomplete and intellisense to assist programmers; however, programmers still need to visit search engines like Google to find API (Application Program Interface) documentation and samples. This paper evaluates Redprint - a browser based development environment for PHP that integrates API specific \"Instant Example\" and \"Instant Documentation\" display interfaces. A comparative laboratory study shows that integrating API specific \"Instant Example\" and \"Instant Documentation\" display interfaces into a development environment significantly reduces the cost of searching and thus significantly reduces the time to develop software.\n",
      "=============================\n",
      "Blui: low-cost localized blowable user interfaces\n",
      "We describe a unique form of hands-free interaction that can be implemented on most commodity computing platforms. Our approach supports blowing at a laptop or computer screen to directly control certain interactive applications. Localization estimates are produced in real-time to determine where on the screen the person is blowing. Our approach relies solely on a single microphone, such as those already embedded in a standard laptop or one placed near a computer monitor, which makes our approach very cost-effective and easy-to-deploy. We show example interaction techniques that leverage this approach.\n",
      "=============================\n",
      "Bimanual and unimanual image alignment: an evaluation of mouse-based techniques\n",
      "We present an evaluation of three mouse-based techniques for aligning digital images. We investigate the physical image alignment task and discuss the implications for interacting with virtual images. In a formal evaluation we show that a symmetric bimanual technique outperforms an asymmetric bimanual technique which in turn outperforms a unimanual technique. We show that even after mode switching times are removed, the symmetric technique outperforms the single mouse technique. Subjects also exhibited more parallel interaction using the symmetric technique than when using the asymmetric technique.\n",
      "=============================\n",
      "CommunityCommands: command recommendations for software applications\n",
      "We explore the use of modern recommender system technology to address the problem of learning software applications. Before describing our new command recommender system, we first define relevant design considerations. We then discuss a 3 month user study we conducted with professional users to evaluate our algorithms which generated customized recommendations for each user. Analysis shows that our item-based collaborative filtering algorithm generates 2.1 times as many good suggestions as existing techniques. In addition we present a prototype user interface to ambiently present command recommendations to users, which has received promising initial user feedback.\n",
      "=============================\n",
      "Global beautification of layouts with interactive ambiguity resolution\n",
      "Automatic global beautification methods have been proposed for sketch-based interfaces, but they can lead to undesired results due to ambiguity in the user's input. To facilitate ambiguity resolution in layout beautification, we present a novel user interface for visualizing and editing inferred relationships. First, our interface provides a preview of the beautified layout with inferred constraints, without directly modifying the input layout. In this way, the user can easily keep refining beautification results by interactively repositioning and/or resizing elements in the input layout. Second, we present a gestural interface for editing automatically inferred constraints by directly interacting with the visualized constraints via simple gestures. Our efficient implementation of the beautification system provides the user instant feedback. Our user studies validate that our tool is capable of creating, editing and refining layouts of graphic elements and is significantly faster than the standard snap-dragging and command-based alignment tools.\n",
      "=============================\n",
      "Pen + touch = new tools\n",
      "We describe techniques for direct pen+touch input. We observe people's manual behaviors with physical paper and notebooks. These serve as the foundation for a prototype Microsoft Surface application, centered on note-taking and scrapbooking of materials. Based on our explorations we advocate a division of labor between pen and touch: the pen writes, touch manipulates, and the combination of pen + touch yields new tools. This articulates how our system interprets unimodal pen, unimodal touch, and multimodal pen+touch inputs, respectively. For example, the user can hold a photo and drag off with the pen to create and place a copy; hold a photo and cross it in a freeform path with the pen to slice it in two; or hold selected photos and tap one with the pen to staple them all together. Touch thus unifies object selection with mode switching of the pen, while the muscular tension of holding touch serves as the \"glue\" that phrases together all the inputs into a unitary multimodal gesture. This helps the UI designer to avoid encumbrances such as physical buttons, persistent modes, or widgets that detract from the user's focus on the workspace.\n",
      "=============================\n",
      "Active bone-conducted sound sensing for wearable interfaces\n",
      "In this paper, we propose a wearable sensor system that measures an angle of an elbow and position tapped by finger using bone-conducted sound. Our system consists of two microphones and a speaker, and they are attached on forearm. A novelty of this paper is to use active sensing for measuring an angle of an elbow. In this paper, active sensing means to emit sounds to a bone, and a microphone receives the sounds reflected at the elbow. The reflection of sound depends on the angle of elbow. Since frequencies of bone-conducted sound by tapping and from the speaker are different, these proposed techniques can be used simultaneously. We confirmed the feasibility of proposed system through experiments.\n",
      "=============================\n",
      "World-stabilized annotations and virtual scene navigation for remote collaboration\n",
      "We present a system that supports an augmented shared visual space for live mobile remote collaboration on physical tasks. The remote user can explore the scene independently of the local user's current camera position and can communicate via spatial annotations that are immediately visible to the local user in augmented reality. Our system operates on off-the-shelf hardware and uses real-time visual tracking and modeling, thus not requiring any preparation or instrumentation of the environment. It creates a synergy between video conferencing and remote scene exploration under a unique coherent interface. To evaluate the collaboration with our system, we conducted an extensive outdoor user study with 60 participants comparing our system with two baseline interfaces. Our results indicate an overwhelming user preference (80%) for our system, a high level of usability, as well as performance benefits compared with one of the two baselines.\n",
      "=============================\n",
      "A survey of design issues in spatial input\n",
      "We present a survey of design issues for developing effective free-space three-dimensional (3D) user interfaces. Our survey is based upon previous work in 3D interaction, our experience in developing free-space interfaces, and our informal observations of test users. We illustrate our design issues using examples drawn from instances of 3D interfaces.For example, our first issue suggests that users have difficulty understanding three-dimensional space. We offer a set of strategies which may help users to better perceive a 3D virtual environment, including the use of spatial references, relative gesture, two-handed interaction, multisensory feedback, physical constraints, and head tracking. We describe interfaces which employ these strategies.Our major contribution is the synthesis of many scattered results, observations, and examples into a common framework. This framework should serve as a guide to researchers or systems builders who may not be familiar with design issues in spatial input. Where appropriate, we also try to identify areas in free-space 3D interaction which we see as likely candidates for additional research.An extended and annotated version of the references list for this paper is available on-line through mosaic at address http://uvacs.cs.virginia.edu/~kph2q/.\n",
      "=============================\n",
      "Calibration games: making calibration tasks enjoyable by adding motivating game elements\n",
      "Interactive systems often require calibration to ensure that input and output are optimally configured. Without calibration, user performance can degrade (e.g., if an input device is not adjusted for the user's abilities), errors can increase (e.g., if color spaces are not matched), and some interactions may not be possible (e.g., use of an eye tracker). The value of calibration is often lost, however, because many calibration processes are tedious and unenjoyable, and many users avoid them altogether. To address this problem, we propose calibration games that gather calibration data in an engaging and entertaining manner. To facilitate the creation of calibration games, we present design guidelines that map common types of calibration to core tasks, and then to well-known game mechanics. To evaluate the approach, we developed three calibration games and compared them to standard procedures. Users found the game versions significantly more enjoyable than regular calibration procedures, without compromising the quality of the data. Calibration games are a novel way to motivate users to carry out calibrations, thereby improving the performance and accuracy of many human-computer systems.\n",
      "=============================\n",
      "The metaDESK: models and prototypes for tangible user interfaces\n",
      "The metaDESK is a user interface platform demonstrating new interaction techniques we call \"tangible user inter- faces.\" We explore the physical instantiation of interface elements from the graphical user interface paradigm, giving physical form to windows, icons, handles, menus, and controls. The design and implementation of the metaDESK display, sensor, and software architectures is discussed. A prototype application driving an interaction with geographi- cal space, Tangible Geospace, is presented to demonstrate these concepts.\n",
      "=============================\n",
      "Controlling widgets with one power-up button\n",
      "The Power-up Button is a physical button that combines pressure and proximity sensing to enable gestural interaction with one thumb. Combined with a gesture recognizer that takes the hand's anatomy into account, the Power-up Button can recognize six different mid-air gestures performed on the side of a mobile device. This gives it, for instance, enough expressive power to provide full one-handed control of interface widgets displayed on screen. This technology can complement touch input, and can be particularly useful when interacting eyes-free. It also opens up a larger design space for widget organization on screen: the button enables a more compact layout of interface components than what touch input alone would allow. This can be useful when, e.g., filling the numerous fields of a long Web form, or for very small devices.\n",
      "=============================\n",
      "Who cares?: reflecting who is reading what on distributed community bulletin boards\n",
      "In this paper, we describe the YeTi information sharing system that has been designed to foster community building through informal digital content sharing. The YeTi system is a general information parsing, hosting and distribution infrastructure, with interfaces designed for individual and public content reading. In this paper we describe the YeTi public display interface, with a particular focus on tools we have designed to provide lightweight awareness of others' interactions with posted content. Our tools augment content with metadata that reflect people's reading of content - captured video clips of who's reading and interacting with content, tools to allow people to leave explicit freehand annotations about content, and a visualization of the content access history to show when content is interacted with. Results from an initial evaluation are presented and discussed.\n",
      "=============================\n",
      "Simple vs. compound mark hierarchical marking menus\n",
      "We present a variant of hierarchical marking menus where items are selected using a series of inflection-free simple marks, rather than the single \"zig-zag\" compound mark used in the traditional design. Theoretical analysis indicates that this simple mark approach has the potential to significantly increase the number of items in a marking menu that can be selected efficiently and accurately. A user experiment is presented that compares the simple and compound mark techniques. Results show that the simple mark technique allows for significantly more accurate and faster menu selections overall, but most importantly also in menus with a large number of items where performance of the compound mark technique is particularly poor. The simple mark technique also requires significantly less physical input space to perform the selections, making it particularly suitable for small footprint pen-based input devices. Visual design alternatives are also discussed.\n",
      "=============================\n",
      "Bringing everyday applications to interactive surfaces\n",
      "This paper presents ongoing work that intends to simplify the introduction of everyday applications to interactive tabletops. SLAP Widgets bring tangible general-purpose widgets to tabletops while providing the flexibility of on-screen controls. Madgets maintain consistency between physical controls and their digital state. BendDesk represents our vision of a multi-touch enabled office environment. Our pattern language captures knowledge for the design of interactive tabletops. For each project, we describe its technical background, present the current state of research, and discuss future work.\n",
      "=============================\n",
      "Augmenting interactive tables with mice & keyboards\n",
      "This note examines the role traditional input devices can play in surface computing. Mice and keyboards can enhance tabletop technologies since they support high fidelity input, facilitate interaction with distant objects, and serve as a proxy for user identity and position. Interactive tabletops, in turn, can enhance the functionality of traditional input devices: they provide spatial sensing, augment devices with co-located visual content, and support connections among a plurality of devices. We introduce eight interaction techniques for a table with mice and keyboards, and we discuss the design space of such interactions.\n",
      "=============================\n",
      "Interpreting strokes on paper with a mobile assistant\n",
      "Digital pen technology has allowed for the easy transfer of pen data from paper to the computer. However, linking handwritten content with the digital world remains a hard problem as it requires the translation of unstructured and highly personal vocabularies into structured ones that computers can easily understand and process. Automatic recognition can help to this direction, but as it is not always reliable, solutions require the active cooperation between users and recognition algorithms. This work examines the use of portable touch-screen devices in connection with pen and paper to help users direct and refine the interpretation of their strokes on paper. We explore four techniques of bi-manual interaction that combine touch and pen-writing, where user attention is divided between the original strokes on paper and their interpretation by the electronic device. We demonstrate the techniques through a mobile interface for writing music that complements the automatic recognition with interactive user-driven interpretation. An experiment evaluates the four techniques and provides insights about their strengths and limitations.\n",
      "=============================\n",
      "ViziCal: accurate energy expenditure prediction for playing exergames\n",
      "In recent years, exercise games have been criticized for not being able to engage their players into levels of physical activity that are high enough to yield health benefits. A major challenge in the design of exergames, however, is that it is difficult to assess the amount of physical activity an exergame yields due to limitations of existing techniques to assess energy expenditure of exergaming activities. With recent advances in commercial depth sensing technology to accurately track players' motions in 3D, we present a technique called Vizical that uses a non-linear regression approach to accurately predict energy expenditure in real-time. Vizical may allow for creating exergames that can report energy expenditure while playing, and whose intensity can be adjusted in real-time to stimulate larger health benefits.\n",
      "=============================\n",
      "User guided audio selection from complex sound mixtures\n",
      "In this paper we present a novel interface for selecting sounds in audio mixtures. Traditional interfaces in audio editors provide a graphical representation of sounds which is either a waveform, or some variation of a time/frequency transform. Although with these representations a user might be able to visually identify elements of sounds in a mixture, they do not facilitate object-specific editing (e.g. selecting only the voice of a singer in a song). This interface uses audio guidance from a user in order to select a target sound within a mixture. The user is asked to vocalize (or otherwise sonically represent) the desired target sound, and an automatic process identifies and isolates the elements of the mixture that best relate to the user's input. This way of pointing to specific parts of an audio stream allows a user to perform audio selections which would have been infeasible otherwise.\n",
      "=============================\n",
      "The actuated workbench: computer-controlled actuation in tabletop tangible interfaces\n",
      "The Actuated Workbench is a device that uses magnetic forces to move objects on a table in two dimensions. It is intended for use with existing tabletop tangible interfaces, providing an additional feedback loop for computer output, and helping to resolve inconsistencies that otherwise arise from the computer's inability to move objects on the table. We describe the Actuated Workbench in detail as an enabling technology, and then propose several applications in which this technology could be useful.\n",
      "=============================\n",
      "Rapid construction of functioning physical interfaces from cardboard, thumbtacks, tin foil and masking tape\n",
      "Rapid, early, but rough system prototypes are becoming a standard and valued part of the user interface design process. Pen, paper, and tools like Flash™ and Director™ are well suited to creating such prototypes. However, in the case of physical forms with embedded technology, there is a lack of tools for developing rapid, early prototypes. Instead, the process tends to be fragmented into prototypes exploring forms that look like the intended product or explorations of functioning interactions that work like the intended product - bringing these aspects together into full design concepts only later in the design process. To help alleviate this problem, we present a simple tool for very rapidly creating functioning, rough physical prototypes early in the design process - supporting what amounts to interactive physical sketching. Our tool allows a designer to combine exploration of form and interactive function, using objects constructed from materials such as thumbtacks, foil, cardboard and masking tape, enhanced with a small electronic sensor board. By means of a simple and fluid tool for delivering events to \"screen clippings,\" these physical sketches can then be easily connected to any existing (or new) program running on a PC to provide real or Wizard of Oz supported functionality.\n",
      "=============================\n",
      "System components for embedded information retrieval from multiple disparate information sources\n",
      "Current information retrieval interfaces only address a small pant of the reality of rich interactions amongst user, task, and information sources. We view information gathering as an interactive, iterative activity involving multiple disparate information sources and embedded in the context of broader processes of information use. We have developed two key system components that enable information workspaces that adhere to this reformulation of information retrieval. The tlrst is a design for a user/system interaction model for retrieval from multiple, disparate information sources. The second is a repository modeling system, called Repo, that represents meta-information about different information repositories in a manner that supports system operation as well as provides a direct information resource to the user. To test these embodied the interaction system called Labrador.\n",
      "=============================\n",
      "An experimental evaluation of transparent user interface tools and information content\n",
      "The central research issue addressed by this paper is how we can design computer interfaces that better support human attention and better maintain the fluency of work. To accomplish this we propose to use semi-transparent user interface objects. This paper reports on an experimental evaluation which provides both valuable insights into design parameters and suggests a systematic evaluation methodology. For this study, we used a variablytransparent tool palette superimposed over different background content, combining text, wire-frame or line art images, and solid images. The experiment explores the issue of focused attention and interference, by varying both visual distinctiveness and levels of transparency.\n",
      "=============================\n",
      "Kinetic tiles: modular construction units for interactive kinetic surfaces\n",
      "We propose and demonstrate Kinetic Tiles, modular con-struction units for Interactive Kinetic Surfaces (IKSs). We aimed to design Kinetic Tiles to be accessible and available so that users can construct IKSs easily and rapidly. The components of Kinetic Tiles are inexpensive and easily available. In addition, the use of magnetic force enables the separation of the surface material and actuators so that users only interact with the tile modules as if constructing a tile mosaic. Kinetic Tiles can be utilized as a new design and architectural material that allows the surfaces of everyday objects and spaces to convey ambient and pleasurable kinetic expressions.\n",
      "=============================\n",
      "Demonstration of a reading coach that listens\n",
      "Project LISTEN stands for “Literacy Innovation that Speech Technology ENables.” We will demonstrate a prototype automated reading coach that displays text on a screen, listens to a child read it aloud, and helps where needed. We have tested successive prototypes of the coach on several dozen second gmders. [1] reports implementation details and evaluation results. Here we summarize its functionality, the issues it raises in human-computer interaction, and how it addresses them. We are redesigning the coach based on our experience, and will demonstrate its successor at UIST ’95.\n",
      "=============================\n",
      "Prefab layers and prefab annotations: extensible pixel-based interpretation of graphical interfaces\n",
      "Pixel-based methods have the potential to fundamentally change how we build graphical interfaces, but remain difficult to implement. We introduce a new toolkit for pixel based enhancements, focused on two areas of support. Prefab Layers helps developers write interpretation logic that can be composed, reused, and shared to manage the multi-faceted nature of pixel-based interpretation. Prefab Annotations supports robustly annotating interface elements with metadata needed to enable runtime enhancements. Together, these help developers overcome subtle but critical dependencies between code and data. We validate our toolkit with (1) demonstrative applications and (2) a lab study that compares how developers build an enhancement using our toolkit versus state of the art methods. Our toolkit addresses core challenges faced by developers when building pixel based enhancements, potentially opening up pixel based systems to broader adoption.\n",
      "=============================\n",
      "TapSongs: tapping rhythm-based passwords on a single binary sensor\n",
      "TapSongs are presented, which enable user authentication on a single \"binary\" sensor (e.g., button) by matching the rhythm of tap down/up events to a jingle timing model created by the user. We describe our matching algorithm, which employs absolute match criteria and learns from successful logins. We also present a study of 10 subjects showing that after they created their own TapSong models from 12 examples (< 2 minutes), their subsequent login attempts were 83.2% successful. Furthermore, aural and visual eavesdropping of the experimenter's logins resulted in only 10.7% successful imposter logins by subjects. Even when subjects heard the target jingles played by a synthesized piano, they were only 19.4% successful logging in as imposters. These results are attributable to subtle but reliable individual differences in people's tapping, which are supported by prior findings in music psychology.\n",
      "=============================\n",
      "Generating emotionally relevant musical scores for audio stories\n",
      "Highly-produced audio stories often include musical scores that reflect the emotions of the speech. Yet, creating effective musical scores requires deep expertise in sound production and is time-consuming even for experts. We present a system and algorithm for re-sequencing music tracks to generate emotionally relevant music scores for audio stories. The user provides a speech track and music tracks and our system gathers emotion labels on the speech through hand-labeling, crowdsourcing, and automatic methods. We develop a constraint-based dynamic programming algorithm that uses these emotion labels to generate emotionally relevant musical scores. We demonstrate the effectiveness of our algorithm by generating 20 musical scores for audio stories and showing that crowd workers rank their overall quality significantly higher than stories without music.\n",
      "=============================\n",
      "MAI painting brush++: augmenting the feeling of painting with new visual and tactile feedback mechanisms\n",
      "We have developed a mixed-reality (MR) painting system named the MR-based Artistic Interactive (MAI) Painting Expert and MAI Painting Brush which simulates the painting of physical objects in the real world. In this paper, we describe how the MAI Painting Brush was upgraded to the \"MAI Painting Brush++,\" enabling virtual painting on virtual objects. The improved system has a visual and tactile feedback mechanism that simulates the effect of touch when used on a virtual painting target. This is achieved using deformation of the brush tip and reaction force on the hand.\n",
      "=============================\n",
      "SqueezeBlock: using virtual springs in mobile devices for eyes-free interaction\n",
      "Haptic feedback provides an additional interaction channel when auditory and visual feedback may not be appropriate. We present a novel haptic feedback system that changes its elasticity to convey information for eyes-free interaction. SqueezeBlock is an electro-mechanical system that can realize a virtual spring having a programmatically controlled spring constant. It also allows for additional haptic modalities by altering the Hooke's Law linear-elastic force- displacement equation, such as non-linear springs, size changes, and spring length (range of motion) variations. This ability to program arbitrarily spring constants also allows for \"click\" and button-like feedback. We present several potential applications along with results from a study showing how well participants can distinguish between several levels of stiffness, size, and range of motion. We conclude with implications for interaction design.\n",
      "=============================\n",
      "SideSwipe: detecting in-air gestures around mobile devices using actual GSM signal\n",
      "Current smartphone inputs are limited to physical buttons, touchscreens, cameras or built-in sensors. These approaches either require a dedicated surface or line-of-sight for interaction. We introduce SideSwipe, a novel system that enables in-air gestures both above and around a mobile device. Our system leverages the actual GSM signal to detect hand gestures around the device. We developed an algorithm to convert the discrete and bursty GSM pulses to a continuous wave that can be used for gesture recognition. Specifically, when a user waves their hand near the phone, the hand movement disturbs the signal propagation between the phone's transmitter and added receiving antennas. Our system captures this variation and uses it for gesture recognition. To evaluate our system, we conduct a study with 10 participants and present robust gesture recognition with an average accuracy of 87.2% across 14 hand gestures.\n",
      "=============================\n",
      "Eddi: interactive topic-based browsing of social status streams\n",
      "Twitter streams are on overload: active users receive hundreds of items per day, and existing interfaces force us to march through a chronologically-ordered morass to find tweets of interest. We present an approach to organizing a user's own feed into coherently clustered trending topics for more directed exploration. Our Twitter client, called Eddi, groups tweets in a user's feed into topics mentioned explicitly or implicitly, which users can then browse for items of interest. To implement this topic clustering, we have developed a novel algorithm for discovering topics in short status updates powered by linguistic syntactic transformation and callouts to a search engine. An algorithm evaluation reveals that search engine callouts outperform other approaches when they employ simple syntactic transformation and backoff strategies. Active Twitter users evaluated Eddi and found it to be a more efficient and enjoyable way to browse an overwhelming status update feed than the standard chronological interface.\n",
      "=============================\n",
      "PhantomPen: virtualization of pen head for digital drawing free from pen occlusion & visual parallax\n",
      "We present PhantomPen, a direct pen input device whose pen head is virtualized onto the tablet display surface and visually connected to a graspable pen barrel in order to achieve digital drawing free from pen occlusion and visual parallax. As the pen barrel approaches the display, the virtual pen head smoothly appears as if the rendered virtual pen head and the physical pen barrel are in unity. The virtual pen head provides visual feedback by changing its virtual form according to pen type, color, and thickness while the physical pen tip, hidden in the user's sight, provides tactile feedback. Three experiments were carefully designed based on an analysis of drawings by design professionals and observations of design drawing classes. With these experiments that simulate natural drawing we proved significant performance advantages of PhantomPen. PhantomPen was at least as usable as the normal stylus in basic line drawing, and was 17 % faster in focus region drawing (26% faster in extreme focus region drawing). PhantomPen also reduced error rate by 40 % in a typical drawing setup where users have to manage a complex combination of pen and stroke properties.\n",
      "=============================\n",
      "The kinetic typography engine: an extensible system for animating expressive text\n",
      "Kinetic typography --- text that uses movement or other temporal change --- has recently emerged as a new form of communication. As we hope to illustrate in this paper, kinetic typography can be seen as bringing some of the expressive power of film --- such as its ability to convey emotion, portray compelling characters, and visually direct attention --- to the strong communicative properties of text. Although kinetic typography offers substantial promise for expressive communications, it has not been widely exploited outside a few limited application areas (most notably in TV advertising). One of the reasons for this has been the lack of tools directly supporting it, and the accompanying difficulty in creating dynamic text. This paper presents a first step in remedying this situation --- an extensible and robust system for animating text in a wide variety of forms. By supporting an appropriate set of carefully factored abstractions, this engine provides a relatively small set of components that can be plugged together to create a wide range of different expressions. It provides new techniques for automating effects used in traditional cartoon animation, and provides specific support for typographic manipulations.\n",
      "=============================\n",
      "Conducting a realistic electronic orchestra\n",
      "Personal Orchestra is the first system to let users conduct an actual audio and video recording of an orchestra, using an infrared baton to control tempo, volume, and instrument sections. A gesture recognition algorithm interprets user input, and a novel high-fidelity playback algorithm renders audio and video data at variable speed without time-stretching artifacts. The system is installed as a public exhibit in the HOUSE OF MUSIC VIENNA.\n",
      "=============================\n",
      "Artistic resizing: a technique for rich scale-sensitive vector graphics\n",
      "When involved in the visual design of graphical user interfaces, graphic designers can do more than providing static graphics for programmers to incorporate into applications. We describe a technique that allows them to provide examples of graphical objects at various key sizes using their usual drawing tool, then let the system interpolate their resizing behavior. We relate this technique to current practices of graphic designers, provide examples of its use and describe the underlying inference algorithm. We show how the mathematical properties of the algorithm allows the system to be predictable and explain how it can be combined with more traditional layout mechanisms.\n",
      "=============================\n",
      "HyperSource: bridging the gap between source and code-related web sites\n",
      "Programmers frequently use the Web while writing code: they search for libraries, code examples, tutorials, documentation, and engage in discussions on Q&A forums. This link between code and visited Web pages largely remains implicit today. Connecting source code and (selective) browsing history can help programmers maintain context, reduce the cost of Web content re-retrieval, and enhance understanding when code is shared. This paper introduces HyperSource, an IDE augmentation that associates browsing histories with source code edits. HyperSource comprises a browser extension that logs visited pages; a novel source document format that maps visited pages to individual characters; and a user interface that enables interaction with these histories.\n",
      "=============================\n",
      "UltraHaptics: multi-point mid-air haptic feedback for touch surfaces\n",
      "We introduce UltraHaptics, a system designed to provide multi-point haptic feedback above an interactive surface. UltraHaptics employs focused ultrasound to project discrete points of haptic feedback through the display and directly on to users' unadorned hands. We investigate the desirable properties of an acoustically transparent display and demonstrate that the system is capable of creating multiple localised points of feedback in mid-air. Through psychophysical experiments we show that feedback points with different tactile properties can be identified at smaller separations. We also show that users are able to distinguish between different vibration frequencies of non-contact points with training. Finally, we explore a number of exciting new interaction possibilities that UltraHaptics provides.\n",
      "=============================\n",
      "SnipMatch: using source code context to enhance snippet retrieval and parameterization\n",
      "Programmers routinely use source code snippets to increase their productivity. However, locating and adapting code snippets to the current context still takes time: for example, variables must be renamed, and dependencies included. We believe that when programmers decide to invest time in creating a new code snippet from scratch, they would also be willing to spend additional effort to make that code snippet configurable and easy to integrate. To explore this insight, we built SnipMatch, a plug-in for the Eclipse IDE. SnipMatch introduces a simple markup that allows snippet authors to specify search patterns and integration instructions. SnipMatch leverages this information, in conjunction with current code context, to improve snippet search and parameterization. For example, when a search query includes local variables, SnipMatch suggests compatible snippets, and automatically adapts them by substituting in these variables. In the lab, we observed that participants integrated snippets faster when using SnipMatch than when using standard Eclipse. Findings from a public deployment to 93 programmers suggest that SnipMatch has become integrated into the work practices of real users.\n",
      "=============================\n",
      "A framework for unifying presentation space\n",
      "Making effective use of the available display space has long been a fundamental issue in user interface design. We live in a time of rapid advances in available CPU power and memory. However, the common sizes of our computational display spaces have only minimally increased or in some cases, such as hand held devices, actually decreased. In addition, the size and scope of the information spaces we wish to explore are also expanding. Representing vast amounts of information on our relatively small screens has become increasingly problematic and has been associated with problems in navigation, interpretation and recognition. User interface research has proposed several differing presentation approaches to address these problems. These methods create displays that vary considerably, visually and algorithmically. We present a unified framework that provides a way of relating seemingly distinct methods, facilitating the inclusion of more than one presentation method in a single interface. Furthermore, it supports extrapolation between the presentation methods it describes. Of particular interest are the presentation possibilities that exist in the ranges between various distortion presentations, magnified insets and detail-in-context presentations, and between detail-in-context presentations and a full-zooming environment. This unified framework offers a geometric presentation library in which presentation variations are available independently of the mode of graphic representation. The intention is to promote the ease of exploration and experimentation into the use of varied presentation combinations.\n",
      "=============================\n",
      "Human-computer interaction for hybrid carving\n",
      "In this paper we explore human-computer interaction for carving, building upon our previous work with the FreeD digital sculpting device. We contribute a new tool design (FreeD V2), with a novel set of interaction techniques for the fabrication of static models: personalized tool paths, manual overriding, and physical merging of virtual models. We also present techniques for fabricating dynamic models, which may be altered directly or parametrically during fabrication. We demonstrate a semi-autonomous operation and evaluate the performance of the tool. We end by discussing synergistic cooperation between human and machine to ensure accuracy while preserving the expressiveness of manual practice.\n",
      "=============================\n",
      "The skweezee system: enabling the design and the programming of squeeze interactions\n",
      "The Skweezee System is an easy, flexible and open system for designing and developing squeeze-based, gestural interactions. It consists of Skweezees, which are soft objects, filled with conductive padding, that can be deformed or squeezed by applying pressure. These objects contain a number of electrodes that are dispersed over the shape. The electrodes sense the shape shifting of the conductive filling by measuring the changing resistance between every possible pair of electrodes. In addition, the Skweezee System contains user-friendly software that allows end-users to define and to record their own squeeze gestures. These gestures are distinguished using a Support Vector Machine (SVM) classifier. In this paper we introduce the concept and the underlying technology of the Skweezee System and we demonstrate the robustness of the SVM based classifier via two experimental user studies. The results of these studies demonstrate accuracies of 81% (8 gestures, user-defined) to 97% (3 gestures, user-defined), with an accuracy of 90% for 7 pre-defined gestures.\n",
      "=============================\n",
      "Revisiting visual interface programming: creating GUI tools for designers and programmers\n",
      "Involving graphic designers in the large-scale development of user interfaces requires tools that provide more graphical flexibility and support efficient software processes. These requirements were analysed and used in the design of the TkZ-inc graphical library and the IntuiKit interface design environment. More flexibility is obtained through a wider palette of visual techniques and support for iterative construction of images, composition and parametric displays. More efficient processes are obtained with the use of the SVG standard to import graphics, support for linking graphics and behaviour, and a unifying model-driven architecture. We describe the corresponding features of our tools, and show their use in the development of an application for airports. Benefits include a wider access to high quality visual interfaces for specialised applications, and shorter prototyping and development cycles for multidisciplinary teams.\n",
      "=============================\n",
      "ARC-Pad: absolute+relative cursor positioning for large displays with a mobile touchscreen\n",
      "We introduce ARC-Pad (Absolute+Relative Cursor pad), a novel technique for interacting with large displays using a mobile phone's touchscreen. In ARC-Pad we combine ab-solute and relative cursor positioning. Tapping with ARC-Pad causes the cursor to jump to the corresponding location on the screen, providing rapid movement across large distances. For fine position control, users can also clutch using relative mode. Unlike prior hybrid cursor positioning techniques, ARC-Pad does not require an explicit switch between relative and absolute modes. We compared ARC-Pad with the relative positioning commonly found on touchpads. Users were given a target acquisition task on a large display, and results showed that they were faster with ARC-Pad, without sacrificing accuracy. Users welcomed the benefits associated with ARC-Pad.\n",
      "=============================\n",
      "Instrumenting the crowd: using implicit behavioral measures to predict task performance\n",
      "Detecting and correcting low quality submissions in crowdsourcing tasks is an important challenge. Prior work has primarily focused on worker outcomes or reputation, using approaches such as agreement across workers or with a gold standard to evaluate quality. We propose an alternative and complementary technique that focuses on the way workers work rather than the products they produce. Our technique captures behavioral traces from online crowd workers and uses them to predict outcome measures such quality, errors, and the likelihood of cheating. We evaluate the effectiveness of the approach across three contexts including classification, generation, and comprehension tasks. The results indicate that we can build predictive models of task performance based on behavioral traces alone, and that these models generalize to related tasks. Finally, we discuss limitations and extensions of the approach.\n",
      "=============================\n",
      "Putting people first: specifying proper names in speech interfaces\n",
      "Communication is about people, not machines. But as firms and families alike spread out geographically, we rely increasingly on telecommunications tools to keep us “connected”. The challenge of such systems is to enable conversation between individuals without computational infrastructure getting in the way. This paper compares two speech-based communication systems, Phoneshell and Chatter, in how they deal with the keys to communication: proper names. Chatter, a conversational system using speech-recognition, improves upon the hierarchical nature of the touch-tone based Phoneshell by maintaining context and enabling use of anaphora. Proper names can present particular problems for speech recognizers, so an interface algorithm for reliable name specification by spelling is offered. Since individual letter recognition is non-robust, Chatter implicitly disambiguates strings of letters based on context. We hypothesize that the right interface can make faulty speech recognition as usable as TouchTones—even more so.\n",
      "=============================\n",
      "Cross-device interaction via micro-mobility and f-formations\n",
      "GroupTogether is a system that explores cross-device interaction using two sociological constructs. First, F-formations concern the distance and relative body orientation among multiple users, which indicate when and how people position themselves as a group. Second, micro-mobility describes how people orient and tilt devices towards one another to promote fine-grained sharing during co-present collaboration. We sense these constructs using: (a) a pair of overhead Kinect depth cameras to sense small groups of people, (b) low-power 8GHz band radio modules to establish the identity, presence, and coarse-grained relative locations of devices, and (c) accelerometers to detect tilting of slate devices. The resulting system supports fluid, minimally disruptive techniques for co-located collaboration by leveraging the proxemics of people as well as the proxemics of devices.\n",
      "=============================\n",
      "Using a low-cost electroencephalograph for task classification in HCI research\n",
      "Modern brain sensing technologies provide a variety of methods for detecting specific forms of brain activity. In this paper, we present an initial step in exploring how these technologies may be used to perform task classification and applied in a relevant manner to HCI research. We describe two experiments showing successful classification between tasks using a low-cost off-the-shelf electroencephalograph (EEG) system. In the first study, we achieved a mean classification accuracy of 84.0% in subjects performing one of three cognitive tasks - rest, mental arithmetic, and mental rotation - while sitting in a controlled posture. In the second study, conducted in more ecologically valid setting for HCI research, we attained a mean classification accuracy of 92.4% using three tasks that included non-cognitive features: a relaxation task, playing a PC based game without opponents, and engaging opponents within the game. Throughout the paper, we provide lessons learned and discuss how HCI researchers may utilize these technologies in their work.\n",
      "=============================\n",
      "Moving markup: repositioning freeform annotations\n",
      "Freeform digital ink annotation allows readers to interact with documents in an intuitive and familiar manner. Such marks are easy to manage on static documents, and provide a familiar annotation experience. In this paper, we describe an implementation of a freeform annotation system that accommodates dynamic document layout. The algorithm preserves the correct position of annotations when documents are viewed with different fonts or font sizes, with different aspect ratios, or on different devices. We explore a range of heuristics and algorithms required to handle common types of annotation, and conclude with a discussion of possible extensions to handle special kinds of annotations and changes to documents.\n",
      "=============================\n",
      "PhoneTouch: a technique for direct phone interaction on surfaces\n",
      "PhoneTouch is a novel technique for integration of mobile phones and interactive surfaces. The technique enables use of phones to select targets on the surface by direct touch, facilitating for instance pick&drop-style transfer of objects between phone and surface. The technique is based on separate detection of phone touch events by the surface, which determines location of the touch, and by the phone, which contributes device identity. The device-level observations are merged based on correlation in time. We describe a proof-of-concept implementation of the technique, using vision for touch detection on the surface (including discrimination of finger versus phone touch) and acceleration features for detection by the phone.\n",
      "=============================\n",
      "Laevo: a temporal desktop interface for integrated knowledge work\n",
      "Prior studies show that knowledge work is characterized by highly interlinked practices, including task, file and window management. However, existing personal information management tools primarily focus on a limited subset of knowledge work, forcing users to perform additional manual configuration work to integrate the different tools they use. In order to understand tool usage, we review literature on how users' activities are created and evolve over time as part of knowledge worker practices. From this we derive the activity life cycle, a conceptual framework describing the different states and transitions of an activity. The life cycle is used to inform the design of Laevo, a temporal activity-centric desktop interface for personal knowledge work. Laevo allows users to structure work within dedicated workspaces, managed on a timeline. Through a centralized notification system which doubles as a to-do list, incoming interruptions can be handled. Our field study indicates how highlighting the temporal nature of activities results in lightweight scalable activity management, while making users more aware about their ongoing and planned work.\n",
      "=============================\n",
      "Lucid touch: a see-through mobile device\n",
      "Touch is a compelling input modality for interactive devices; however, touch input on the small screen of a mobile device is problematic because a user's fingers occlude the graphical elements he wishes to work with. In this paper, we present LucidTouch, a mobile device that addresses this limitation by allowing the user to control the application by touching the back of the device. The key to making this usable is what we call pseudo-transparency: by overlaying an image of the user's hands onto the screen, we create the illusion of the mobile device itself being semi-transparent. This pseudo-transparency allows users to accurately acquire targets while not occluding the screen with their fingers and hand. Lucid Touch also supports multi-touch input, allowing users to operate the device simultaneously with all 10 fingers. We present initial study results that indicate that many users found touching on the back to be preferable to touching on the front, due to reduced occlusion, higher precision, and the ability to make multi-finger input.\n",
      "=============================\n",
      "Hierarchical parsing and recognition of hand-sketched diagrams\n",
      "A long standing challenge in pen-based computer interaction is the ability to make sense of informal sketches. A main difficulty lies in reliably extracting and recognizing the intended set of visual objects from a continuous stream of pen strokes. Existing pen-based systems either avoid these issues altogether, thus resulting in the equivalent of a drawing program, or rely on algorithms that place unnatural constraints on the way the user draws. As one step toward alleviating these difficulties, we present an integrated sketch parsing and recognition approach designed to enable natural, fluid, sketch-based computer interaction. The techniques presented in this paper are oriented toward the domain of network diagrams. In the first step of our approach, the stream of pen strokes is examined to identify the arrows in the sketch. The identified arrows then anchor a spatial analysis which groups the uninterpreted strokes into distinct clusters, each representing a single object. Finally, a trainable shape recognizer, which is informed by the spatial analysis, is used to find the best interpretations of the clusters. Based on these concepts, we have built SimuSketch, a sketch-based interface for Matlab's Simulink software package. An evaluation of SimuSketch has indicated that even novice users can effectively utilize our system to solve real engineering problems without having to know much about the underlying recognition techniques.\n",
      "=============================\n",
      "FingerSkate: making multi-touch operations less constrained and more continuous\n",
      "Multi-touch operations are sometimes difficult to perform due to musculoskeletal constraints. We propose FingerSkate, a variation to the current multi-touch operations to make them less constrained and more continuous. With FingerSkate, once one starts a multi-touch operation, one can continue the operation without having to maintain both fingers on the screen. In a pilot study, we observe that participants could learn to FingerSkate easily and were utilizing the new technique actively.\n",
      "=============================\n",
      "Interaction and modeling techniques for desktop two-handed input\n",
      "We describe input devices and two-handed interaction techniques to support map navigation tasks. We discuss several design variations and user testing of two-handed navigation techniques, including puck and stylus input on a Wacom tablet, as well as a novel design incorporating a touchpad (for the nonpreferred hand) and a mouse (for the preferred hand). To support the latter technique, we introduce a new input device, the TouchMouse, which is a standard mouse augmented with a pair of one-bit touch sensors, one for the palm and one for the index finger. Finally, we propose several enhancements to Buxton’s three-state model of graphical input and extend this model to encompass two-handed input transactions as well.\n",
      "=============================\n",
      "Animation support in a user interface toolkit: flexible, robust, and reusable abstractions\n",
      "Animation can be a very effective mechanism to convey information in visualization and user interface settings. However, integrating animated presentations into user interfaces has typicadly been a difficult task since, to date, there has been little or no explicit support for animation in window systems or user interface toolkits. This paper describes how the Artkit user interface toolkit has been extended with new animation support abstractions designed to overcome this problem. These abstractions provide a powerful but convenient base for building a range of animations, supporting techniques such as simple motion-blur, “squash and stretch”, use of arcing trajectories, anticipation and follow through, and “slow-in / slow-out” transitions. Because these\n",
      "=============================\n",
      "Ultra-lightweight constraints\n",
      "Constraint systems have been used for some time to implement various components of a user interface. High level support for flexible screen layou~t has been among the more important uses; layout constraints in a user interface toolkit provide a declarative mechanism for controlling the size and position of objects in an interactive display, along with an efficient update mechanism for maintaining display layouts automatically in the face of dynamic changes. This paper describes a new technique for implementing one-way layout constraints which overcomes a substantial limitation of previous systems. In particular, it allows constraints to be implemented in an extremely small amount of space — as little as 17 bits per constraint — and still maintain the level of performance needed for good interactive response. These ultralightweight constraints, while not handling all (cases, cover most relationships used for layout, and allow conventional constraints to be applied when needed. This paper will consider both a general technique for ultra-lightweight layout constraints and its specific implementation in a new JavaTM-basecl user interface toolkit.\n",
      "=============================\n",
      "Soylent: a word processor with a crowd inside\n",
      "This paper introduces architectural and interaction patterns for integrating crowdsourced human contributions directly into user interfaces. We focus on writing and editing, complex endeavors that span many levels of conceptual and pragmatic activity. Authoring tools offer help with pragmatics, but for higher-level help, writers commonly turn to other people. We thus present Soylent, a word processing interface that enables writers to call on Mechanical Turk workers to shorten, proofread, and otherwise edit parts of their documents on demand. To improve worker quality, we introduce the Find-Fix-Verify crowd programming pattern, which splits tasks into a series of generation and review stages. Evaluation studies demonstrate the feasibility of crowdsourced editing and investigate questions of reliability, cost, wait time, and work time for edits.\n",
      "=============================\n",
      "The radial scroll tool: scrolling support for stylus- or touch-based document navigation\n",
      "We present radial scroll, an interface widget to support scrolling particularly on either small or large scale touch displays. Instead of dragging a elevator in a scroll bar, or using repetitive key presses to page up or down, users gesture anywhere on the document surface such that clockwise gestures advance the document; counter clockwise gestures reverse the document. We describe our prototype implementation and discuss the results of an initial user study.\n",
      "=============================\n",
      "Steerable augmented reality with the beamatron\n",
      "Steerable displays use a motorized platform to orient a projector to display graphics at any point in the room. Often a camera is included to recognize markers and other objects, as well as user gestures in the display volume. Such systems can be used to superimpose graphics onto the real world, and so are useful in a number of augmented reality and ubiquitous computing scenarios. We contribute the Beamatron, which advances steerable displays by drawing on recent progress in depth camera-based interactions. The Beamatron consists of a computer-controlled pan and tilt platform on which is mounted a projector and Microsoft Kinect sensor. While much previous work with steerable displays deals primarily with projecting corrected graphics onto a discrete set of static planes, we describe computational techniques that enable reasoning in 3D using live depth data. We show two example applications that are enabled by the unique capabilities of the Beamatron: an augmented reality game in which a player can drive a virtual toy car around a room, and a ubiquitous computing demo that uses speech and gesture to move projected graphics throughout the room.\n",
      "=============================\n",
      "Interacting with hidden content using content-aware free-space transparency\n",
      "We present <i>content-aware free-space transparency</i>, an approach to viewing and manipulating the otherwise hidden content of obscured windows through unimportant regions of overlapping windows. Traditional approaches to interacting with otherwise obscured content in a window system render an entire window uniformly transparent. In contrast, content-aware free-space transparency uses opaque-to-transparent gradients and image-processing filters to minimize the interference from overlapping material, based on properties of that material. By increasing the amount of simultaneously visible content and allowing basic interaction with otherwise obscured content, without modifying window geometry, we believe that free-space transparency has the potential to improve user productivity.\n",
      "=============================\n",
      "Using graphical representation of user interfaces as visual references\n",
      "Many user interfaces use indirect references to identify specific objects and devices. My thesis investigates using graphical representations of user interfaces (i.e. screenshots) as direct visual references to support various kinds of applications. Sikuli Script enables users to programmatically control GUIs without the support from the underlying applications. Sikuli Test lets GUI developers and testers create test scripts without coding. Deep Shot introduces a framework and interaction techniques to migrate work states across heterogeneous devices in one action, taking a picture. In addition to these pure pixel-based systems, PAX associates the pixel representation with the internal structures and metadata of the user interface. Based on these building blocks, we propose to develop a visual history system that enables users to search and browse what they have seen on their computer screens. We outline some interesting use cases and discuss the challenges in this ongoing work.\n",
      "=============================\n",
      "Mapping GUIs to auditory interfaces\n",
      "This paper describes work to provide mappings between X-based graphical interfaces and auditory interfaces. In our system, dubbed Mercator, this mapping is transparent to applications. The primary motivation for this work is to provide accessibility to graphical applications for users who are blind or visually impaired. We describe the design of an auditory interface which simulates many of the features of graphical interfaces. We then describe the architecture we have built to model and transform graphical interfaces. Finally, we conclude with some indications of future research for improving our translation mechanisms and for creating an auditory “desktop” environment.\n",
      "=============================\n",
      "In-stroke word completion\n",
      "We present the design and implementation of a word-level stroking system called Fisch, which is intended to improve the speed of character-level unistrokes. Importantly, Fisch does not alter the way in which character-level unistrokes are made, but allows users to gradually ramp up to word-level unistrokes by extending their letters in minimal ways. Fisch relies on in-stroke word completion, a flexible design for fluidly turning unistroke letters into whole words. Fisch can be memorized at the motor level since word completions always appear at the same positions relative to the strokes being made. Our design for Fisch is suitable for use with any unistroke alphabet. We have implemented Fisch for multiple versions of EdgeWrite, and results show that Fisch reduces the number of strokes during entry by 43.9% while increasing the rate of entry. An informal test of \"record speed\" with the stylus version resulted in 50-60 wpm with no uncorrected errors.\n",
      "=============================\n",
      "Boomerang: suspendable drag-and-drop interactions based on a throw-and-catch metaphor\n",
      "We present the boomerang technique, which makes it possible to suspend and resume drag-and-drop operations. A throwing gesture while dragging an object suspends the operation, anytime and anywhere. A drag-and-drop interaction, enhanced with our technique, allows users to switch windows, invoke commands, and even drag other objects during a drag-and-drop operation without using the keyboard or menus. We explain how a throwing gesture can suspend drag-and-drop operations, and describe other features of our technique, including grouping, copying, and deleting dragged objects. We conclude by presenting prototype implementations and initial feedback on the proposed technique.\n",
      "=============================\n",
      "Pssst: side conversations in the Argo telecollaboration system\n",
      "We describe side conversations, a new facility we have added to the Argo telecollaboration system. Side conversations allow subgroups of teleconference participants to whisper to each other. The other participants can see who is whispering to whom, but cannot hear what\n",
      "=============================\n",
      "Ta-Tap: consecutive distant tap operations for one-handed touch screen use\n",
      "Tapping on the same point twice is a common operation known as double tap, but tapping on distant points in sequence is underutilized. In this poster we explore the potential uses of consecutive distant tap operations, which we call Ta-Tap. As a single-touch operation, it is expected to be particularly useful for single-handed touch screen use. We examined three possible uses of Ta-Tap: simulating multi-touch operations, invoking a virtual scroll wheel, and invoking a pie-menu. We verified the feasibility of Ta-Tap through the experiment.\n",
      "=============================\n",
      "Creating contextual help for GUIs using screenshots\n",
      "Contextual help is effective for learning how to use GUIs by showing instructions and highlights on the actual interface rather than in a separate viewer. However, end-users and third-party tech support typically cannot create contextual help to assist other users because it requires programming skill and source code access. We present a creation tool for contextual help that allows users to apply common computer skills-taking screenshots and writing simple scripts. We perform pixel analysis on screenshots to make this tool applicable to a wide range of applications and platforms without source code access. We evaluated the tool's usability with three groups of participants: developers, in-structors, and tech support. We further validated the applicability of our tool with 60 real tasks supported by the tech support of a university campus.\n",
      "=============================\n",
      "SikuliBot: automating physical interface using images\n",
      "We present SikuliBot, an image-based approach to automating user interface. SikuliBot extends the visual programming concept of Sikuli Script[2] from the graphical UIs to the real world of physical UIs, such as mobile devices' touch-screens and hardware buttons. The key to our approach is using a physical robot to see an interface, identify a target, and perform an action on the target using the robot's actuators. We demonstrate working examples on MakerBot 3D printer that could move a stylus to perform multi-touch gestures on touchscreen to automate tasks such as swipe-to unlock, playing a virtual piano, and playing the Angry Bird game. A wide range of automation possibilities are made viable using a simple scripting language based on images of UI components. The benefits of our approach are: generalizability, instrumentation-free, and high-level programming abstraction.\n",
      "=============================\n",
      "Crowd-based recognition of web interaction patterns\n",
      "Web automation often involves users describing complex tasks to a system, with directives generally limited to low-level constituent actions like \"click the search button.\" This level of description is unnatural and makes it difficult to generalize the task across websites. In this paper, we propose a system for automatically recognizing higher-level interaction patterns from user's completion of tasks, such as \"searching for cat videos\" or \"replying to a post\". We present PatFinder, a system that identifies these patterns using the input of crowd workers. We validate the system by generating data for 10 tasks, having 62 crowd workers label them, and automatically extracting 14 interaction patterns. Our results show that the number of patterns grows sublinearly with the number of tasks, suggesting that a small finite set of patterns may suffice to describe the vast majority of tasks on the web.\n",
      "=============================\n",
      "Slack-scroll: sharing sliding operations among scrolling and other GUI functions\n",
      "Sliding is one of the basic touchscreen operations, but is mainly used for scrolling in mobile touchscreen GUIs. As a way to share sliding operations among scrolling and other GUI functions, we propose Slack-Scroll. We implemented two application scenarios of Slack-Scroll, and asserted their feasibility in a user study. All participants could accept and adapt well to the new techniques enabled by Slack-Scroll.\n",
      "=============================\n",
      "A framework for robust and flexible handling of inputs with uncertainty\n",
      "New input technologies (such as touch), recognition based input (such as pen gestures) and next-generation interactions (such as inexact interaction) all hold the promise of more natural user interfaces. However, these techniques all create inputs with some uncertainty. Unfortunately, conventional infrastructure lacks a method for easily handling uncertainty, and as a result input produced by these technologies is often converted to conventional events as quickly as possible, leading to a stunted interactive experience. We present a framework for handling input with uncertainty in a systematic, extensible, and easy to manipulate fashion. To illustrate this framework, we present several traditional interactors which have been extended to provide feedback about uncertain inputs and to allow for the possibility that in the end that input will be judged wrong (or end up going to a different interactor). Our six demonstrations include tiny buttons that are manipulable using touch input, a text box that can handle multiple interpretations of spoken input, a scrollbar that can respond to inexactly placed input, and buttons which are easier to click for people with motor impairments. Our framework supports all of these interactions by carrying uncertainty forward all the way through selection of possible target interactors, interpretation by interactors, generation of (uncertain) candidate actions to take, and a mediation process that decides (in a lazy fashion) which actions should become final.\n",
      "=============================\n",
      "Evaluating user interface systems research\n",
      "The development of user interface systems has languished with the stability of desktop computing. Future systems, however, that are off-the-desktop, nomadic or physical in nature will involve new devices and new software systems for creating interactive applications. Simple usability testing is not adequate for evaluating complex systems. The problems with evaluating systems work are explored and a set of criteria for evaluating new UI systems work is presented.\n",
      "=============================\n",
      "A practical pressure sensitive computer keyboard\n",
      "A pressure sensitive computer keyboard is presented that independently senses the force level on every depressed key. The design leverages existing membrane technologies and is suitable for low-cost, high-volume manufacturing. A number of representative applications are discussed.\n",
      "=============================\n",
      "PacCAM: material capture and interactive 2D packing for efficient material usage on CNC cutting machines\n",
      "The availability of low-cost digital fabrication devices enables new groups of users to participate in the design and fabrication of things. However, software to assist in the transition from design to actual fabrication is currently overlooked. In this paper, we introduce PacCAM, a system for packing 2D parts within a given source material for fabrication using 2D cutting machines. Our solution combines computer vision to capture the source material shape with a user interface that incorporates 2D rigid body simulation and snapping. A user study demonstrated that participants could make layouts faster with our system compared with using traditional drafting tools. PacCAM caters to a variety of 2D fabrication applications and can contribute to the reduction of material waste.\n",
      "=============================\n",
      "Perceptually-supported image editing of text and graphics\n",
      "This paper presents a novel image editing program emphasizing easy selection and manipulation of material found in informal, casual documents such as sketches, handwritten notes, whiteboard images, screen snapshots, and scanned documents. The program, called ScanScribe, offers four significant advances. First, it presents a new, intuitive model for maintaining image objects and groups, along with underlying logic for updating these in the course of an editing session. Second, ScanScribe takes advantage of newly developed image processing algorithms to separate foreground markings from a white or light background, and thus can automatically render the background transparent so that image material can be rearranged without occlusion by background pixels. Third, ScanScribe introduces new interface techniques for selecting image objects with a pointing device without resorting to a palette of tool modes. Fourth, ScanScribe presents a platform for exploiting image analysis and recognition methods to make perceptually significant structure readily available to the user. As a research prototype, ScanScribe has proven useful in the work of members of our laboratory, and has been released on a limited basis for user testing and evaluation.\n",
      "=============================\n",
      "Pad++: a zooming graphical interface for exploring alternate interface physics\n",
      "We describe the current status of Pad++, a zooming graphical interface that we are exploring as an alternative to traditional window and icon-based approaches to interface design. We discuss the motivation for Pad++, describe the implementation, and present prototype applications. In addition, we introduce an informational physics strategy for interface design and briefly compare it with metaphor-based design strategies.\n",
      "=============================\n",
      "Using temporal video annotation as a navigational aid for video browsing\n",
      "Video is a complex information space that requires advanced navigational aids for effective browsing. The increasing number of temporal video annotations offers new opportunities to provide video navigation according to a user's needs. We present a novel video browsing interface called TAV (Temporal Annotation Viewing) that provides the user with a visual overview of temporal video annotations. TAV enables the user to quickly determine the general content of a video, the location of scenes of interest and the type of annotations that are displayed while watching the video. An ongoing user study will evaluate our novel approach.\n",
      "=============================\n",
      "OmniTouch: wearable multitouch interaction everywhere\n",
      "OmniTouch is a wearable depth-sensing and projection system that enables interactive multitouch applications on everyday surfaces. Beyond the shoulder-worn system, there is no instrumentation of the user or environment. Foremost, the system allows the wearer to use their hands, arms and legs as graphical, interactive surfaces. Users can also transiently appropriate surfaces from the environment to expand the interactive area (e.g., books, walls, tables). On such surfaces - without any calibration - OmniTouch provides capabilities similar to that of a mouse or touchscreen: X and Y location in 2D interfaces and whether fingers are \"clicked\" or hovering, enabling a wide variety of interactions. Reliable operation on the hands, for example, requires buttons to be 2.3cm in diameter. Thus, it is now conceivable that anything one can do on today's mobile devices, they could do in the palm of their hand.\n",
      "=============================\n",
      "Histomages: fully synchronized views for image editing\n",
      "We present Histomages, a new interaction model for image editing that considers color histograms as spatial rearrangements of image pixels. Users can select pixels on image histograms as they would select image regions and directly manipulate them to adjust their colors. Histomages are also affected by other image tools such as paintbrushes. We explore some possibilities offered by this interaction model, and discuss the four key principles behind it as well as their implications for the design of feature-rich software in general.\n",
      "=============================\n",
      "The proximity toolkit: prototyping proxemic interactions in ubiquitous computing ecologies\n",
      "People naturally understand and use proxemic relationships (e.g., their distance and orientation towards others) in everyday situations. However, only few ubiquitous computing (ubicomp) systems interpret such proxemic relationships to mediate interaction (proxemic interaction). A technical problem is that developers find it challenging and tedious to access proxemic information from sensors. Our Proximity Toolkit solves this problem. It simplifies the exploration of interaction techniques by supplying fine-grained proxemic information between people, portable devices, large interactive surfaces, and other non-digital objects in a room-sized environment. The toolkit offers three key features. 1) It facilitates rapid prototyping of proxemic-aware systems by supplying developers with the orientation, distance, motion, identity, and location information between entities. 2) It includes various tools, such as a visual monitoring tool, that allows developers to visually observe, record and explore proxemic relationships in 3D space. (3) Its flexible architecture separates sensing hardware from the proxemic data model derived from these sensors, which means that a variety of sensing technologies can be substituted or combined to derive proxemic information. We illustrate the versatility of the toolkit with proxemic-aware systems built by students.\n",
      "=============================\n",
      "Scalable methods to collect and visualize sidewalk accessibility data for people with mobility impairments\n",
      "Poorly maintained sidewalks pose considerable accessibility challenges for mobility impaired persons; however, there are currently few, if any, mechanisms to determine accessible areas of a city a priori. In this paper, I introduce four threads of research that I will conduct for my Ph.D. thesis aimed at creating new methods and tools to provide unprecedented levels of information on the accessibility of streets and sidewalk. Namely, I will (i) conduct a formative study to better understand accessibility problems, (ii) develop and evaluate scalable map-based data collection methods, (iii) integrate computer vision algorithms to increase the scalability of the methods, and (iv) develop accessible-aware map-based tools that demonstrate the utility of our data (Figure 1 and 6).\n",
      "=============================\n",
      "Proactive wrangling: mixed-initiative end-user programming of data transformation scripts\n",
      "Analysts regularly wrangle data into a form suitable for computational tools through a tedious process that delays more substantive analysis. While interactive tools can assist data transformation, analysts must still conceptualize the desired output state, formulate a transformation strategy, and specify complex transforms. We present a model to proactively suggest data transforms which map input data to a relational format expected by analysis tools. To guide search through the space of transforms, we propose a metric that scores tables according to type homogeneity, sparsity and the presence of delimiters. When compared to \"ideal\" hand-crafted transformations, our model suggests over half of the needed steps; in these cases the top-ranked suggestion is preferred 77% of the time. User study results indicate that suggestions produced by our model can assist analysts' transformation tasks, but that users do not always value proactive assistance, instead preferring to maintain the initiative. We discuss some implications of these results for mixed-initiative interfaces.\n",
      "=============================\n",
      "A gesture-based authentication scheme for untrusted public terminals\n",
      "Powerful mobile devices with minimal I/O capabilities increase the likelihood that we will want to annex these devices to I/O resources we encounter in the local environment. This opportunistic annexing will require authentication. We present a sensor-based authentication mechanism for mobile devices that relies on physical possession instead of knowledge to setup the initial connection to a public terminal. Our solution provides a simple mechanism for shaking a device to authenticate with the public infrastructure, making few assumptions about the surrounding infrastructure while also maintaining a reasonable level of security.\n",
      "=============================\n",
      "The cognitive coprocessor architecture for interactive user interfaces\n",
      "The graphics capabilities and speed of current hardware systems allow the exploration of 3D and animation in user interfaces, while improving the degree of interaction as well. In order to fully utilize these capabilities, new software architectures must support multiple, asynchronous, interacting agents (the <italic>Multiple Agent Problem</italic>), <italic>and</italic> support smooth interactive animation (the <italic>Animation Problem</italic>). The <italic>Cognitive Coprocessor</italic> is a new user interface architecture designed to solve these two problems, while supporting highly interactive user interfaces that have 2D and 3D animations. This architecture includes <italic>3D Rooms</italic>, a 3D analogy to the Rooms system with Rooms Buttons extended to <italic>Interactive Objects</italic> that deal with 3D, animation, and gestures. This research is being tested in the domain of <italic>Information Visualization</italic>, which uses 2D and 3D animated artifacts to represent the structure of information. A prototype, called the <italic>Information Visualizer</italic>, has been built.\n",
      "=============================\n",
      "IP-QAT: in-product questions, answers, & tips\n",
      "We present IP-QAT, a new community-based question and answer system for software users. Unlike most community forums, IP-QAT is integrated into the actual software application, allowing users to easily post questions, answers and tips without having to leave the application. Our in-product implementation is context-aware and shows relevant posts based on a user's recent activity. It is also designed with minimal transaction costs to encourage users to easily post, include annotated images and file attachments, as well as tag their posts with relevant UI components. We describe a robust cloud-based system implementation, which allowed us to release IP-QAT to 37 users for a 2 week field study. Our study showed that IP-QAT increased user contributions, and subjectively, users found our system more useful and easier to use, in comparison to the existing commercial discussion board.\n",
      "=============================\n",
      "eyeLook: using attention to facilitate mobile media consumption\n",
      "One of the problems with mobile media devices is that they may distract users during critical everyday tasks, such as navigating the streets of a busy city. We addressed this issue in the design of eyeLook: a platform for attention sensitive mobile computing. eyeLook appliances use embedded low cost eyeCONTACT sensors (ECS) to detect when the user looks at the display. We discuss two eyeLook applications, seeTV and seeTXT, that facilitate courteous media consumption in mobile contexts by using the ECS to respond to user attention. seeTV is an attentive mobile video player that automatically pauses content when the user is not looking. seeTXT is an attentive speed reading application that flashes words on the display, advancing text only when the user is looking. By making mobile media devices sensitive to actual user attention, eyeLook allows applications to gracefully transition users between consuming media, and managing life.\n",
      "=============================\n",
      "Designing adaptive feedback for improving data entry accuracy\n",
      "Data quality is critical for many information-intensive applications. One of the best opportunities to improve data quality is during entry. Usher provides a theoretical, data-driven foundation for improving data quality during entry. Based on prior data, Usher learns a probabilistic model of the dependencies between form questions and values. Using this information, Usher maximizes information gain. By asking the most unpredictable questions first, Usher is better able to predict answers for the remaining questions. In this paper, we use Usher's predictive ability to design a number of intelligent user interface adaptations that improve data entry accuracy and efficiency. Based on an underlying cognitive model of data entry, we apply these modifications before, during and after committing an answer. We evaluated these mechanisms with professional data entry clerks working with real patient data from six clinics in rural Uganda. The results show that our adaptations have the potential to reduce error (by up to 78%), with limited effect on entry time (varying between -14% and +6%). We believe this approach has wide applicability for improving the quality and availability of data, which is increasingly important for decision-making and resource allocation.\n",
      "=============================\n",
      "SideSight: multi-\"touch\" interaction around small devices\n",
      "Interacting with mobile devices using touch can lead to fingers occluding valuable screen real estate. For the smallest devices, the idea of using a touch-enabled display is almost wholly impractical. In this paper we investigate sensing user touch around small screens like these. We describe a prototype device with infra-red (IR) proximity sensors embedded along each side and capable of detecting the presence and position of fingers in the adjacent regions. When this device is rested on a flat surface, such as a table or desk, the user can carry out single and multi-touch gestures using the space around the device. This gives a larger input space than would otherwise be possible which may be used in conjunction with or instead of on-display touch input. Following a detailed description of our prototype, we discuss some of the interactions it affords.\n",
      "=============================\n",
      "A cuttable multi-touch sensor\n",
      "We propose cutting as a novel paradigm for ad-hoc customization of printed electronic components. As a first instantiation, we contribute a printed capacitive multi-touch sensor, which can be cut by the end-user to modify its size and shape. This very direct manipulation allows the end-user to easily make real-world objects and surfaces touch-interactive, to augment physical prototypes and to enhance paper craft. We contribute a set of technical principles for the design of printable circuitry that makes the sensor more robust against cuts, damages and removed areas. This includes novel physical topologies and printed forward error correction. A technical evaluation compares different topologies and shows that the sensor remains functional when cut to a different shape.\n",
      "=============================\n",
      "A thin stretchable interface for tangential force measurement\n",
      "We have developed a simple skin-like user interface that can be easily attached to curved as well as flat surfaces and used to measure tangential force generated by pinching and dragging interactions. The interface consists of several photoreflectors that consist of an IR LED and a phototransistor and elastic fabric such as stocking and rubber membrane. The sensing method used is based on our observation that photoreflectors can be used to measure the ratio of expansion and contraction of a stocking using the changes in transmissivity of IR light passing through the stocking. Since a stocking is thin, stretchable, and nearly transparent, it can be easily attached to various types of objects such as mobile devices, robots, and different parts of the body as well as to various types of conventional pressure sensors without altering the original shape of the object. It can also present natural haptic feedback in accordance with the amount of force exerted. A system using several such sensors can determine the direction of a two-dimensional force. A variety of example applications illustrated the utility of this sensing system.\n",
      "=============================\n",
      "Designing for low-latency direct-touch input\n",
      "Software designed for direct-touch interfaces often utilize a metaphor of direct physical manipulation of pseudo \"real-world\" objects. However, current touch systems typically take 50-200ms to update the display in response to a physi-cal touch action. Utilizing a high performance touch de-monstrator, subjects were able to experience touch latencies ranging from current levels down to about 1ms. Our tests show that users greatly prefer lower latencies, and noticea-ble improvement continued well below 10ms. This level of performance is difficult to achieve in commercial compu-ting systems using current technologies. As an alternative, we propose a hybrid system that provides low-fidelity visu-al feedback immediately, followed by high-fidelity visuals at standard levels of latency.\n",
      "=============================\n",
      "Robust computer vision-based detection of pinching for one and two-handed gesture input\n",
      "We present a computer vision technique to detect when the user brings their thumb and forefinger together (a pinch gesture) for close-range and relatively controlled viewing circumstances. The technique avoids complex and fragile hand tracking algorithms by detecting the hole formed when the thumb and forefinger are touching; this hole is found by simple analysis of the connected components of the background segmented against the hand. Our Thumb and Fore-Finger Interface (TAFFI) demonstrates the technique for cursor control as well as map navigation using one and two-handed interactions.\n",
      "=============================\n",
      "An architecture for an extensible 3D interface toolkit\n",
      "This paper presents the architecture for an extensible toolkit used in construction and rapid prototyping of three dimensional interfaces, interactive illustrations, and three dimensional widgets. The toolkit provides methods for the direct manipulation of 3D primitives which can be linked together through a visual programming language to create complex constrained behavior. Features of the toolkit include the ability to visually build, encapsulate, and parameterize complex models, and impose limits on the models. The toolkit's constraint resolution technique is based on a dynamic object model similar to those in prototype delegation object systems. The toolkit has been used to rapidly prototype tools for mechanical modelling, scientific visualization, construct 3D widgets, and build mathematical illustrations.\n",
      "=============================\n",
      "The metropolis keyboard - an exploration of quantitative techniques for virtual keyboard design\n",
      "Text entry user interfaces have been a bottleneck of non traditional computing devices. One of the promising methods is the virtual keyboard on touch screens. Various layouts have been manually designed to replace the dominant QWERTY layout. This paper presents two computerized quantitative design techniques to search for the optimal virtual keyboard. The first technique simulated the dynamics of a keyboard with “digraph springs” between keys, which produced a “Hooke’s” keyboard with 41.6 wpm performance. The second technique used a Metropolis random walk algorithm guided by a “Fitts energy” objective function, which produced a “Metropolis” keyboard with 43.1 wpm performance. The paper also models and evaluates the perfo rmance of four existing keyboard layouts. We corrected erroneous estimates in the literature and predicted the performance of QWERTY, CHUBON, FITALY, OPTI to be in the neighborhood of 30, 33, 36 and 38 wpm respectively. Our best design was 40% faster than QWERTY and 10% faster than OPTI, illustrating the advantage of quantitative user interface design techniques based on models of human performance over traditional trial and error designs guided by heuristics.\n",
      "=============================\n",
      "Zoom-and-pick: facilitating visual zooming and precision pointing with interactive handheld projectors\n",
      "Designing interfaces for interactive handheld projectors is an exiting new area of research that is currently limited by two problems: hand jitter resulting in poor input control, and possible reduction of image resolution due to the needs of image stabilization and warping algorithms. We present the design and evaluation of a new interaction technique, called zoom-and-pick, that addresses both problems by allowing the user to fluidly zoom in on areas of interest and make accurate target selections. Subtle design features of zoom-and-pick enable pixel-accurate pointing, which is not possible in most freehand interaction techniques. Our evaluation results indicate that zoom-and-pick is significantly more accurate than the standard pointing technique described in our previous work.\n",
      "=============================\n",
      "Sketch-sketch revolution: an engaging tutorial system for guided sketching and application learning\n",
      "We describe Sketch-Sketch Revolution, a new tutorial system that allows any user to experience the success of drawing content previously created by an expert artist. Sketch-Sketch Revolution not only guides users through the application user interface, it also provides assistance with the actual sketching. In addition, the system offers an authoring tool that enables artists to create content and then automatically generates a tutorial from their recorded workflow history. Sketch-Sketch Revolution is a unique hybrid tutorial system that combines in-product, content-centric and reactive tutorial methods to provide an engaging learning experience. A qualitative user study showed that our system successfully taught users how to interact with a drawing application user interface, gave users confidence they could recreate expert content, and was uniformly considered useful and easy to use.\n",
      "=============================\n",
      "User interface façades: towards fully adaptable user interfaces\n",
      "User interfaces are becoming more and more complex. Adaptable and adaptive interfaces have been proposed to address this issue and previous studies have shown that users prefer interfaces that they can adapt to self-adjusting ones. However, most existing systems provide users with little support for adapting their interfaces. Interface customization techniques are still very primitive and usually constricted to particular applications. In this paper, we present User Interface Façades, a system that provides users with simple ways to adapt, reconfigure, and re-combine existing graphical interfaces, through the use of direct manipulation techniques. The paper describes the user's view of the system, provides some technical details, and presents several examples to illustrate its potential.\n",
      "=============================\n",
      "Pacers: time-elastic objects\n",
      "Current time-based presentation systems are rigid in that they assumethe running time of all components of a presentation is constant. Furthermore, most systems offer little or no supportfordynamically adapting the presentation quality to the (lack of) available system resources. In this paper, we introduce an object called a Pacer that is “timeelastic” in that it can adjust the quality of its presentation according to the amount of time available. We have implemented a direct manipulation graphical interface using pacersthat can automatically degradepresentation quality in a controlled fashion depending on the user’s input rate and the speed of the rendering system.\n",
      "=============================\n",
      "Hands-on demonstration: interacting with SpeechSkimmer\n",
      "SpeechSkimmer is an interactive system for quickly browsing and finding information in speech recordings. Skimming speech recordings is much more difficult than visually scanning images, text, or video because of the slow, linear, temporal nature of the audio channel. The SpeechSkimmer system uses a combination of (1) time compression and pause removal, (2) automatically finding segments that summarize a recording, and (3) interaction techniques, to enable a speech recording to be heard quickly and at several levels of detail.\n",
      "=============================\n",
      "Adding structured data in unstructured web chat conversation\n",
      "Web chat is becoming the primary customer contact channel in customer relationship management (CRM), and Question/Answer/Lookup (QAL) is the dominant communication pattern in CRM agent-to-customer chat. Text-based web chat for QAL has two main usability problems. Chat transcripts between agents and customers are not tightly integrated into agent-side applications, requiring customer service agents to re-enter customer typed data. Also, sensitive information posted in chat sessions in plain text raises security concerns. The addition of web form widgets to web chat not only solves both of these problems but also adds new usability benefits to QAL. Forms can be defined beforehand or, more flexibly, dynamically composed. Two preliminary user studies were conducted. An agent-side study showed that adding inline forms to web chat decreased overall QAL completion time by 47 percent and increased QAL accuracy by removing all potential human errors. A customer-side study showed that web chat with inline forms is intuitive to customers.\n",
      "=============================\n",
      "Crowd-scale interactive formal reasoning and analytics\n",
      "Large online courses often assign problems that are easy to grade because they have a fixed set of solutions (such as multiple choice), but grading and guiding students is more difficult in problem domains that have an unbounded number of correct answers. One such domain is derivations: sequences of logical steps commonly used in assignments for technical, mathematical and scientific subjects. We present DeduceIt, a system for creating, grading, and analyzing derivation assignments in any formal domain. DeduceIt supports assignments in any logical formalism, provides students with incremental feedback, and aggregates student paths through each proof to produce instructor analytics. DeduceIt benefits from checking thousands of derivations on the web: it introduces a proof cache, a novel data structure which leverages a crowd of students to decrease the cost of checking derivations and providing real-time, constructive feedback. We evaluate DeduceIt with 990 students in an online compilers course, finding students take advantage of its incremental feedback and instructors benefit from its structured insights into course topics. Our work suggests that automated reasoning can extend online assignments and large-scale education to many new domains.\n",
      "=============================\n",
      "Scopemate: a robotic microscope\n",
      "Scopemate is a robotic microscope that tracks the user for inspection microscopy. The novel input device combines an optically augmented web-cam with a head tracker. A head tracker controls the inspection angle of a webcam fitted with appropriate microscope optics. This allows an operator the full use of their hands while intuitively looking at the work area from different perspectives.\n",
      "=============================\n",
      "Argohalls: adding support for group awareness to the Argo telecollaboration system\n",
      "Members of geographically distributed work groups often complain of a feeling of isolation and of not knowing “who is around”. Argohalls attempt to solve this problem by integrating video icons, clustered into groups representing physical hallways, into the Argo telecollaboration system. Argo users can “hang out” in hallways in order to keep track of the co-workers on their projects, and they can roam other hallways to “run into” whoever happens to be there.\n",
      "=============================\n",
      "Performance optimizations of virtual keyboards for stroke-based text entry on a touch-based tabletop\n",
      "Efficiently entering text on interactive surfaces, such as touch-based tabletops, is an important concern. One novel solution is shape writing - the user strokes through all the letters in the word on a virtual keyboard without lifting his or her finger. While this technique can be used with any keyboard layout, the layout does impact the expected performance. In this paper, I investigate the influence of keyboard layout on expert text-entry performance for stroke-based text entry. Based on empirical data, I create a model of stroking through a series of points based on Fitts's law. I then use that model to evaluate various keyboard layouts for both tapping and stroking input. While the stroke-based technique seems promising by itself (i.e., there is a predicted gain of 17.3% for a Qwerty layout), significant additional gains can be made by using a more-suitable keyboard layout (e.g., the OPTI II layout is predicted to be 29.5% faster than Qwerty).\n",
      "=============================\n",
      "ImpAct: enabling direct touch and manipulation for surface computing\n",
      "This paper explores direct touch and manipulation techniques for surface computing platforms using a special force feedback stylus named ImpAct(Immersive Haptic Augmentation for Direct Touch). Proposed haptic stylus can change its length when it is pushed against a display surface. Correspondingly, a virtual stem is rendered inside the display area so that user perceives the stylus immersed through to the digital space below the screen. We propose ImpAct as a tool to probe and manipulate digital objects in the shallow region beneath display surface. ImpAct creates a direct touch interface by providing kinesthetic haptic sensations along with continuous visual contact to digital objects below the screen surface.\n",
      "=============================\n",
      "Videotater: an approach for pen-based digital video segmentation and tagging\n",
      "The continuous growth of media databases necessitates development of novel visualization and interaction techniques to support management of these collections. We present Videotater, an experimental tool for a Tablet PC that supports the efficient and intuitive navigation, selection, segmentation, and tagging of video. Our veridical representation immediately signals to the user where appropriate segment boundaries should be placed and allows for rapid review and refinement of manually or automatically generated segments. Finally, we explore a distribution of modalities in the interface by using multiple timeline representations, pressure sensing, and a tag painting/erasing metaphor with the pen.\n",
      "=============================\n",
      "FOCUS: the interactive table for product comparison and selection\n",
      "FOCUS, the Feature-Oriented Catalog USer interface, is an interactive table viewer for a common kind of table, namely the object-attribute table, also called cases-by-attribute table or relational table. Typical examples of these tables are the Roll Calls in BYTE where the features and test results of a family of hardware or software products are c ompared. FOCUS supports data exploration by a combination of a focus+context or fisheye technique, a hierarchical outliner for large attribute sets, and a general and easy-to-use dynamic query mechanism where the user simply clicks on desired values found in the table. A PC/Windows implementation of FOCUS is publicly available (http://www.gmd.de/fit/projects/focus.html). It is suited for tables with up to a few hundred rows and columns, which are typically stored and maintained by spreadsheet applications. Since we use a simple data format, existing tables can be easily inspected with FOCUS. With the rapidly increasing public interest in on-line services like the W orld Wide We b we e xpect a growing d emand for access to o n-line c atalogues and d atabases. FOCUS satisfies this demand, allowing formulation of simple database queries with an interface as easy to use as a Web browser.\n",
      "=============================\n",
      "Activity analysis enabling real-time video communication on mobile phones for deaf users\n",
      "We describe our system called MobileASL for real-time video communication on the current U.S. mobile phone network. The goal of MobileASL is to enable Deaf people to communicate with Sign Language over mobile phones by compressing and transmitting sign language video in real-time on an off-the-shelf mobile phone, which has a weak processor, uses limited bandwidth, and has little battery capacity. We develop several H.264-compliant algorithms to save system resources while maintaining ASL intelligibility by focusing on the important segments of the video. We employ a dynamic skin-based region-of-interest (ROI) that encodes the skin at higher quality at the expense of the rest of the video. We also automatically recognize periods of signing versus not signing and raise and lower the frame rate accordingly, a technique we call variable frame rate (VFR). We show that our variable frame rate technique results in a 47% gain in battery life on the phone, corresponding to an extra 68 minutes of talk time. We also evaluate our system in a user study. Participants fluent in ASL engage in unconstrained conversations over mobile phones in a laboratory setting. We find that the ROI increases intelligibility and decreases guessing. VFR increases the need for signs to be repeated and the number of conversational breakdowns, but does not affect the users' perception of adopting the technology. These results show that our sign language sensitive algorithms can save considerable resources without sacrificing intelligibility.\n",
      "=============================\n",
      "Video collections in panoramic contexts\n",
      "Video collections of places show contrasts and changes in our world, but current interfaces to video collections make it hard for users to explore these changes. Recent state-of-the-art interfaces attempt to solve this problem for 'outside->in' collections, but cannot connect 'inside->out' collections of the same place which do not visually overlap. We extend the focus+context paradigm to create a video-collections+context interface by embedding videos into a panorama. We build a spatio-temporal index and tools for fast exploration of the space and time of the video collection. We demonstrate the flexibility of our representation with interfaces for desktop and mobile flat displays, and for a spherical display with joypad and tablet controllers. We study with users the effect of our video-collection+context system to spatio-temporal localization tasks, and find significant improvements to accuracy and completion time in visual search tasks compared to existing systems. We measure the usability of our interface with System Usability Scale (SUS) and task-specific questionnaires, and find our system scores higher.\n",
      "=============================\n",
      "Ambiguous intentions: a paper-like interface for creative design\n",
      "Interfaces for conceptual and creative design should recognize and interpret drawings. They should also capture users’ intended ambiguity, vagueness, and imprecision and convey these qualities visually and through interactive behavior. Freehand drawing can provide this information and it is a natural input mode for design. We describe a pen-based interface that acquires information about ambiguity and precision from freehand input, represents it internally, and echoes it to users visually and through constraint based edit behavior.\n",
      "=============================\n",
      "Real-time crowd control of existing interfaces\n",
      "Crowdsourcing has been shown to be an effective approach for solving difficult problems, but current crowdsourcing systems suffer two main limitations: (i) tasks must be repackaged for proper display to crowd workers, which generally requires substantial one-off programming effort and support infrastructure, and (ii) crowd workers generally lack a tight feedback loop with their task. In this paper, we introduce Legion, a system that allows end users to easily capture existing GUIs and outsource them for collaborative, real-time control by the crowd. We present mediation strategies for integrating the input of multiple crowd workers in real-time, evaluate these mediation strategies across several applications, and further validate Legion by exploring the space of novel applications that it enables.\n",
      "=============================\n",
      "Associating the visual representation of user interfaces with their internal structures and metadata\n",
      "Pixel-based methods are emerging as a new and promising way to develop new interaction techniques on top of existing user interfaces. However, in order to maintain platform independence, other available low-level information about GUI widgets, such as accessibility metadata, was neglected intentionally. In this paper, we present a hybrid framework, PAX, which associates the visual representation of user interfaces (i.e. the pixels) and their internal hierarchical metadata (i.e. the content, role, and value). We identify challenges to building such a framework. We also develop and evaluate two new algorithms for detecting text at arbitrary places on the screen, and for segmenting a text image into individual word blobs. Finally, we validate our framework in implementations of three applications. We enhance an existing pixel-based system, Sikuli Script, and preserve the readability of its script code at the same time. Further, we create two novel applications, Screen Search and Screen Copy, to demonstrate how PAX can be applied to development of desktop-level interactive systems.\n",
      "=============================\n",
      "Chronicle: capture, exploration, and playback of document workflow histories\n",
      "We describe Chronicle, a new system that allows users to explore document workflow histories. Chronicle captures the entire video history of a graphical document, and provides links between the content and the relevant areas of the history. Users can indicate specific content of interest, and see the workflows, tools, and settings needed to reproduce the associated results, or to better understand how it was constructed to allow for informed modification. Thus, by storing the rich information regarding the document's history workflow, Chronicle makes any working document a potentially powerful learning tool. We outline some of the challenges surrounding the development of such a system, and then describe our implementation within an image editing application. A qualitative user study produced extremely encouraging results, as users unanimously found the system both useful and easy to use.\n",
      "=============================\n",
      "Personalizing the capture of public experiences\n",
      "In this paper, we describe our work on developing a system to support the personalization of a captured public experience. Specifically, we are interested in providing students with the ability to personalize the capture of the lecture experiences as part of the Classroom 2000 project. We discuss the issues and challenges involved in designing a system that performs live integration of personal streams of information with multiple other streams of information made available to it through an environment designed to capture public information.\n",
      "=============================\n",
      "Mouse 2.0: multi-touch meets the mouse\n",
      "In this paper we present novel input devices that combine the standard capabilities of a computer mouse with multi-touch sensing. Our goal is to enrich traditional pointer-based desktop interactions with touch and gestures. To chart the design space, we present five different multi-touch mouse implementations. Each explores a different touch sensing strategy, which leads to differing form-factors and hence interactive possibilities. In addition to the detailed description of hardware and software implementations of our prototypes, we discuss the relative strengths, limitations and affordances of these novel input devices as informed by the results of a preliminary user study.\n",
      "=============================\n",
      "Hybrid infrared and visible light projection for location tracking\n",
      "A number of projects within the computer graphics, computer vision, and human-computer interaction communities have recognized the value of using projected structured light patterns for the purposes of doing range finding, location dependent data delivery, projector adaptation, or object discovery and tracking. However, most of the work exploring these concepts has relied on visible structured light patterns resulting in a caustic visual experience. In this work, we present the first design and implementation of a high-resolution, scalable, general purpose invisible near-infrared projector that can be manufactured in a practical manner. This approach is compatible with simultaneous visible light projection and integrates well with future Digital Light Processing (DLP) projector designs -- the most common type of projectors today. By unifying both the visible and non-visible pattern projection into a single device, we can greatly simply the implementation and execution of interactive projection systems. Additionally, we can inherently provide location discovery and tracking capabilities that are unattainable using other approaches.\n",
      "=============================\n",
      "Interactive record/replay for web application debugging\n",
      "During debugging, a developer must repeatedly and manually reproduce faulty behavior in order to inspect different facets of the program's execution. Existing tools for reproducing such behaviors prevent the use of debugging aids such as breakpoints and logging, and are not designed for interactive, random-access exploration of recorded behavior. This paper presents Timelapse, a tool for quickly recording, reproducing, and debugging interactive behaviors in web applications. Developers can use Timelapse to browse, visualize, and seek within recorded program executions while simultaneously using familiar debugging tools such as breakpoints and logging. Testers and end-users can use Timelapse to demonstrate failures in situ and share recorded behaviors with developers, improving bug report quality by obviating the need for detailed reproduction steps. Timelapse is built on Dolos, a novel record/replay infrastructure that ensures deterministic execution by capturing and reusing program inputs both from the user and from external sources such as the network. Dolos introduces negligible overhead and does not interfere with breakpoints and logging. In a small user evaluation, participants used Timelapse to accelerate existing reproduction activities, but were not significantly faster or more successful in completing the larger tasks at hand. Together, the Dolos infrastructure and Timelapse developer tool support systematic bug reporting and debugging practices.\n",
      "=============================\n",
      "Dynamic ambient lighting for mobile devices\n",
      "The information a small mobile device can show via its display has been always limited by its size. In large information spaces, relevant information, such as important locations on a map can get clipped when a user starts zooming and panning. Dynamic ambient lighting allows mobile devices to visualize off-screen objects by illuminating the background without compromising valuable display space. The lighted spots can be used to show the direction and distance of such objects by varying the spot's position and intensity. Dynamic ambient lighting also provides a new way of displaying the state of a mobile device. Illumination is provided by a prototype rear of device shell which contains LEDs and requires the device to be placed on a surface, such as a table or desk.\n",
      "=============================\n",
      "Pen-top feedback for paper-based interfaces\n",
      "Current paper-based interfaces such as PapierCraft, provide very little feedback and this limits the scope of possible interactions. So far, there has been little systematic exploration of the structure, constraints, and contingencies of feedback-mechanisms in paper-based interaction systems for paper-only environments. We identify three levels of feedback: discovery feedback (e.g., to aid with menu learning), status-indication feedback (e.g., for error detection), and task feedback (e.g., to aid in a search task). Using three modalities (visual, tactile, and auditory) which can be easily implemented on a pen-sized computer, we introduce a conceptual matrix to guide systematic research on pen-top feedback for paper-based interfaces. Using this matrix, we implemented a multimodal pen prototype demonstrating the potential of our approach. We conducted an experiment that confirmed the efficacy of our design in helping users discover a new interface and identify and correct their errors.\n",
      "=============================\n",
      "GLEAN: a computer-based tool for rapid GOMS model usability evaluation of user interface designs\n",
      "Engineering models of human performance permit some aspects of usability of interface designs to be predicted from an analysis of the task, and thus can replace to some extent expensive user testing data. The best developed such tools are GOMS models, which have been shown to be accurate and effective in predicting usability of the procedural aspects of interface designs. This paper describes a computer-based tool, GLEAN, that generates quantitative predictions from a supplied GOMS model and a set of benchmark tasks. GLEAN is demonstrated to reproduce the results of a case study of GOMS model application with considerable time savings over both manual modeling as well as empirical testing.\n",
      "=============================\n",
      "Bringing physics to the surface\n",
      "This paper explores the intersection of emerging surface technologies, capable of sensing multiple contacts and of-ten shape information, and advanced games physics engines. We define a technique for modeling the data sensed from such surfaces as input within a physics simulation. This affords the user the ability to interact with digital objects in ways analogous to manipulation of real objects. Our technique is capable of modeling both multiple contact points and more sophisticated shape information, such as the entire hand or other physical objects, and of mapping this user input to contact forces due to friction and collisions within the physics simulation. This enables a variety of fine-grained and casual interactions, supporting finger-based, whole-hand, and tangible input. We demonstrate how our technique can be used to add real-world dynamics to interactive surfaces such as a vision-based tabletop, creating a fluid and natural experience. Our approach hides from application developers many of the complexities inherent in using physics engines, allowing the creation of applications without preprogrammed interaction behavior or gesture recognition.\n",
      "=============================\n",
      "ThinSight: versatile multi-touch sensing for thin form-factor displays\n",
      "ThinSight is a novel optical sensing system, fully integrated into a thin form factor display, capable of detecting multi-ple fingers placed on or near the display surface. We describe this new hardware in detail, and demonstrate how it can be embedded behind a regular LCD, allowing sensing without degradation of display capability. With our approach, fingertips and hands are clearly identifiable through the display. The approach of optical sensing also opens up the exciting possibility for detecting other physical objects and visual markers through the display, and some initial experiments are described. We also discuss other novel capabilities of our system: interaction at a distance using IR pointing devices, and IR-based communication with other electronic devices through the display. A major advantage of ThinSight over existing camera and projector based optical systems is its compact, thin form-factor making such systems even more deployable. We therefore envisage using ThinSight to capture rich sensor data through the display which can be processed using computer vision techniques to enable both multi-touch and tangible interaction.\n",
      "=============================\n",
      "Brain-based target expansion\n",
      "The bubble cursor is a promising cursor expansion technique, improving a user's movement time and accuracy in pointing tasks. We introduce a brain-based target expansion system, which improves the efficacy of bubble cursor by increasing the expansion of high importance targets at the optimal time based on brain measurements correlated to a particular type of multitasking. We demonstrate through controlled experiments that brain-based target expansion can deliver a graded and continuous level of assistance to a user according to their cognitive state, thereby improving task and speed-accuracy metrics, even without explicit visual changes to the system. Such an adaptation is ideal for use in complex systems to steer users toward higher priority goals during times of increased demand.\n",
      "=============================\n",
      "deForm: an interactive malleable surface for capturing 2.5D arbitrary objects, tools and touch\n",
      "We introduce a novel input device, deForm, that supports 2.5D touch gestures, tangible tools, and arbitrary objects through real-time structured light scanning of a malleable surface of interaction. DeForm captures high-resolution surface deformations and 2D grey-scale textures of a gel surface through a three-phase structured light 3D scanner. This technique can be combined with IR projection to allow for invisible capture, providing the opportunity for co-located visual feedback on the deformable surface. We describe methods for tracking fingers, whole hand gestures, and arbitrary tangible tools. We outline a method for physically encoding fiducial marker information in the height map of tangible tools. In addition, we describe a novel method for distinguishing between human touch and tangible tools, through capacitive sensing on top of the input surface. Finally we motivate our device through a number of sample applications.\n",
      "=============================\n",
      "A viewer for PostScript documents\n",
      "We describe a PostScript viewer that provides navigation and annotation functionality similar to that of paper documents using simple unified user-interface techniques.\n",
      "=============================\n",
      "Low-cost multi-touch sensing through frustrated total internal reflection\n",
      "This paper describes a simple, inexpensive, and scalable technique for enabling high-resolution multi-touch sensing on rear-projected interactive surfaces based on frustrated total internal reflection. We review previous applications of this phenomenon to sensing, provide implementation details, discuss results from our initial prototype, and outline future directions.\n",
      "=============================\n",
      "Using fNIRS brain sensing in realistic HCI settings: experiments and guidelines\n",
      "Because functional near-infrared spectroscopy (fNIRS) eases many of the restrictions of other brain sensors, it has potential to open up new possibilities for HCI research. From our experience using fNIRS technology for HCI, we identify several considerations and provide guidelines for using fNIRS in realistic HCI laboratory settings. We empirically examine whether typical human behavior (e.g. head and facial movement) or computer interaction (e.g. keyboard and mouse usage) interfere with brain measurement using fNIRS. Based on the results of our study, we establish which physical behaviors inherent in computer usage interfere with accurate fNIRS sensing of cognitive state information, which can be corrected in data analysis, and which are acceptable. With these findings, we hope to facilitate further adoption of fNIRS brain sensing technology in HCI research.\n",
      "=============================\n",
      "Sikuli: using GUI screenshots for search and automation\n",
      "We present Sikuli, a visual approach to search and automation of graphical user interfaces using screenshots. Sikuli allows users to take a screenshot of a GUI element (such as a toolbar button, icon, or dialog box) and query a help system using the screenshot instead of the element's name. Sikuli also provides a visual scripting API for automating GUI interactions, using screenshot patterns to direct mouse and keyboard events. We report a web-based user study showing that searching by screenshot is easy to learn and faster to specify than keywords. We also demonstrate several automation tasks suitable for visual scripting, such as map navigation and bus tracking, and show how visual scripting can improve interactive help systems previously proposed in the literature.\n",
      "=============================\n",
      "Tapping and rubbing: exploring new dimensions of tactile feedback with voice coil motors\n",
      "Tactile feedback allows devices to communicate with users when visual and auditory feedback are inappropriate. Unfortunately, current vibrotactile feedback is abstract and not related to the content of the message. This often clash-es with the nature of the message, for example, when sending a comforting message. We propose addressing this by extending the repertoire of haptic notifications. By moving an actuator perpendicular to the user's skin, our prototype device can tap the user. Moving the actuator parallel to the user's skin induces rub-bing. Unlike traditional vibrotactile feedback, tapping and rubbing convey a distinct emotional message, similar to those induced by human-human touch. To enable these techniques we built a device we call soundTouch. It translates audio wave files into lateral motion using a voice coil motor found in computer hard drives. SoundTouch can produce motion from below 1Hz to above 10kHz with high precision and fidelity. We present the results of two exploratory studies. We found that participants were able to distinguish a range of taps and rubs. Our findings also indicate that tapping and rubbing are perceived as being similar to touch interactions exchanged by humans.\n",
      "=============================\n",
      "Windows on the world: 2D windows for 3D augmented reality\n",
      "INTRODUCTION We describe the design and implementation of a prototype When we think of the use of head-mounted displays and 3D heads-up window system intended for use in a 3D environinteraction devices to present virtual worlds, it is often in ment. Our system includes a see-through head-mounted terms of environments populated solely by 3D objects. display that runs a full X server whose image is overlaid on There are many situations, however, in which 2D text and the user’s view of the physical world. The user’s head is graphics of the sort supported by current window systems tracked so that the display indexes into a large X bitmap, can be useful components of these environments. This is effectively placing the user inside a display space that is especially true in the case of the many applications that run mapped onto part of a surrounding virtual sphere. By under an industry standard window system such as X [13]. tracking the user’s body, and interpreting head motion relaWhile we might imagine porting or enhancing a significant tive to it, we create a portable information surround that X application to take advantage of the 3D capabilities of a envelopes the user as they move about. virtual world, the effort and cost may not be worth the return, especially if the application is inherently 2D. We support three kinds of windows implemented on top of Therefore, we have been exploring how we can incorporate the X server: windows fixed to the head-mounted display, an existing 2D window system within a 3D virtual world. windows fixed to the information surround, and windows fixed to locations and objects in the 3D world. Objects can We are building an experimental system that supports a full also be tracked, allowing windows to move with them. To X11 server on a see-through head-mounted display. Our demonstrate the utility of this model, we describe a small display overlays a selected portion of the X bitmap on the hypermedia system that allows links to be made between user’s view of the world, creating an X-based augmented windows and windows to be attached to objects. Thus, our reality. Depending on the situation and application, the hypermedia system can forge links between any combinauser may wish to treat a window as a stand-alone entity or tion of physical objects and virtual windows. to take advantage of the potential relationships that can be made between it and the visible physical world. To make this possible, we have developed facilities that allow X\n",
      "=============================\n",
      "Sketch-editing games: human-machine communication, game theory and applications\n",
      "We study uncertainty in graphical-based interaction (with special attention to sketches). We argue that a comprehensive model for the problem must include the interaction participants (and their current beliefs), their possible actions and their past sketches. It's yet unclear how to frame and solve the former problem, considering all the latter elements. We suggest framing the problem as a game and solving it with a game-theoretical solution, which leads to a framework for the design of new two-way, sketch-based user interfaces. In special, we use the framework to design a game that can progressively learn visual models of objects from user sketches, and use the models in real-world interactions. Instead of an abstract visual criterion, players in this game learn models to optimize interaction (the game's duration). This two-way sketching game addresses problems essential in emerging interfaces (such as learning and how to deal with interpretation errors). We review possible applications in robotic sketch-to-command, hand gesture recognition, media authoring and visual search, and evaluate two. Evaluations demonstrate how players improve performance with repeated play, and the influence of interaction aspects on learning.\n",
      "=============================\n",
      "Mnemonic rendering: an image-based approach for exposing hidden changes in dynamic displays\n",
      "Managing large amounts of dynamic visual information involves understanding changes happening out of the user's sight. In this paper, we show how current software does not adequately support users in this task, and motivate the need for a more general approach. We propose an image-based storage, visualization, and implicit interaction paradigm called mnemonic rendering that provides better support for handling visual changes. Once implemented on a system, mnemonic rendering techniques can benefit all applications. We explore its rich design space and discuss its expected benefits as well as limitations based on feedback from users of a small-screen and a wall-size prototype.\n",
      "=============================\n",
      "StackBlock: block-shaped interface for flexible stacking\n",
      "We propose a novel building-block interface called StackBlock that allows users to precisely construct 3D shapes by stacking blocks at arbitrary positions and angles. Infrared LEDs and phototransistors are laid in a matrix on each surface of a block to detect the areas contacted by other blocks. Contact-area information is transmitted to the bottom block by the relay of infrared communication between the stacked blocks, and then the bottom block sends all information to the host computer for recognizing the 3D shape. We implemented a prototype of StackBlock with several blocks and evaluated the accuracy and latency of 3D shape recognition. As a result, StackBlock could sufficiently perform 3D shape recognition for users' flexible stacking.\n",
      "=============================\n",
      "Changing how people view changes on the web\n",
      "The Web is a dynamic information environment. Web content changes regularly and people revisit Web pages frequently. But the tools used to access the Web, including browsers and search engines, do little to explicitly support these dynamics. In this paper we present DiffIE, a browser plug-in that makes content change explicit in a simple and lightweight manner. DiffIE caches the pages a person visits and highlights how those pages have changed when the person returns to them. We describe how we built a stable, reliable, and usable system, including how we created compact, privacy-preserving page representations to support fast difference detection. Via a longitudinal user study, we explore how DiffIE changed the way people dealt with changing content. We find that much of its benefit came not from exposing expected change, but rather from drawing attention to unexpected change and helping people build a richer understanding of the Web content they frequent.\n",
      "=============================\n",
      "Enabling always-available input with muscle-computer interfaces\n",
      "Previous work has demonstrated the viability of applying offline analysis to interpret forearm electromyography (EMG) and classify finger gestures on a physical surface. We extend those results to bring us closer to using muscle-computer interfaces for always-available input in real-world applications. We leverage existing taxonomies of natural human grips to develop a gesture set covering interaction in free space even when hands are busy with other objects. We present a system that classifies these gestures in real-time and we introduce a bi-manual paradigm that enables use in interactive systems. We report experimental results demonstrating four-finger classification accuracies averaging 79% for pinching, 85% while holding a travel mug, and 88% when carrying a weighted bag. We further show generalizability across different arm postures and explore the tradeoffs of providing real-time visual feedback.\n",
      "=============================\n",
      "Clip-on gadgets: expanding multi-touch interaction area with unpowered tactile controls\n",
      "Virtual keyboards and controls, commonly used on mobile multi-touch devices, occlude content of interest and do not provide tactile feedback. Clip-on Gadgets solve these issues by extending the interaction area of multi-touch devices with physical controllers. Clip-on Gadgets use only conductive materials to map user input on the controllers to touch points on the edges of screens; therefore, they are battery-free, lightweight, and low-cost. In addition, they can be used in combination with multi-touch gestures. We present several hardware designs and a software toolkit, which enable users to simply attach Clip-on Gadgets to an edge of a device and start interacting with it.\n",
      "=============================\n",
      "Bonfire: a nomadic system for hybrid laptop-tabletop interaction\n",
      "We present Bonfire, a self-contained mobile computing system that uses two laptop-mounted laser micro-projectors to project an interactive display space to either side of a laptop keyboard. Coupled with each micro-projector is a camera to enable hand gesture tracking, object recognition, and information transfer within the projected space. Thus, Bonfire is neither a pure laptop system nor a pure tabletop system, but an integration of the two into one new nomadic computing platform. This integration (1) enables observing the periphery and responding appropriately, e.g., to the casual placement of objects within its field of view, (2) enables integration between physical and digital objects via computer vision, (3) provides a horizontal surface in tandem with the usual vertical laptop display, allowing direct pointing and gestures, and (4) enlarges the input/output space to enrich existing applications. We describe Bonfire's architecture, and offer scenarios that highlight Bonfire's advantages. We also include lessons learned and insights for further development and use.\n",
      "=============================\n",
      "YouMove: enhancing movement training with an augmented reality mirror\n",
      "YouMove is a novel system that allows users to record and learn physical movement sequences. The recording system is designed to be simple, allowing anyone to create and share training content. The training system uses recorded data to train the user using a large-scale augmented reality mirror. The system trains the user through a series of stages that gradually reduce the user's reliance on guidance and feedback. This paper discusses the design and implementation of YouMove and its interactive mirror. We also present a user study in which YouMove was shown to improve learning and short-term retention by a factor of 2 compared to a traditional video demonstration.\n",
      "=============================\n",
      "Authoring multi-stage code examples with editable code histories\n",
      "Multi-stage code examples present multiple versions of a program where each stage increases the overall complexity of the code. In order to acquire strategies of program construction using a new language or API, programmers consult multi-stage code examples in books, tutorials and online videos. Authoring multi-stage code examples is currently a tedious process, as it involves keeping several stages of code synchronized in the face of edits and error corrections. We document these difficulties with a formative study examining how programmers author multi-stage code examples. We then present an IDE extension that helps authors create multi-stage code examples by propagating changes (insertions, deletions and modifications) to multiple saved versions of their code. Our system adapts revision control algorithms to the specific task of evolving example code. An informal evaluation finds that taking snapshots of a program as it is being developed and editing these snapshots in hindsight help users in creating multi-stage code examples.\n",
      "=============================\n",
      "inFORM: dynamic physical affordances and constraints through shape and object actuation\n",
      "Past research on shape displays has primarily focused on rendering content and user interface elements through shape output, with less emphasis on dynamically changing UIs. We propose utilizing shape displays in three different ways to mediate interaction: to facilitate by providing dynamic physical affordances through shape change, to restrict by guiding users with dynamic physical constraints, and to manipulate by actuating physical objects. We outline potential interaction techniques and introduce Dynamic Physical Affordances and Constraints with our inFORM system, built on top of a state-of-the-art shape display, which provides for variable stiffness rendering and real-time user input through direct touch and tangible interaction. A set of motivating examples demonstrates how dynamic affordances, constraints and object actuation can create novel interaction possibilities.\n",
      "=============================\n",
      "Predictive translation memory: a mixed-initiative system for human language translation\n",
      "The standard approach to computer-aided language translation is post-editing: a machine generates a single translation that a human translator corrects. Recent studies have shown this simple technique to be surprisingly effective, yet it underutilizes the complementary strengths of precision-oriented humans and recall-oriented machines. We present Predictive Translation Memory, an interactive, mixed-initiative system for human language translation. Translators build translations incrementally by considering machine suggestions that update according to the user's current partial translation. In a large-scale study, we find that professional translators are slightly slower in the interactive mode yet produce slightly higher quality translations despite significant prior experience with the baseline post-editing condition. Our analysis identifies significant predictors of time and quality, and also characterizes interactive aid usage. Subjects entered over 99% of characters via interactive aids, a significantly higher fraction than that shown in previous work.\n",
      "=============================\n",
      "Bayesian touch: a statistical criterion of target selection with finger touch\n",
      "To improve the accuracy of target selection for finger touch, we conceptualize finger touch input as an uncertain process, and derive a statistical target selection criterion, Bayesian Touch Criterion, by combining the basic Bayes' rule of probability with the generalized dual Gaussian distribution hypothesis of finger touch. The Bayesian Touch Criterion selects the intended target as the candidate with the shortest Bayesian Touch Distance to the touch point, which is computed from the touch point to the target center distance and the target size. We give the derivation of the Bayesian Touch Criterion and its empirical evaluation with two experiments. The results showed that for 2-dimensional circular target selection, the Bayesian Touch Criterion is significantly more accurate than the commonly used Visual Boundary Criterion (i.e., a target is selected if and only if the touch point falls within its boundary) and its two variants.\n",
      "=============================\n",
      "VB2: an architecture for interaction in synthetic worlds\n",
      "The paper describes the VB2 architecture for the construction of three-dimensional interactive applications. The system's state and behavior are uniformly represented as a network of interrelated objects. Dynamic components are modeled by active variables, while multi-way relations are modeled by hierarchical constraints. Daemons are used to sequence between system states in reaction to changes in variable values. The constraint network is efficiently maintained by an incremental constraint solver based on an enhancement of SkyBlue. Multiple devices are used to interact with the synthetic world through the use of various interaction paradigms, including immersive environments with visual and audio feedback. Interaction techniques range from direct manipulation, to gestural input and three-dimensional virtual tools. Adaptive pattern recognition is used to increase input device expressiveness by enhancing sensor data with classification information. Virtual tools, which are encapsulations of visual appearance and behavior, present a selective view of manipulated models' information and offer an interaction metaphor to control it. Since virtual tools are first class objects, they can be assembled into more complex tools, much in the same way that simple tools are built on top of a modeling hierarchy. The architecture is currently being used to build a virtual reality animation system\n",
      "=============================\n",
      "Real-time collaborative coding in a web IDE\n",
      "This paper describes Collabode, a web-based Java integrated development environment designed to support close, synchronous collaboration between programmers. We examine the problem of collaborative coding in the face of program compilation errors introduced by other users which make collaboration more difficult, and describe an algorithm for error-mediated integration of program code. Concurrent editors see the text of changes made by collaborators, but the errors reported in their view are based only on their own changes. Editors may run the program at any time, using only error-free edits supplied so far, and ignoring incomplete or otherwise error-generating changes. We evaluate this algorithm and interface on recorded data from previous pilot experiments with Collabode, and via a user study with student and professional programmers. We conclude that it offers appreciable benefits over naive continuous synchronization without regard to errors and over manual version control.\n",
      "=============================\n",
      "EM-Sense: Touch Recognition of Uninstrumented, Electrical and Electromechanical Objects\n",
      "Most everyday electrical and electromechanical objects emit small amounts of electromagnetic (EM) noise during regular operation. When a user makes physical contact with such an object, this EM signal propagates through the user, owing to the conductivity of the human body. By modifying a small, low-cost, software-defined radio, we can detect and classify these signals in real-time, enabling robust on-touch object detection. Unlike prior work, our approach requires no instrumentation of objects or the environment; our sensor is self-contained and can be worn unobtrusively on the body. We call our technique EM-Sense and built a proof-of-concept smartwatch implementation. Our studies show that discrimination between dozens of objects is feasible, independent of wearer, time and local environment.\n",
      "=============================\n",
      "PrintScreen: fabricating highly customizable thin-film touch-displays\n",
      "PrintScreen is an enabling technology for digital fabrication of customized flexible displays using thin-film electroluminescence (TFEL). It enables inexpensive and rapid fabrication of highly customized displays in low volume, in a simple lab environment, print shop or even at home. We show how to print ultra-thin (120 µm) segmented and passive matrix displays in greyscale or multi-color on a variety of deformable and rigid substrate materials, including PET film, office paper, leather, metal, stone, and wood. The displays can have custom, unconventional 2D shapes and can be bent, rolled and folded to create 3D shapes. We contribute a systematic overview of graphical display primitives for customized displays and show how to integrate them with static print and printed electronics. Furthermore, we contribute a sensing framework, which leverages the display itself for touch sensing. To demonstrate the wide applicability of PrintScreen, we present application examples from ubiquitous, mobile and wearable computing.\n",
      "=============================\n",
      "A design space analysis of availability-sharing systems\n",
      "Workplace collaboration often requires interruptions, which can happen at inopportune times. Designing a successful availability-sharing system requires finding the right balance to optimize the benefits and reduce costs for both the interrupter and interruptee. In this paper, we examine the design space of availability-sharing systems and identify six relevant design dimensions: abstraction, presentation, information delivery, symmetry, obtrusiveness and temporal gradient. We describe these dimensions in terms of the tensions between interrupters and interruptees revealed in previous studies of workplace collaboration and deployments of awareness systems. As a demonstration of the utility of our design space, we introduce InterruptMe, a novel availability-sharing system that represents a previously unexplored point in the design space and that balances the tensions between interrupters and interruptees. InterruptMe differs from previous systems in that it displays availability information only when needed by monitoring implicit inputs from the system's users, implements a traceable asymmetry structure, and introduces the notion of per-communications channel availability.\n",
      "=============================\n",
      "Capacitive fingerprinting: exploring user differentiation by sensing electrical properties of the human body\n",
      "At present, touchscreens can differentiate multiple points of contact, but not who is touching the device. In this work, we consider how the electrical properties of humans and their attire can be used to support user differentiation on touchscreens. We propose a novel sensing approach based on Swept Frequency Capacitive Sensing, which measures the impedance of a user to the environment (i.e., ground) across a range of AC frequencies. Different people have different bone densities and muscle mass, wear different footwear, and so on. This, in turn, yields different impedance profiles, which allows for touch events, including multitouch gestures, to be attributed to a particular user. This has many interesting implications for interactive design. We describe and evaluate our sensing approach, demonstrating that the technique has considerable promise. We also discuss limitations, how these might be overcome, and next steps.\n",
      "=============================\n",
      "Designer's augmented reality toolkit, ten years later: implications for new media authoring tools\n",
      "The Designer's Augmented Reality Toolkit (DART) was an augmented (AR) and mixed reality (MR) authoring tool targeted at new media designers. It was released in 2003 and was heavily used by a diverse population of creators for the next several years [28]. Ten years later, we approached a group of users to collect reflections on their use of DART, the artifacts they produced, their subsequent AR/MR authoring, their thoughts on the challenges of AR/MR authoring in general, and the state of modern tools. In this paper we present the findings from in-depth interviews with these DART developers and other AR experts. Their reflections provide insights on how to successfully engage non-technologists with new media and the challenges they face during authoring, the unique requirements of new media authoring, and how modern tools are still not meeting the needs of this type of author, highlighting where additional research is needed.\n",
      "=============================\n",
      "Carpus: a non-intrusive user identification technique for interactive surfaces\n",
      "Interactive surfaces have great potential for co-located collaboration because of their ability to track multiple inputs simultaneously. However, the multi-user experience on these devices could be enriched significantly if touch points could be associated with a particular user. Existing approaches to user identification are intrusive, require users to stay in a fixed position, or suffer from poor accuracy. We present a non-intrusive, high-accuracy technique for mapping touches to their corresponding user in a collaborative environment. By mounting a high-resolution camera above the interactive surface, we are able to identify touches reliably without any extra instrumentation, and users are able to move around the surface at will. Our technique, which leverages the back of users' hands as identifiers, supports walk-up-and-use situations in which multiple people interact on a shared surface.\n",
      "=============================\n",
      "ILoveSketch: as-natural-as-possible sketching system for creating 3d curve models\n",
      "We present ILoveSketch, a 3D curve sketching system that captures some of the affordances of pen and paper for professional designers, allowing them to iterate directly on concept 3D curve models. The system coherently integrates existing techniques of sketch-based interaction with a number of novel and enhanced features. Novel contributions of the system include automatic view rotation to improve curve sketchability, an axis widget for sketch surface selection, and implicitly inferred changes between sketching techniques. We also improve on a number of existing ideas such as a virtual sketchbook, simplified 2D and 3D view navigation, multi-stroke NURBS curve creation, and a cohesive gesture vocabulary. An evaluation by a professional designer shows the potential of our system for deployment within a real design process.\n",
      "=============================\n",
      "Surround-see: enabling peripheral vision on smartphones during active use\n",
      "Mobile devices are endowed with significant sensing capabilities. However, their ability to 'see' their surroundings, during active use, is limited. We present Surround-See, a self-contained smartphone equipped with an omni-directional camera that enables peripheral vision around the device to augment daily mobile tasks. Surround-See provides mobile devices with a field-of-view collinear to the device screen. This capability facilitates novel mobile tasks such as, pointing at objects in the environment to interact with content, operating the mobile device at a physical distance and allowing the device to detect user activity, even when the user is not holding it. We describe Surround-See's architecture, and demonstrate applications that exploit peripheral 'seeing' capabilities during active use of a mobile device. Users confirm the value of embedding peripheral vision capabilities on mobile devices and offer insights for novel usage methods.\n",
      "=============================\n",
      "Lumitrack: low cost, high precision, high speed tracking with projected m-sequences\n",
      "We present Lumitrack, a novel motion tracking technology that uses projected structured patterns and linear optical sensors. Each sensor unit is capable of recovering 2D location within the projection area, while multiple sensors can be combined for up to six degree of freedom (DOF) tracking. Our structured light approach is based on special patterns, called m-sequences, in which any consecutive sub-sequence of m bits is unique. Lumitrack can utilize both digital and static projectors, as well as scalable embedded sensing configurations. The resulting system enables high-speed, high precision, and low-cost motion tracking for a wide range of interactive applications. We detail the hardware, operation, and performance characteristics of our approach, as well as a series of example applications that highlight its immediate feasibility and utility.\n",
      "=============================\n",
      "Portico: tangible interaction on and around a tablet\n",
      "We present Portico, a portable system for enabling tangible interaction on and around tablet computers. Two cameras on small foldable arms are positioned above the display to recognize a variety of physical objects placed on or around the tablet. These cameras have a larger field-of-view than the screen, allowing Portico to extend interaction significantly beyond the tablet itself. Our prototype, which uses a 12\" tablet, delivers an interaction space six times the size of the tablet screen. Portico thus allows tablets to extend both their sensing capabilities and interaction space without sacrificing portability. We describe the design of our system and present a number of applications that demonstrate Portico's unique capability to track objects. We focus on a number of fun applications that demonstrate how such a device can be used as a low-cost way to create personal surface computing experiences. Finally, we discuss the challenges in supporting tangible interaction beyond the screen and describe possible mechanisms for overcoming them.\n",
      "=============================\n",
      "A perceptually-supported sketch editor\n",
      "The human visual system makes a great deal more of images than the elemental marks on a surface. In the course of viewing, creating, or editing a picture, we actively construct a host of visual structures and relationships as components of sensible interpretations. This paper shows how some of these computational processes can be incorporated into perceptually-supported image editing tools, enabling machines to better engage users at the level of their own percepts. We focus on the domain of freehand sketch editors, such as an electronic whiteboard application for a pen-based computer. By using computer vision techniques to perform covert recognition of visual structure as it emerges during the course of a drawing/editing session, a perceptually supported image editor gives users access to visual objects as they are perceived by the human visual system. We present a flexible image interpretation architecture based on token grouping in a multiscale blackboard data structure. This organization supports multiple perceptual interpretations of line drawing data, domain-specific knowledge bases for interpretable visual structures, and gesture-based selection of visual objects. A system implementing these ideas, called PerSketch, begins to explore a new space of WYPIWYG (What You Perceive Is What You Get) image editing tools.\n",
      "=============================\n",
      "Fix and float: object movement by egocentric navigation\n",
      "c The two traditional techniques for moving objects in graphical workspaces are dragging and cut and paste. Each method has some disadvantages. We introduce a new method, calledfixandfloat, formoving objects in graphical workspaces. The new methodfies the object(s) to the gaze or viewpoint, thereby letting the user move objects implicitly while,doing egocentric navigation. We describe the advantages this new method has over previous techniques, and give an example of its use in a 3D graphical workspace. )’ .:,\n",
      "=============================\n",
      "Printed optics: 3D printing of embedded optical elements for interactive devices\n",
      "We present an approach to 3D printing custom optical elements for interactive devices labelled Printed Optics. Printed Optics enable sensing, display, and illumination elements to be directly embedded in the casing or mechanical structure of an interactive device. Using these elements, unique display surfaces, novel illumination techniques, custom optical sensors, and embedded optoelectronic components can be digitally fabricated for rapid, high fidelity, highly customized interactive devices. Printed Optics is part of our long term vision for interactive devices that are 3D printed in their entirety. In this paper we explore the possibilities for this vision afforded by fabrication of custom optical elements using today's 3D printing technology.\n",
      "=============================\n",
      "Overview based example selection in end user interactive concept learning\n",
      "Interaction with large unstructured datasets is difficult because existing approaches, such as keyword search, are not always suited to describing concepts corresponding to the distinctions people want to make within datasets. One possible solution is to allow end users to train machine learning systems to identify desired concepts, a strategy known as interactive concept learning. A fundamental challenge is to design systems that preserve end user flexibility and control while also guiding them to provide examples that allow the machine learning system to effectively learn the desired concept. This paper presents our design and evaluation of four new overview based approaches to guiding example selection. We situate our explorations within CueFlik, a system examining end user interactive concept learning in Web image search. Our evaluation shows our approaches not only guide end users to select better training examples than the best performing previous design for this application, but also reduce the impact of not knowing when to stop training the system. We discuss challenges for end user interactive concept learning systems and identify opportunities for future research on the effective design of such systems.\n",
      "=============================\n",
      "ScreenCrayons: annotating anything\n",
      "ScreenCrayons is a system for collecting annotations on any type of document or visual information from any application. The basis for the system is a screen capture upon which the user can highlight the relevant portions of the image. The user can define any number of topics for organizing notes. Each topic is associated with a highlighting \"crayon.\" In addition the user can supply annotations in digital ink or text. Algorithms are described that summarize captured images based on the highlight strokes so as to provide overviews of many annotations as well as being able to \"zoom in\" on particular information about a given note and the context of that note.\n",
      "=============================\n",
      "TurKit: human computation algorithms on mechanical turk\n",
      "Mechanical Turk (MTurk) provides an on-demand source of human computation. This provides a tremendous opportunity to explore algorithms which incorporate human computation as a function call. However, various systems challenges make this difficult in practice, and most uses of MTurk post large numbers of independent tasks. TurKit is a toolkit for prototyping and exploring algorithmic human computation, while maintaining a straight-forward imperative programming style. We present the crash-and-rerun programming model that makes TurKit possible, along with a variety of applications for human computation algorithms. We also present case studies of TurKit used for real experiments across different fields.\n",
      "=============================\n",
      "Systematic output modification in a 2D user interface toolkit\n",
      "In this paper we present a simple but general set of techniques for modifying output in a 2D user interface toolkit. We use a combination of simple subclassing, wrapping, and collusion between parent and output objects to produce arbitrary sets of composable output transformations. The techniques described here allow rich output effects to be added to most, if not all, existing interactors in an application, without the knowledge of the interactors themselves. This paper explains how the approach works, discusses a number of example effects that have been built, and describes how the techniques presented here could be extended to work with other toolkits. We address issues of input by examining a number of extensions to the toolkit input subsystem to accommodate transformed graphical output. Our approach uses a set of “hooks” to undo output transformations when input is to be dispatched.\n",
      "=============================\n",
      "PlayAnywhere: a compact interactive tabletop projection-vision system\n",
      "We introduce PlayAnywhere, a front-projected computer vision-based interactive table system which uses a new commercially available projection technology to obtain a compact, self-contained form factor. PlayAnywhere's configuration addresses installation, calibration, and portability issues that are typical of most vision-based table systems, and thereby is particularly motivated in consumer applications. PlayAnywhere also makes a number of contributions related to image processing techniques for front-projected vision-based table systems, including a shadow-based touch detection algorithm, a fast, simple visual bar code scheme tailored to projection-vision table systems, the ability to continuously track sheets of paper, and an optical flow-based algorithm for the manipulation of onscreen objects that does not rely on fragile tracking algorithms.\n",
      "=============================\n",
      "HybridPointing: fluid switching between absolute and relative pointing with a direct input device\n",
      "We present HybridPointing, a technique that lets users easily switch between absolute and relative pointing with a direct input device such as a pen. Our design includes a new graphical element, the Trailing Widget, which remains \"close at hand\" but does not interfere with normal cursor operation. The use of visual feedback to aid the user's understanding of input state is discussed, and several novel visual aids are presented. An experiment conducted on a large, wall-sized display validates the benefits of HybridPointing under certain conditions. We also discuss other situations in which HybridPointing may be useful. Finally, we present an extension to our technique that allows for switching between absolute and relative input in the middle of a single drag-operation.\n",
      "=============================\n",
      "InterTwine: creating interapplication information scent to support coordinated use of software\n",
      "Users often make continued and sustained use of online resources to complement use of a desktop application. For example, users may reference online tutorials to recall how to perform a particular task. While often used in a coordinated fashion, the browser and desktop application provide separate, independent mechanisms for helping users find and re-find task-relevant information. In this paper, we describe InterTwine, a system that links information in the web browser with relevant elements in the desktop application to create interapplication information scent. This explicit link produces a shared interapplication history to assist in re-finding information in both applications. As an example, InterTwine marks all menu items in the desktop application that are currently mentioned in the front-most web page. This paper introduces the notion of interapplication information scent, demonstrates the concept in InterTwine, and describes results from a formative study suggesting the utility of the concept.\n",
      "=============================\n",
      "ConstraintJS: programming interactive behaviors for the web by integrating constraints and states\n",
      "Interactive behaviors in GUIs are often described in terms of states, transitions, and constraints, where the constraints only hold in certain states. These constraints maintain relationships among objects, control the graphical layout, and link the user interface to an underlying data model. However, no existing Web implementation technology provides direct support for all of these, so the code for maintaining constraints and tracking state may end up spread across multiple languages and libraries. In this paper we describe ConstraintJS, a system that integrates constraints and finite-state machines (FSMs) with Web languages. A key role for the FSMs is to enable and disable constraints based on the interface's current mode, making it possible to write constraints that sometimes hold. We illustrate that constraints combined with FSMs can be a clearer way of defining many interactive behaviors with a series of examples.\n",
      "=============================\n",
      "The rise of the expert amateur: DIY culture and citizen science\n",
      "We are at an important technological inflection point. Most of our computing systems have been designed and built by professionally trained experts (i.e. us--computer scientists, engineers, and designers) for use in specific domains and to solve explicit problems. Artifacts often called \"user manuals\" traditionally prescribed the appropriate usage of these tools and implied an acceptable etiquette for interaction and experience. A fringe group of individuals usually labeled \"hackers\" or \"nerds\" have challenged this producer-consumer model of technology by hacking novel hardware and software features to \"improve\" our research and products while a similar creative group of technicians called \"artists\" have re-directed the techniques, tools, and tenets of accepted technological usage away from their typical manifestations in practicality and product. Over time the technological artifacts of these fringe groups and the support for their rhetoric have gained them a foothold into computing culture and eroded the established power discontinuities within the practice of computing research. We now expect our computing tools to be driven by an architecture of open participation and democracy that encourages users to add value to their tools and applications as they use them. Similarly, the bar for enabling the design of novel, personal computing systems and \"hardware remixes\" has fallen to the point where many non-experts and novices are readily embracing and creating fascinating and ingenious computing artifacts outside of our official and traditionally sanctioned academic research communities. But how have we as \"expert\" practitioners been influencing this discussion? By constructing a practice around the design and development of technology for task based and problem solving applications, we have unintentionally established such work as the status quo for the human computing experience. We have failed in our duty to open up alternate forums for technology to express itself and touch our lives beyond productivity and efficiency. Blinded by our quest for \"smart technologies\" we have forgotten to contemplate the design of technologies to inspire us to be smarter, more curious, and more inquisitive. We owe it to ourselves to rethink the impact we desire to have on this historic moment in computing culture. We must choose to participate in and perhaps lead a dialogue that heralds an expansive new acceptable practice of designing to enable participation by experts and non-experts alike. We are in the milieu of the rise of the \"expert amateur\". We must change our mantra: \"not just usability but usefulness and relevancy to our world, its citizens, and our environment\". We must design for the world and what matters. This means discussing our computing research alongside new keywords such as the economy, the environment, activism, poverty, healthcare, famine, homelessness, literacy, religion, and politics. This talk will explore the design territory and potential opportunities for all of us to collaborate and benefit as a society from this cultural movement.\n",
      "=============================\n",
      "CrowdLearner: rapidly creating mobile recognizers using crowdsourcing\n",
      "Mobile applications can offer improved user experience through the use of novel modalities and user context. However, these new input dimensions often require recognition-based techniques, with which mobile app developers or designers may not be familiar. Furthermore, the recruiting, data collection and labeling, necessary for using these techniques, are usually time-consuming and expensive. We present CrowdLearner, a framework based on crowdsourcing to automatically generate recognizers using mobile sensor input such as accelerometer or touchscreen readings. CrowdLearner allows a developer to easily create a recognition task, distribute it to the crowd, and monitor its progress as more data becomes available. We deployed CrowdLearner to a crowd of 72 mobile users over a period of 2.5 weeks. We evaluated the system by experimenting with 6 recognition tasks concerning motion gestures, touchscreen gestures, and activity recognition. The experimental results indicated that CrowdLearner enables a developer to quickly acquire a usable recognizer for their specific application by spending a moderate amount of money, often less than $10, in a short period of time, often in the order of 2 hours. Our exploration also revealed challenges and provided insights into the design of future crowdsourcing systems for machine learning tasks.\n",
      "=============================\n",
      "FoveAR: Combining an Optically See-Through Near-Eye Display with Projector-Based Spatial Augmented Reality\n",
      "Optically see-through (OST) augmented reality glasses can overlay spatially-registered computer-generated content onto the real world. However, current optical designs and weight considerations limit their diagonal field of view to less than 40 degrees, making it difficult to create a sense of immersion or give the viewer an overview of the augmented reality space. We combine OST glasses with a projection-based spatial augmented reality display to achieve a novel display hybrid, called FoveAR, capable of greater than 100 degrees field of view, view dependent graphics, extended brightness and color, as well as interesting combinations of public and personal data display. We contribute details of our prototype implementation and an analysis of the interactive design space that our system enables. We also contribute four prototype experiences showcasing the capabilities of FoveAR as well as preliminary user feedback providing insights for enhancing future FoveAR experiences.\n",
      "=============================\n",
      "Automatic projector calibration with embedded light sensors\n",
      "Projection technology typically places several constraints on the geometric relationship between the projector and the projection surface to obtain an undistorted, properly sized image. In this paper we describe a simple, robust, fast, and low-cost method for automatic projector calibration that eliminates many of these constraints. We embed light sensors in the target surface, project Gray-coded binary patterns to discover the sensor locations, and then prewarp the image to accurately fit the physical features of the projection surface. This technique can be expanded to automatically stitch multiple projectors, calibrate onto non-planar surfaces for object decoration, and provide a method for simple geometry acquisition.\n",
      "=============================\n",
      "User interfaces when and where they are needed: an infrastructure for recombinant computing\n",
      "Users in ubiquitous computing environments need to be able to make serendipitous use of resources that they did not anticipate and of which they have no prior knowledge. The Speakeasy recombinant computing framework is designed to support such ad hoc use of resources on a network. In addition to other facilities, the framework provides an infrastructure through which device and service user interfaces can be made available to users on multiple platforms. The framework enables UIs to be provided for connections involving multiple entities, allows these UIs to be delivered asynchronously, and allows them to be injected by any party participating in a connection.\n",
      "=============================\n",
      "Proton++: a customizable declarative multitouch framework\n",
      "Proton++ is a declarative multitouch framework that allows developers to describe multitouch gestures as regular expressions of touch event symbols. It builds on the Proton framework by allowing developers to incorporate custom touch attributes directly into the gesture description. These custom attributes increase the expressivity of the gestures, while preserving the benefits of Proton: automatic gesture matching, static analysis of conflict detection, and graphical gesture creation. We demonstrate Proton++'s flexibility with several examples: a direction attribute for describing trajectory, a pinch attribute for detecting when touches move towards one another, a touch area attribute for simulating pressure, an orientation attribute for selecting menu items, and a screen location attribute for simulating hand ID. We also use screen location to simulate user ID and enable simultaneous recognition of gestures by multiple users. In addition, we show how to incorporate timing into Proton++ gestures by reporting touch events at a regular time interval. Finally, we present a user study that suggests that users are roughly four times faster at interpreting gestures written using Proton++ than those written in procedural event-handling code commonly used today.\n",
      "=============================\n",
      "RevMiner: an extractive interface for navigating reviews on a smartphone\n",
      "Smartphones are convenient, but their small screens make searching, clicking, and reading awkward. Thus, perusing product reviews on a smartphone is difficult. In response, we introduce RevMiner - a novel smartphone interface that utilizes Natural Language Processing techniques to analyze and navigate reviews. RevMiner was run over 300K Yelp restaurant reviews extracting attribute-value pairs, where attributes represent restaurant attributes such as sushi and service, and values represent opinions about the attributes such as fresh or fast. These pairs were aggregated and used to: 1) answer queries such as \"cheap Indian food\", 2) concisely present information about each restaurant, and 3) identify similar restaurants. Our user studies demonstrate that on a smartphone, participants preferred RevMiner's interface to tag clouds and color bars, and that they preferred RevMiner's results to Yelp's, particularly for conjunctive queries (e.g., \"great food and huge portions\"). Demonstrations of RevMiner are available at revminer.com.\n",
      "=============================\n",
      "A user interface using fingerprint recognition: holding commands and data objects on fingers\n",
      "This paper describes a new user interface, called a fingerprint user interface (FUI), which employs fingerprint recognition. While the unique feature patterns of fingerprints have mainly been used for personal identification, the FUI is unique in that it uses not only the differences among fingerprint patterns of different persons, but also the differences among the ten fingers of a single person. In the FUI, the system identifies the finger that has operated (touched) an input device through pattern matching of fingerprints and it performs the task assigned to the identified finger. Since users are able to specify different tasks by using different fingers, they feel as if commands and data objects were actually held on their fingers.\n",
      "=============================\n",
      "PneUI: pneumatically actuated soft composite materials for shape changing interfaces\n",
      "This paper presents PneUI, an enabling technology to build shape-changing interfaces through pneumatically-actuated soft composite materials. The composite materials integrate the capabilities of both input sensing and active shape output. This is enabled by the composites' multi-layer structures with different mechanical or electrical properties. The shape changing states are computationally controllable through pneumatics and pre-defined structure. We explore the design space of PneUI through four applications: height changing tangible phicons, a shape changing mobile, a transformable tablet case and a shape shifting lamp.\n",
      "=============================\n",
      "Clui: a platform for handles to rich objects\n",
      "On the desktop, users are accustomed to having visible handles to objects that they want to organize, share, or manipulate. Web applications today feature many classes of such objects, like flight itineraries, products for sale, people, recipes, and businesses, but there are no interoperable handles for high-level semantic objects that users can grab. This paper proposes Clui, a platform for exploring a new data type, called a Webit, that provides uniform handles to rich objects. Clui uses plugins to 1) create Webits on existing pages by extracting semantic data from those pages, and 2) augmenting existing sites with drag and drop targets that accept and interpret Webits. Users drag and drop Webits between sites to transfer data, auto-fill search forms, map associated locations, or share Webits with others. Clui enables experimentation with handles to semantic objects and the standards that underlie them.\n",
      "=============================\n",
      "Programming by a sample: rapidly creating web applications with d.mix\n",
      "Source-code examples of APIs enable developers to quickly gain a gestalt understanding of a library's functionality, and they support organically creating applications by incrementally modifying a functional starting point. As an increasing number of web sites provide APIs, significantlatent value lies in connecting the complementary representations between site and service - in essence, enabling sites themselves to be the example corpus. We introduce d.mix, a tool for creating web mashups that leverages this site-to-service correspondence. With d.mix, users browse annotated web sites and select elements to sample. d.mix's sampling mechanism generates the underlying service calls that yield those elements. This code can be edited, executed, and shared in d.mix's wiki-based hosting environment. This sampling approach leverages pre-existing web sites as example sets and supports fluid composition and modification of examples. An initial study with eight participants found d.mix to enable rapid experimentation, and suggested avenues for improving its annotation mechanism.\n",
      "=============================\n",
      "Foldable interactive displays\n",
      "Modern computer displays tend to be in fixed size, rigid, and rectilinear rendering them insensitive to the visual area demands of an application or the desires of the user. Foldable displays offer the ability to reshape and resize the interactive surface at our convenience and even permit us to carry a very large display surface in a small volume. In this paper, we implement four interactive foldable display designs using image projection with low-cost tracking and explore display behaviors using orientation sensitivity.\n",
      "=============================\n",
      "A toolkit for managing user attention in peripheral displays\n",
      "Traditionally, computer interfaces have been confined to conventional displays and focused activities. However, as displays become embedded throughout our environment and daily lives, increasing numbers of them must operate on the periphery of our attention. <i>Peripheral displays</i> can allow a person to be aware of information while she is attending to some other primary task or activity. We present the Peripheral Displays Toolkit (PTK), a toolkit that provides structured support for managing user attention in the development of peripheral displays. Our goal is to enable designers to explore different approaches to managing user attention. The PTK supports three issues specific to conveying information on the periphery of human attention. These issues are <i>abstraction</i> of raw input, rules for assigning <i>notification levels</i> to input, and <i>transitions</i> for updating a display when input arrives. Our contribution is the investigation of issues specific to attention in peripheral display design and a toolkit that encapsulates support for these issues. We describe our toolkit architecture and present five sample peripheral displays demonstrating our toolkit's capabilities.\n",
      "=============================\n",
      "Monte carlo methods for managing interactive state, action and feedback under uncertainty\n",
      "Current input handling systems provide effective techniques for modeling, tracking, interpreting, and acting on user input. However, new interaction technologies violate the standard assumption that input is certain. Touch, speech recognition, gestural input, and sensors for context often produce uncertain estimates of user inputs. Current systems tend to remove uncertainty early on. However, information available in the user interface and application can help to resolve uncertainty more appropriately for the end user. This paper presents a set of techniques for tracking the state of interactive objects in the presence of uncertain inputs. These techniques use a Monte Carlo approach to maintain a probabilistically accurate description of the user interface that can be used to make informed choices about actions. Samples are used to approximate the distribution of possible inputs, possible interactor states that result from inputs, and possible actions (callbacks and feedback) interactors may execute. Because each sample is certain, the developer can specify most of the behavior of interactors in a familiar, non-probabilistic fashion. This approach retains all the advantages of maintaining information about uncertainty while minimizing the need for the developer to work in probabilistic terms. We present a working implementation of our framework and illustrate the power of these techniques within a paint program that includes three different kinds of uncertain input.\n",
      "=============================\n",
      "CINCH: a cooperatively designed marking interface for 3D pathway selection\n",
      "To disentangle and analyze neural pathways estimated from magnetic resonance imaging data, scientists need an interface to select 3D pathways. Broad adoption of such an interface requires the use of commodity input devices such as mice and pens, but these devices offer only two degrees of freedom. CINCH solves this problem by providing a marking interface for 3D pathway selection. CINCH interprets pen strokes as pathway selections in 3D using a marking language designed together with scientists. Its bimanual interface employs a pen and a trackball (see Figure 1), allowing alternating selections and scene rotations without changes of mode. CINCH was evaluated by observing four scientists using the tool over a period of three weeks as part of their normal work activity. Event logs and interviews revealed dramatic improvements in both the speed and quality of scientists' everyday work, and a set of principles that should inform the design of future 3D marking interfaces. More broadly, CINCH demonstrates the value of the iterative, participatory design process that catalyzed its evolution.\n",
      "=============================\n",
      "3D magic lenses\n",
      "This work extends the metaphor of a see-through interface embodied in Magic LensesTM to 3D environments. We present two new see-through visualization techniques: jlat lenses in 3D and volumetric lenses. We discuss implementation concerns for platforms that have programmer accessible hardware clipping planes and show several examples of each visualization technique. We also examine composition of multiple lenses in 3D environments, which strengthens the flat lens metaphor, but may have no meaningful semantics in the case of volumetric lenses.\n",
      "=============================\n",
      "Outlier finding: focusing user attention on possible errors\n",
      "When users handle large amounts of data, errors are hard to notice. Outlier finding is a new way to reduce errors by directing the user's attention to inconsistent data which may indicate errors. We have implemented an outlier finder for text, which can detect both unusual matches and unusual mismatches to a text pattern. When integrated into the user interface of a PBD text editor and tested in a user study, outlier finding substantially reduced errors.\n",
      "=============================\n",
      "Bubble clusters: an interface for manipulating spatial aggregation of graphical objects\n",
      "Spatial layout is frequently used for managing loosely organized information, such as desktop icons and digital ink. To help users organize this type of information efficiently, we propose an interface for manipulating spatial aggregations of objects. The aggregated objects are automatically recognized as a group, and the group structure is visualized as a two-dimensional bubble surface that surrounds the objects. Users can drag, copy, or delete a group by operating on the bubble. Furthermore, to help pick out individual objects in a dense aggregation, the system spreads the objects to avoid overlapping when requested. This paper describes the design of this interface and its implementation. We tested our technique in icon grouping and ink relocation tasks and observed improvements in user performance.\n",
      "=============================\n",
      "BallCam!: dynamic view synthesis from spinning cameras\n",
      "We are interested in generating novel video sequences from a ball's point of view for sports domains. Despite the challenge of extreme camera motion, we show that we can leverage the periodicity of spinning cameras to generate a stabilized ball point-of-view video. We present preliminary results of image stabilization and view synthesis from a single camera being hurled in the air at 600 RPM.\n",
      "=============================\n",
      "UnderScore: musical underlays for audio stories\n",
      "Audio producers often use musical underlays to emphasize key moments in spoken content and give listeners time to reflect on what was said. Yet, creating such underlays is time-consuming as producers must carefully (1) mark an emphasis point in the speech (2) select music with the appropriate style, (3) align the music with the emphasis point, and (4) adjust dynamics to produce a harmonious composition. We present UnderScore, a set of semi-automated tools designed to facilitate the creation of such underlays. The producer simply marks an emphasis point in the speech and selects a music track. UnderScore automatically refines, aligns and adjusts the speech and music to generate a high-quality underlay. UnderScore allows producers to focus on the high-level design of the underlay; they can quickly try out a variety of music and test different points of emphasis in the story. Amateur producers, who may lack the time or skills necessary to author underlays, can quickly add music to their stories. An informal evaluation of UnderScore suggests that it can produce high-quality underlays for a variety of examples while significantly reducing the time and effort required of radio producers.\n",
      "=============================\n",
      "A conversational interface to web automation\n",
      "This paper presents CoCo, a system that automates web tasks on a user's behalf through an interactive conversational interface. Given a short command such as \"get road conditions for highway 88,\" CoCo synthesizes a plan to accomplish the task, executes it on the web, extracts an informative response, and returns the result to the user as a snippet of text. A novel aspect of our approach is that we leverage a repository of previously recorded web scripts and the user's personal web browsing history to determine how to complete each requested task. This paper describes the design and implementation of our system, along with the results of a brief user study that evaluates how likely users are to understand what CoCo does for them.\n",
      "=============================\n",
      "Shoe-shaped i/o interface\n",
      "In this research, we propose a shoe-shaped I/O interface. The benefits to users of wearable devices are significantly reduced if they are aware of them. Wearable devices should have the ability to be worn without requiring any attention from the user. However, previous wearable systems required users to be careful and be aware of wearing or carrying them. To solve this problem, we propose a shoe-shaped I/O interface. By wearing the shoes throughout the day, users soon cease to be conscious of them. Electromechanical devices are potentially easy to install in shoes. This report describes the concept of a shoe-shaped I/O interface, the development of a prototype system, and possible applications.\n",
      "=============================\n",
      "Mixture model based label association techniques for web accessibility\n",
      "An important aspect of making the Web accessible to blind users is ensuring that all important web page elements such as links, clickable buttons, and form fields have explicitly assigned labels. Properly labeled content is then correctly read out by screen readers, a dominant assistive technology used by blind users. In particular, improperly labeled form fields can critically impede online transactions such as shopping, paying bills, etc. with screen readers. Very often labels are not associated with form fields or are missing altogether, making form filling a challenge for blind users. Algorithms for associating a form element with one of several candidate labels in its vicinity must cope with the variability of the element's features including label's location relative to the element, distance to the element, etc. Probabilistic models provide a natural machinery to reason with such uncertainties. In this paper we present a Finite Mixture Model (FMM) formulation of the label association problem. The variability of feature values are captured in the FMM by a mixture of random variables that are drawn from parameterized distributions. Then, the most likely label to be paired with a form element is computed by maximizing the log-likelihood of the feature data using the Expectation-Maximization algorithm. We also adapt the FMM approach for two related problems: assigning labels (from an external Knowledge Base) to form elements that have no candidate labels in their vicinity and for quickly identifying clickable elements such as add-to-cart, checkout, etc., used in online transactions even when these elements do not have textual captions (e.g., image buttons w/o alternative text). We provide a quantitative evaluation of our techniques, as well as a user study with two blind subjects who used an aural web browser implementing our approach.\n",
      "=============================\n",
      "Topiary: a tool for prototyping location-enhanced applications\n",
      "Location-enhanced applications use the location of people, places, and things to augment or streamline interaction. Location-enhanced applications are just starting to emerge in several different domains, and many people believe that this type of application will experience tremendous growth in the near future. However, it currently requires a high level of technical expertise to build location-enhanced applications, making it hard to iterate on designs. To address this problem we introduce Topiary, a tool for rapidly prototyping location-enhanced applications. Topiary lets designers create a map that models the location of people, places, and things; use this active map to demonstrate scenarios depicting location contexts; use these scenarios in creating storyboards that describe interaction sequences; and then run these storyboards on mobile devices, with a wizard updating the location of people and things on a separate device. We performed an informal evaluation with seven researchers and interface designers and found that they reacted positively to the concept.\n",
      "=============================\n",
      "Kinematic templates: end-user tools for content-relative cursor manipulations\n",
      "This paper introduces kinematic templates, an end-user tool for defining content-specific motor space manipulations in the context of editing 2D visual compositions. As an example, a user can choose the \"sandpaper\" template to define areas within a drawing where cursor movement should slow down. Our current implementation provides templates that amplify or dampen the cursor's speed, attenuate jitter in a user's movement, guide movement along paths, and add forces to the cursor. Multiple kinematic templates can be defined within a document, with overlapping templates resulting in a form of function composition. A template's strength can also be varied, enabling one to improve one's strokes without losing the human element. Since kinematic templates guide movements, rather than strictly prescribe them, they constitute a visual composition aid that lies between unaided freehand drawing and rigid drawing aids such as snapping guides, masks, and perfect geometric primitives.\n",
      "=============================\n",
      "Dyadic projected spatial augmented reality\n",
      "Mano-a-Mano is a unique spatial augmented reality system that combines dynamic projection mapping, multiple perspective views and device-less interaction to support face to face, or dyadic, interaction with 3D virtual objects. Its main advantage over more traditional AR approaches, such as handheld devices with composited graphics or see-through head worn displays, is that users are able to interact with 3D virtual objects and each other without cumbersome devices that obstruct face to face interaction. We detail our prototype system and a number of interactive experiences. We present an initial user experiment that shows that participants are able to deduce the size and distance of a virtual projected object. A second experiment shows that participants are able to infer which of a number of targets the other user indicates by pointing.\n",
      "=============================\n",
      "RichReview: blending ink, speech, and gesture to support collaborative document review\n",
      "This paper introduces a novel document annotation system that aims to enable the kinds of rich communication that usually only occur in face-to-face meetings. Our system, RichReview, lets users create annotations on top of digital documents using three main modalities: freeform inking, voice for narration, and deictic gestures in support of voice. RichReview uses novel visual representations and time-synchronization between modalities to simplify annotation access and navigation. Moreover, RichReview's versatile support for multi-modal annotations enables users to mix and interweave different modalities in threaded conversations. A formative evaluation demonstrates early promise for the system finding support for voice, pointing, and the combination of both to be especially valuable. In addition, initial findings point to the ways in which both content and social context affect modality choice.\n",
      "=============================\n",
      "Galaxy of news: an approach to visualizing and understanding expansive news landscapes\n",
      "The Galaxy of News system embodies an approach to visualizing large quantities of independently authored pieces of information, in this case news stories. At the heart of this system is a powerful relationship construction engine that constructs an associative relation network to automatically build implicit links between related articles. To visualize these relationships, and hence the news information space, the Galaxy of News uses pyramidal structuring and visual presentation, semantic zooming and panning, animated visual cues that are dynamically constructed to illustrate relationships between articles, and fluid interaction in a three dimensional information space to browse and search through large databases of news articles. The result is a tool that allows people to quickly gain a broad understanding of a news base by providing an abstracted presentation that covers the entire information base, and through interaction, progressively refines the details of the information space. This research has been generalized into a model for news access and visualization to provide automatic construction of news information spaces and derivation of an interactive news experience.\n",
      "=============================\n",
      "Augmenting the SCOPE of interactions with implicit and explicit graphical structures\n",
      "When using interactive graphical tools, users often have to manage a structure, i.e. the arrangement of and relations between the parts or elements of the content. However, the interaction with structures may be complex, and not well integrated with the interaction with the content. Based on contextual inquiries and past work, we have identified a number of concepts and requirements about interaction with structure. We have explored a number of interactive tools and we present one of them in this paper: a new kind of property sheet that relies on the implicit structure of graphics. The interactions with the tool augment the scope of interactions to multiple objects.\n",
      "=============================\n",
      "Rapid serial visual presentation techniques for consumer digital video devices\n",
      "In this paper we propose a new model for a class of rapid serial visual presentation (RSVP) interfaces [16] in the context of consumer video devices. The basic spatial layout \"explodes\" a sequence of image frames into a 3D trail in order to provide more context for a spatial/temporal presentation. As the user plays forward or back, the trail advances or recedes while the image in the foreground focus position is replaced. The design is able to incorporate a variety of methods for analyzing or highlighting images in the trail. Our hypotheses are that users can navigate more quickly and precisely to points of interest when compared to conventional consumer-based browsing, channel flipping, or fast-forwarding techniques. We report on an experiment testing our hypotheses in which we found that subjects were more accurate but not faster in browsing to a target of interest in recorded television content with a TV remote.\n",
      "=============================\n",
      "Citrine: providing intelligent copy-and-paste\n",
      "We present Citrine, a system that extends the widespread copy-and-paste interaction technique with intelligent transformations, making it useful in more situations. Citrine uses text parsing to find the structure in copied text and allows users to paste the structured information, which might have many pieces, in a single paste operation. For example, using Citrine, a user can copy the text of a meeting request and add it to the Outlook calendar with a single paste. In applications such as Excel, users can teach Citrine by example how to copy and paste data by showing it which fields go into which columns, and can use this to copy or paste many items at a time in a user-defined manner. Citrine can be used with a wide variety of applications and types of data and can be easily extended to work with more. It currently includes parsers that recognize contact information, calendar appointments and bibliographic citations. It works with Internet Explorer, Outlook, Excel, Palm Desktop, EndNote and other applications. Citrine is available to download on the internet.\n",
      "=============================\n",
      "Social activity indicators: interface components for CSCW systems\n",
      "Knowing what social activity is occurring within and through a Computer-Supported Cooperative Work (CSCW) system is often very useful. This is especially true for computer-mediated communication systems such as chat and other synchronous applications. People will attend to these systems more closely when they know that there is interesting activity on them. Interface mechanisms for indicating social activity, however, are often ad-hoc, if present at all. This paper argues for the importance of displaying social activity as well as proposes a generalized mechanism for doing so. This social activity indication mechanism is built upon a new CSCW toolkit, the Cafe ConstructionKit, and the Cafe ConstructionKit provides a number of important facilities for making construction of these indicators easy and straight-forward. Accordingly, this paper presents both the Cafe ConstructionKit as a CSCW toolkit as well as a mechanism for creating activity indicators.\n",
      "=============================\n",
      "Bimanual gesture keyboard\n",
      "Gesture keyboards represent an increasingly popular way to input text on mobile devices today. However, current gesture keyboards are exclusively unimanual. To take advantage of the capability of modern multi-touch screens, we created a novel bimanual gesture text entry system, extending the gesture keyboard paradigm from one finger to multiple fingers. To address the complexity of recognizing bimanual gesture, we designed and implemented two related interaction methods, finger-release and space-required, both based on a new multi-stroke gesture recognition algorithm. A formal experiment showed that bimanual gesture behaviors were easy to learn. They improved comfort and reduced the physical demand relative to unimanual gestures on tablets. The results indicated that these new gesture keyboards were valuable complements to unimanual gesture and regular typing keyboards.\n",
      "=============================\n",
      "Tools for supporting the collaborative process\n",
      "Collaborative software has been divided into two temporal categories: synchronous and asynchronous. We argue that this binary distinction is unnecessary and harmful, and present a model for collaboration processes (i.e. the temporal record of the actions of the group members) which includes both synchronous and asynchronous software as submodels. We outline an object–oriented toolkit which implements the model, and present an application of its use in a pen–based conferencing tool.\n",
      "=============================\n",
      "Physical embodiments for mobile communication agents\n",
      "This paper describes a physically embodied and animated user interface to an interactive call handling agent, consisting of a small wireless animatronic device in the form of a squirrel, bunny, or parrot. A software tool creates movement primitives, composes these primitives into complex behaviors, and triggers these behaviors dynamically at state changes in the conversational agent's finite state machine. Gaze and gestural cues from the animatronics alert both the user and co-located third parties of incoming phone calls, and data suggests that such alerting is less intrusive than conventional telephones.\n",
      "=============================\n",
      "sleepyWhispers: sharing goodnights within distant relationships\n",
      "There is a growing body of work in HCI on the design of communication technologies to help support lovers in long distance relationships. We build upon this work by presenting an exploratory study of a prototype device intended to allow distant lovers to share goodnight messages. Our work distinguishes itself by basing distance communication metaphors on elements of familiar, simple co-located behaviours. We argue that voice remains an under-utilised media when designing interactive technologies for long-distant couples. Through exploring the results of a 2-month case study we present some of the unique challenges that using voice entails.\n",
      "=============================\n",
      "Taskposé: exploring fluid boundaries in an associative window visualization\n",
      "Window management research has aimed to leverage users' tasks to organize the growing number of open windows in a useful manner. This research has largely assumed task classifications to be binary -- either a window is in a task, or not -- and context-independent. We suggest that the continual evolution of tasks can invalidate this approach and instead propose a fuzzy association model in which windows are related to one another by varying degrees. Task groupings are an emergent property of our approach. To support the association model, we introduce the WindowRank algorithm and its use in determining window association. We then describe Taskposé, a prototype window switch visualization embodying these ideas, and report on a week-long user study of the system.\n",
      "=============================\n",
      "MAI painting brush: an interactive device that realizes the feeling of real painting\n",
      "Many digital painting systems have been proposed and their quality is improving. In these systems, graphics tablets are widely used as input devices. However, because of its rigid nib and indirect manipulation, the operational feeling of a graphics tablet is different from that of real paint brush. We solved this problem by developing the MR-based Artistic Interactive (MAI) Painting Brush, which imitates a real paint brush, and constructed a mixed reality (MR) painting system that enables direct painting on physical objects in the real world.\n",
      "=============================\n",
      "Distributed mediation of ambiguous context in aware environments\n",
      "Many context-aware services make the assumption that the context they use is completely accurate. However, in reality, both sensed and interpreted context is often ambiguous. A challenge facing the development of realistic and deployable context-aware services, therefore, is the ability to handle ambiguous context. In this paper, we describe an architecture that supports the building of context-aware services that assume context is ambiguous and allows for mediation of ambiguity by mobile users in aware environments. We illustrate the use of our architecture and evaluate it through three example context-aware services, a word predictor system, an In/Out Board, and a reminder tool.\n",
      "=============================\n",
      "Rethinking the progress bar\n",
      "Progress bars are prevalent in modern user interfaces. Typically, a linear function is employed such that the progress of the bar is directly proportional to how much work has been completed. However, numerous factors cause progress bars to proceed at non-linear rates. Additionally, humans perceive time in a non-linear way. This paper explores the impact of various progress bar behaviors on user perception of process duration. The results are used to suggest several design considerations that can make progress bars appear faster and ultimately improve users' computing experience.\n",
      "=============================\n",
      "HIPerPaper: introducing pen and paper interfaces for ultra-scale wall displays\n",
      "While recent advances in graphics, display, and computer hardware support ultra-scale visualizations of a tremendous amount of data sets, mechanisms for interacting with this information on large high-resolution wall displays are still under investigation. Different issues in terms of user interface, ergonomics, multi-user interaction, and system flexibility arise while facing ultra-scale wall displays and none of the introduced approaches fully address them. We introduce HIPerPaper, a novel digital pen and paper interface that enables natural interaction with the HIPerSpace wall, a 31.8 by 7.5 foot tiled wall display of 268,720,000 pixels. HIPerPaper provides a flexible, portable, and inexpensive medium for interacting with large high-resolution wall displays.\n",
      "=============================\n",
      "Understanding the design of a flying jogging companion\n",
      "Jogging can offer many health benefits, and mobile phone apps have recently emerged that aim to support the jogging experience. We believe that jogging is an embodied experience, and therefore present a contrasting approach to these existing systems by arguing that any supporting technology should also take on an embodied approach. In order to exemplify this approach, we detail the technical specifications of a flying quadcopter that has successfully been used with joggers in order to explore the design of embodied systems to support physical exertion activities. Based on interviews with five joggers running with our system, we present preliminary insights about the experience of jogging with a flying robot. With our work, we hope to inspire and guide designers who are interested in developing embodied systems to support exertion activities.\n",
      "=============================\n",
      "An application-independent system for visualizing user operation history\n",
      "A history-of-user-operations function helps make applications easier to use. For example, users may have access to an operation history list in an application to undo or redo a past operation. To provide an overview of a long operation history and help users find target interactions or application states quickly, visual representations of operation history have been proposed. However, most previous systems are tightly integrated with target applications and difficult to apply to new applications. We propose an application-independent method that can visualize the operation history of arbitrary GUI applications by monitoring the input and output GUI events from outside of the target application. We implemented a prototype system that visualizes operation sequences of generic Java Awt/Swing applications using an annotated comic strip metaphor. We tested the system with various applications and present results from a user study.\n",
      "=============================\n",
      "MediaMosaic—a multimedia editing environment\n",
      "MediaMosaic is an editing environment developed to provide several features that are either unavailable or not adequately addressed in current editing systems. First, it is a multimedia editor of an open architecture. General media are inserted in documents by embedded virtual screens. Second, it allows users to do markup editing in context. The marked comments are overlapped and attached to the commented areas. Third, it provides a mechanism to allow users to bring data from more than one source to a single document. The views of the included data can be tailored. Fourth, users can work on an included medium through its embedded view or through another complete and duplicated view. It isolates and simplifies the interface design of individual media editors.\n",
      "=============================\n",
      "Elasticurves: exploiting stroke dynamics and inertia for the real-time neatening of sketched 2D curves\n",
      "Elasticurves present a novel approach to neaten sketches in real-time, resulting in curves that combine smoothness with user-intended detail. Inspired by natural variations in stroke speed when drawing quickly or with precision, we exploit stroke dynamics to distinguish intentional fine detail from stroke noise. Combining inertia and stroke dynamics, elasticurves can be imagined as the trace of a pen attached to the user by an oscillation-free elastic band. Sketched quickly, the elasticurve spatially lags behind the stroke, smoothing over stroke detail, but catches up and matches the input stroke at slower speeds. Connectors, such as lines or circular-arcs link the evolving elasticurve to the next input point, growing the curve by a responsiveness fraction along the connector. Responsiveness is calibrated, to reflect drawing skill or device noise. Elasticurves are theoretically sound and robust to variations in stroke sampling. Practically, they neaten digital strokes in real-time while retaining the modeless and visceral feel of pen on paper.\n",
      "=============================\n",
      "The omni-directional treadmill: a locomotion device for virtual worlds\n",
      "The Omni-Directional Treadmill (ODT) is a revolutionary device for locomotion in large-scale virtual environments. The device allows its user to walk or jog in any direction of travel. It is the third generation in a series of devices built for this purpose for the U.S. Army’s Dismounted Infantry Training Program. We first describe the device in terms of its construction and operating characteristics. We then report on an analysis consisting of a series of locomotion and maneuvering tasks on the ODT. We observed user motions and system responses to those motions from the perspective of the user. Each task is described in terms of what causes certain motions to trigger unpredictable responses causing loss of balance or at least causing the user to become consciously aware of their movements. We conclude that the two primary shortcomings in the ODT are its tracking system and machine control mechanism for centering the user on the treads.\n",
      "=============================\n",
      "Hands-on math: a page-based multi-touch and pen desktop for technical work and problem solving\n",
      "Students, scientists and engineers have to choose between the flexible, free-form input of pencil and paper and the computational power of Computer Algebra Systems (CAS) when solving mathematical problems. Hands-On Math is a multi-touch and pen-based system which attempts to unify these approaches by providing virtual paper that is enhanced to recognize mathematical notations as a means of providing in situ access to CAS functionality. Pages can be created and organized on a large pannable desktop, and mathematical expressions can be computed, graphed and manipulated using a set of uni- and bi-manual interactions which facilitate rapid exploration by eliminating tedious and error prone transcription tasks. Analysis of a qualitative pilot evaluation indicates the potential of our approach and highlights usability issues with the novel techniques used.\n",
      "=============================\n",
      "Automatically generating user interfaces adapted to users' motor and vision capabilities\n",
      "Most of today's GUIs are designed for the typical, able-bodied user; atypical users are, for the most part, left to adapt as best they can, perhaps using specialized assistive technologies as an aid. In this paper, we present an alternative approach: SUPPLE++ automatically generates interfaces which are tailored to an individual's motor capabilities and can be easily adjusted to accommodate varying vision capabilities. SUPPLE++ models users. motor capabilities based on a onetime motor performance test and uses this model in an optimization process, generating a personalized interface. A preliminary study indicates that while there is still room for improvement, SUPPLE++ allowed one user to complete tasks that she could not perform using a standard interface, while for the remaining users it resulted in an average time savings of 20%, ranging from an slowdown of 3% to a speedup of 43%.\n",
      "=============================\n",
      "DiamondTouch: a multi-user touch technology\n",
      "A technique for creating a touch-sensitive input device is proposed which allows multiple, simultaneous users to interact in an intuitive fashion. Touch location information is determined independently for each user, allowing each touch on a common surface to be associated with a particular user. The surface generates location dependent, modulated electric fields which are capacitively coupled through the users to receivers installed in the work environment. We describe the design of these systems and their applications. Finally, we present results we have obtained with a small prototype device.\n",
      "=============================\n",
      "Search Vox: leveraging multimodal refinement and partial knowledge for mobile voice search\n",
      "Internet usage on mobile devices continues to grow as users seek anytime, anywhere access to information. Because users frequently search for businesses, directory assistance has been the focus of many voice search applications utilizing speech as the primary input modality. Unfortunately, mobile settings often contain noise which degrades performance. As such, we present Search Vox, a mobile search interface that not only facilitates touch and text refinement whenever speech fails, but also allows users to assist the recognizer via text hints. Search Vox can also take advantage of any partial knowledge users may have about the business listing by letting them express their uncertainty in an intuitive way using verbal wildcards. In simulation experiments conducted on real voice search data, leveraging multimodal refinement resulted in a 28% relative reduction in error rate. Providing text hints along with the spoken utterance resulted in even greater relative reduction, with dramatic gains in recovery for each additional character.\n",
      "=============================\n",
      "Anywhere touchtyping: text input on arbitrary surface using depth sensing\n",
      "In this paper, touch typing enabled virtual keyboard system using depth sensing on arbitrary surface is proposed. Keystroke event detection is conducted using 3-dimensional hand appearance database matching combined with fingertip's surface touch sensing. Our prototype system acquired hand posture depth map by implementing phase shift algorithm for Digital Light Processor (DLP) fringe projection on arbitrary flat surface. The system robustly detects hand postures on the sensible surface with no requirement of hand position alignment on virtual keyboard frame. The keystroke feedback is the physical touch to the surface, thus no specific hardware must be worn. The system works real-time in average of 20 frames per second.\n",
      "=============================\n",
      "What interfaces mean: a history and sociology of computer windows\n",
      "This poster presents a cursory look at the history of windows in Graphical User Interfaces. It examines the controversy between tiling and overlapping window managers and explains that controversy's sociological importance: windows are control devices, enabling their users to manage their activity and attention. It then explores a few possible reasons for the relative disappearance of windowing in recent computing devices. It concludes with a recapitulative typology.\n",
      "=============================\n",
      "DART: a toolkit for rapid design exploration of augmented reality experiences\n",
      "In this paper, we describe The Designer's Augmented Reality Toolkit (DART). DART is built on top of Macromedia Director, a widely used multimedia development environment. We summarize the most significant problems faced by designers working with AR in the real world, and discuss how DART addresses them. Most of DART is implemented in an interpreted scripting language, and can be modified by designers to suit their needs. Our work focuses on supporting early design activities, especially a rapid transition from story-boards to working experience, so that the experiential part of a design can be tested early and often. DART allows designers to specify complex relationships between the physical and virtual worlds, and supports 3D animatic actors (informal, sketch-based content) in addition to more polished content. Designers can capture and replay synchronized video and sensor data, allowing them to work off-site and to test specific parts of their experience more effectively.\n",
      "=============================\n",
      "Sensing from the basement: a feasibility study of unobtrusive and low-cost home activity recognition\n",
      "The home deployment of sensor-based systems offers many opportunities, particularly in the area of using sensor-based systems to support aging in place by monitoring an elder's activities of daily living. But existing approaches to home activity recognition are typically expensive, difficult to install, or intrude into the living space. This paper considers the feasibility of a new approach that \"reaches into the home\" via the existing infrastructure. Specifically, we deploy a small number of low-cost sensors at critical locations in a home's water distribution infrastructure. Based on water usage patterns, we can then infer activities in the home. To examine the feasibility of this approach, we deployed real sensors into a real home for six weeks. Among other findings, we show that a model built on microphone-based sensors that are placed away from systematic noise sources can identify 100% of clothes washer usage, 95% of dishwasher usage, 94% of showers, 88% of toilet flushes, 73% of bathroom sink activity lasting ten seconds or longer, and 81% of kitchen sink activity lasting ten seconds or longer. While there are clear limits to what activities can be detected when analyzing water usage, our new approach represents a sweet spot in the tradeoff between what information is collected at what cost.\n",
      "=============================\n",
      "SpeckleSense: fast, precise, low-cost and compact motion sensing using laser speckle\n",
      "Motion sensing is of fundamental importance for user interfaces and input devices. In applications, where optical sensing is preferred, traditional camera-based approaches can be prohibitive due to limited resolution, low frame rates and the required computational power for image processing. We introduce a novel set of motion-sensing configurations based on laser speckle sensing that are particularly suitable for human-computer interaction. The underlying principles allow these configurations to be fast, precise, extremely compact and low cost. We provide an overview and design guidelines for laser speckle sensing for user interaction and introduce four general speckle projector/sensor configurations. We describe a set of prototypes and applications that demonstrate the versatility of our laser speckle sensing techniques.\n",
      "=============================\n",
      "Robust, low-cost, non-intrusive sensing and recognition of seated postures\n",
      "In this paper, we present a methodology for recognizing seated postures using data from pressure sensors installed on a chair. Information about seated postures could be used to help avoid adverse effects of sitting for long periods of time or to predict seated activities for a human-computer interface. Our system design displays accurate near-real-time classification performance on data from subjects on which the posture recognition system was not trained by using a set of carefully designed, subject-invariant signal features. By using a near-optimal sensor placement strategy, we keep the number of required sensors low thereby reducing cost and computational complexity. We evaluated the performance of our technology using a series of empirical methods including (1) cross-validation (classification accuracy of 87% for ten postures using data from 31 sensors), and (2) a physical deployment of our system (78% classification accuracy using data from 19 sensors).\n",
      "=============================\n",
      "RecipeSheet: creating, combining and controlling information processors\n",
      "Many tasks require users to extract information from diverse sources, to edit or process this information locally, and to explore how the end results are affected by changes in the information or in its processing. We present the RecipeSheet, a general-purpose tool for assisting users in such tasks. The RecipeSheet lets users create information processors, called recipes, which may take input in a variety of forms such as text, Web pages, or XML, and produce results in a similar variety of forms. The processing carried out by a recipe may be specified using a macro or query language, of which we currently support Rexx, Smalltalk and XQuery, or by capturing the behaviour of a Web application or Web service. In the RecipeSheet's spreadsheet-inspired user interface, information appears in cells, with inter-cell dependencies defined by recipes rather than formulas. Users can also intervene manually to control which information flows through the dependency connections. Through a series of examples we illustrate how tasks that would be challenging in existing environments are supported by the RecipeSheet.\n",
      "=============================\n",
      "THAW: tangible interaction with see-through augmentation for smartphones on computer screens\n",
      "In this paper, we present a novel interaction system that allows a collocated large display and small handheld devices to seamlessly work together. The smartphone acts both as a physical interface and as an additional graphics layer for near-surface interaction on a computer screen. Our system enables accurate position tracking of a smartphone placed on or over any screen by displaying a 2D color pattern that is captured using the smartphone's back-facing camera. The proposed technique can be implemented on existing devices without the need for additional hardware.\n",
      "=============================\n",
      "Sphere: multi-touch interactions on a spherical display\n",
      "Sphere is a multi-user, multi-touch-sensitive spherical display in which an infrared camera used for touch sensing shares the same optical path with the projector used for the display. This novel configuration permits: (1) the enclosure of both the projection and the sensing mechanism in the base of the device, and (2) easy 360-degree access for multiple users, with a high degree of interactivity without shadowing or occlusion. In addition to the hardware and software solution, we present a set of multi-touch interaction techniques and interface concepts that facilitate collaborative interactions around Sphere. We designed four spherical application concepts and report on several important observations of collaborative activity from our initial Sphere installation in three high-traffic locations.\n",
      "=============================\n",
      "Touch sensing by partial shadowing of PV module\n",
      "A novel touch sensing technique is proposed. By utilizing partial shadowing of a photovoltaic (PV) module, touch events are accurately detected. Since the PV module also works as a power source, a battery-less touch sensing device is easily realized. We develop a wireless touch commander consisting of 6 PV modules so the user can input by using both touch and swipe actions.\n",
      "=============================\n",
      "CommandSpace: modeling the relationships between tasks, descriptions and features\n",
      "Users often describe what they want to accomplish with an application in a language that is very different from the application's domain language. To address this gap between system and human language, we propose modeling an application's domain language by mining a large corpus of Web documents about the application using deep learning techniques. A high dimensional vector space representation can model the relationships between user tasks, system commands, and natural language descriptions and supports mapping operations, such as identifying likely system commands given natural language queries and identifying user tasks given a trace of user operations. We demonstrate the feasibility of this approach with a system, CommandSpace, for the popular photo editing application Adobe Photoshop. We build and evaluate several applications enabled by our model showing the power and flexibility of this approach.\n",
      "=============================\n",
      "Inkjet-printed conductive patterns for physical manipulation of audio signals\n",
      "In this demo paper, we present the realization of a completely aesthetically driven conductive image as a multi-modal music controller. Combining two emerging technologies - rapid prototyping with an off-the-shelf inkjet printer using conductive ink and parametric graphic design, we are able to create an interactive surface that is thin, flat, and flexible. This sensate surface can be conformally wrapped around a simple curved surface, and unlike touch screens, can accommodate complex structures and shapes such as holes on a surface. We present the design and manufacturing flow and discuss the technology behind this multi-modal sensing design. Our work seeks to offer a new dimension of designing sonic interaction with graphic tools, playing and learning music from a visual perspective and performing with expressive physical manipulation.\n",
      "=============================\n",
      "PICL: portable in-circuit learner\n",
      "This paper introduces the PICL, the portable in-circuit learner. The PICL explores the possibility of providing standalone, low-cost, programming-by-demonstration machine learning capabilities to circuit prototyping. To train the PICL, users attach a sensor to the PICL, demonstrate example input, then specify the desired output (expressed as a voltage) for the given input. The current version of the PICL provides two learning modes, binary classification and linear regression. To streamline training and also make it possible to train on highly transient signals (such as those produced by a camera flash or a hand clap), the PICL includes a number of input inferencing techniques. These techniques make it possible for the PICL to learn with as few as one example. The PICL's behavioural repertoire can be expanded by means of various output adapters, which serve to transform the output in useful ways when prototyping. Collectively, the PICL's capabilities allow users of systems such as the Arduino or littleBits electronics kit to quickly add basic sensor-based behaviour, with little or no programming required.\n",
      "=============================\n",
      "SketchREAD: a multi-domain sketch recognition engine\n",
      "We present SketchREAD, a multi-domain sketch recognition engine capable of recognizing freely hand-drawn diagrammatic sketches. Current computer sketch recognition systems are difficult to construct, and either are fragile or accomplish robustness by severely limiting the designer's drawing freedom. Our system can be applied to a variety of domains by providing structural descriptions of the shapes in that domain; no training data or programming is necessary. Robustness to the ambiguity and uncertainty inherent in complex, freely-drawn sketches is achieved through the use of context. The system uses context to guide the search for possible interpretations and uses a novel form of dynamically constructed Bayesian networks to evaluate these interpretations. This process allows the system to recover from low-level recognition errors (e.g., a line misclassified as an arc) that would otherwise result in domain level recognition errors. We evaluated Sketch-READ on real sketches in two domains--family trees and circuit diagrams--and found that in both domains the use of context to reclassify low-level shapes significantly reduced recognition error over a baseline system that did not reinterpret low-level classifications. We also discuss the system's potential role in sketch based user interfaces.\n",
      "=============================\n",
      "Imaginary phone: learning imaginary interfaces by transferring spatial memory from a familiar device\n",
      "We propose a method for learning how to use an imaginary interface (i.e., a spatial non-visual interface) that we call \"transfer learning\". By using a physical device (e.g. an iPhone) a user inadvertently learns the interface and can then transfer that knowledge to an imaginary interface. We illustrate this concept with our Imaginary Phone prototype. With it users interact by mimicking the use of a physical iPhone by tapping and sliding on their empty non-dominant hand without visual feedback. Pointing on the hand is tracked using a depth camera and touch events are sent wirelessly to an actual iPhone, where they invoke the corresponding actions. Our prototype allows the user to perform everyday task such as picking up a phone call or launching the timer app and setting an alarm. Imaginary Phone thereby serves as a shortcut that frees users from the necessity of retrieving the actual physical device. We present two user studies that validate the three assumptions underlying the transfer learning method. (1) Users build up spatial memory automatically while using a physical device: participants knew the correct location of 68% of their own iPhone home screen apps by heart. (2) Spatial memory transfers from a physical to an imaginary inter-face: participants recalled 61% of their home screen apps when recalling app location on the palm of their hand. (3) Palm interaction is precise enough to operate a typical mobile phone: Participants could reliably acquire 0.95cm wide iPhone targets on their palm-sufficiently large to operate any iPhone standard widget.\n",
      "=============================\n",
      "A text entry technique for wrist-worn watches with tiny touchscreens\n",
      "We consider a text entry technique for wrist-worn watches with inch-scale touchscreens. Most of the watches which are commercially available, for example, Galaxy Gear, Omate, etc., have around 1.5-inch touchscreens that is too small for the shrinked Qwerty keyboard. Moreover, the virtual button-based techniques determine input-letters by distinguishing touched locations on touchscreens which continuously demands a user to carefully touch certain locations. Thus, they are not suitable to tiny-touchscreen devices in mobile environment. Instead, the proposed text entry technique allows a user to touch almost anywhere on the touchscreen for text entry by determining input-letters based on drag direction regardless of touched location. We implemented the proposed method on a commercial watch with 1.54-inch touchscreen for validating its feasibility.\n",
      "=============================\n",
      "Waken: reverse engineering usage information and interface structure from software videos\n",
      "We present Waken, an application-independent system that recognizes UI components and activities from screen captured videos, without any prior knowledge of that application. Waken can identify the cursors, icons, menus, and tooltips that an application contains, and when those items are used. Waken uses frame differencing to identify occurrences of behaviors that are common across graphical user interfaces. Candidate templates are built, and then other occurrences of those templates are identified using a multi-phase algorithm. An evaluation demonstrates that the system can successfully reconstruct many aspects of a UI without any prior application-dependant knowledge. To showcase the design opportunities that are introduced by having this additional meta-data, we present the Waken Video Player, which allows users to directly interact with UI components that are displayed in the video.\n",
      "=============================\n",
      "OctoPocus: a dynamic guide for learning gesture-based command sets\n",
      "We describe OctoPocus, an example of a dynamic guide that combines on-screen feedforward and feedback to help users learn, execute and remember gesture sets. OctoPocus can be applied to a wide range of single-stroke gestures and recognition algorithms and helps users progress smoothly from novice to expert performance. We provide an analysis of the design space and describe the results of two experi-ments that show that OctoPocus is significantly faster and improves learning of arbitrary gestures, compared to con-ventional Help menus. It can also be adapted to a mark-based gesture set, significantly improving input time compared to a two-level, four-item Hierarchical Marking menu.\n",
      "=============================\n",
      "Under the table interaction\n",
      "We explore the design space of a two-sided interactive touch table, designed to receive touch input from both the top and bottom surfaces of the table. By combining two registered touch surfaces, we are able to offer a new dimension of input for co-located collaborative groupware. This design accomplishes the goal of increasing the relative size of the input area of a touch table while maintaining its direct-touch input paradigm. We describe the interaction properties of this two-sided touch table, report the results of a controlled experiment examining the precision of user touches to the underside of the table, and a series of application scenarios we developed for use on inverted and two-sided tables. Finally, we present a list of design recommendations based on our experiences and observations with inverted and two-sided tables.\n",
      "=============================\n",
      "Detecting and leveraging finger orientation for interaction with direct-touch surfaces\n",
      "Current interactions on direct-touch interactive surfaces are often modeled based on properties of the input channel that are common in traditional graphical user interfaces (GUI) such as x-y coordinate information. Leveraging additional information available on the surfaces could potentially result in richer and novel interactions. In this paper we specifically explore the role of finger orientation. This property is typically ignored in touch-based interactions partly because of the ambiguity in determining it solely from the contact shape. We present a simple algorithm that unambiguously detects the directed finger orientation vector in real-time from contact information only, by considering the dynamics of the finger landing process. Results of an experimental evaluation show that our algorithm is stable and accurate. We then demonstrate how finger orientation can be leveraged to enable novel interactions and to infer higher-level information such as hand occlusion or user position. We present a set of orientation-aware interaction techniques and widgets for direct-touch surfaces.\n",
      "=============================\n",
      "Huddle: automatically generating interfaces for systems of multiple connected appliances\n",
      "Systems of connected appliances, such as home theaters and presentation rooms, are becoming commonplace in our homes and workplaces. These systems are often difficult to use, in part because users must determine how to split the tasks they wish to perform into sub-tasks for each appliance and then find the particular functions of each appliance to complete their sub-tasks. This paper describes Huddle, a new system that automatically generates task-based interfaces for a system of multiple appliances based on models of the content flow within the multi-appliance system.\n",
      "=============================\n",
      "Chorus: a crowd-powered conversational assistant\n",
      "Despite decades of research attempting to establish conversational interaction between humans and computers, the capabilities of automated conversational systems are still limited. In this paper, we introduce Chorus, a crowd-powered conversational assistant. When using Chorus, end users converse continuously with what appears to be a single conversational partner. Behind the scenes, Chorus leverages multiple crowd workers to propose and vote on responses. A shared memory space helps the dynamic crowd workforce maintain consistency, and a game-theoretic incentive mechanism helps to balance their efforts between proposing and voting. Studies with 12 end users and 100 crowd workers demonstrate that Chorus can provide accurate, topical responses, answering nearly 93% of user queries appropriately, and staying on-topic in over 95% of responses. We also observed that Chorus has advantages over pairing an end user with a single crowd worker and end users completing their own tasks in terms of speed, quality, and breadth of assistance. Chorus demonstrates a new future in which conversational assistants are made usable in the real world by combining human and machine intelligence, and may enable a useful new way of interacting with the crowds powering other systems.\n",
      "=============================\n",
      "SwingStates: adding state machines to the swing toolkit\n",
      "This article describes SwingStates, a library that adds state machines to the Java Swing user interface toolkit. Unlike traditional approaches, which use callbacks or listeners to define interaction, state machines provide a powerful control structure and localize all of the interaction code in one place. SwingStates takes advantage of Java's inner classes, providing programmers with a natural syntax and making it easier to follow and debug the resulting code. SwingStates tightly integrates state machines, the Java language and the Swing toolkit. It reduces the potential for an explosion of states by allowing multiple state machines to work together. We show how to use SwingStates to add new interaction techniques to existing Swing widgets, to program a powerful new Canvas widget and to control high-level dialogues.\n",
      "=============================\n",
      "Gestalt: integrated support for implementation and analysis in machine learning\n",
      "We present Gestalt, a development environment designed to support the process of applying machine learning. While traditional programming environments focus on source code, we explicitly support both code and data. Gestalt allows developers to implement a classification pipeline, analyze data as it moves through that pipeline, and easily transition between implementation and analysis. An experiment shows this significantly improves the ability of developers to find and fix bugs in machine learning systems. Our discussion of Gestalt and our experimental observations provide new insight into general-purpose support for the machine learning process.\n",
      "=============================\n",
      "Distant freehand pointing and clicking on very large, high resolution displays\n",
      "We explore the design space of freehand pointing and clicking interaction with very large high resolution displays from a distance. Three techniques for gestural pointing and two for clicking are developed and evaluated. In addition, we present subtle auditory and visual feedback techniques to compensate for the lack of kinesthetic feedback in freehand interaction, and to promote learning and use of appropriate postures.\n",
      "=============================\n",
      "Touch scrolling transfer functions\n",
      "Touch scrolling systems use a transfer function to transform gestures on a touch sensitive surface into scrolling output. The design of these transfer functions is complex as they must facilitate precise direct manipulation of the underlying content as well as rapid scrolling through large datasets. However, researchers' ability to refine them is impaired by: (1) limited understanding of how users express scrolling intentions through touch gestures; (2) lack of knowledge on proprietary transfer functions, causing researchers to evaluate techniques that may misrepresent the state of the art; and (3) a lack of tools for examining existing transfer functions. To address these limitations, we examine how users express scrolling intentions in a human factors experiment; we describe methods to reverse engineer existing `black box' transfer functions, including use of an accurate robotic arm; and we use the methods to expose the functions of Apple iOS and Google Android, releasing data tables and software to assist replication. We discuss how this new understanding can improve experimental rigour and assist iterative improvement of touch scrolling.\n",
      "=============================\n",
      "A negotiation architecture for fluid documents\n",
      "The information presented in a document often consists of primary content as well as supporting material such as explanatory notes, detailed derivations, illustrations, and the like. We introduce a class of user interface techniques for fluid documents that supports the reader’s shift to supporting material while maintaining the context of the primary material. Our approach initially minimizes the intrusion of supporting material by presenting it as a small visual cue near the annotated primary material. When the user expresses interest in the annotation, it expands smoothly to a readable size. At the same time, the primary material makes space for the expanded annotation. The expanded supporting material must be given space to occupy, and it must be made salient with respect to the surrounding primary material. These two aspects, space and salience, are subject to a negotiation between the primary and supporting material. This paper presents the components of our fluid document techniques and describes the negotiation architecture for ensuring that the presentations of both primary and supporting material are honored.\n",
      "=============================\n",
      "Preference elicitation for interface optimization\n",
      "Decision-theoretic optimization is becoming a popular tool in the user interface community, but creating accurate cost (or utility) functions has become a bottleneck --- in most cases the numerous parameters of these functions are chosen manually, which is a tedious and error-prone process. This paper describes ARNAULD, a general interactive tool for eliciting user preferences concerning concrete outcomes and using this feedback to automatically learn a factored cost function. We empirically evaluate our machine learning algorithm and two automatic query generation approaches and report on an informal user study.\n",
      "=============================\n",
      "SideBySide: ad-hoc multi-user interaction with handheld projectors\n",
      "We introduce SideBySide, a system designed for ad-hoc multi-user interaction with handheld projectors. SideBySide uses device-mounted cameras and hybrid visible/infrared light projectors to track multiple independent projected images in relation to one another. This is accomplished by projecting invisible fiducial markers in the near-infrared spectrum. Our system is completely self-contained and can be deployed as a handheld device without instrumentation of the environment. We present the design and implementation of our system including a hybrid handheld projector to project visible and infrared light, and techniques for tracking projected fiducial markers that move and overlap. We introduce a range of example applications that demonstrate the applicability of our system to real-world scenarios such as mobile content exchange, gaming, and education.\n",
      "=============================\n",
      "Going beyond the display: a surface technology with an electronically switchable diffuser\n",
      "We introduce a new type of interactive surface technology based on a switchable projection screen which can be made diffuse or clear under electronic control. The screen can be continuously switched between these two states so quickly that the change is imperceptible to the human eye. It is then possible to rear-project what is perceived as a stable image onto the display surface, when the screen is in fact transparent for half the time. The clear periods may be used to project a second, different image through the display onto objects held above the surface. At the same time, a camera mounted behind the screen can see out into the environment. We explore some of the possibilities this type of screen technology affords, allowing surface computing interactions to extend 'beyond the display'. We present a single self-contained system that combines these off-screen interactions with more typical multi-touch and tangible surface interactions. We describe the technical challenges in realizing our system, with the aim of allowing others to experiment with these new forms of interactive surfaces.\n",
      "=============================\n",
      "Cheaper by the dozen: group annotation of 3D data\n",
      "This paper proposes a group annotation approach to interactive semantic labeling of data and demonstrates the idea in a system for labeling objects in 3D LiDAR scans of a city. In this approach, the system selects a group of objects, predicts a semantic label for it, and highlights it in an interactive display. In response, the user either confirms the predicted label, provides a different label, or indicates that no single label can be assigned to all objects in the group. This sequence of interactions repeats until a label has been confirmed for every object in the data set. The main advantage of this approach is that it provides faster interactive labeling rates than alternative approaches, especially in cases where all labels must be explicitly confirmed by a person. The main challenge is to provide an algorithm that selects groups with many objects all of the same label type arranged in patterns that are quick to recognize, which requires models for predicting object labels and for estimating times for people to recognize objects in groups. We address these challenges by defining an objective function that models the estimated time required to process all unlabeled objects and approximation algorithms to minimize it. Results of user studies suggest that group annotation can be used to label objects in LiDAR scans of cities significantly faster than one-by-one annotation with active learning.\n",
      "=============================\n",
      "Query-feature graphs: bridging user vocabulary and system functionality\n",
      "This paper introduces query-feature graphs, or QF-graphs. QF-graphs encode associations between high-level descriptions of user goals (articulated as natural language search queries) and the specific features of an interactive system relevant to achieving those goals. For example, a QF-graph for the GIMP graphics manipulation software links the query \"GIMP black and white\" to the commands \"desaturate\" and \"grayscale.\" We demonstrate how QF-graphs can be constructed using search query logs, search engine results, web page content, and localization data from interactive systems. An analysis of QF-graphs shows that the associations produced by our approach exhibit levels of accuracy that make them eminently usable in a range of real-world applications. Finally, we present three hypothetical user interface mechanisms that illustrate the potential of QF-graphs: search-driven interaction, dynamic tooltips, and app-to-app analogy search.\n",
      "=============================\n",
      "Customizable physical interfaces for interacting with conventional applications\n",
      "When using today's productivity applications, people rely heavily on graphical controls (GUI widgets) as the way to invoke application functions and to obtain feedback. Yet we all know that certain controls can be difficult or tedious to find and use. As an alternative, a customizable physical interface lets an end-user easily bind a modest number of physical controls to similar graphical counterparts. The user can then use the physical control to invoke the corresponding graphical control's function, or to display its graphical state in a physical form. To show how customizable physical interfaces work, we present examples that illustrate how our combined phidgets® and widget tap packages are used to link existing application widgets to physical controls. While promising, our implementation prompts a number of issues relevant to others pursuing interface customization.\n",
      "=============================\n",
      "The ProD framework for proactive displays\n",
      "A proactive display is an application that selects content to display based on the set of users who have been detected nearby. For example, the Ticket2Talk [17] proactive display application presented content for users so that other people would know something about them. It is our view that promising patterns for proactive display applications have been discovered, and now we face the need for frameworks to support the range of applications that are possible in this design space. In this paper, we present the Proactive Display (ProD) Framework, which allows for the easy construction of proactive display applications. It allows a range of proactive display applications, including ones already in the literature. ProD also enlarges the design space of proactive display systems by allowing a variety of new applications that incorporate different views of social life and community.\n",
      "=============================\n",
      "1 thumb, 4 buttons, 20 words per minute: design and evaluation of H4-writer\n",
      "We present what we believe is the most efficient and quickest four-key text entry method available. H4-Writer uses Huffman coding to assign minimized key sequences to letters, with full access to error correction, punctuation, digits, modes, etc. The key sequences are learned quickly, and support eyes-free entry. With KSPC = 2.321, the effort to enter text is comparable to multitap on a mobile phone keypad; yet multitap requires nine keys. In a longitudinal study with six participants, an average text entry speed of 20.4 wpm was observed in the 10th session. Error rates were under 1%. To improve external validity, an extended session was included that required input of punctuation and other symbols. Entry speed dropped only by about 3 wpm, suggesting participants quickly leveraged their acquired skill with H4-Writer to access advanced features.\n",
      "=============================\n",
      "CrowdScape: interactively visualizing user behavior and output\n",
      "Crowdsourcing has become a powerful paradigm for accomplishing work quickly and at scale, but involves significant challenges in quality control. Researchers have developed algorithmic quality control approaches based on either worker outputs (such as gold standards or worker agreement) or worker behavior (such as task fingerprinting), but each approach has serious limitations, especially for complex or creative work. Human evaluation addresses these limitations but does not scale well with increasing numbers of workers. We present CrowdScape, a system that supports the human evaluation of complex crowd work through interactive visualization and mixed initiative machine learning. The system combines information about worker behavior with worker outputs, helping users to better understand and harness the crowd. We describe the system and discuss its utility through grounded case studies. We explore other contexts where CrowdScape's visualizations might be useful, such as in user studies.\n",
      "=============================\n",
      "Socially augmenting employee profiles with people-tagging\n",
      "Employee directories play a valuable role in helping people find others to collaborate with, solve a problem, or provide needed expertise. Serving this role successfully requires accurate and up-to-date user profiles, yet few users take the time to maintain them. In this paper, we present a system that enables users to tag other users with key words that are displayed on their profiles. We discuss how people-tagging is a form of social bookmarking that enables people to organize their contacts into groups, annotate them with terms supporting future recall, and search for people by topic area. In addition, we show that people-tagging has a valuable side benefit: it enables the community to collectively maintain each others' interest and expertise profiles. Our user studies suggest that people tag other people as a form of contact management and that the tags they have been given are accurate descriptions of their interests and expertise. Moreover, none of the people interviewed reported offensive or inappropriate tags. Based on our results, we believe that peopletagging will become an important tool for relationship management in an organization.\n",
      "=============================\n",
      "A series of tubes: adding interactivity to 3D prints using internal pipes\n",
      "3D printers offer extraordinary flexibility for prototyping the shape and mechanical function of objects. We investigate how 3D models can be modified to facilitate the creation of interactive objects that offer dynamic input and output. We introduce a general technique for supporting the rapid prototyping of interactivity by removing interior material from 3D models to form internal pipes. We describe this new design space of pipes for interaction design, where variables include openings, path constraints, topologies, and inserted media. We then present PipeDream, a tool for routing such pipes through the interior of 3D models, integrated within a 3D modeling program. We use two distinct routing algorithms. The first has users define pipes' terminals, and uses path routing and physics-based simulation to minimize pipe bending energy, allowing easy insertion of media post-print. The second allows users to supply a desired internal shape to which we fit a pipe route: for this we describe a graph-routing algorithm. We present several prototypes created using our tool to show its flexibility and potential.\n",
      "=============================\n",
      "Haptic feedback design for a virtual button along force-displacement curves\n",
      "In this paper, we present a haptic feedback method for a virtual button based on the force-displacement curves of a physical button. The original feature of the proposed method is that it provides haptic feedback, not only for the \"click\" sensation but also for the moving sensation before and after transition points in a force-displacement curve. The haptic feedback is by vibrotactile stimulations only and does not require a force feedback mechanism. We conducted user experiments to show that the resultant haptic feedback is realistic and distinctive. Participants were able to distinguish among six different virtual buttons, with 94.1% accuracy even in a noisy environment. In addition, participants were able to associate four virtual buttons with their physical counterparts, with a correct answer rate of 79.2%.\n",
      "=============================\n",
      "Searching for software learning resources using application context\n",
      "Users of complex software applications frequently need to consult documentation, tutorials, and support resources to learn how to use the software and further their understand-ing of its capabilities. Existing online help systems provide limited context awareness through \"what's this?\" and simi-lar techniques. We examine the possibility of making more use of the user's current context in a particular application to provide useful help resources. We provide an analysis and taxonomy of various aspects of application context and how they may be used in retrieving software help artifacts with web browsers, present the design of a context-aware augmented web search system, and describe a prototype implementation and initial user study of this system. We conclude with a discussion of open issues and an agenda for further research.\n",
      "=============================\n",
      "Animation: from cartoons to the user interface\n",
      "User interfaces are often based on static presentations, a model ill suited for conveying change. Consequently, events on the screen frequently startle and confuse users. Cartoon animation, in contrast, is exceedingly successful at engaging its audience; even the most bizarre events are easily comprehended. The Self user interface has served as a testbed for the application of cartoon animation techniques as a means of making the interface easier to understand and more pleasant to use. Attention to timing and transient detail allows Self objects to move solidly. Use of cartoon-style motion blur allows Self objects to move quickly and still maintain their comprehensibility. Self objects arrive and depart smoothly, without sudden materializations and disappearances, and they rise to the front of overlapping objects smoothly through the use of dissolve. Anticipating motion with a small contrary motion and pacing the middle of transitions faster than the endpoints results in smoother and clearer movements. Despite the differences between user interfaces and cartoons --cartoons are frivolous, passive entertainment and user interfaces are serious, interactive tools -- cartoon animation has much to lend to user interfaces to realize both affective and cognitive benefits. *This work was originally supported by Sun Microsystems Laboratories, an NSF Graduate Fellowship, National Science Foundation Presidential Young Investigator Grant #CCR-8657631, IBM Powell Foundation, Apple Computer, Inc., Cray Laboratories, Tandem, NCR Corporation, Texas Instruments, Inc., and Digital Equipment Corporation.\n",
      "=============================\n",
      "Query-by-critique: spoken language access to large lists\n",
      "Spoken language interfaces provide highly mobile, small form-factor, hands-free, eyes-free interaction with information. Uniform access to large lists of information using spoken interfaces is highly desirable, but problematic due to inherent limitations of speech. A speech widget for lists of attributed objects is described that provides for approximate queries to retrieve desired items. User tests demonstrate that this is an effective technique for accessing information using speech.\n",
      "=============================\n",
      "Video-based document tracking: unifying your physical and electronic desktops\n",
      "This paper presents an approach for tracking paper documents on the desk over time and automatically linking them to the corresponding electronic documents using an overhead video camera. We demonstrate our system in the context of two scenarios, <i>paper tracking</i> and <i>photo sorting</i>. In the paper tracking scenario, the system tracks changes in the stacks of printed documents and books on the desk and builds a complete representation of the spatial structure of the desktop. When users want to find a printed document buried in the stacks, they can query the system based on appearance, keywords, or access time. The system also provides a <i>remote desktop</i> interface for directly browsing the physical desktop from a remote location. In the photo sorting scenario, users sort printed photographs into physical stacks on the desk. The systemautomatically recognizes the photographs and organizes the corresponding digital photographs into separate folders according to the physical arrangement. Our framework provides a way to unify the physical and electronic desktops without the need for a specialized physical infrastructure except for a video camera.\n",
      "=============================\n",
      "The multiplayer: multi-perspective social video navigation\n",
      "We present a multi-perspective video \"multiplayer\" designed to organize social video aggregated from online sites like YouTube. Our system automatically time-aligns videos using audio fingerprinting, thus bringing them into a unified temporal frame. The interface utilizes social metadata to visually aid navigation and cue users to more interesting portions of an event. We provide details about the visual and interaction design rationale of the multiplayer.\n",
      "=============================\n",
      "Sensing techniques for mobile interaction\n",
      "We describe sensing techniques motivated by unique aspects of human-computer interaction with handheld devices in mobile settings. Special features of mobile interaction include changing orientation and position, changing venues, the use of computing as auxiliary to ongoing, real-world activities like talking to a colleague, and the general intimacy of use for such devices. We introduce and integrate a set of sensors into a handheld device, and demonstrate several new functionalities engendered by the sensors, such as recording memos when the device is held like a cell phone, switching between portrait and landscape display modes by holding the device in the desired orientation, automatically powering up the device when the user picks it up the device to start using it, and scrolling the display using tilt. We present an informal experiment, initial usability testing results, and user reactions to these techniques.\n",
      "=============================\n",
      "Citrus: a language and toolkit for simplifying the creation of structured editors for code and data\n",
      "Direct-manipulation editors for structured data are increasingly common. While such editors can greatly simplify the creation of structured data, there are few tools to simplify the creation of the editors themselves. This paper presents Citrus, a new programming language and user interface toolkit designed for this purpose. Citrus offers language-level support for constraints, restrictions and change notifications on primitive and aggregate data, mechanisms for automatically creating, removing, and reusing views as data changes, a library of widgets, layouts and behaviors for defining interactive views, and two comprehensive interactive editors as an interface to the language and toolkit itself. Together, these features support the creation of editors for a large class of data and code.\n",
      "=============================\n",
      "ThickPad: a hover-tracking touchpad for a laptop\n",
      "We explored the use of a hover tracking touchpad in a laptop environment. In order to study the new experience, we implemented a prototype touchpad consisting of infrared LEDs and photo-transistors, which can track fingers as far as 10mm over the surface. We demonstrate here three major interaction techniques that would become possible when a hover-tracking touchpad meets a laptop\n",
      "=============================\n",
      "IrCube tracker: an optical 6-DOF tracker based on LED directivity\n",
      "Six-degrees-of-freedom (6-DOF) trackers, which were mainly for professional computer applications, are now in demand by everyday consumer applications. With the requirements of consumer electronics in mind, we designed an optical 6-DOF tracker where a few photo-sensors can track the position and orientation of an LED cluster. The operating principle of the tracker is basically source localization by solving an inverse problem. We implemented a prototype system for a TV viewing environment, verified the feasibility of the operating principle, and evaluated the basic performance of the prototype system in terms of accuracy and speed. We also examined its application possibility to different environments, such as a tabletop computer, a tablet computer, and a mobile spatial interaction environment.\n",
      "=============================\n",
      "Dynamic space management for user interfaces\n",
      "We present a general approach to the dynamic representation of 2D space that is well suited for userinterface layout. We partition space into two distinct categories: full and empty. The user can explicitly specify a set of possibly overlapping upright rectangles that represent the objects of interest. These full-space rectangles are processed by the system to create a representation of the remaining empty space. This representation makes it easy for users to develop customized spatial allocation strategies that avoid overlapping the full-space rectangles. We describe the representation; provide efficient incremental algorithms for adding and deleting full-space rectangles, and for querying the empty-space representation; and show several allocation strategies that the representation makes possible. We present two testbed applications that incorporate an implementation of the algorithm; one shows the utility of our representation for window management tasks; the other applies it to the layout of components in a 3D user interface, based on the upright 2D bounding boxes of their projections.\n",
      "=============================\n",
      "Graphstract: minimal graphical help for computers\n",
      "We explore the use of abstracted screenshots as part of a new help interface. Graphstract, an implementation of a graphical help system, extends the ideas of textually oriented Minimal Manuals to the use of screenshots, allowing multiple small graphical elements to be shown in a limited space. This allows a user to get an overview of a complex sequential task as a whole. The ideas have been developed by three iterations of prototyping and evaluation. A user study shows that Graphstract helps users perform tasks faster on some but not all tasks. Due to their graphical nature, it is possible to construct Graphstracts automatically from pre-recorded interactions. A second study shows that automated capture and replay is a low-cost method for authoring Graphstracts, and the resultant help is as understandable as manually constructed help.\n",
      "=============================\n",
      "Dranimate: Rapid Real-time Gestural Rigging and Control of Animation\n",
      "Dranimate is an interactive animation system that allows users to rapidly and intuitively rig and control animations based on a still image or drawing, using hand gestures. Dranimate combines two complementary methods of shape manipulation: bone-joint-based physics simulation, and the as-rigid-as-possible deformation algorithm. Dranimate also introduces a number of designed interactions that focus the users attention on the animated content, as opposed to computer keyboard or mouse.\n",
      "=============================\n",
      "Gaze-Shifting: Direct-Indirect Input with Pen and Touch Modulated by Gaze\n",
      "Modalities such as pen and touch are associated with direct input but can also be used for indirect input. We propose to combine the two modes for direct-indirect input modulated by gaze. We introduce gaze-shifting as a novel mechanism for switching the input mode based on the alignment of manual input and the user's visual attention. Input in the user's area of attention results in direct manipulation whereas input offset from the user's gaze is redirected to the visual target. The technique is generic and can be used in the same manner with different input modalities. We show how gaze-shifting enables novel direct-indirect techniques with pen, touch, and combinations of pen and touch input.\n",
      "=============================\n",
      "GazeProjector: Accurate Gaze Estimation and Seamless Gaze Interaction Across Multiple Displays\n",
      "Mobile gaze-based interaction with multiple displays may occur from arbitrary positions and orientations. However, maintaining high gaze estimation accuracy in such situa-tions remains a significant challenge. In this paper, we present GazeProjector, a system that combines (1) natural feature tracking on displays to determine the mobile eye tracker's position relative to a display with (2) accurate point-of-gaze estimation. GazeProjector allows for seam-less gaze estimation and interaction on multiple displays of arbitrary sizes independently of the user's position and orientation to the display. In a user study with 12 partici-pants we compare GazeProjector to established methods (here: visual on-screen markers and a state-of-the-art video-based motion capture system). We show that our approach is robust to varying head poses, orientations, and distances to the display, while still providing high gaze estimation accuracy across multiple displays without re-calibration for each variation. Our system represents an important step towards the vision of pervasive gaze-based interfaces.\n",
      "=============================\n",
      "FlexiBend: Enabling Interactivity of Multi-Part, Deformable Fabrications Using Single Shape-Sensing Strip\n",
      "This paper presents FlexiBend, an easily installable shape-sensing strip that enables interactivity of multi-part, deformable fabrications. The flexible sensor strip is composed of a dense linear array of strain gauges, therefore it has shape sensing capability. After installation, FlexiBend can simultaneously sense user inputs in different parts of a fabrication or even capture the geometry of a deformable fabrication.\n",
      "=============================\n",
      "Gaze vs. Mouse: A Fast and Accurate Gaze-Only Click Alternative\n",
      "Eye gaze tracking is a promising input method which is gradually finding its way into the mainstream. An obvious question to arise is whether it can be used for point-and-click tasks, as an alternative for mouse or touch. Pointing with gaze is both fast and natural, although its accuracy is limited. There are still technical challenges with gaze tracking, as well as inherent physiological limitations. Furthermore, providing an alternative to clicking is challenging. We are considering use cases where input based purely on gaze is desired, and the click targets are discrete user interface (UI) elements which are too small to be reliably resolved by gaze alone, e.g., links in hypertext. We present Actigaze, a new gaze-only click alternative which is fast and accurate for this scenario. A clickable user interface element is selected by dwelling on one of a set of confirm buttons, based on two main design contributions: First, the confirm buttons stay on fixed positions with easily distinguishable visual identifiers such as colors, enabling procedural learning of the confirm button position. Secondly, UI elements are associated with confirm buttons through the visual identifiers in a way which minimizes the likelihood of inadvertent clicks. We evaluate two variants of the proposed click alternative, comparing them against the mouse and another gaze-only click alternative.\n",
      "=============================\n",
      "Corona: Positioning Adjacent Device with Asymmetric Bluetooth Low Energy RSSI Distributions\n",
      "We introduce Corona, a novel spatial sensing technique that implicitly locates adjacent mobile devices in the same plane by examining asymmetric Bluetooth Low Energy RSSI distributions. The underlying phenomenon is that the off-center BLE antenna and asymmetric radio frequency topology create a characteristic Bluetooth RSSI distribution around the device. By comparing the real-time RSSI readings against a RSSI distribution model, each device can derive the relative position of the other adjacent device. Our experiments using an iPhone and iPad Mini show that Corona yields position estimation at 50% accuracy within a 2cm range, or 85% for the best two candidates. We developed an application to combine Corona with accelerometer readings to mitigate ambiguity and enable cross-device interactions on adjacent devices.\n",
      "=============================\n",
      "Orbits: Gaze Interaction for Smart Watches using Smooth Pursuit Eye Movements\n",
      "We introduce Orbits, a novel gaze interaction technique that enables hands-free input on smart watches. The technique relies on moving controls to leverage the smooth pursuit movements of the eyes and detect whether and at which control the user is looking at. In Orbits, controls include targets that move in a circular trajectory in the face of the watch, and can be selected by following the desired one for a small amount of time. We conducted two user studies to assess the technique's recognition and robustness, which demonstrated how Orbits is robust against false positives triggered by natural eye movements and how it presents a hands-free, high accuracy way of interacting with smart watches using off-the-shelf devices. Finally, we developed three example interfaces built with Orbits: a music player, a notifications face plate and a missed call menu. Despite relying on moving controls -- very unusual in current HCI interfaces -- these were generally well received by participants in a third and final study.\n",
      "=============================\n",
      "Tiltcasting: 3D Interaction on Large Displays using a Mobile Device\n",
      "We develop and formally evaluate a metaphor for smartphone interaction with 3D environments: Tiltcasting. Under the Tiltcasting metaphor, users interact within a rotatable 2D plane that is \"cast\" from their phone's interactive display into 3D space. Through an empirical validation, we show that Tiltcasting supports efficient pointing, interaction with occluded objects, disambiguation between nearby objects, and object selection and manipulation in fully addressable 3D space. Our technique out-performs existing target agnostic pointing implementations, and approaches the performance of physical pointing with an off-the-shelf smartphone.\n",
      "=============================\n",
      "Pin-and-Cross: A Unimanual Multitouch Technique Combining Static Touches with Crossing Selection\n",
      "We define, explore, and demonstrate a new multitouch interaction space called \"pin-and-cross.\" It combines one or more static touches (\"pins\") with another touch to cross a radial target, all performed with one hand. A formative study reveals pin-and-cross kinematic characteristics and evaluates fundamental performance and preference for target angles. These results are used to form design guidelines and recognition heuristics for pin-and-cross menus invoked with one and two pin fingers on first touch or after a drag. These guidelines are used to implement different pin-and-cross techniques. A controlled experiment compares a one finger pin-and-cross contextual menu to a Marking Menu and partial Pie Menu: pin-and-cross is just as accurate and 27% faster when invoked on a draggable object. A photo app demonstrates more pin-and-cross variations for extending two-finger scrolling, selecting modes while drawing, constraining two-finger transformations, and combining pin-and-cross with a Marking Menu.\n",
      "=============================\n",
      "Biometric Touch Sensing: Seamlessly Augmenting Each Touch with Continuous Authentication\n",
      "Current touch devices separate user authentication from regular interaction, for example by displaying modal login screens before device usage or prompting for in-app passwords, which interrupts the interaction flow. We propose biometric touch sensing, a new approach to representing touch events that enables commodity devices to seamlessly integrate authentication into interaction: From each touch, the touchscreen senses the 2D input coordinates and at the same time obtains biometric features that identify the user. Our approach makes authentication during interaction transparent to the user, yet ensures secure interaction at all times. To implement this on today's devices, our watch prototype Bioamp senses the impedance profile of the user's wrist and modulates a signal onto the user's body through skin using a periodic electric signal. This signal affects the capacitive values touchscreens measure upon touch, allowing devices to identify users on each touch. We integrate our approach into Windows 8 and discuss and demonstrate it in the context of various use cases, including access permissions and protecting private screen contents on personal and shared devices.\n",
      "=============================\n",
      "Push-Push: A Drag-like Operation Overlapped with a Page Transition Operation on Touch Interfaces\n",
      "A page transition operation on touch interfaces is a common and frequent subtask when one conducts a drag-like operation such as selecting text and dragging an icon. Traditional page transition gestures such as scrolling and flicking gestures, however, cannot be conducted while conducting the drag-like operation since they have a confliction. We proposed Push-Push that is a new drag-like operation not in conflict with page transition operations. Thus, page transition operations could be conducted while performing Push-Push. To design Push-Push, we utilized the hover and pressed states as additional input states of touch interfaces. The results from two experiments showed that Push-Push has an advantage on increasing performance and qualitative opinions of users while reducing the subjective overload.\n",
      "=============================\n",
      "uniMorph: Fabricating Thin Film Composites for Shape-Changing Interfaces\n",
      "Researchers have been investigating shape-changing interfaces, however technologies for thin, reversible shape change remain complicated to fabricate. uniMorph is an enabling technology for rapid digital fabrication of customized thin-film shape-changing interfaces. By combining the thermoelectric characteristics of copper with the high thermal expansion rate of ultra-high molecular weight polyethylene, we are able to actuate the shape of flexible circuit composites directly. The shape-changing actuation is enabled by a temperature driven mechanism and reduces the complexity of fabrication for thin shape-changing interfaces. In this paper we describe how to design and fabricate thin uniMorph composites. We present composites that are actuated by either environmental temperature changes or active heating of embedded structures and provide a systematic overview of shape-changing primitives. Finally, we present different sensing techniques that leverage the existing copper structures or can be seamlessly embedded into the uniMorph composite. To demonstrate the wide applicability of uniMorph, we present several applications in ubiquitous and mobile computing.\n",
      "=============================\n",
      "Tracko: Ad-hoc Mobile 3D Tracking Using Bluetooth Low Energy and Inaudible Signals for Cross-Device Interaction\n",
      "While current mobile devices detect the presence of surrounding devices, they lack a truly spatial awareness to bring them into the user's natural 3D space. We present Tracko, a 3D tracking system between two or more commodity devices without added components or device synchronization. Tracko achieves this by fusing three signal types. 1) Tracko infers the presence of and rough distance to other devices from the strength of Bluetooth low energy signals. 2) Tracko exchanges a series of inaudible stereo sounds and derives a set of accurate distances between devices from the difference in their arrival times. A Kalman filter integrates both signal cues to place collocated devices in a shared 3D space, combining the robustness of Bluetooth with the accuracy of audio signals for relative 3D tracking. 3) Tracko incorporates inertial sensors to refine 3D estimates and support quick interactions. Tracko robustly tracks devices in 3D with a mean error of 6.5 cm within 0.5 m and a 15.3 cm error within 1 m, which validates Trackoffs suitability for cross-device interactions.\n",
      "=============================\n",
      "Foobaz: Variable Name Feedback for Student Code at Scale\n",
      "Current traditional feedback methods, such as hand-grading student code for substance and style, are labor intensive and do not scale. We created a user interface that addresses feedback at scale for a particular and important aspect of code quality: variable names. We built this user interface on top of an existing back-end that distinguishes variables by their behavior in the program. Therefore our interface not only allows teachers to comment on poor variable names, they can comment on names that mislead the reader about the variable's role in the program. We ran two user studies in which 10 teachers and 6 students created and received feedback, respectively. The interface helped teachers give personalized variable name feedback on thousands of student solutions from an edX introductory programming MOOC. In the second study, students composed solutions to the same programming assignments and immediately received personalized quizzes composed by teachers in the previous user study.\n",
      "=============================\n",
      "Looking through the Eye of the Mouse: A Simple Method for Measuring End-to-end Latency using an Optical Mouse\n",
      "We present a simple method for measuring end-to-end latency in graphical user interfaces. The method works with most optical mice and allows accurate and real time latency measures up to 5 times per second. In addition, the technique allows easy insertion of probes at different places in the system I.e. mouse events listeners - to investigate the sources of latency. After presenting the measurement method and our methodology, we detail the measures we performed on different systems, toolkits and applications. Results show that latency is affected by the operating system and system load. Substantial differences are found between C++/GLUT and C++/Qt or Java/Swing implementations, as well as between web browsers.\n",
      "=============================\n",
      "Capacitive Blocks: A Block System that Connects the Physical with the Virtual using Changes of Capacitance\n",
      "We propose a block-stacking system based on capacitance. The system, called Capacitive Blocks, allows users to build 3D models in a virtual space by stacking physical blocks. The construction of the block-stacking system is simple, and fundamental components including physical blocks can be made with a 3D printer. The block is a capacitor that consists of two layers made of conductive plastic filament and between them a layer made of non-conductive plastic filament. In this paper, we present a prototype of this block-stacking system and the mechanism that detects the height of blocks (i.e., the number of stacked blocks) by measuring the capacitance of the stacked blocks, which changes in accordance with the number of stacked blocks.\n",
      "=============================\n",
      "Protopiper: Physically Sketching Room-Sized Objects at Actual Scale\n",
      "Physical sketching of 3D wireframe models, using a hand-held plastic extruder, allows users to explore the design space of 3D models efficiently. Unfortunately, the scale of these devices limits users' design explorations to small-scale objects. We present protopiper, a computer aided, hand-held fabrication device, that allows users to sketch room-sized objects at actual scale. The key idea behind protopiper is that it forms adhesive tape into tubes as its main building material, rather than extruded plastic or photopolymer lines. Since the resulting tubes are hollow they offer excellent strength-to-weight ratio, thus scale well to large structures. Since the tape is pre-coated with adhesive it allows connecting tubes quickly, unlike extruded plastic that would require heating and cooling in the kilowatt range. We demonstrate protopiper's use through several demo objects, ranging from more constructive objects, such as furniture, to more decorative objects, such as statues. In our exploratory user study, 16 participants created objects based on their own ideas. They rated the device as being \"useful for creative exploration\", \"its ability to sketch at actual scale helped judge fit\", and \"fun to use.\"\n",
      "=============================\n",
      "AirFlip-Undo: Quick Undo using a Double Crossing In-Air Gesture in Hover Zone\n",
      "In this work, we use AirFlip to undo text input on mobile touchscreen devices. AirFlip involves a quick double crossing in-air gesture in the boundary surfaces of hover zone of devices that have hover sensing capability. To evaluate the effectiveness of undoing text input with AirFlip, we implemented two QWERTY soft keyboards (AirFlip keyboard and Typical keyboard). With these keyboards, we conducted a user study to investigate the users? workload and to collect subjective opinions. The results show that there is no significant difference in workload between keyboards.\n",
      "=============================\n",
      "LineFORM: Actuated Curve Interfaces for Display, Interaction, and Constraint\n",
      "In this paper we explore the design space of actuated curve interfaces, a novel class of shape changing-interfaces. Physical curves have several interesting characteristics from the perspective of interaction design: they have a variety of inherent affordances; they can easily represent abstract data; and they can act as constraints, boundaries, or borderlines. By utilizing such aspects of lines and curves, together with the added capability of shape-change, new possibilities for display, interaction and body constraint are possible. In order to investigate these possibilities we have implemented two actuated curve interfaces at different scales. LineFORM, our implementation, inspired by serpentine robotics, is comprised of a series chain of 1DOF servo motors with integrated sensors for direct manipulation. To motivate this work we present various applications such as shape changing cords, mobiles, body constraints, and data manipulation tools.\n",
      "=============================\n",
      "Kinetic Blocks: Actuated Constructive Assembly for Interaction and Display\n",
      "Pin-based shape displays not only give physical form to digital information, they have the inherent ability to accurately move and manipulate objects placed on top of them. In this paper we focus on such object manipulation: we present ideas and techniques that use the underlying shape change to give kinetic ability to otherwise inanimate objects. First, we describe the shape display's ability to assemble, disassemble, and reassemble structures from simple passive building blocks through stacking, scaffolding, and catapulting. A technical evaluation demonstrates the reliability of the presented techniques. Second, we introduce special kinematic blocks that are actuated and sensed through the underlying pins. These blocks translate vertical pin movements into other degrees of freedom like rotation or horizontal movement. This interplay of the shape display with objects on its surface allows us to render otherwise inaccessible forms, like overhangs, and enables richer input and output.\n",
      "=============================\n",
      "Form Follows Function(): An IDE to Create Laser-cut Interfaces and Microcontroller Programs from Single Code Base\n",
      "During the development of physical computing devices, physical object models and programs for microcontrollers are usually created with separate tools with distinct files. As a result, it is difficult to track the changes in hardware and software without discrepancy. Moreover, the software cannot directly access hardware metrics. Designing hardware interface cannot benefit from the source code information either. This demonstration proposes a browser-based IDE named f3.js that enables development of both as a single JavaScript code base. The demonstration allows audiences to play with the f3.js IDE and showcases example applications such as laser-cut interfaces generated from the same code but with different parameters. Programmers can experience the full feature and designers can interact with preset projects with a mouse or touch to customize laser-cut interfaces. More information is available at http://f3js.org.\n",
      "=============================\n",
      "NanoStylus: Enhancing Input on Ultra-Small Displays with a Finger-Mounted Stylus\n",
      "Due to their limited input area, ultra-small devices, such as smartwatches, are even more prone to occlusion or the fat finger problem, than their larger counterparts, such as smart phones, tablets, and tabletop displays. We present NanoStylus -- a finger-mounted fine-tip stylus that enables fast and accurate pointing on a smartwatch with almost no occlusion. The NanoStylus is built from the circuitry of an active capacitive stylus, and mounted within a custom 3D-printed thimble-shaped housing unit. A sensor strip is mounted on each side of the device to enable additional gestures. A user study shows that NanoStylus reduces error rate by 80%, compared to traditional touch interaction and by 45%, compared to a traditional stylus. This high precision pointing capability, coupled with the implemented gesture sensing, gives us the opportunity to explore a rich set of interactive applications on a smartwatch form factor.\n",
      "=============================\n",
      "Foldio: Digital Fabrication of Interactive and Shape-Changing Objects With Foldable Printed Electronics\n",
      "Foldios are foldable interactive objects with embedded input sensing and output capabilities. Foldios combine the advantages of folding for thin, lightweight and shape-changing objects with the strengths of thin-film printed electronics for embedded sensing and output. To enable designers and end-users to create highly custom interactive foldable objects, we contribute a new design and fabrication approach. It makes it possible to design the foldable object in a standard 3D environment and to easily add interactive high-level controls, eliminating the need to manually design a fold pattern and low-level circuits for printed electronics. Second, we contribute a set of printable user interface controls for touch input and display output on folded objects. Moreover, we contribute controls for sensing and actuation of shape-changeable objects. We demonstrate the versatility of the approach with a variety of interactive objects that have been fabricated with this framework.\n",
      "=============================\n",
      "Tactile Animation by Direct Manipulation of Grid Displays\n",
      "Chairs, wearables, and handhelds have become popular sites for spatial tactile display. Visual animators, already expert in using time and space to portray motion, could readily transfer their skills to produce rich haptic sensations if given the right tools. We introduce the tactile animation object, a directly manipulated phantom tactile sensation. This abstraction has two key benefits: 1) efficient, creative, iterative control of spatiotemporal sensations, and 2) the potential to support a variety of tactile grids, including sparse displays. We present Mango, an editing tool for animators, including its rendering pipeline and perceptually-optimized interpolation algorithm for sparse vibrotactile grids. In our evaluation, professional animators found it easy to create a variety of vibrotactile patterns, with both experts and novices preferring the tactile animation object over controlling actuators individually.\n",
      "=============================\n",
      "Capricate: A Fabrication Pipeline to Design and 3D Print Capacitive Touch Sensors for Interactive Objects\n",
      "3D printing is widely used to physically prototype the look and feel of 3D objects. Interaction possibilities of these prototypes, however, are often limited to mechanical parts or post-assembled electronics. In this paper, we present Capricate, a fabrication pipeline that enables users to easily design and 3D print highly customized objects that feature embedded capacitive multi-touch sensing. The object is printed in a single pass using a commodity multi-material 3D printer. To enable touch input on a wide variety of 3D printable surfaces, we contribute two techniques for designing and printing embedded sensors of custom shape. The fabrication pipeline is technically validated by a series of experiments and practically validated by a set of example applications. They demonstrate the wide applicability of Capricate for interactive objects.\n",
      "=============================\n",
      "Encore: 3D printed augmentation of everyday objects with printed-over, affixed and interlocked attachments\n",
      "One powerful aspect of 3D printing is its ability to extend, repair, or more generally modify everyday objects. However, nearly all existing work implicitly assumes that whole objects are to be printed from scratch. Designing objects as extensions or enhancements of existing ones is a laborious process in most of today's 3D authoring tools. This paper presents a framework for 3D printing to augment existing objects that covers a wide range of attachment options. We illustrate the framework through three exemplar attachment techniques - print-over, print-to-affix, and print-through. We implemented these techniques in Encore, a design tool that supports a range of analysis with visualization for users to explore design options and tradeoffs among these metrics. Encore also generates 3D models for production, addressing issues such as support jigs and contact geometry between the attached part and the original object.\n",
      "=============================\n",
      "TurkDeck: Physical Virtual Reality Based on People\n",
      "TurkDeck is an immersive virtual reality system that reproduces not only what users see and hear, but also what users feel. TurkDeck produces the haptic sensation using props, i.e., when users touch or manipulate an object in the virtual world, they simultaneously also touch or manipulate a corresponding object in the physical world. Unlike previous work on prop-based virtual reality, however, TurkDeck allows creating arbitrarily large virtual worlds in finite space and using a finite set of physical props. The key idea behind TurkDeck is that it creates these physical representations on the fly by making a group of human workers present and operate the props only when and where the user can actually reach them. TurkDeck manages these so-called \"human actuators\" by displaying visual instructions that tell the human actuators when and where to place props and how to actuate them. We demonstrate TurkDeck at the example of an immersive 300m2 experience in 25m2 physical space. We show how to simulate a wide range of physical objects and effects, including walls, doors, ledges, steps, beams, switches, stompers, portals, zip lines, and wind. In a user study, participants rated the realism/immersion of TurkDeck higher than a traditional prop-less baseline condition (4.9 vs. 3.6 on 7 item Likert).\n",
      "=============================\n",
      "Tomo: Wearable, Low-Cost Electrical Impedance Tomography for Hand Gesture Recognition\n",
      "We present Tomo, a wearable, low-cost system using Electrical Impedance Tomography (EIT) to recover the interior impedance geometry of a user's arm. This is achieved by measuring the cross-sectional impedances between all pairs of eight electrodes resting on a user's skin. Our approach is sufficiently compact and low-powered that we integrated the technology into a prototype wrist- and armband, which can monitor and classify gestures in real-time. We conducted a user study that evaluated two gesture sets, one focused on gross hand gestures and another using thumb-to-finger pinches. Our wrist location achieved 97% and 87% accuracies on these gesture sets respectively, while our arm location achieved 93% and 81%. We ultimately envision this technique being integrated into future smartwatches, allowing hand gestures and direct touch manipulation to work synergistically to support interactive tasks on small screens.\n",
      "=============================\n",
      "Paper generators: harvesting energy from touching, rubbing and sliding\n",
      "We present a new energy harvesting technology that generates electrical energy from a user's interactions with paper-like materials. The energy harvesters are flexible, light, and inexpensive, and they utilize a user's gestures such as tapping, touching, rubbing and sliding to generate electrical energy. The harvested energy is then used to actuate LEDs, e-paper displays and various other devices to create novel interactive applications, such as enhancing books and other printed media with interactivity.\n",
      "=============================\n",
      "Impacto: Simulating Physical Impact by Combining Tactile Stimulation with Electrical Muscle Stimulation\n",
      "We present impacto, a device designed to render the haptic sensation of hitting or being hit in virtual reality. The key idea that allows the small and light impacto device to simulate a strong hit is that it decomposes the stimulus: it renders the tactile aspect of being hit by tapping the skin using a solenoid; it adds impact to the hit by thrusting the user's arm backwards using electrical muscle stimulation. The device is self-contained, wireless, and small enough for wearable use, thus leaves the user unencumbered and able to walk around freely in a virtual environment. The device is of generic shape, allowing it to also be worn on legs, so as to enhance the experience of kicking, or merged into props, such as a baseball bat. We demonstrate how to assemble multiple impacto units into a simple haptic suit. Participants of our study rated impact simulated using impacto's combination of solenoid hit and electrical muscle stimulation as more realistic than either technique in isolation.\n",
      "=============================\n",
      "RevoMaker: Enabling Multi-directional and Functionally-embedded 3D printing using a Rotational Cuboidal Platform\n",
      "In recent years, 3D printing has gained significant attention from the maker community, academia, and industry to support low-cost and iterative prototyping of designs. Current unidirectional extrusion systems require printing sacrificial material to support printed features such as overhangs. Furthermore, integrating functions such as sensing and actuation into these parts requires additional steps and processes to create \"functional enclosures\", since design functionality cannot be easily embedded into prototype printing. All of these factors result in relatively high design iteration times. We present \"RevoMaker\", a self-contained 3D printer that creates direct out-of-the-printer functional prototypes, using less build material and with substantially less reliance on support structures. By modifying a standard low-cost FDM printer with a revolving cuboidal platform and printing partitioned geometries around cuboidal facets, we achieve a multidirectional additive prototyping process to reduce the print and support material use. Our optimization framework considers various orientations and sizes for the cuboidal base. The mechanical, electronic, and sensory components are preassembled on the flattened laser-cut facets and enclosed inside the cuboid when closed. We demonstrate RevoMaker directly printing a variety of customized and fully-functional product prototypes, such as computer mice and toys, thus illustrating the new affordances of 3D printing for functional product design.\n",
      "=============================\n",
      "GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\n",
      "We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (>32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\n",
      "=============================\n",
      "Printem: Instant Printed Circuit Boards with Standard Office Printers & Inks\n",
      "Printem film, a novel method for the fabrication of Printed Circuit Boards (PCBs) for small batch/prototyping use, is presented. Printem film enables a standard office inkjet or laser printer, using standard inks, to produce a PCB: the user prints a negative of the PCB onto the film, exposes it to UV or sunlight, and then tears-away the unneeded portion of the film, leaving-behind a copper PCB. PCBs produced with Printem film are as conductive as PCBs created using standard industrial methods. Herein, the composition of Printem film is described, and advantages of various materials discussed. Sample applications are also described, each of which demonstrates some unique advantage of Printem film over current prototyping methods: conductivity, flexibility, the ability to be cut with a pair of scissors, and the ability to be mounted to a rigid backplane. NOTE: publication of full-text held until November 9, 2015.\n",
      "=============================\n",
      "Candid Interaction: Revealing Hidden Mobile and Wearable Computing Activities\n",
      "The growth of mobile and wearable technologies has made it often difficult to understand what people in our surroundings are doing with their technology. In this paper, we introduce the concept of candid interaction: techniques for providing awareness about our mobile and wearable device usage to others in the vicinity. We motivate and ground this exploration through a survey on current attitudes toward device usage during interpersonal encounters. We then explore a design space for candid interaction through seven prototypes that leverage a wide range of technological enhancements, such as Augmented Reality, shape memory muscle wire, and wearable projection. Preliminary user feedback of our prototypes highlights the trade-offs between the benefits of sharing device activity and the need to protect user privacy.\n",
      "=============================\n",
      "SensorTape: Modular and Programmable 3D-Aware Dense Sensor Network on a Tape\n",
      "SensorTape is a modular and dense sensor network in a form factor of a tape. SensorTape is composed of interconnected and programmable sensor nodes on a flexible electronics substrate. Each node can sense its orientation with an inertial measurement unit, allowing deformation self-sensing of the whole tape. Also, nodes sense proximity using time-of-flight infrared. We developed network architecture to automatically determine the location of each sensor node, as SensorTape is cut and rejoined. Also, we made an intuitive graphical interface to program the tape. Our user study suggested that SensorTape enables users with different skill sets to intuitively create and program large sensor network arrays. We developed diverse applications ranging from wearables to home sensing, to show low deployment effort required by the user. We showed how SensorTape could be produced at scale using current technologies and we made a 2.3-meter long prototype.\n",
      "=============================\n",
      "Virtual Replicas for Remote Assistance in Virtual and Augmented Reality\n",
      "In many complex tasks, a remote subject-matter expert may need to assist a local user to guide actions on objects in the local user's environment. However, effective spatial referencing and action demonstration in a remote physical environment can be challenging. We introduce two approaches that use Virtual Reality (VR) or Augmented Reality (AR) for the remote expert, and AR for the local user, each wearing a stereo head-worn display. Both approaches allow the expert to create and manipulate virtual replicas of physical objects in the local environment to refer to parts of those physical objects and to indicate actions on them. This can be especially useful for parts that are occluded or difficult to access. In one approach, the expert points in 3D to portions of virtual replicas to annotate them. In another approach, the expert demonstrates actions in 3D by manipulating virtual replicas, supported by constraints and annotations. We performed a user study of a 6DOF alignment task, a key operation in many physical task domains, comparing both approaches to an approach in which the expert uses a 2D tablet-based drawing system similar to ones developed for prior work on remote assistance. The study showed the 3D demonstration approach to be faster than the others. In addition, the 3D pointing approach was faster than the 2D tablet in the case of a highly trained expert.\n",
      "=============================\n",
      "Anger-based BCI Using fNIRS Neurofeedback\n",
      "Functional near-infrared spectroscopy (fNIRS) holds increasing potential for Brain-Computer Interfaces (BCI) due to its portability, ease of application, robustness to movement artifacts, and relatively low cost. The use of fNIRS to support the development of affective BCI has received comparatively less attention, despite the role played by the prefrontal cortex in affective control, and the appropriateness of fNIRS to measure prefrontal activity. We present an active, fNIRS-based neurofeedback (NF) interface, which uses differential changes in oxygenation between the left and right sides of the dorsolateral prefrontal cortex to operationalize BCI input. The system is activated by users generating a state of anger, which has been previously linked to increased left prefrontal asymmetry. We have incorporated this NF interface into an experimental platform adapted from a virtual 3D narrative, in which users can express anger at a virtual character perceived as evil, causing the character to disappear progressively. Eleven subjects used the system and were able to successfully perform NF despite minimal training. Extensive analysis confirms that success was associated with the intent to express anger. This has positive implications for the design of affective BCI based on prefrontal asymmetry.\n",
      "=============================\n",
      "Daemo: A Self-Governed Crowdsourcing Marketplace\n",
      "Crowdsourcing marketplaces provide opportunities for autonomous and collaborative professional work as well as social engagement. However, in these marketplaces, workers feel disrespected due to unreasonable rejections and low payments, whereas requesters do not trust the results they receive. The lack of trust and uneven distribution of power among workers and requesters have raised serious concerns about sustainability of these marketplaces. To address the challenges of trust and power, this paper introduces Daemo, a self-governed crowdsourcing marketplace. We propose a prototype task to improve the work quality and open-governance model to achieve equitable representation. We envisage Daemo will enable workers to build sustainable careers and provide requesters with timely, quality labor for their businesses.\n",
      "=============================\n",
      "Scope+: A Stereoscopic Video See-Through Augmented Reality Microscope\n",
      "During the process of using conventional stereo microscope, users need to move their head away from the eyepieces repeatedly to access more information, such as anatomy structures from atlas. It happens during microsurgery if surgeons want to check patient?s data again. You might lose your target and your concentration after this kind of disruption. To solve this critical problem and to improve the user experience of stereo microscope, we present Scope+, a stereoscopic video see-through augmented reality system. Scope+ is designed for biological procedures, education and surgical training. While performing biological procedures, for example, dissection of a frog, anatomical atlas will show up inside the head mounted display (HMD) overlaid onto the magnified images. For education purpose, the specimens will no longer be silent under Scope+. When their body parts are pointed by a marked stick, related animation or transparent background video will merge with the real object and interact with observers. If surgeons want to improve their techniques of microsurgery, they can practice with Scope+ which provides complete foot pedal control functions identical to standard surgical microscope. Moreover, cooperating with special designed phantom models, this augmented reality system will guide you to perform some key steps of operation, such as Continuous Curvilinear Capsulorhexis in cataract surgery. Video see-through rather than optical see-through technology is adopt by Scope+ system, therefore remote observation via another Scope+ or web applications can be achieved. This feature can not only assist teachers during experiment classes, but also help researchers keep their eyes on the observables after work. Array mode is powered by the motor-driven stage plate which allows users to load multiple samples at the same time. Quick comparison between samples is possible when switching them by the foot pedal.\n",
      "=============================\n",
      "Juggling the Effects of Latency: Software Approaches to Minimizing Latency in Dynamic Projector-Camera Systems\n",
      "Projector-camera (pro-cam) systems afford a wide range of interactive possibilities, combining both natural and mixed-reality 3D interaction. However, the latency inherent within these systems can cause the projection to ?slip? from any moving target, so pro-cam systems have typically shied away from truly dynamic scenarios. We explore software-only techniques to reduce latency; considering the best achievable results with widely adopted commodity devices (e.g. 30Hz depth cameras and 60Hz projectors). We achieve 50% projection alignment on objects in free flight (a 34% improvement) and 69% alignment on dynamic human movement (a 40% improvement).\n",
      "=============================\n",
      "A Study on Grasp Recognition Independent of Users' Situations Using Built-in Sensors of Smartphones\n",
      "There are many hand postures of smartphone according to the users? situations. In order to support appropriate inter-face, it is important to know user?s hand posture. To recognize grasp postures, which is not depend on users? situations, we consider using smartphone?s touchscreen and their built-in gyroscope and accelerometer and use support vector machine (SVM). In order to evaluate our system, we described the result of the experiments when users are using the devices in the room and on the train. We knew that our system could be feasible for personal use only system by improving the information from the accelerometer. We also collected users? data when users are sitting in the room. Results showed that grasp recognition accuracy under 5 and 4 hand postures were 87.7%, 92.4% respectively when training and testing on 6 users.\n",
      "=============================\n",
      "Codeopticon: Real-Time, One-To-Many Human Tutoring for Computer Programming\n",
      "One-on-one tutoring from a human expert is an effective way for novices to overcome learning barriers in complex domains such as computer programming. But there are usually far fewer experts than learners. To enable a single expert to help more learners at once, we built Codeopticon, an interface that enables a programming tutor to monitor and chat with dozens of learners in real time. Each learner codes in a workspace that consists of an editor, compiler, and visual debugger. The tutor sees a real-time view of each learner's actions on a dashboard, with each learner's workspace summarized in a tile. At a glance, the tutor can see how learners are editing and debugging their code, and what errors they are encountering. The dashboard automatically reshuffles tiles so that the most active learners are always in the tutor's main field of view. When the tutor sees that a particular learner needs help, they can open an embedded chat window to start a one-on-one conversation. A user study showed that 8 first-time Codeopticon users successfully tutored anonymous learners from 54 countries in a naturalistic online setting. On average, in a 30-minute session, each tutor monitored 226 learners, started 12 conversations, exchanged 47 chats, and helped 2.4 learners.\n",
      "=============================\n",
      "MetaSpace: Full-body Tracking for Immersive Multiperson Virtual Reality\n",
      "Most current virtual reality (VR) interactions are mediated by hand-held input devices or hand gestures and they usually display only a partial representation of the user in the synthetic environment. We believe, representing the user as a full avatar that is controlled by natural movements of the person in the real world will lead to a greater sense of presence in VR. Possible applications exist in various domains such as entertainment, therapy, travel, real estate, education, social interaction and professional assistance. In this demo, we present MetaSpace, a virtual reality system that allows co-located users to explore a VR world together by walking around in physical space. Each user's body is represented by an avatar that is dynamically controlled by their body movements. We achieve this by tracking each user's body with a Kinect device such that their physical movements are mirrored in the virtual world. Users can see their own avatar and the other person's avatar allowing them to perceive and act intuitively in the virtual environment.\n",
      "=============================\n",
      "Color Sommelier: Interactive Color Recommendation System Based on Community-Generated Color Palettes\n",
      "We present Color Sommelier, an interactive color recommendation system based on community-generated color palettes that helps users to choose harmonious colors on the fly. We used an item-based collaborative filtering technique with Adobe Color CC palettes in order to take advantage of their ratings that reflect the general public?s color harmony preferences. Every time a user chooses a color(s), Color Sommelier calculates how harmonious each of the remaining colors is with the chosen color(s). This interactive recommendation enables users to choose colors iteratively until they are satisfied. To illustrate the usefulness of the algorithm, we implemented a coloring application with a specially designed color chooser. With the chooser, users can intuitively recognize the harmony score of each color based on its bubble size and use the recommendations at their discretion. The Color Sommelier algorithm is flexible enough to be applicable to any color chooser in any software package and is easy to implement.\n",
      "=============================\n",
      "MagPad: A Near Surface Augmented Reading System for Physical Paper and Smartphone Coupling\n",
      "In this paper, we present a novel near surface augmented reading system that brings digital content to physical papers. Our system allows a collocated mobile phone to provide augmented content based on its position on top of paper. Our system utilizes built-in magnetometer of a smartphone together with six constantly spinning magnets that generate designed patterns of magnetic flux, to detect 2D location of phone and render dynamic interactive content on the smartphone screen. The proposed technique could be implemented on most of mobile platforms without external sensing hardware.\n",
      "=============================\n",
      "Spotlights: Facilitating Skim Reading with Attention-Optimized Highlights\n",
      "This demo presents Spotlights, a technique to facilitate skim reading, or the activity of rapidly comprehending long documents such as webpages or PDFs. Users mainly use continuous rate-based scrolling to skim. However, visual attention fails when scrolling rapidly due to excessive number of objects and brief exposure per object. Spotlights supports continuous scrolling at high speeds. It selects a small number of objects and raises them to transparent overlays (spotlights) in the viewer. Spotlights stay static for a prolonged time and then fade away. The technical contribution is novel method for ?brokering? user?s attentional resources in a way that guarantees sufficient attentional resources for some objects, even at very high scrolling rates. It facilitates visual attention by (1) decreasing the number of objects competing for divided attention and (2) by ensuring sufficient processing time per object.\n",
      "=============================\n",
      "Fix and Slide: Caret Navigation with Movable Background\n",
      "We present a ?Fix and Slide? technique, which is a concept to use a movable background to place a caret insertion point and to select text on a mobile device. Standard approach to select text on the mobile devices is touching to the text where a user wants to select, and sometimes pop-up menu is displayed and s/he choose ?select? mode and then start to specify an area to be selected. A big problem is that the user?s finger hides the area to select; this is called a \"fat finger problem.\" We use the movable background to navigate a caret. First a user places a caret by tapping on a screen and then moves the background by touching and dragging on a screen. In this situation, the caret is fixed on the screen so that the user can move the background to navigate the caret where the user wants to move the caret. We implement the Fix and Slide technique on iOS device (iPhone) to demonstrate the impact of this text selection technique on small mobile devices.\n",
      "=============================\n",
      "KickSoul: A Wearable System for Feet Interactions with Digital Devices\n",
      "In this paper we present a wearable device that maps natural feet movements into inputs for digital devices. KickSoul consists of an insole with sensors embedded that tracks movements and triggers actions in devices that surround us. We present a novel approach to use our feet as input devices in mobile situations when our hands are busy. We analyze natural feet?s movements and their meaning before activating an action. This paper discusses different applications for this technology as well as the implementation of our prototype.\n",
      "=============================\n",
      "ATK: Enabling Ten-Finger Freehand Typing in Air Based on 3D Hand Tracking Data\n",
      "Ten-finger freehand mid-air typing is a potential solution for post-desktop interaction. However, the absence of tactile feedback as well as the inability to accurately distinguish tapping finger or target keys exists as the major challenge for mid-air typing. In this paper, we present ATK, a novel interaction technique that enables freehand ten-finger typing in the air based on 3D hand tracking data. Our hypothesis is that expert typists are able to transfer their typing ability from physical keyboards to mid-air typing. We followed an iterative approach in designing ATK. We first empirically investigated users' mid-air typing behavior, and examined fingertip kinematics during tapping, correlated movement among fingers and 3D distribution of tapping endpoints. Based on the findings, we proposed a probabilistic tap detection algorithm, and augmented Goodman's input correction model to account for the ambiguity in distinguishing tapping finger. We finally evaluated the performance of ATK with a 4-block study. Participants typed 23.0 WPM with an uncorrected word-level error rate of 0.3% in the first block, and later achieved 29.2 WPM in the last block without sacrificing accuracy.\n",
      "=============================\n",
      "Haptic-enabled Active Bone-Conducted Sound Sensing\n",
      "In this study, we propose active bone-conducted sound sens- ing for estimating a joint angle of a finger and simultaneous use as a haptic interface. For estimating the joint angle, an unnoticeable vibration is input to the finger, and a perceptible vibration is additionally input to the finger for providing hap- tic feedback. The joint angle is estimated by switching the estimation model depending on the haptic feedback and the average error of the estimation is within about seven degree.\n",
      "=============================\n",
      "Reconfiguring and Fabricating Special-Purpose Tangible Controls\n",
      "Unlike regular interfaces on touch screens or desktop computers, tangible user interfaces allow for more physically rich interactions that better uses the capacity of our motor system. On the flipside, the physicality of tangibles comes with rigidity. This makes it hard to (1) use tangibles on systems that require a variety of controls and interaction styles, and (2) make changes to physical interfaces once manufactured. In my research, I explore techniques that allow users to reconfigure and fabricate tangible interfaces in order to mitigate these issues.\n",
      "=============================\n",
      "Gunslinger: Subtle Arms-down Mid-air Interaction\n",
      "We describe Gunslinger, a mid-air interaction technique using barehand postures and gestures. Unlike past work, we explore a relaxed arms-down position with both hands interacting at the sides of the body. It features \"hand-cursor\" feedback to communicate recognized hand posture, command mode and tracking quality; and a simple, but flexible hand posture recognizer. Although Gunslinger is suitable for many usage contexts, we focus on integrating mid-air gestures with large display touch input. We show how the Gunslinger form factor enables an interaction language that is equivalent, coherent, and compatible with large display touch input. A four-part study evaluates Midas Touch, posture recognition feedback, pointing and clicking, and general usability.\n",
      "=============================\n",
      "Elastic Cursor and Elastic Edge: Applying Simulated Resistance to Interface Elements for Seamless Edge-scroll\n",
      "We present elastic cursor and elastic edge, new interaction techniques for seamless edge-scroll. Through the use of light-weight physical simulations of elastic behavior on interface elements, we can improve precision, usability, and cueing on the use of edge-scroll in scrollable windows or screens, and make experiences more playful and easier to learn.\n",
      "=============================\n",
      "Machine Intelligence and Human Intelligence\n",
      "There has been a stellar rise in computational power since 2006 in part thanks to GPUs, yet today, we are as an intelligent species essentially singular. There are of course some other brainy species, like chimpanzees, dolphins, crows and octopuses, but if anything they only emphasize our unique position on Earth -- as animals richly gifted with self-awareness, language, abstract thought, art, mathematical capability, science, technology and so on. Many of us have staked our entire self-concept on the idea that to be human is to have a mind, and that minds are the unique province of humans. For those of us who are not religious, this could be interpreted as the last bastion of dualism. Our economic, legal and ethical systems are also implicitly built around this idea. Now, we're well along the road to really understanding the fundamental principles of how a mind can be built, and Moore's Law will put brain-scale computing within reach this decade. (We need to put some asterisks next to Moore's Law, since we are already running up against certain limits in computational scale using our present-day approaches, but I'll stand behind the broader statement.) In this talk I will discuss the relationships between engineered neurally inspired systems and brains today, between humans and machines tomorrow, and how these relationships will alter user interfaces, software and technology.\n",
      "=============================\n",
      "BitDrones: Towards Levitating Programmable Matter Using Interactive 3D Quadcopter Displays\n",
      "In this paper, we present BitDrones, a platform for the construction of interactive 3D displays that utilize nano quadcopters as self-levitating tangible building blocks. Our prototype is a first step towards supporting interactive mid-air, tangible experiences with physical interaction techniques through multiple building blocks capable of physically representing interactive 3D data.\n",
      "=============================\n",
      "Extreme Computational Photography\n",
      "The Camera Culture Group at the MIT Media Lab aims to create a new class of imaging platforms. This talk will discuss three tracks of research: femto photography, retinal imaging, and 3D displays. Femto Photography consists of femtosecond laser illumination, picosecond-accurate detectors and mathematical reconstruction techniques allowing researchers to visualize propagation of light. Direct recording of reflected or scattered light at such a frame rate with sufficient brightness is nearly impossible. Using an indirect 'stroboscopic' method that records millions of repeated measurements by careful scanning in time and viewpoints we can rearrange the data to create a 'movie' of a nanosecond long event. Femto photography and a new generation of nano-photography (using ToF cameras) allow powerful inference with computer vision in presence of scattering. EyeNetra is a mobile phone attachment that allows users to test their own eyesight. The device reveals corrective measures thus bringing vision to billions of people who would not have had access otherwise. Another project, eyeMITRA, is a mobile retinal imaging solution that brings retinal exams to the realm of routine care, by lowering the cost of the imaging device to a 10th of its current cost and integrating the device with image analysis software and predictive analytics. This provides early detection of Diabetic Retinopathy that can change the arc of growth of the world's largest cause of blindness. Finally the talk will describe novel lightfield cameras and lightfield displays that require a compressive optical architecture to deal with high bandwidth requirements of 4D signals\n",
      "=============================\n",
      "Procedural Modeling Using Autoencoder Networks\n",
      "Procedural modeling systems allow users to create high quality content through parametric, conditional or stochastic rule sets. While such approaches create an abstraction layer by freeing the user from direct geometry editing, the nonlinear nature and the high number of parameters associated with such design spaces result in arduous modeling experiences for non-expert users. We propose a method to enable intuitive exploration of such high dimensional procedural modeling spaces within a lower dimensional space learned through autoencoder network training. Our method automatically generates a representative training dataset from the procedural modeling rule set based on shape similarity features. We then leverage the samples in this dataset to train an autoencoder neural network, while also structuring the learned lower dimensional space for continuous exploration with respect to shape features. We demonstrate the efficacy our method with user studies where designers create content with more than 10-fold faster speeds using our system compared to the classic procedural modeling interface.\n",
      "=============================\n",
      "Improving Haptic Feedback on Wearable Devices through Accelerometer Measurements\n",
      "Many variables have been shown to impact whether a vibration stimulus will be perceived. We present a user study that takes into account not only previously investigated predictors such as vibration intensity and duration along with the age of the person receiving the stimulus, but also the amount of motion, as measured by an accelerometer, at the site of vibration immediately preceding the stimulus. This is a more specific measure than in previous studies showing an effect on perception due to gross conditions such as walking. We show that a logistic regression model including prior acceleration is significantly better at predicting vibration perception than a model including only vibration intensity, duration and participant age. In addition to the overall regression, we discuss individual participant differences and measures of classification performance for real-world applications. Our expectation is that haptic interface designers will be able to use such results to design better vibrations that are perceivable under the user's current activity conditions, without being annoyingly loud or jarring, eventually approaching ``perceptually equivalent' feedback independent of motion.\n",
      "=============================\n",
      "Patching Physical Objects\n",
      "Personal fabrication is currently a one-way process: Once an object has been fabricated with a 3D printer, it cannot be changed anymore; any change requires printing a new version from scratch. The problem is that this approach ignores the nature of design iteration, i.e. that in subsequent iterations large parts of an object stay the same and only small parts change. This makes fabricating from scratch feel unnecessary and wasteful. In this paper, we propose a different approach: instead of re-printing the entire object from scratch, we suggest patching the existing object to reflect the next design iteration. We built a system on top of a 3D printer that accomplishes this: Users mount the existing object into the 3D printer, then load both the original and the modified 3D model into our software, which in turn calculates how to patch the object. After identifying which parts to remove and what to add, our system locates the existing object in the printer using the system's built-in 3D scanner. After calibrating the orientation, a mill first removes the outdated geometry, then a print head prints the new geometry in place. Since only a fraction of the entire object is refabricated, our approach reduces material consumption and plastic waste (for our example objects by 82% and 93% respectively).\n",
      "=============================\n",
      "Codo: Fundraising with Conditional Donations\n",
      "Crowdfunding websites like Kickstarter and Indiegogo offer project organizers the ability to market, fund, and build a community around their campaign. While offering support and flexibility for organizers, crowdfunding sites provide very little control to donors. In this paper, we investigate the idea of empowering donors by allowing them to specify conditions for their crowdfunding contributions. We introduce a crowdfunding system, Codo, that allows donors to specify conditional donations. Codo allow donors to contribute to a campaign but hold off on their contribution until certain specific conditions are met (e.g. specific members or groups contribute a certain amount). We begin with a micro study to assess several specific conditional donations based on their comprehensibility and usage likelihood. Based on this study, we formalize conditional donations into a general grammar that captures a broad set of useful conditions. We demonstrate the feasibility of resolving conditions in our grammar by elegantly transforming conditional donations into a system of linear inequalities that are efficiently resolved using off-the-shelf linear program solvers. Finally, we designed a user-friendly crowdfunding interface that supports conditional donations for an actual fund raising campaign and assess the potential of conditional donations through this campaign. We find preliminary evidence that roughly 1 in 3 donors make conditional donations and that conditional donors donate more compared to direct donors.\n",
      "=============================\n",
      "From Papercraft to Paper Mechatronics: Exploring a New Medium and Developing a Computational Design Tool\n",
      "Paper Mechatronics is a novel interdisciplinary design medium, enabled by recent advances in craft technologies: the term refers to a reappraisal of traditional papercraft in combination with accessible mechanical, electronic, and computational elements. I am investigating the design space of paper mechatronics as a new hands-on medium by developing a series of examples and building a computational tool, FoldMecha, to support non-experts to design and construct their own paper mechatronics models. This paper describes how I used the tool to create two kinds of paper mechatronics models: walkers and flowers and discuss next steps.\n",
      "=============================\n",
      "Investigating the \"Wisdom of Crowds\" at Scale\n",
      "In a variety of problem domains, it has been observed that the aggregate opinions of groups are often more accurate than those of the constituent individuals, a phenomenon that has been termed the \"wisdom of the crowd.\" Yet, perhaps surprisingly, there is still little consensus on how generally the phenomenon holds, how best to aggregate crowd judgements, and how social influence affects estimates. We investigate these questions by taking a meta wisdom of crowds approach. With a distributed team of over 100 student researchers across 17 institutions in the United States and India, we develop a large-scale online experiment to systematically study the wisdom of crowds effect for 1,000 different tasks in 50 subject domains. These tasks involve various types of knowledge (e.g., explicit knowledge, tacit knowledge, and prediction), question formats (e.g., multiple choice and point estimation), and inputs (e.g., text, audio, and video). To examine the effect of social influence, participants are randomly assigned to one of three different experiment conditions in which they see varying degrees of information on the responses of others. In this ongoing project, we are now preparing to recruit participants via Amazon?s Mechanical Turk.\n",
      "=============================\n",
      "Supporting Collaborative Innovation at Scale\n",
      "Emerging online innovation platforms have enabled large groups of people to collaborate and generate ideas together in ways that were not possible before. However, these platforms also introduce new challenges in finding inspiration from a large number of ideas, and coordinating the collective effort. In my dissertation, I address the challenges of large scale idea generation platforms by developing methods and systems for helping people make effective use of each other's ideas, and for orchestrating collective effort to reduce redundancy and increase the quality and breadth of generated ideas.\n",
      "=============================\n",
      "PERCs: Persistently Trackable Tangibles on Capacitive Multi-Touch Displays\n",
      "Tangible objects on capacitive multi-touch surfaces are usually only detected while the user is touching them. When the user lets go of such a tangible, the system cannot distinguish whether the user just released the tangible, or picked it up and removed it from the surface. We introduce PERCs, persistent capacitive tangibles that \"know\" whether they are currently on a capacitive touch surface or not. This is achieved by adding a small field sensor to the tangible to detect the touch screen's own, weak electromagnetic touch detection probing signal. Thus, unlike previous designs, PERCs do not get filtered out over time by the adaptive signal filters of the touch screen. We provide a technical overview of the theory be- hind PERCs and our prototype construction, and we evaluate detection rates, timing performance, and positional and angular accuracy for PERCs on a variety of unmodified, commercially available multi-touch devices.Through their affordable circuitry and high accuracy, PERCs open up the potential for a variety of new applications that use tangibles on today's ubiquitous multi-touch devices.\n",
      "=============================\n",
      "Explaining Visual Changes in Web Interfaces\n",
      "Web developers often want to repurpose interactive behaviors from third-party web pages, but struggle to locate the specific source code that implements the behavior. This task is challenging because developers must find and connect all of the non-local interactions between event-based JavaScript code, declarative CSS styles, and web page content that combine to express the behavior. The Scry tool embodies a new approach to locating the code that implements interactive behaviors. A developer selects a page element; whenever the element changes, Scry captures the rendering engine's inputs (DOM, CSS) and outputs (screenshot) for the element. For any two captured element states, Scry can compute how the states differ and which lines of JavaScript code were responsible. Using Scry, a developer can locate an interactive behavior's implementation by picking two output states; Scry indicates the JavaScript code directly responsible for their differences.\n",
      "=============================\n",
      "GravitySpot: Guiding Users in Front of Public Displays Using On-Screen Visual Cues\n",
      "Users tend to position themselves in front of interactive public displays in such a way as to best perceive its content. Currently, this sweet spot is implicitly defined by display properties, content, the input modality, as well as space constraints in front of the display. We present GravitySpot - an approach that makes sweet spots flexible by actively guiding users to arbitrary target positions in front of displays using visual cues. Such guidance is beneficial, for example, if a particular input technology only works at a specific distance or if users should be guided towards a non-crowded area of a large display. In two controlled lab studies (n=29) we evaluate different visual cues based on color, shape, and motion, as well as position-to-cue mapping functions. We show that both the visual cues and mapping functions allow for fine-grained control over positioning speed and accuracy. Findings are complemented by observations from a 3-month real-world deployment.\n",
      "=============================\n",
      "CyclopsRing: Enabling Whole-Hand and Context-Aware Interactions Through a Fisheye Ring\n",
      "This paper presents CyclopsRing, a ring-style fisheye imaging wearable device that can be worn on hand webbings to en- able whole-hand and context-aware interactions. Observing from a central position of the hand through a fisheye perspective, CyclopsRing sees not only the operating hand, but also the environmental contexts that involve with the hand-based interactions. Since CyclopsRing is a finger-worn device, it also allows users to fully preserve skin feedback of the hands. This paper demonstrates a proof-of-concept device, reports the performance in hand-gesture recognition using random decision forest (RDF) method, and, upon the gesture recognizer, presents a set of interaction techniques including on-finger pinch-and-slide input, in-air pinch-and-motion input, palm-writing input, and their interactions with the environ- mental contexts. The experiment obtained an 84.75% recognition rate of hand gesture input from a database of seven hand gestures collected from 15 participants. To our knowledge, CyclopsRing is the first ring-wearable device that supports whole-hand and context-aware interactions.\n",
      "=============================\n",
      "Improving Automated Email Tagging with Implicit Feedback\n",
      "Tagging email is an important tactic for managing information overload. Machine learning methods can help the user with this task by predicting tags for incoming email messages. The natural user interface displays the predicted tags on the email message, and the user doesn't need to do anything unless those predictions are wrong (in which case, the user can delete the incorrect tags and add the missing tags). From a machine learning perspective, this means that the learning algorithm never receives confirmation that its predictions are correct---it only receives feedback when it makes a mistake. This can lead to slower learning, particularly when the predictions were not very confident, and hence, the learning algorithm would benefit from positive feedback. One could assume that if the user never changes any tag, then the predictions are correct, but users sometimes forget to correct the tags, presumably because they are focused on the content of the email messages and fail to notice incorrect and missing tags. The aim of this paper is to determine whether implicit feedback can provide useful additional training examples to the email prediction subsystem of TaskTracer, known as EP2 (Email Predictor 2). Our hypothesis is that the more time a user spends working on an email message, the more likely it is that the user will notice tag errors and correct them. If no corrections are made, then perhaps it is safe for the learning system to treat the predicted tags as being correct and train accordingly. This paper proposes three algorithms (and two baselines) for incorporating implicit feedback into the EP2 tag predictor. These algorithms are then evaluated using email interaction and tag correction events collected from 14 user-study participants as they performed email-directed tasks while using TaskTracer EP2. The results show that implicit feedback produces important increases in training feedback, and hence, significant reductions in subsequent prediction errors despite the fact that the implicit feedback is not perfect. We conclude that implicit feedback mechanisms can provide a useful performance boost for email tagging systems.\n",
      "=============================\n",
      "LaserStacker: Fabricating 3D Objects by Laser Cutting and Welding\n",
      "Laser cutters are useful for rapid prototyping because they are fast. However, they only produce planar 2D geometry. One approach to creating non-planar objects is to cut the object in horizontal slices and to stack and glue them. This approach, however, requires manual effort for the assembly and time for the glue to set, defeating the purpose of using a fast fabrication tool. We propose eliminating the assembly step with our system LaserStacker. The key idea is to use the laser cutter to not only cut but also to weld. Users place not one acrylic sheet, but a stack of acrylic sheets into their cutter. In a single process, LaserStacker cuts each individual layer to shape (through all layers above it), welds layers by melting material at their interface, and heals undesired cuts in higher layers. When users take out the object from the laser cutter, it is already assembled. To allow users to model stacked objects efficiently, we built an extension to a commercial 3D editor (SketchUp) that provides tools for defining which parts should be connected and which remain loose. When users hit the export button, LaserStacker converts the 3D model into cutting, welding, and healing instructions for the laser cutter. We show how LaserStacker does not only allow making static objects, such as architectural models, but also objects with moving parts and simple mechanisms, such as scissors, a simple pinball machine, and a mechanical toy with gears.\n",
      "=============================\n",
      "SHOCam: A 3D Orbiting Algorithm\n",
      "In this paper we describe a new orbiting algorithm, called SHOCam, which enables simple, safe and visually attractive control of a camera moving around 3D objects. Compared with existing methods, SHOCam provides a more consistent mapping between the user's interaction and the path of the camera by substantially reducing variability in both camera motion and look direction. Also, we present a new orbiting method that prevents the camera from penetrating object(s), making the visual feedback -- and with it the user experience -- more pleasing and also less error prone. Finally, we present new solutions for orbiting around multiple objects and multi-scale environments.\n",
      "=============================\n",
      "On Sounder Ground: CAAT, a Viable Widget for Affective Reaction Assessment\n",
      "The reliable assessment of affective reactions to stimuli is paramount in a variety of scientific fields, including HCI (Human-Computer Interaction). Variation of emotional states over time, however, warrants the need for quick measurements of emotions. To address it, new tools for quick assessments of affective states have been developed. In this work, we explore the CAAT (Circumplex Affective Assessment Tool), an instrument with a unique design in the scope of affect assessment -- a graphical control element -- that makes it amenable to seamless integration in user interfaces. We briefly describe the CAAT and present a multi-dimensional evaluation that evidences the tool's viability. We have assessed its test-retest reliability, construct validity and quickness of use, by collecting data through an unsupervised, web-based user study. Results show high test-retest reliability, evidence the tool's construct validity and confirm its quickness of use, making it a good fit for longitudinal studies and systems requiring quick assessments of emotional reactions.\n",
      "=============================\n",
      "Fisheye keyboard : whole keyboard displayed on small device\n",
      "In this article, we propose a soft keyboard with interaction inspired by research on visualisation information. Our goal is to find a compromise between readability and usability on a whole character layout for an Ultra mobile PC. The proposed interactions allow to display all keys on a small screen while making pointing easier for the user by expanding any given key as a function of its distance from the stylus.\n",
      "=============================\n",
      "SmartTokens: Embedding Motion and Grip Sensing in Small Tangible Objects\n",
      "SmartTokens are small-sized tangible tokens that can sense multiple types of motion, multiple types of touch/grip, and send input events wirelessly as state-machine transitions. By providing an open platform for embedding basic sensing capabilities within small form-factors, SmartTokens extend the design space of tangible user interfaces. We describe the design and implementation of SmartTokens and illustrate how they can be used in practice by introducing a novel TUI design for event notification and personal task management.\n",
      "=============================\n",
      "Unravel: Rapid Web Application Reverse Engineering via Interaction Recording, Source Tracing, and Library Detection\n",
      "Professional websites with complex UI features provide real world examples for developers to learn from. Yet despite the availability of source code, it is still difficult to understand how these features are implemented. Existing tools such as the Chrome Developer Tools and Firebug offer debugging and inspection, but reverse engineering is still a time consuming task. We thus present Unravel, an extension of the Chrome Developer Tools for quickly tracking and visualizing HTML changes, JavaScript method calls, and JavaScript libraries. Unravel injects an observation agent into websites to monitor DOM interactions in real-time without functional interference or external dependencies. To manage potentially large observations of events, the Unravel UI provides affordances to reduce, sort, and scope observations. Testing Unravel with 13 web developers on 5 large-scale websites, we found a 53% decrease in time to discovering the first key source behind a UI feature and a 32% decrease in time to understanding how to fully recreate a feature.\n",
      "=============================\n",
      "Projectibles: Optimizing Surface Color For Projection\n",
      "Typically video projectors display images onto white screens, which can result in a washed out image. Projectibles algorithmically control the display surface color to increase the contrast and resolution. By combining a printed image with projected light, we can create animated, high resolution, high dynamic range visual experiences for video sequences. We present two algorithms for separating an input video sequence into a printed component and projected component, maximizing the combined contrast and resolution while minimizing any visual artifacts introduced from the decomposition. We present empirical measurements of real-world results of six example video sequences, subjective viewer feedback ratings, and we discuss the benefits and limitations of Projectibles. This is the first approach to combine a static display with a dynamic display for the display of video, and the first to optimize surface color for projection of video.\n",
      "=============================\n",
      "cLuster: Smart Clustering of Free-Hand Sketches on Large Interactive Surfaces\n",
      "Structuring and rearranging free-hand sketches on large interactive surfaces typically requires making multiple stroke selections. This can be both time-consuming and fatiguing in the absence of well-designed selection tools. Investigating the concept of automated clustering, we conducted a background study which highlighted the fact that people have varying perspectives on how elements in sketches can and should be grouped. In response to these diverse user expectations, we present cLuster, a flexible, domain-independent clustering approach for free-hand sketches. Our approach is designed to accept an initial user selection, which is then used to calculate a linear combination of pre-trained perspectives in real-time. The remaining elements are then clustered. An initial evaluation revealed that in many cases, only a few corrections were necessary to achieve the desired clustering results. Finally, we demonstrate the utility of our approach in a variety of application scenarios.\n",
      "=============================\n",
      "Capture-Time Feedback for Recording Scripted Narration\n",
      "Well-performed audio narrations are a hallmark of captivating podcasts, explainer videos, radio stories, and movie trailers. To record these narrations, professional voiceover actors follow guidelines that describe how to use low-level vocal components---volume, pitch, timbre, and tempo---to deliver performances that emphasize important words while maintaining variety, flow and diction. Yet, these techniques are not well-known outside the professional voiceover community, especially among hobbyist producers looking to create their own narrations. We present Narration Coach, an interface that assists novice users in recording scripted narrations. As a user records her narration, our system synchronizes the takes to her script, provides text feedback about how well she is meeting the expert voiceover guidelines, and resynthesizes her recordings to help her hear how she can speak better.\n",
      "=============================\n",
      "Zensei: Augmenting Objects with Effortless User Recognition Capabilities through Bioimpedance Sensing\n",
      "As interactions with everyday handheld devices and objects become increasingly common, a more seamless and effortless identification and personalization technique will be essential to an uninterrupted user experience. In this paper, we present Zensei, a user identification and customization system using human body bioimpedance sensing through multiple electrodes embedded into everyday objects. Zensei provides for an uninterrupted user-device personalization experience that is difficult to forge because it uses both the unique physiological and behavioral characteristics of the user. We demonstrate our measurement system in three exemplary device configurations that showcase different levels of constraint via environment-based, whole-body-based, and handheld-based identification scenarios. We evaluated Zensei's classification accuracy among 12 subjects on each configuration over 22 days of collected data and report our promising results.\n",
      "=============================\n",
      "EMG Sensor-based Two-Hand Smart Watch Interaction\n",
      "These days, smart watches have drawn more attention of users, and many smart watch products have been launched (Samsung Gear series, apple watch and etc.). Since a smart watch is put on the wrist, the device should be small and unobtrusive. Because of these features, display of the smart watch is small and there is a limitation to interaction. To overcome the limitation, many studies are conducted. In this paper, we propose a two-hand interaction technique that obtains posture information of a hand using electromyography (EMG) sensor attached to the arm and to make input interaction to a smart watch different depending on each posture. EMG sensors recognize information about a user's hand posture, and the non-dominant hand is used for smart watch inputs. In this way, different function is executed depending on postures. As a result, a smart watch that has limited input methods is given a variety of interaction functions with users.\n",
      "=============================\n",
      "HapticPrint: Designing Feel Aesthetics for Digital Fabrication\n",
      "Digital fabrication has enabled massive creativity in hobbyist communities and professional product design. These emerging technologies excel at realizing an arbitrary shape or form; however these objects are often rigid and lack the feel desired by designers. We aim to enable physical haptic design in passive 3D printed objects. This paper identifies two core areas for extending physical design into digital fabrication: designing the external and internal haptic characteristics of an object. We present HapticPrint as a pair of design tools to easily modify the feel of a 3D model. Our external tool maps textures and UI elements onto arbitrary shapes, and our internal tool modifies the internal geometry of models for novel compliance and weight characteristics. We demonstrate the value of HapticPrint with a range of applications that expand the aesthetics of feel, usability, and interactivity in 3D artifacts.\n",
      "=============================\n",
      "3D Printed Hair: Fused Deposition Modeling of Soft Strands, Fibers, and Bristles\n",
      "We introduce a technique for furbricating 3D printed hair, fibers and bristles, by exploiting the stringing phenomena inherent in 3D printers using fused deposition modeling. Our approach offers a range of design parameters for controlling the properties of single strands and also of hair bundles. We further detail a list of post-processing techniques for refining the behavior and appearance of printed strands. We provide several examples of output, demonstrating the immediate feasibility of our approach using a low cost, commodity printer. Overall, this technique extends the capabilities of 3D printing in a new and interesting way, without requiring any new hardware.\n",
      "=============================\n",
      "Improving Virtual Keyboards When All Finger Positions Are Known\n",
      "Current virtual keyboards are known to be slower and less convenient than physical QWERTY keyboards because they simply imitate the traditional QWERTY keyboards on touchscreens. In order to improve virtual keyboards, we consider two reasonable assumptions based on the observation of skilled typists. First, the keys are already assigned to each finger for typing. Based on this assumption, we suggest restricting each finger to entering pre-allocated keys only. Second, non-touching fingers move in correlation with the touching finger because of the intrinsic structure of human hands. To verify of our assumptions, we conducted two experiments with skilled typists. In the first experiment, we statistically verified the second assumption. We then suggest a novel virtual keyboard using our observations. In the second experiment, we show that our suggested keyboard outperforms existing virtual keyboards.\n",
      "=============================\n",
      "BackHand: Sensing Hand Gestures via Back of the Hand\n",
      "In this paper, we explore using the back of hands for sensing hand gestures, which interferes less than glove-based approaches and provides better recognition than sensing at wrists and forearms. Our prototype, BackHand, uses an array of strain gauge sensors affixed to the back of hands, and applies machine learning techniques to recognize a variety of hand gestures. We conducted a user study with 10 participants to better understand gesture recognition accuracy and the effects of sensing locations. Results showed that sensor reading patterns differ significantly across users, but are consistent for the same user. The leave-one-user-out accuracy is low at an average of 27.4%, but reaches 95.8% average accuracy for 16 popular hand gestures when personalized for each participant. The most promising location spans the 1/8~1/4 area between the metacarpophalangeal joints (MCP, the knuckles between the hand and fingers) and the head of ulna (tip of the wrist).\n",
      "=============================\n",
      "Webstrates: Shareable Dynamic Media\n",
      "We revisit Alan Kay's early vision of dynamic media that blurs the distinction between documents and applications. We introduce shareable dynamic media that are malleable by users, who may appropriate them in idiosyncratic ways; shareable among users, who collaborate on multiple aspects of the media; and distributable across diverse devices and platforms. We present Webstrates, an environment for exploring shareable dynamic media. Webstrates augment web technology with real-time sharing. They turn web pages into substrates, i.e. software entities that act as applications or documents depending upon use. We illustrate Webstrates with two implemented case studies: users collaboratively author an article with functionally and visually different editors that they can personalize and extend at run-time; and they orchestrate its presentation and audience participation with multiple devices. We demonstrate the simplicity and generative power of Webstrates with three additional prototypes and evaluate it from a systems perspective.\n",
      "=============================\n",
      "WearWrite: Orchestrating the Crowd to Complete Complex Tasks from Wearables\n",
      "Smartwatches are becoming increasingly powerful, but limited input makes completing complex tasks impractical. Our WearWrite system introduces a new paradigm for enabling a watch user to contribute to complex tasks, not through new hardware or input methods, but by directing a crowd to work on their behalf from their wearable device. WearWrite lets authors give writing instructions and provide bits of expertise and big picture directions from their smartwatch, while crowd workers actually write the document on more powerful devices. We used this approach to write three academic papers, and found it was effective at producing reasonable drafts.\n",
      "=============================\n",
      "ReForm: Integrating Physical and Digital Design through Bidirectional Fabrication\n",
      "Digital fabrication machines such as 3D printers and laser-cutters allow users to produce physical objects based on virtual models. The creation process is currently unidirectional: once an object is fabricated it is separated from its originating virtual model. Consequently, users are tied into digital modeling tools, the virtual design must be completed before fabrication, and once fabricated, re-shaping the physical object no longer influences the digital model. To provide a more flexible design process that allows objects to iteratively evolve through both digital and physical input, we introduce bidirectional fabrication. To demonstrate the concept, we built ReForm, a system that integrates digital modeling with shape input, shape output, annotation for machine commands, and visual output. By continually synchronizing the physical object and digital model it supports object versioning to allow physical changes to be undone. Through application examples, we demonstrate the benefits of ReForm to the digital fabrication process.\n",
      "=============================\n",
      "Self-Calibrating Head-Mounted Eye Trackers Using Egocentric Visual Saliency\n",
      "Head-mounted eye tracking has significant potential for gaze-based applications such as life logging, mental health monitoring, or the quantified self. A neglected challenge for the long-term recordings required by these applications is that drift in the initial person-specific eye tracker calibration, for example caused by physical activity, can severely impact gaze estimation accuracy and thus system performance and user experience. We first analyse calibration drift on a new dataset of natural gaze data recorded using synchronised video-based and Electrooculography-based eye trackers of 20 users performing everyday activities in a mobile setting. Based on this analysis we present a method to automatically self-calibrate head-mounted eye trackers based on a computational model of bottom-up visual saliency. Through evaluations on the dataset we show that our method 1) is effective in reducing calibration drift in calibrated eye trackers and 2) given sufficient data, can achieve gaze estimation accuracy competitive with that of a calibrated eye tracker, without any manual calibration.\n",
      "=============================\n",
      "These Aren't the Commands You're Looking For: Addressing False Feedforward in Feature-Rich Software\n",
      "The names, icons, and tooltips of commands in feature-rich software are an important source of guidance when locating and selecting amongst commands. Unfortunately, these cues can mislead users into believing that a command is appropriate for a given task, when another command would be more appropriate, resulting in wasted time and frustration. In this paper, we present command disambiguation techniques that inform the user of alternative commands before, during, and after an incorrect command has been executed. To inform the design of these techniques, we define categories of false-feedforward errors caused by misleading interface cues, and identify causes for each. Our techniques are the first designed explicitly to solve this problem in feature-rich software. A user study showed enthusiasm for the techniques, and revealed their potential to play a key role in learning of feature-rich software.\n",
      "=============================\n",
      "DataTone: Managing Ambiguity in Natural Language Interfaces for Data Visualization\n",
      "Answering questions with data is a difficult and time-consuming process. Visual dashboards and templates make it easy to get started, but asking more sophisticated questions often requires learning a tool designed for expert analysts. Natural language interaction allows users to ask questions directly in complex programs without having to learn how to use an interface. However, natural language is often ambiguous. In this work we propose a mixed-initiative approach to managing ambiguity in natural language interfaces for data visualization. We model ambiguity throughout the process of turning a natural language query into a visualization and use algorithmic disambiguation coupled with interactive ambiguity widgets. These widgets allow the user to resolve ambiguities by surfacing system decisions at the point where the ambiguity matters. Corrections are stored as constraints and influence subsequent queries. We have implemented these ideas in a system, DataTone. In a comparative study, we find that DataTone is easy to learn and lets users ask questions without worrying about syntax and proper question form.\n",
      "=============================\n",
      "Hand Biometrics Using Capacitive Touchscreens\n",
      "Biometric methods for authentication on mobile devices are becoming popular. Some methods such as face and voice biometrics are problematic in noisy mobile environments, while others such as fingerprint require specialized hardware to operate. We present a novel biometric authentication method that uses raw touch capacitance data captured from the hand touching a display. Performance results using a moderate sample size (N = 40) yielded an equal error rate (EER) of 2.5%, while a 1-month longitudinal study using a smaller sample (N = 10) yielded an EER = 2.3%. Overall, our results provide evidence for biometric uniqueness, permanence and user acceptance.\n",
      "=============================\n",
      "Enriching Online Classroom Communication with Collaborative Multi-Modal Annotations\n",
      "In massive open online courses, peer discussion is a scalable solution for offering interactive and engaging learning experiences to a large number of students. On the other hand, the quality of communication mediated through online discussion tools, such as discussion forums, is far less expressive than that of face-to-face communication. As a solution, I present RichReview, a multi-modal annotation system through which distant students can exchange ideas using versatile combinations of voice, text, and pointing gestures. A series of lab and deployment studies of RichReview promised that the expressive multimedia mixture and lightweight audio browsing feature help students better understand commentators? intention. For the large-scale deployment, I redesigned RichReview as a web applet in edX?s courseware framework. By deploying the system at scale, I will investigate (1) the optimal group assignment scheme that maximizes overall diversities of group members, (2) educational data mining applications based on user-generated rich discussion data, and (3) the impact of the rich discussion to students? retention of knowledge. Throughout these studies, I will argue that a multi-modal anchored digital document annotation system enables rich online peer discussion at scale.\n",
      "=============================\n",
      "Graphical Passwords for Older Computer Users\n",
      "Computers and the internet have been challenging for many computer users over the age of 60. We conducted a survey of older users which revealed that the creation, management and recall of strong text passwords were some of the challenging aspects of modern technology. In practice, this user group based passwords on familiar facts such as family member names, pets, phone numbers and important personal dates. Graphical passwords formed from abstract graphical symbols or anonymous facial images are feasible, but harder for older computers users to grasp and recall. In this paper we describe initial results for our graphical password system based on recognition of culturally-familiar facial images that are age-relevant to the life experiences of older users. Our goals are to design an easy-to-memorize, graphical password system intended specifically for older users, and achieve a level of password entropy comparable to traditional PINs and text passwords. We are also conducting a user study to demonstrate our technique and capture performance and recall metrics for comparison with traditional password systems.\n",
      "=============================\n",
      "Perspective-dependent Indirect Touch Input for 3D Polygon Extrusion\n",
      "We present a two-handed indirect touch interaction technique for the extrusion of polygons within a 3D modeling tool that we have built for a horizontal/vertical dual touch screen setup. In particular, we introduce perspective-dependent touch gestures: using several graphical input areas on the horizontal display, the non-dominant hand navigates the virtual camera and thus continuously updates the spatial frame of reference within which the dominant hand performs extrusions with dragging gestures.\n",
      "=============================\n",
      "LegionTools: A Toolkit + UI for Recruiting and Routing Crowds to Synchronous Real-Time Tasks\n",
      "We introduce LegionTools, a toolkit and interface for managing large, synchronous crowds of online workers for experiments. This poster contributes the design and implementation of a state-of-the-art crowd management tool, along with a publicly-available, open-source toolkit that future system builders can use to coordinate synchronous crowds of online workers for their systems and studies. We describe the toolkit itself, along with the underlying design rationale, in order to make it clear to the community of system builders at UIST when and how this tool may be beneficial to their project. We also describe initial deployments of the system in which workers were synchronously recruited to support real-time crowdsourcing systems, including the largest synchronous recruitment and routing of workers from Mechanical Turk that we are aware of. While the version of LegionTools discussed here focuses on Amazon's Mechanical Turk platform, it can be easily extended to other platforms as APIs become available.\n",
      "=============================\n",
      "Joint 5D Pen Input for Light Field Displays\n",
      "Light field displays allow viewers to see view-dependent 3D content as if looking through a window; however, existing work on light field display interaction is limited. Yet, they have the potential to parallel 2D pen and touch screen systems, which present a joint input and display surface for natural interaction. We propose a 4D display and interaction space using a dual-purpose lenslet array, which combines light field display and light field pen sensing, and allows us to estimate the 3D position and 2D orientation of the pen. This method is simple, fast (150Hz), with position accuracy of 2-3mm and precision of 0.2-0.6mm from 0-350mm away from the lenslet array, and orientation accuracy of 2 degrees and precision of 0.2-0.3 degrees within a 45 degree field of view. Further, we 3D print the lenslet array with embedded baffles to reduce out-of-bounds cross-talk, and use an optical relay to allow interaction behind the focal plane. We demonstrate our joint display/sensing system with interactive light field painting.\n",
      "=============================\n",
      "Creating a Mobile Head-mounted Display with Proprietary Controllers for Interactive Virtual Reality Content\n",
      "A method to create a mobile head-mounted display (HMD) a proprietary controller for interactive virtual reality (VR) content is proposed. The proposed method uses an interface cartridge printed with a conductive pattern. This allows the user to operate a smartphone by touching on the face of the mobile HMD. In addition, the user can easily create a mobile HMD and interface cartridge using a laser cutter and inkjet printer. Changing the form of the conductive pattern allows the user to create a variety of controllers. The proposed method can realize an environment that can deliver a variety of interactions with VR content.\n",
      "=============================\n",
      "Adding Body Motion and Intonation to Instant Messaging with Animation\n",
      "Digital text communication (DTC) has transformed the way people communicate. Static typographical cues like emoticons, punctuation, letter case, and word lengthening (ie. Hellooo?) are regularly employed to convey intonation and affect. However, DTC platforms like instant messaging still suffer from a lack of nonverbal communication cues. This paper introduces an Animated Text Instant Messenger (ATIM), which uses text animations to add another distinct layer of cues to existing plaintext. ATIM builds upon previous research by using kinetic typography in communication. This paper describes the design principles and features of ATIM and discusses how animated text can add more nuanced communication cues of intonation and body motion.\n",
      "=============================\n",
      "Responsive Facilitation of Experiential Learning Through Access to Attentional State\n",
      "The planned thesis presents a vision of the future of learning, where learners explore environments, physical and virtual, in a curiosity-driven or intrinsically motivated way, and receive contextual information from a companion facilitator or teacher. Learners are instrumented with sensors that convey their cognitive and attentional state to the companion, who can then accurately judge what is interesting or relevant, and when is a good moment to jump in. I provide a broad definition of the possible types of sensor input as well as the modalities of intervention, and then present a specific proof-of-concept system that uses gaze behavior as a means of communication between the learner and a human companion.\n",
      "=============================\n",
      "Methods of 3D Printing Micro-pillar Structures on Surfaces\n",
      "This work presents a method of 3D printing hair-like structures on both flat and curved surfaces. It allows a user to design and fabricate hair geometry that is smaller than 100 micron. We built a software platform to let one quickly define a hair's angle, thickness, density, and height. The ability to fabricate customized hair-like structures expands the library of 3D-printable shape. We then present several applications to show how the 3D-printed hair can be used for designing toy objects.\n",
      "=============================\n",
      "Effective Interactions for Personalizing Spatial Visualizations of Collections\n",
      "Interactive spatial visualizations powered by machine learning will help us explore and understand large collections in meaningful ways, but little is yet known about the design space of interactions. We ran a pilot user study to compare two different interaction techniques: a \"grouping?\" interaction adapted from interactive clustering, and an existing \"positioning?\" interaction. We identified three important dimensions of the interaction design space that inform future design of more intuitive and expressive interactions.\n",
      "=============================\n",
      "Enhanced Motion Robustness from ToF-based Depth Sensing Cameras\n",
      "Depth sensing cameras that can acquire RGB and depth information are being widely used. They can expand and enhance various camera-based applications and are cheap but strong tools for computer human interaction. RGB and depth sensing cameras have quite different key parameters, such as exposure time. We focus on the differences in their motion robustness; the RGB camera has relatively long exposure times while those of ToF (Time-of-flight) based depth sensing camera are relatively short. An experiment on visual tag reading, one typical application, shows that depth sensing cameras can robustly decode moving tags. The proposed technique will yield robust tag reading, indoor localization, and color image stabilization while walking and jogging or even glancing momentarily without requiring any special additional devices.\n",
      "=============================\n",
      "TMotion: Embedded 3D Mobile Input using Magnetic Sensing Technique\n",
      "We present TMotion, a self-contained 3D input that enables spatial interactions around mobile using a magnetic sensing technique. Using a single magnetometer from the mobile device, we can track the 3D position of the permanent magnet embedded in the prototype along with an inertial measurement unit. By numerically solving non-linear magnetic field equations with known orientation from inertial measurement unit (IMU), we attain a tracking rate greater than 30Hz based solely on the mobile device computation. We describe the working principle of TMotion and example applications illustrating its capability.\n",
      "=============================\n",
      "MoveableMaker: Facilitating the Design, Generation, and Assembly of Moveable Papercraft\n",
      "In this work, we explore moveables, i.e., interactive papercraft that harness user interaction to generate visual effects. First, we present a survey of children's books that captured the state of the art of moveables. The results of this survey were synthesized into a moveable taxonomy and informed MoveableMaker, a new tool to assist users in designing, generating, and assembling moveable papercraft. MoveableMaker supports the creation and customization of a number of moveable effects and employs moveable-specific features including animated tooltips, automatic instruction generation, constraint-based rendering, techniques to reduce material waste, and so on. To understand how MoveableMaker encourages creativity and enhances the workflow when creating moveables, a series of exploratory workshops were conducted. The results of these explorations, including the content participants created and their impressions, are discussed, along with avenues for future research involving moveables.\n",
      "=============================\n",
      "Using Personal Devices to Facilitate Multi-user Interaction with Large Display Walls\n",
      "Large display walls and personal devices such as Smartphones have complementary characteristics. While large displays are well-suited to multi-user interaction (potentially with complex data), they are inherently public and generally cannot present an interface adapted to the individual user. However, effective multi-user interaction in many cases depends on the ability to tailor the interface, to interact without interfering with others, and to access and possibly share private data. The combination with personal devices facilitates exactly this. Multi-device interaction concepts enable data transfer and include moving parts of UIs to the personal device. In addition, hand-held devices can be used to present personal views to the user. Our work will focus on using personal devices for true multi-user interaction with interactive display walls. It will cover appropriate interaction techniques as well as the technical foundation and will be validated with corresponding application cases.\n",
      "=============================\n",
      "RFlow: User Interaction Beyond Walls\n",
      "Current user-interaction with optical gesture tracking technologies suffer from occlusions, limiting the functionality to direct line-of-sight. We introduce RFlow, a compact, medium-range interface based on Radio Frequency (RF) that enables camera-free tracking of the position of a moving hand through drywall and other occluders. Our system uses Time of Flight (TOF) RF sensors and speed-based segmentation to localize the hand of a single user with 5cm accuracy (as measured to the closest ground-truth point), enabling an interface which is not restricted to a training set.\n",
      "=============================\n",
      "GaussStarter: Prototyping Analog Hall-Sensor Grids with Breadboards\n",
      "This work presents GaussStarter, a pluggable and tileable analog Hall-sensor grid module for easy and scalable bread- board prototyping. In terms of ease-of-use, the graspable units allow users to easily plug them on or remove them from a breadboard. In terms of scalability, tiling the units on the breadboard can easily expand the sensing area. A software development kit is also provided for designing applications based on this hardware module.\n",
      "=============================\n",
      "Multi-Modal Peer Discussion with RichReview on edX\n",
      "In this demo, we present RichReview, a multi-modal peer discussion system, implemented as an XBlock in the edX courseware platform. The system brings richness similar to face-to-face communication into online learning at scale. With this demonstration, we discuss the system?s scalable back-end architecture, semantic voice editing user interface, and a future research plan for the profile based group-assignment scheme.\n",
      "=============================\n",
      "Workload Assessment with eye Movement Monitoring Aided by Non-invasive and Unobtrusive Micro-fabricated Optical Sensors\n",
      "Mental state or workload of a person are very relevant when the person is executing delicate tasks such as piloting an aircraft, operating a crane because the high level of workload could prevent accomplishing the task and lead to disastrous results. Some frameworks have been developed to assess the workload and determine whether the person is capable of executing a new task. However, such methodologies are applied when the operator finished the task. Another feature that these methodologies share is that are based on paper and pencil tests. Therefore, human-friendly devices that could assess the workload in real time are in high demand. In this paper, we report a wearable device that can correlate physical eye behavior with the mental state for the workload assessment.\n",
      "=============================\n",
      "Connected environments\n",
      "Can new interfaces contribute to social and environmental improvement? For all the care, wit and brilliance that UIST innovations can contribute, can they actually make things better - better in the sense of public good - not merely lead to easier to use or more efficient consumer goods? This talk will explore the impact of interface technology on society and the environment, and examine engineered systems that invite participation, document change over time, and suggest alternative courses of action that are ethical and sustainable, drawing on examples from a diverse series of experimental designs and site-specific work Natalie has created throughout her career.\n",
      "=============================\n",
      "Migratory applications\n",
      "We introduce a new genre of user interface applications that can migrate from one machine to another, taking their user interface and application contexts with them, and continue from where they left off. Such applications are not tied to one user or one machine, and can roam freely over the network, rendering service to a community of users, gathering human input and interacting with people. We envisage that this will support many new agent-based collaboration metaphors. The ability to migrate executing programs has applicability to mobile computing as well. Users can have their applications travel with them, as they move from one computing environment to another. We present an elegant programming model for creating migratory applications and describe an implementation. The biggest strength of our implementation is that the details of migration are completely hidden from the application programmer; arbitrary user interface applications can be migrated by a single ÒmigrationÓ command. We address system issues such as robustness, persistence and memory usage, and also human factors relating to application design, the interaction metaphor and safety.\n",
      "=============================\n",
      "Wait-Learning: Leveraging Wait Time for Education\n",
      "Competing priorities in daily life make it difficult for those with a casual interest in learning to set aside time for regular practice. Yet, learning often requires significant time and effort, with repeated exposures to learning material on a recurring basis. Despite the struggle to find time for learning, there are numerous times in a day that are wasted due to micro-waiting. In my research, I develop systems for wait-learning, leveraging wait time for education. Combining wait time with productive work opens up a new class of software systems that overcomes the problem of limited time while addressing the frustration often associated with waiting. My research tackles several challenges in learning and task management, such as identifying which waiting moments to leverage; how to encourage learning unobtrusively; how to integrate learning across a diversity of waiting moments; and how to extend wait-learning to more complex domains. In the development process, I hope to understand how to manage these waiting moments, and describe essential design principles for wait-learning systems.\n",
      "=============================\n",
      "Interactive viscosity\n",
      "Your conscious experience is not a function of the world, it is a function of the neural networks of your brain. Therefore, the biology of the brain and consciousness is as fundamental to understanding the universe, as we know it, as the high-energy physics of subatomic particles. This is especially true for the study of sensory and cognitive illusions, since they represent effects that clearly stand out as not representing the real world. That is, since illusions don't match reality we can know that by studying illusions we are studying exactly what the brain is actually doing, and not just what we think the brain should be doing. Your brain does a staggering amount of pragmatic self-dealing guesswork and outright confabulation in order to construct the highly imperfect mental simulation of reality known as \"consciousness.\" This is not to say that objective reality isn't \"out there\" in a very real sense--but no one lives there. No one's ever even been there for a visit. Ironically, the fact that consciousness feels like a solid, robust, fact-rich transcript of reality is just one of the countless illusions your brain creates for itself. Illusions are not errors of the brain. Far from it. Illusions arise from processes that are critical to our survival. Our brains have developed illusory processes so that we may experience the world in a ready-to-consume manner. Remove the machinery of illusion, and you unwind the entire tapestry of human awareness. Illusions are those perceptual experiences that do not match the physical reality. They are therefore exquisite tools with which to analyze the neural correlates of human perception and consciousness. Neuroscientists have long known that they can only be sure of where they stand, in terms of correlating neural responses to awareness, when they correlate the awareness of an illusion to the brain's response, specifically because of the illusions' mismatch with reality. The study of illusions is therefore of critical importance to the understanding of the basic mechanisms of sensory perception and conscious awareness. If you've ever seen a good magician perform, you know how thrilling it is to watch the impossible happening before your eyes. The laws of physics, probability, psychology and common sense--the four trusty compass points in your mental map of reality--are suddenly turned into liabilities. Objects and people appear, vanish, levitate, transpose, transform, and with all your smarts you can't imagine how it's being done. Magicians are the premier artists of attention and awareness, and they manipulate our cognition like clay on a potter's wheel. And the mechanisms underlying magic perception have implications for our daily lives. The magical arts work because humans have hardwired processes of attention and awareness that are hackable. By understanding how magicians hack our brains, we can better understand how we work.\n",
      "=============================\n",
      "Fisheye menus\n",
      "We introduce “fisheye menus” which apply traditional fisheye graphical visualization techniques to linear menus. This provides for an efficient mechanism to select items from long menus, which are becoming more common as menus are used to select data items in, for example, ecommerce applications. Fisheye menus dynamically change the size of menu items to provide a focus area around the mouse pointer. This makes it possible to present the entire menu on a single screen without requiring buttons, scrollbars, or hierarchies. A pilot study with 10 users compared user preference of fisheye menus with traditional pull-down menus that use scrolling arrows, scrollbars, and hierarchies. Users preferred the fisheye menus for browsing tasks, and hierarchical menus for goal-directed tasks.\n",
      "=============================\n",
      "Attribute gates\n",
      "Attribute gates are a new user interface element designed to address the problem of concurrently setting attributes and moving objects between territories on a digital tabletop. Motivated by the notion of task levels in activity theory, and crossing interfaces, attribute gates allow users to operationalize multiple subtasks in one smooth movement. We present two configurations of attribute gates; (1) grid gates which spatially distribute attribute values in a regular grid, and require users to draw trajectories through the attributes; (2) polar gates which distribute attribute values on segments of concentric rings, and require users to align segments when setting attribute combinations. The layout of both configurations was optimised based on targeting and steering laws derived from Fitts' Law. A study compared the use of attribute gates with traditional contextual menus. Users of attribute gates demonstrated both increased performance and higher mutual awareness.\n",
      "=============================\n",
      "Personalizing routes\n",
      "Navigation services (e.g., in-car navigation systems and online mapping sites) compute routes between two locations to help users navigate. However, these routes may direct users along an unfamiliar path when a familiar path exists, or, conversely, may include redundant information that the user already knows. These overly complicated directions increase the cognitive load of the user, which may lead to a dangerous driving environment. Since the level of detail is user specific and depends on their familiarity with a region, routes need to be personalized. We have developed a system, called MyRoute, that reduces route complexity by creating user specific routes based on a priori knowledge of familiar routes and landmarks. MyRoute works by compressing well known steps into a single contextualized step and rerouting users along familiar routes.\n",
      "=============================\n",
      "FoldMecha: Design for Linkage-Based Paper Toys\n",
      "We present FoldMecha, a computational tool to help non-experts design and build paper mechanical toys. By customizing templates a user can experiment with basic mechanisms, design their own model, print and cut out a folding net to construct the toy. We used the tool to build two kinds of paper automata models: walkers and flowers.\n",
      "=============================\n",
      "Inductive groups\n",
      "The notion of inductive groups is presented as a mechanism for manipulating sets of related objects. The interactive behavior of such groups is discussed along with extensible algorithms for discovering inductive relationships in general. An application using these techniques is shown.\n",
      "=============================\n",
      "Restorable backspace\n",
      "This paper presents Restorable Backspace, an input helper for mistyping correction. It stores characters deleted by backspace keystrokes, and restores them in the retyping phase. We developed Restoration algorithm that compares deleted characters and retyped characters, and makes a suggestion while retyping. In a pilot study we could observe the algorithm work as expected for most of the cases. All participants in the pilot study showed satisfaction about the concept of Restorable Backspace.\n",
      "=============================\n",
      "Leveraging Dual-Observable Input for Fine-Grained Thumb Interaction Using Forearm EMG\n",
      "We introduce the first forearm-based EMG input system that can recognize fine-grained thumb gestures, including left swipes, right swipes, taps, long presses, and more complex thumb motions. EMG signals for thumb motions sensed from the forearm are quite weak and require significant training data to classify. We therefore also introduce a novel approach for minimally-intrusive collection of labeled training data for always-available input devices. Our dual-observable input approach is based on the insight that interaction observed by multiple devices allows recognition by a primary device (e.g., phone recognition of a left swipe gesture) to create labeled training examples for another (e.g., forearm-based EMG data labeled as a left swipe). We implement a wearable prototype with dry EMG electrodes, train with labeled demonstrations from participants using their own phones, and show that our prototype can recognize common fine-grained thumb gestures and user-defined complex gestures.\n",
      "=============================\n",
      "Interactive viscosity\n",
      "When the Macintosh first made graphical user interfaces popular the notion of each person having their own computer was novel. Today's technology landscape is characterized by multiple computers per person many with far more capacity than that original Mac. The world of input devices, display devices and interactive techniques is far richer than those Macintosh days. Despite all of this diversity in possible interactions very few of these integrate well with each other. The monolithic isolated user interface architecture that characterized the Macintosh still dominates a great deal of today's personal computing. This talk will explore how possible ways to change that architecture so that information, interaction and communication flows more smoothly among our devices and those of our associates.\n",
      "=============================\n",
      "Skeletal strokes\n",
      "A skeletal stroke is a kind of general brush stroke for changing the shape of pictures as if by bending, shearing, twisting, while conservating the aspect ratio of selected features on the picture. It is neither a simple warping nor texture mapping technique, but a new method for controlling the deformation of a picture. A deformation model of a coordinate system has been proposed taking into account cases of discontinuous or extreme bending. Complicated pictures can be built up hierarchically by defining higher order strokes and recursive strokes. It is therefore a powerful general drawing tool and extended image transformation instrument. The use of skeletal strokes as a replacement for affine transformations in IFS coding has been explored. A novel general anchoring mechanism is proposed, which allows arbitrary control of any point in the picture. This control flexibility is particularly desirable in computer animation and digital typography. As a result, virtual ‘2-D models’ of cartoon characters as well as pseudo 3-D objects can be created and manipulated with ease.\n",
      "=============================\n",
      "Interactive shadows\n",
      "It is often difficult in computer graphics applications to understand spatial relationships between objects in a 3D scene or effect changes to those objects without specialized visualization and manipulation techniques. We present a set of three-dimensional tools (widgets) called “shadows” that not only provide valuable perceptual cues about the spatial relationships between objects, but also provide a direct manipulation interface to constrained transformation techniques. These shadow widgets provide two advances over previous techniques. First, they provide high correlation between their own geometric feedback and their effects on the objects they control. Second, unlike some other 3D widgets, they do not obscure the objects they control.\n",
      "=============================\n",
      "Tracking menus\n",
      "We describe a new type of graphical user interface widget, known as a \"tracking menu.\" A tracking menu consists of a cluster of graphical buttons, and as with traditional menus, the cursor can be moved within the menu to select and interact with items. However, unlike traditional menus, when the cursor hits the edge of the menu, the menu moves to continue tracking the cursor. Thus, the menu always stays under the cursor and close at hand.In this paper we define the behavior of tracking menus, show unique affordances of the widget, present a variety of examples, and discuss design characteristics. We examine one tracking menu design in detail, reporting on usability studies and our experience integrating the technique into a commercial application for the Tablet PC. While user interface issues on the Tablet PC, such as preventing round trips to tool palettes with the pen, inspired tracking menus, the design also works well with a standard mouse and keyboard configuration.\n",
      "=============================\n",
      "Olfactory display\n",
      "The last twenty years have seen enormous leaps forward in computers' abilities to generate sound and video. What happens when computers can produce scents on demand? In this talk, I present three approaches to this question. I first look at human olfactory processing: what is our olfactory bandwidth, and what are the limitations of our sense of smell? I then explore the use of scent to accompany other media, from historical examples like Sense-o-Rama and Aromarama, to more recent work including firefighter training systems, augmented gaming, and food and beverage applications. Finally, I look at the possibilities of olfactory output as an ambient display medium. I conclude with an overview of current computer-controlled olfactory output devices: off the shelf solutions for incorporating scent into user interface applications.\n",
      "=============================\n",
      "Clothing manipulation\n",
      "This paper presents interaction techniques (and the underlying implementations) for putting clothes on a 3D character and manipulating them. The user paints freeform marks on the clothes and corresponding marks on the 3D character; the system then puts the clothes around the body so that corresponding marks match. Internally, the system grows the clothes on the body surface around the marks while maintaining basic cloth constraints via simple relaxation steps. The entire computation takes a few seconds. After that, the user can adjust the placement of the clothes by an enhanced dragging operation. Unlike standard dragging where the user moves a set of vertices in a single direction in 3D space, our dragging operation moves the cloth along the body surface to make possible more flexible operations. The user can apply pushpins to fix certain cloth points during dragging. The techniques are ideal for specifying an initial cloth configuration before applying a more sophisticated cloth simulation.\n",
      "=============================\n",
      "Remot-IO: a System for Reaching into the Environment of a Remote Collaborator\n",
      "In this paper we present Remot-IO, a system for mobile collaboration and remote assistance around Internet connected devices. The system uses two Head Mounted Displays, cameras and depth sensors to enable a remote expert to be immersed in a local user's point of view and control devices in that user?s environment. The remote expert can provide guidance through the use of hand gestures that appear in real-time in the local user?s field of view as superimposed 3D hands. In addition, the remote expert is able to operate devices in the novice?s environment and bring about physical changes by using the same hand gestures the novice would use. We describe a smart radio where the knobs of the radio can be controlled by local and remote user alike. Moreover, the user can visualize, interact and modify properties of sound waves in real time by using intuitive hand gestures.\n",
      "=============================\n",
      "Private Webmail 2.0: Simple and Easy-to-Use Secure Email\n",
      "Private Webmail 2.0 (Pwm 2.0) improves upon the current state of the art by increasing the usability and practical security of secure email for ordinary users. More users are able to send and receive encrypted emails without mistakenly revealing sensitive information. In this paper we describe four user interface traits that positively affect the usability and security of Pwm 2.0. In a user study involving 51 participants we validate that these interface modifications result in high usability, few mistakes, and a strong understanding of the protection provided to secure email messages. We also show that the use of manual encryption has no effect on usability or security.\n",
      "=============================\n",
      "Flying User Interface\n",
      "This paper describes a special type of drone called \"Flying User Interface\", comprised of a robotic projector-camera system, an onboard digital computer connected with the Internet, sensors, and a hardware interface capable of sticking to any surface such as wall, ceilings, etc. Computer further consists of other subsystems, devices, and sensors such as accelerometer, compass, gyroscope, flashlight, etc. Drone flies from one place to another, detects a surface, and attaches itself to it. After a successful attachment, the device stops all its rotators; it then projects or augments images, information, and user interfaces on nearby surfaces and walls. User interface may contain applications, information about object being augmented and information from Internet. User can interact with user-interface using commands and gestures such as hand, body, feet, voice, etc.\n",
      "=============================\n",
      "Semi-Automated SVG Programming via Direct Manipulation\n",
      "Direct manipulation interfaces provide intuitive and interactive features to a broad range of users, but they often exhibit two limitations: the built-in features cannot possibly cover all use cases, and the internal representation of the content is not readily exposed. We believe that if direct manipulation interfaces were to (a) use general-purpose programs as the representation format, and (b) expose those programs to the user, then experts could customize these systems in powerful new ways and non-experts could enjoy some of the benefits of programmable systems. In recent work, we presented a prototype SVG editor called Sketch-n-Sketch that offered a step towards this vision. In that system, the user wrote a program in a general-purpose lambda-calculus to generate a graphic design and could then directly manipulate the output to indirectly change design parameters (i.e. constant literals) in the program in real-time during the manipulation. Unfortunately, the burden of programming the desired relationships rested entirely on the user. In this paper, we design and implement new features for Sketch-n-Sketch that assist in the programming process itself. Like typical direct manipulation systems, our extended Sketch-n-Sketch now provides GUI-based tools for drawing shapes, relating shapes to each other, and grouping shapes together. Unlike typical systems, however, each tool carries out the user's intention by transforming their general-purpose program. This novel, semi-automated programming workflow allows the user to rapidly create high-level, reusable abstractions in the program while at the same time retaining direct manipulation capabilities. In future work, our approach may be extended with more graphic design features or realized for other application domains.\n",
      "=============================\n",
      "JOLED: A Mid-air Display based on Electrostatic Rotation of Levitated Janus Objects\n",
      "We present JOLED, a mid-air display for interactive physical visualization using Janus objects as physical voxels. The Janus objects have special surfaces that have two or more asymmetric physical properties at different areas. In JOLED, they are levitated in mid-air and controllably rotated to reveal their different physical properties. We made voxels by coating the hemispheres of expanded polystyrene beads with different materials, and applied a thin patch of titanium dioxide to induce electrostatic charge on them. Transparent indium tin oxide electrodes are used around the levitation volume to create a tailored electric field to control the orientation of the voxels. We propose a novel method to control the angular position of individual voxels in a grid using electrostatic rotation and their 3D position using acoustic levitation. We present a display in which voxels can be flipped independently, and two mid-air physical games with a voxel as the playable character that moves in 3D across other physical structures and rotates to reflect its status in the games. We demonstrate a voxel update speed of 37.8 ms/flip, which is video-rate.\n",
      "=============================\n",
      "Orchestrated Informal Care Coordination: Designing a Connected Network of Tools in Support of Collective Care Activities for Informal Caregivers\n",
      "Often, family caregivers experience difficulties in coordinating older adults' health care because it requires not only a lot of time but also a diverse set of responsibilities to coordinate care for their loved ones. While many can reduce their individual burden by sharing care tasks with other family members, there are still many challenges to overcome in maintaining the quality of care when they work together. As they increase their informal care network, it becomes more difficult for them to stay informed and coordinated. Coordination breakdowns caused by having multiple caregivers who are cooperating to care for the same care recipient result in reduced quality of care. I explored opportunities for \"Internet of Things (IoT)\" technologies to help informal caregivers better coordinate and communicate care with each other for their loved ones. Based on identified design opportunities, I propose the concept of CareBot, a smart home platform consisting of interactive tools in support of collective care activities of family caregivers. \\\n",
      "=============================\n",
      "Expanding the Field-of-View of Head-Mounted Displays with Peripheral Blurred Images\n",
      "Head-mounted displays are rapidly becoming popular. Field-of-view is one of the key parameters of head-mounted displays, because a wider field-of-view gives higher presence and immersion in the virtual environment. However, wider field-of-view often increase device cost and weight because it needs complicated optics or expensive modules such as multi high-resolution displays or complex lenses. This paper proposes a method that expands the field-of-view by using two kinds of lenses with different levels of magnification. The principle of the proposed method is that Fresnel lenses with high magnification surround convex lenses to fill the peripheral vision with a blurred image. The proposed method doesn't need complicated optics, and is advantageous in terms of device cost and weight, because only two additional Fresnel lenses are necessary. We implement a prototype and confirm that the Fresnel lenses fill the peripheral with a blurred image, and effectively expand the field-of-view.\n",
      "=============================\n",
      "LIME: LIquid MEtal Interfaces for Non-Rigid Interaction\n",
      "Room-temperature liquid metal GaIn25 (Eutectic Gallium- Indium alloy, 75% gallium and 25% indium) has distinctive properties of reversible deformation and controllable locomotion under an external electric field stimulus. Liquid metal's newly discovered properties imply great possibilities in developing new technique for interface design. In this paper, we present LIME, LIquid MEtal interfaces for non-rigid interaction. We first discuss the interaction potential of LIME interfaces. Then we introduce the development of LIME cells and the design of some LIME widgets.\n",
      "=============================\n",
      "Gushed Diffusers: Fast-moving, Floating, and Lightweight Midair Display\n",
      "We present a novel method for fast-moving aerial imaging using aerosol-based fog screens. Conventional systems of aerial imaging cannot move fast because they need large and heavy setup. In this study, we propose to add new tradeoffs between limited display time and payloads. This system employ aerosol distribution from off-the-shelf spray as a fog screen that can resist the wind, and have high portability. As application examples, we present wearable application and aerial imaging on objects with high speed movements such as a drone, a radio-controlled model car, and performers. We believe that our study contribute to the exploration of new application areas for fog displays and expand expressions of entertainments and interactivity.\n",
      "=============================\n",
      "Data-driven Mobile App Design\n",
      "Design is becoming a key differentiating factor for successful apps in today's crowded app marketplaces. This thesis describes how data-driven approaches can enable useful tools for mobile app design. It presents interaction mining -- capturing both static (UI layouts, visual details) and dynamic (user flows, motion details) components of an app's design. It develops two approaches for interaction mining existing Android apps and presents three applications enabled by the resultant data: a search engine for user flows, lightweight usability testing at scale, and automated generation of mobile UIs.\n",
      "=============================\n",
      "FaceTouch: Enabling Touch Interaction in Display Fixed UIs for Mobile Virtual Reality\n",
      "We present FaceTouch, a novel interaction concept for mobile Virtual Reality (VR) head-mounted displays (HMDs) that leverages the backside as a touch-sensitive surface. With FaceTouch, the user can point at and select virtual content inside their field-of-view by touching the corresponding location at the backside of the HMD utilizing their sense of proprioception. This allows for rich interaction (e.g. gestures) in mobile and nomadic scenarios without having to carry additional accessories (e.g. a gamepad). We built a prototype of FaceTouch and conducted two user studies. In the first study we measured the precision of FaceTouch in a display-fixed target selection task using three different selection techniques showing a low error rate of 2% indicate the viability for everyday usage. To asses the impact of different mounting positions on the user performance we conducted a second study. We compared three mounting positions of the touchpad (face, hand and side) showing that mounting the touchpad at the back of the HMD resulted in a significantly lower error rate, lower selection time and higher usability. Finally, we present interaction techniques and three example applications that explore the FaceTouch design space.\n",
      "=============================\n",
      "SkyAnchor: Optical Design for Anchoring Mid-air Images onto Physical Objects\n",
      "For glass-free mixed reality (MR), mid-air imaging is a promising way of superimposing a virtual image onto a real object. We focus on attaching virtual images to non-static real life objects. In previous work, moving the real object causes latency in the superimposing system, and the virtual image seems to follow the object with a delay. This is caused by delays due to sensors, displays and computational devices for position sensing, and occasionally actuators for moving the image generation source. In order to avoid this problem, this paper proposes to separate the object-anchored imaging effect from the position sensing. Our proposal is a retro-reflective system called \"SkyAnchor,\" which consists of only optical devices: two mirrors and an aerial-imaging plate. The system reflects light from a light source anchored under the physical object itself, and forms an image anchored around the object. This optical solution does not cause any latency in principle and is effective for high-quality mixed reality applications. We consider two types of light sources to be attached to physical objects: reflecting content from a touch table on which the object rests, or attaching the source directly on the object. As for position sensing, we utilize a capacitive marker on the bottom of the object, tracked on a touch table. We have implemented a prototype, where mid-air images move with the object, and whose content may change based on its position.\n",
      "=============================\n",
      "Bootstrapping User-Defined Body Tapping Recognition with Offline-Learned Probabilistic Representation\n",
      "To address the increasing functionality (or information) overload of smartphones, prior research has explored a variety of methods to extend the input vocabulary of mobile devices. In particular, body tapping has been previously proposed as a technique that allows the user to quickly access a target functionality by simply tapping at a specific location of the body with a smartphone. Though compelling, prior work often fell short in enabling users' unconstrained tapping locations or behaviors. To address this problem, we developed a novel recognition method that combines both offline-before the system sees any user-defined gestures and online learning to reliably recognize arbitrary, user-defined body tapping gestures, only using a smartphone's built-in sensors. Our experiment indicates that our method significantly outperforms baseline approaches in several usage conditions. In particular, provided only with a single sample per location, our accuracy is 30.8% over an SVM baseline and 24.8% over a template matching method. Based on these findings, we discuss how our approach can be generalized to other user-defined gesture problems.\n",
      "=============================\n",
      "A Tangible Interface to Realize Touch Operations on the Face of a Physical Object\n",
      "In this paper, we describe a tangible interface that can realize touch operations on a physical object. We printed physical objects that have conductive striped patterns using a multi-material 3D printer. The ExtensionSticker technique allows the user to operate capacitive touch-panel devices by tapping, scrolling, and swiping the physical object. By shaping the structure of conductive wiring inside a physical object, a variety of interfaces can be realized. We examined the conditions for using our proposed method on touch-panel devices.\n",
      "=============================\n",
      "Holoportation: Virtual 3D Teleportation in Real-time\n",
      "We present an end-to-end system for augmented and virtual reality telepresence, called Holoportation. Our system demonstrates high-quality, real-time 3D reconstructions of an entire space, including people, furniture and objects, using a set of new depth cameras. These 3D models can also be transmitted in real-time to remote users. This allows users wearing virtual or augmented reality displays to see, hear and interact with remote participants in 3D, almost as if they were present in the same physical space. From an audio-visual perspective, communicating and interacting with remote users edges closer to face-to-face communication. This paper describes the Holoportation technical system in full, its key interactive capabilities, the application scenarios it enables, and an initial qualitative study of using this new communication medium.\n",
      "=============================\n",
      "Interaction Technique Using Acoustic Sensing for Different Squeak Sounds Caused by Number of Rubbing Fingers\n",
      "Various studies have been conducted for developing interaction techniques in a smart house. Some of our previous studies [1, 2] focused on bathrooms and we converted an existing normal bathtub system into a user interface by using embedded sensors. A system called Bathcratch [2] detects squeak sounds by rubbing on a bathtub edge. To generate squeaks, it requires some conditions to cause the Stick-slip phenomenon. A bathtub has a smooth surface, and water can cause the phenomenon with human skins. Bathcratch uses it as an interaction technique to play DJ-scratching. Here, we extended the interaction technique using squeaks to recognize rubbing states, rubbing events including sequence, and the difference between squeaks caused by the number of fingers. This can be used in various wet environments including kitchen, washbowls in a restroom, swimming pool, and spa. This paper describes the method and its performance for identifying the number of rubbing fingers by using frequency analysis. In addition, we illustrate some smart home applications by using the proposed technique.\n",
      "=============================\n",
      "Energy-Brushes: Interactive Tools for Illustrating Stylized Elemental Dynamics\n",
      "Dynamic effects such as waves, splashes, fire, smoke, and explosions are an integral part of stylized animations. However, such dynamics are challenging to produce, as manually sketching key-frames requires significant effort and artistic expertise while physical simulation tools lack sufficient expressiveness and user control. We present an interactive interface for designing these elemental dynamics for animated illustrations. Users draw with coarse-scale energy brushes which serve as control gestures to drive detailed flow particles which represent local velocity fields. These fields can convey both realistic and artistic effects based on user specification. This painting metaphor for creating elemental dynamics simplifies the process, providing artistic control, and preserves the fluidity of sketching. Our system is fast, stable, and intuitive. An initial user evaluation shows that even novice users with no prior animation experience can create intriguing dynamics using our system.\n",
      "=============================\n",
      "QuickCut: An Interactive Tool for Editing Narrated Video\n",
      "We present QuickCut, an interactive video editing tool designed to help authors efficiently edit narrated video. QuickCut takes an audio recording of the narration voiceover and a collection of raw video footage as input. Users then review the raw footage and provide spoken annotations describing the relevant actions and objects in the scene. QuickCut time-aligns a transcript of the annotations with the raw footage and a transcript of the narration to the voiceover. These aligned transcripts enable authors to quickly match story events in the narration with semantically relevant video segments and form alignment constraints between them. Given a set of such constraints, QuickCut applies dynamic programming optimization to choose frame-level cut points between the video segments while maintaining alignments with the narration and adhering to low-level film editing guidelines. We demonstrate QuickCut's effectiveness by using it to generate a variety of short (less than 2 minutes) narrated videos. Each result required between 14 and 52 minutes of user time to edit (i.e. between 8 and 31 minutes for each minute of output video), which is far less than typical authoring times with existing video editing workflows.\n",
      "=============================\n",
      "Touchscreen Overlay Augmented with the Stick-Slip Phenomenon to Generate Kinetic Energy\n",
      "Kinesthetic feedback requires linkage-based high-powered multi-dimensional manipulators, which are currently not possible to integrate with mobile devices. To overcome this challenge, we developed a novel system that can utilize a wide range of actuation components and apply various techniques to optimize stick-slip motion of a tangible object on a display surface. The current setup demonstrates how it may be possible to generate directional forces on an interactive display in order to move a linkage-free stylus over a touchscreen in a fully controlled and efficient manner. The technology described in this research opens up new possibilities for interacting with displays and tangible surfaces such as continuously supervised learning; active feed-forward systems as well as dynamic gaming environments that predict user behavior and are able modify and physically react to human input at real-time.\n",
      "=============================\n",
      "Beyond Snapping: Persistent, Tweakable Alignment and Distribution with StickyLines\n",
      "Aligning and distributing graphical objects is a common, but cumbersome task. In a preliminary study (six graphic designers, six non-designers), we identified three key problems with current tools: lack of persistence, unpredictability of results, and inability to 'tweak' the layout. We created StickyLines, a tool that treats guidelines as first-class objects: Users can create precise, predictable and persistent interactive alignment and distribution relationships, and 'tweaked' positions can be maintained for subsequent interactions. We ran a [2x2] within-participant experiment to compare StickyLines with standard commands, with two levels of layout difficulty. StickyLines performed 40% faster and required 49% fewer actions than traditional alignment and distribution commands for complex layouts. In study three, six professional designers quickly adopted StickyLines and identified novel uses, including creating complex compound guidelines and using them for both spatial and semantic grouping.\n",
      "=============================\n",
      "OmniEyeball: Spherical Display Embedded With Omnidirectional Camera Using Dynamic Spherical Mapping\n",
      "Recently, 360-degree panorama and spherical displays have received more and more attention due to their unique panoramic properties. Compared with existing works, we plan to utilize omnidirectional cameras in our spherical display system to enable omnidirectional panoramic image as input and output. In our work, we present a novel movable spherical display embedded with omnidirectional cameras. Our system can use embedded cameras to shoot 360-degree panoramic video and project the live stream from its cameras onto its spherical display in real time. In addition, we implemented an approach to achieve the dynamic spherical projection mapping in order to project to moving spherical devices. We have also been creating applications utilizing system's features by using 360-degree panoramic image as input and output.\n",
      "=============================\n",
      "Mavo: Creating Interactive Data-Driven Web Applications by Authoring HTML\n",
      "Many people can author static web pages with HTML and CSS but find it hard or impossible to program persistent, interactive web applications. We show that for a broad class of CRUD (Create, Read, Update, Delete) applications, this gap can be bridged. Mavo extends the declarative syntax of HTML to describe Web applications that manage, store and transform data. Using Mavo, authors with basic HTML knowledge define complex data schemas implicitly as they design their HTML layout. They need only add a few attributes and expressions to their HTML elements to transform their static design into a persistent, data-driven web application whose data can be edited by direct manipulation of the content in the browser. We evaluated Mavo with 20 users who marked up static designs---some provided by us, some their own creation---to transform them into fully functional web applications. Even users with no programming experience were able to quickly craft Mavo applications.\n",
      "=============================\n",
      "Phyxel: Realistic Display of Shape and Appearance using Physical Objects with High-speed Pixelated Lighting\n",
      "A computer display that is sufficiently realistic such that the difference between a presented image and a real object cannot be discerned is in high demand in a wide range of fields, such as entertainment, digital signage, and design industry. To achieve such a level of reality, it is essential to reproduce the three-dimensional (3D) shape and material appearances simultaneously; however, to date, developing a display that can satisfy both conditions has been difficult. To address this problem, we propose a system that places physical elements at desired locations to create a visual image that is perceivable by the naked eye. This configuration can be realized by exploiting characteristics of human visual perception. Humans perceive light modulation as perfectly steady light if the modulation rate is sufficiently high. Therefore, if high-speed spatially varying illumination is projected to the actuated physical elements possessing various appearances at the desired timing, a realistic visual image that can be transformed dynamically by simply modifying the lighting pattern can be obtained. We call the proposed display technology Phyxel. This paper describes the proposed configuration and required performance for Phyxel. We also demonstrate three applications: dynamic stop motion, a layered 3D display, and shape mixture.\n",
      "=============================\n",
      "3D Printed Physical Interfaces that can Extend Touch Devices\n",
      "We propose a method to create a physical interface that allows touch input to be transferred from an external surface attached to a touch panel. Our technique prints a grid having multiple conductive points using an FDM-based 3D printer. When the user touches the conductive points, touch input is generated. This allows the user to control the touch input at arbitrary locations on an XY plane. By arranging the structure of the conductive wiring inside a physical object, a variety of interfaces can be realized.\n",
      "=============================\n",
      "Initial Trials of ofxEpilog: From Real Time Operation to Dynamic Focus of Epilog Laser Cutter\n",
      "This paper describes ofxEpilog which enable people to control a laser cutter of Epilog in real time. ofxEpilog is an add on of open Frameworks, an open source C++ toolkit for creative coding. With the add on, people could directly send their image object to a laser cutter through Ethernet. By alternating the generation and transmission of the command of cutting, the add on could sequentially control a laser cutter in real time. This paper introduces our initial trials of ofxEpilog with a real time operation (A), dynamic focus (z-axis) control with a given 3D object (B), and a scanned 3D object (C). Technical limitations and our upcoming challenges are also discussed.\n",
      "=============================\n",
      "Ballumiere: Real-Time Tracking and Projection for High-Speed Moving Balls\n",
      "Projection onto moving objects has a serious slipping problem due to delay between tracking and projection. We propose a new method to overcome the delay problem, and we succeed in increasing the accuracy of projection. We present Ballumiere as a demo for projection to volleyballs and juggling balls.\n",
      "=============================\n",
      "SleepCoacher: A Personalized Automated Self-Experimentation System for Sleep Recommendations\n",
      "We present SleepCoacher, an integrated system implementing a framework for effective self-experiments. SleepCoacher automates the cycle of single-case experiments by collecting raw mobile sensor data and generating personalized, data-driven sleep recommendations based on a collection of template recommendations created with input from clinicians. The system guides users through iterative short experiments to test the effect of recommendations on their sleep. We evaluate SleepCoacher in two studies, measuring the effect of recommendations on the frequency of awakenings, self-reported restfulness, and sleep onset latency, concluding that it is effective: participant sleep improves as adherence with SleepCoacher's recommendations and experiment schedule increases. This approach presents computationally-enhanced interventions leveraging the capacity of a closed feedback loop system, offering a method for scaling guided single-case experiments in real time.\n",
      "=============================\n",
      "NeverMind: Using Augmented Reality for Memorization\n",
      "NeverMind is an interface and application designed to support human memory. We combine the memory palace memorization method with augmented reality technology to create a tool to help anyone memorize more effectively. Preliminary experiments show that content memorized with NeverMind remains longer in memory compared to general memorization techniques. With this project, we hope to make the memory palace method accessible to novices and demonstrate one way augmented reality can support learning.\n",
      "=============================\n",
      "Representing Gaze Direction in Video Communication Using Eye-Shaped Display\n",
      "A long-standing challenge of video-mediated communication systems is to correctly represent a remote participant's gaze direction in local environments. To address this problem, we developed a video communication system using an \"eye-shaped display.\" This display is made of an artificial ulexite (TV rock) that is cut into a hemispherical shape, enabling the light from the bottom surface to be projected onto the curved surface. By displaying a simulated iris onto the eye-shaped display, we theorize that our system can represent the gaze direction as accurately as a real human eye.\n",
      "=============================\n",
      "Computational Design Driven by Aesthetic Preference\n",
      "Tweaking design parameters is one of the most fundamental tasks in many design domains. In this paper, we describe three computational design methods for parameter tweaking tasks in which aesthetic preference---how aesthetically preferable the design looks---is used as a criterion to be maximized. The first method estimates a preference distribution in the target parameter space using crowdsourced human computation. The estimated preference distribution is then used in a design interface to facilitate interactive design exploration. The second method also estimates a preference distribution and uses it in an interface, but the distribution is estimated using the editing history of the target user. In contrast to these two methods, the third method automatically finds the best parameter that maximizes aesthetic preference, without requiring the user of this method to manually tweak parameters. This is enabled by implementing optimization algorithms using crowdsourced human computation. We validated these methods mainly in the scenario of photo color enhancement where parameters, such as brightness and contrast, need to be tweaked.\n",
      "=============================\n",
      "Asymmetric Design Approach and Collision Avoidance Techniques For Room-scale Multiplayer Virtual Reality\n",
      "Recent advances in consumer virtual reality (VR) technology have made it easy to accurately capture users' motions over room-sized areas allowing natural locomotion for navigation in VR. While this helps create a stronger match between proprioceptive information from human body movements for enhancing immersion and reducing motion sickness, it introduces a few challenges. Walking is only possible within virtual environments (VEs) that fit inside the boundaries of the tracked physical space which for most users is quite small. Within this space the potential for colliding with physical objects around the play area is high. Additionally, only limited haptic feedback is available. In this paper, I focus on the problem of variations in the size and shape of each user's tracked physical space for multiplayer interactions. As part of the constrained physical space problem, I also present an automated system for steering the user away from play area boundaries using Galvanic Vestibular Stimulation (GVS). In my thesis, I will build techniques to enable the system to intelligently apply redirection and GVS-based steering as users explore virtual environments of arbitrary sizes.\n",
      "=============================\n",
      "Promoting Natural Interactions Through Embedded Input Using Novel Sensing Techniques\n",
      "From mobile devices to interactive objects, various input methods are provided using built-in motion and capacitive touch sensors. These inputs are offered in effective and efficient manner where users can operate interface quickly and easily. However, they do not fully explore the input space supported by human's natural motion behavior. As a solution, my work focuses on promoting natural interaction through hand-driven embedded input powered by multimodal and magnetic sensing techniques. In my previous works, embedded inputs were implemented in the form of smart textile, stylus, and ring supporting from mobile devices to everyday objects. Throughout the paper, I will briefly go over implemented systems along with evaluated results and potential applications. Future research direction is highlighted at the end of the paper.\n",
      "=============================\n",
      "floatio: Floating Tangible User Interface Based on Animacy Perception\n",
      "In this study, we propose floatio: a floating tangible user interface that makes it easy to create a perception of animacy (lifelike movement). It has been pointed out that there are three requirements that make animacy more likely to be perceived: interactivity, irregularities, and automatic movement resisting the force of gravity. Based on these requirements, floatio provides a tangible user interface where a polystyrene ball resembling a pixel is suspended in a stream of air where it can be positioned passively by the user, or autonomously by the system itself. To implement floatio, we developed three mechanisms: a floating field mechanism, a pointer input/output mechanism and a hand-over mechanism. We also measured the precision of the pointer input/output and hand-over mechanisms.\n",
      "=============================\n",
      "A Rapid Prototyping Approach to Synthetic Data Generation for Improved 2D Gesture Recognition\n",
      "Training gesture recognizers with synthetic data generated from real gestures is a well known and powerful technique that can significantly improve recognition accuracy. In this paper we introduce a novel technique called gesture path stochastic resampling (GPSR) that is computationally efficient, has minimal coding overhead, and yet despite its simplicity is able to achieve higher accuracy than competitive, state-of-the-art approaches. GPSR generates synthetic samples by lengthening and shortening gesture subpaths within a given sample to produce realistic variations of the input via a process of nonuniform resampling. As such, GPSR is an appropriate rapid prototyping technique where ease of use, understandability, and efficiency are key. Further, through an extensive evaluation, we show that accuracy significantly improves when gesture recognizers are trained with GPSR synthetic samples. In some cases, mean recognition errors are reduced by more than 70%, and in most cases, GPSR outperforms two other evaluated state-of-the-art methods.\n",
      "=============================\n",
      "Polyspector™: An Interactive Visualization Platform Optimized for Visual Analysis of Big Data\n",
      "With the advent of the 'big data' era, there are unprecedented opportunities and challenges to explore complex and large datasets. In the paper, we introduce Polyspector, a web-based interactive visualization platform optimized for interactive visual analysis with two distinguishing features. Firstly, a visualization-specific database engine based on pixel-aware aggregation is implemented to generate views of hundreds of millions of data items within a second even with an off-the-shelf PC. Secondly, a novel deep-linking mechanism, combined with the pixel-aware aggregation, is exploited to realize interactive visual analysis interfaces such as zooming, overview + detail, context + focus etc.\n",
      "=============================\n",
      "An Input Switching Interface Using Carbon Copy Metaphor\n",
      "This paper proposes a novel input technique that aims to switch between relative and absolute coordinates input methods seamlessly based on the \"carbon copy\" metaphor. We display a small workspace (``carbon copy area'') on a computer screen that corresponds one-to-one with the handy trackpad. The user can input hand-written characters or images using absolute coordinates input on this virtual carbon copy paper and move it anywhere using relative coordinates. Our technique allows a user to call both absolute and relative coordinates input methods and use them appropriately with arbitrary timing. Several advantages can be obtained by combining these methods. We developed a desktop application software to utilize this technique in a real GUI environment based on a user's evaluation.\n",
      "=============================\n",
      "Zooids: Building Blocks for Swarm User Interfaces\n",
      "This paper introduces swarm user interfaces, a new class of human-computer interfaces comprised of many autonomous robots that handle both display and interaction. We describe the design of Zooids, an open-source open-hardware platform for developing tabletop swarm interfaces. The platform consists of a collection of custom-designed wheeled micro robots each 2.6 cm in diameter, a radio base-station, a high-speed DLP structured light projector for optical tracking, and a software framework for application development and control. We illustrate the potential of tabletop swarm user interfaces through a set of application scenarios developed with Zooids, and discuss general design considerations unique to swarm user interfaces.\n",
      "=============================\n",
      "Sidetap & Slingshot Gestures on Unmodified Smartwatches\n",
      "We present a technique for detecting gestures on the edge of an unmodified smartwatch. We demonstrate two exemplary gestures, i) Sidetap -- tapping on any side and ii) Slingshot -- pressing on the edge and then releasing quickly. Our technique is lightweight, as it relies on measuring the data from the internal Inertial measurement unit (IMU) only. With these two gestures, we expand the input expressiveness of a smartwatch, allowing users to use intuitive gestures with natural tactile feedback, e.g., for the rapid navigation of a long list of items with a tap, or act as shortcut commands to launch applications. It can also allow for eyes-free interaction or subtle interaction where visual attention is not available.\n",
      "=============================\n",
      "aeroMorph - Heat-sealing Inflatable Shape-change Materials for Interaction Design\n",
      "This paper presents a design, simulation, and fabrication pipeline for making transforming inflatables with various materials. We introduce a bending mechanism that creates multiple, programmable shape-changing behaviors with inextensible materials, including paper, plastics and fabrics. We developed a software tool that generates these bending mechanism for a given geometry, simulates its transformation, and exports the compound geometry as digital fabrication files. We show a range of fabrication methods, from manual sealing, to heat pressing with custom stencils and a custom heat-sealing head that can be mounted on usual 3-axis CNC machines to precisely fabricate the designed transforming material. Finally, we present three applications to show how this technology could be used for designing interactive wearables, toys, and furniture.\n",
      "=============================\n",
      "ERICA: Interaction Mining Mobile Apps\n",
      "Design plays an important role in adoption of apps. App design, however, is a complex process with multiple design activities. To enable data-driven app design applications, we present interaction mining -- capturing both static (UI layouts, visual details) and dynamic (user flows, motion details) components of an app's design. We present ERICA, a system that takes a scalable, human-computer approach to interaction mining existing Android apps without the need to modify them in any way. As users interact with apps through ERICA, it detects UI changes, seamlessly records multiple data-streams in the background, and unifies them into a user interaction trace. Using ERICA we collected interaction traces from over a thousand popular Android apps. Leveraging this trace data, we built machine learning classifiers to detect elements and layouts indicative of 23 common user flows. User flows are an important component of UX design and consists of a sequence of UI states that represent semantically meaningful tasks such as searching or composing. With these classifiers, we identified and indexed more than 3000 flow examples, and released the largest online search engine of user flows in Android apps.\n",
      "=============================\n",
      "Advancing Hand Gesture Recognition with High Resolution Electrical Impedance Tomography\n",
      "Electrical Impedance Tomography (EIT) was recently employed in the HCI domain to detect hand gestures using an instrumented smartwatch. This prior work demonstrated great promise for non-invasive, high accuracy recognition of gestures for interactive control. We introduce a new system that offers improved sampling speed and resolution. In turn, this enables superior interior reconstruction and gesture recognition. More importantly, we use our new system as a vehicle for experimentation ' we compare two EIT sensing methods and three different electrode resolutions. Results from in-depth empirical evaluations and a user study shed light on the future feasibility of EIT for sensing human input.\n",
      "=============================\n",
      "Supporting Mobile Sensemaking Through Intentionally Uncertain Highlighting\n",
      "Patients researching medical diagnoses, scientist exploring new fields of literature, and students learning about new domains are all faced with the challenge of capturing information they find for later use. However, saving information is challenging on mobile devices, where the small screen and font sizes combined with the inaccuracy of finger based touch screens makes it time consuming and stressful for people to select and save text for future use. Furthermore, beyond the challenge of simply selecting a region of bounded text on a mobile device, in many learning and data exploration tasks the boundaries of what text may be relevant and useful later are themselves uncertain for the user. In contrast to previous approaches which focused on speeding up the selection process by making the identification of hard boundaries faster, we introduce the idea of intentionally supporting uncertain input in the context of saving information during complex reading and information exploration. We embody this idea in a system that uses force touch and fuzzy bounding boxes along with posthoc expandable context to support identifying and saving information in an intentionally uncertain way on mobile devices. In a two part user study we find that this approach reduced selection time and was preferred by participants over the default system text selection method.\n",
      "=============================\n",
      "AmbioTherm: Simulating Ambient Temperatures and Wind Conditions in VR Environments\n",
      "As Virtual Reality (VR) experiences become increasingly popular, simulating sensory perceptions of environmental conditions is essential for providing an immersive user experience. In this paper, we present Ambiotherm, a wearable accessory for existing Head Mounted Displays (HMD), which simulates real-world environmental conditions such as ambient temperatures and wind conditions. The system consists of a wearable accessory for the HMD and a mobile application, which generates interactive VR environments and controls the thermal and wind stimuli. The thermal stimulation module is attached to the user's neck while two fans are focused on the user's face to simulate wind conditions. We demonstrate the Ambiotherm system with two VR environments, a desert and a snowy mountain, to showcase the different types of ambient temperatures and wind conditions that can be simulated. Results from initial user experiments show that the participants perceive VR environments to be more immersive when external thermal and wind stimuli are presented as part of the VR experience.\n",
      "=============================\n",
      "MagTacS: Delivering Tactile Sensation over an Object\n",
      "A system that can deliver tactile sensation despite an object existing between an actuator and human was developed. This system composed of a control part, power part, output part, and coil. The control part controls the overall system using a microcontroller. The power part generates electric current to create a magnetic field. The output part delivers high energies to the coil. The coil generates a time-varying magnetic field to induce current flow within the body. Through the tactile sensation recognition test, delivery of tactile sensation was confirmed in the air even an object existed between actuator and human skin.\n",
      "=============================\n",
      "RunPlay: Action Recognition Using Wearable Device Apply on Parkour Game\n",
      "In this paper, we present an action recognition system which consists of pressure insoles, with 16 pressure sensors, and an inertial measurement unit. By analysing the data measured from these sensors, we are able to recognised several human activities. In this circumstance, we focus on the detection of jumping, squatting, moving left and right. We also designed a parkour game on a mobile device to demonstrate the in-game control of an avatar by human action.\n",
      "=============================\n",
      "Nomadic Virtual Reality: Exploring New Interaction Concepts for Mobile Virtual Reality Head-Mounted Displays\n",
      "Technical progress and miniaturization enables virtual reality (VR) head-mounted displays (HMDs) now to be solely operated using a smartphone as a display, processing unit and sensor unit. These mobile VR HMDs (e.g. Samsung GearVR) allow for a whole new interaction scenario, where users can bring their HMD with them wherever they want and immerse themselves anytime at any place (nomadic VR). However, most of the early research on interaction with VR HMDs focused around stationary setups. My research revolves around enabling new forms of interaction for these nomadic VR scenarios. In my research I choose a user-centered design approach where I build research prototypes to solve potential problems of nomadic VR and evaluate those prototypes in user studies. I am going to present three prototypes revolving around current challenges of nomadic VR (input and feedback).\n",
      "=============================\n",
      "CodeMend: Assisting Interactive Programming with Bimodal Embedding\n",
      "Software APIs often contain too many methods and parameters for developers to memorize or navigate effectively. Instead, developers resort to finding answers through online search engines and systems such as Stack Overflow. However, the process of finding and integrating a working solution is often very time-consuming. Though code search engines have increased in quality, there remain significant language- and workflow-gaps in meeting end-user needs. Novice and intermediate programmers often lack the language to query, and the expertise in transferring found code to their task. To address this problem, we present CodeMend, a system to support finding and integration of code. CodeMend leverages a neural embedding model to jointly model natural language and code as mined from large Web and code datasets. We also demonstrate a novel, mixed-initiative, interface to support query and integration steps. Through CodeMend, end-users describe their goal in natural language. The system makes salient the relevant API functions, the lines in the end-user's program that should be changed, as well as proposing the actual change. We demonstrate the utility and accuracy of CodeMend through lab and simulation studies.\n",
      "=============================\n",
      "M.Sketch: Prototyping Tool for Linkage-Based Mechanism Design\n",
      "We present M.Sketch, a prototyping tool to support non-experts to design and build linkage-based mechanism prototype. It enables users to draw and simulate arbitrary mechanisms as well as to make physical prototype for testing actual movement. Mix of bottom-up and top-down sketching approaches, real-time movement visualization, and functions for digital fabrication can make the users to design the desired mechanism easily and effectively. M.Sketch can be used to design customized products with kinetic movement, such as interactive robot, toys, and sculptures.\n",
      "=============================\n",
      "Study on Control Method of Virtual Food Texture by Electrical Muscle Stimulation\n",
      "We propose Electric Food Texture System, which can present virtual food texture such as hardness and elasticity by electrical muscle stimulation (EMS) to the masseter muscle. In our previous study, we investigated the feasibility to detect user's bite with a photoreflector and that to construct database of food texture with electromyography sensors. In this paper, we investigated the feasibility to control virtual food texture by EMS. We conducted an experiment to reveal the relationship of the parameters of EMS and those of virtual food texture. The experimental results show that the higher strength of EMS is, the harder virtual food texture is, and the longer duration of EMS is, the more elastic virtual food texture is.\n",
      "=============================\n",
      "Wearables as Context for Guiard-abiding Bimanual Touch\n",
      "We explore the contextual details afforded by wearable devices to support multi-user, direct-touch interaction on electronic whiteboards in a way that-unlike previous work-can be fully consistent with natural bimanual-asymmetric interaction as set forth by Guiard. Our work offers the following key observation. While Guiard's framework has been widely applied in HCI, for bimanual interfaces where each hand interacts via direct touch, subtle limitations of multi-touch technologies as well as limitations in conception and design-mean that the resulting interfaces often cannot fully adhere to Guiard's principles even if they want to. The interactions are fundamentally ambiguous because the system does not know which hand, left or right, contributes each touch. But by integrating additional context from wearable devices, our system can identify which user is touching, as well as distinguish what hand they use to do so. This enables our prototypes to respect lateral preference the assignment of natural roles to each hand as advocated by Guiard in a way that has not been articulated before.\n",
      "=============================\n",
      "Designing a Non-contact Wearable Tactile Display Using Airflows\n",
      "Traditional wearable tactile displays transfer tactile stimulations through a firm contact between the stimulator and the skin. We conjecture that a firm contact may not be always possible and acceptable. Therefore, we explored the concept of a non-contact wearable tactile display using an airflow, which can transfer information without a firm contact. To secure an empirical ground for the design of a wearable airflow display, we conducted a series of psychophysical experiments to estimate the intensity thresholds, duration thresholds, and distance thresholds of airflow perception on various body locations, and report the resulting empirical data in this paper. We then built a 4-point airflow display, compared its performance with that of a vibrotactile display, and could show that the two tactile displays are comparable in information transfer performance. User feedback was also positive and revealed many unique expressions describing airflow-based tactile experiences. Lastly, we demonstrate the feasibility of an airflow-based wearable tactile display with a prototype using micro-fans.\n",
      "=============================\n",
      "Sparkle: Towards Haptic Hover-Feedback with Electric Arcs\n",
      "We demonstrate a method for stimulating the fingertip with touchable electric arcs above a hover sensing input device. We built a hardware platform using a high-voltage resonant transformer for which we control the electric discharge to create in-air haptic feedback up to 4 mm in height, and combined this technology with infrared proximity sensing. Our method is a first step towards supporting novel in-air haptic experiences for hover input that does not require the user to wear haptic feedback stimulators.\n",
      "=============================\n",
      "Eviza: A Natural Language Interface for Visual Analysis\n",
      "Natural language interfaces for visualizations have emerged as a promising new way of interacting with data and performing analytics. Many of these systems have fundamental limitations. Most return minimally interactive visualizations in response to queries and often require experts to perform modeling for a set of predicted user queries before the systems are effective. Eviza provides a natural language interface for an interactive query dialog with an existing visualization rather than starting from a blank sheet and asking closed-ended questions that return a single text answer or static visualization. The system employs a probabilistic grammar based approach with predefined rules that are dynamically updated based on the data from the visualization, as opposed to computationally intensive deep learning or knowledge based approaches. The result of an interaction is a change to the view (e.g., filtering, navigation, selection) providing graphical answers and ambiguity widgets to handle ambiguous queries and system defaults. There is also rich domain awareness of time, space, and quantitative reasoning built in, and linking into existing knowledge bases for additional semantics. Eviza also supports pragmatics and exploring multi-modal interactions to help enhance the expressiveness of how users can ask questions about their data during the flow of visual analysis.\n",
      "=============================\n",
      "Aesthetic Electronics: Designing, Sketching, and Fabricating Circuits through Digital Exploration\n",
      "As interactive electronics become increasingly intimate and personal, the design of circuitry is correspondingly developing a more playful and creative aesthetic. Circuit sketching and design is a multidimensional activity which combines the arts, crafts, and engineering broadening participation of electronic creation to include makers of diverse backgrounds. In order to support this design ecology, we present Ellustrate, a digital design tool that enables the functional and aesthetic design of electronic circuits with multiple conductive and dielectric materials. Ellustrate guides users through the fabrication and debugging process, easing the task of practical circuit creation while supporting designers' aesthetic decisions throughout the circuit authoring workflow. In a formal user study, we demonstrate how Ellustrate enables a new electronic design conversation that combines electronics, materials, and visual aesthetic concerns.\n",
      "=============================\n",
      "Wolverine: A Wearable Haptic Interface for Grasping in VR\n",
      "The Wolverine is a mobile, wearable haptic device designed for simulating the grasping of rigid objects in virtual environment. In contrast to prior work on force feedback gloves, we focus on creating a low cost, lightweight, and wireless device that renders a force directly between the thumb and three fingers to simulate objects held in pad opposition type grasps. Leveraging low-power brake-based locking sliders, the system can withstand over 100N of force between each finger and the thumb, and only consumes 2.78 Wh(10 mJ) for each braking interaction. Integrated sensors are used both for feedback control and user input: time-of-flight sensors provide the position of each finger and an IMU provides overall orientation tracking. This design enables us to use the device for roughly 6 hours with 5500 full fingered grasping events. The total weight is 55g including a 350 mAh battery.\n",
      "=============================\n",
      "Multi-Device Storyboards for Cinematic Narratives in VR\n",
      "Virtual Reality (VR) narratives have the unprecedented potential to connect with an audience through presence, placing viewers within the narrative. The onset of consumer VR has resulted in an explosion of interest in immersive storytelling. Planning narratives for VR, however, is a grand challenge due to its unique affordances, its evolving cinematic vocabulary, and most importantly the lack of supporting tools to explore the creative process in VR. In this paper, we distill key considerations with the planning process for VR stories, collected through a formative study conducted with film industry professionals. Based on these insights we propose a workflow, specific to the needs of professionals creating storyboards for VR film, and present a multi-device (tablet and head-mounted display) storyboard tool supporting this workflow. We discuss our design and report on feedback received from interviews following demonstration of our tool to VR film professionals.\n",
      "=============================\n",
      "2.5 Dimensional Panoramic Viewing Technique utilizing a Cylindrical Mirror Widget\n",
      "We propose a panoramic viewing system, which applies the technique of Anamorphosis, mapping a 2D display onto a cylindrical mirror. In this system, a distorted scene image is shown on a flat panel display or tabletop surface. When a user places the cylindrical mirror on the display, the original image appears on the cylindrical mirror.By simply rotating the cylinder, a user can decide the direction to walk-through in VR world.\n",
      "=============================\n",
      "A 3D Printer for Interactive Electromagnetic Devices\n",
      "We introduce a new form of low-cost 3D printer to print interactive electromechanical objects with wound in place coils. At the heart of this printer is a mechanism for depositing wire within a five degree of freedom (5DOF) fused deposition modeling (FDM) 3D printer. Copper wire can be used with this mechanism to form coils which induce magnetic fields as a current is passed through them. Soft iron wire can additionally be used to form components with high magnetic permeability which are thus able to shape and direct these magnetic fields to where they are needed. When fabricated with structural plastic elements, this allows simple but complete custom electromagnetic devices to be 3D printed. As examples, we demonstrate the fabrication of a solenoid actuator for the arm of a Lucky Cat figurine, a 6-pole motor stepper stator, a reluctance motor rotor and a Ferrofluid display. In addition, we show how printed coils which generate small currents in response to user actions can be used as input sensors in interactive devices.\n",
      "=============================\n",
      "Immersive Scuba Diving Simulator Using Virtual Reality\n",
      "We present Amphibian, a simulator to experience scuba diving virtually in a terrestrial setting. While existing diving simulators mostly focus on visual and aural displays, Amphibian simulates a wider variety of sensations experienced underwater. Users rest their torso on a motion platform to feel buoyancy. Their outstretched arms and legs are placed in a suspended harness to simulate drag as they swim. An Oculus Rift head-mounted display (HMD) and a pair of headphones delineate the visual and auditory ocean scene. Additional senses simulated in Amphibian are breath motion, temperature changes, and tactile feedback through various sensors. Twelve experienced divers compared Amphibian to real-life scuba diving. We analyzed the system factors that influenced the users' sense of being there while using our simulator. We present future UI improvements for enhancing immersion in VR diving simulators.\n",
      "=============================\n",
      "Switch++: An Output Device of the Switches by the Finger Gestures\n",
      "Regarding human-machine-interfaces, switches have not changed significantly despite the machines themselves evolving constantly. In this paper, we propose a new method of operability for devices by providing multiple switches dynamically, and users choose the switch that has the functionality that they want to use. Switch++ senses the mental model of the operating sensation of switches against the user's finger gestures and changes the shape of the switch and its affordances accordingly. We design the interface based on the raw data.\n",
      "=============================\n",
      "Muscle-plotter: An Interactive System based on Electrical Muscle Stimulation that Produces Spatial Output\n",
      "We explore how to create interactive systems based on electrical muscle stimulation that offer expressive output. We present muscle-plotter, a system that provides users with input and output access to a computer system while on the go. Using pen-on-paper interaction, muscle-plotter allows users to engage in cognitively demanding activities, such as writing math. Users write formulas using a pen and the system responds by making the users' hand draw charts and widgets. While Anoto technology in the pen tracks users' input, muscle-plotter uses electrical muscle stimulation (EMS) to steer the user's wrist so as to plot charts, fit lines through data points, find data points of interest, or fill in forms. We demonstrate the system at the example of six simple applications, including a wind tunnel simulator. The key idea behind muscle-plotter is to make the user's hand sweep an area on which muscle-plotter renders curves, i.e., series of values, and to persist this EMS output by means of the pen. This allows the system to build up a larger whole. Still, the use of EMS allows muscle-plotter to achieve a compact and mobile form factor. In our user study, muscle-plotter made participants draw random plots with an accuracy of ±4.07 mm and preserved the frequency of functions to be drawn up to 0.3 cycles per cm.\n",
      "=============================\n",
      "MlioLight: Multi-Layered Image Overlay using Multiple Flashlight Devices\n",
      "We propose a technique that overlays natural images on the real world using the information from multiple flashlight devices. We focus on finding areas of overlapping lights in a multiple light-source scenario and overlaying multi-layered information on a real world object in these areas.In order to mix multiple images, we developed a light identification and overlapping area detection technique using rapid synchronization between high-speed cameras and multiple light devices.In this paper, we describe the concept of our system and a prototype implementation.We also describe two different applications.\n",
      "=============================\n",
      "CloakingNote: A Novel Desktop Interface for Subtle Writing Using Decoy Texts\n",
      "We present CloakingNote, a novel desktop interface for subtle writing. The main idea of CloakingNote is to misdirect observers' attention away from a real text by using a prominent decoy text. To assess the subtlety of CloakingNote, we conducted a subtlety test while varying the contrast ratio between the real text and its background. Our results demonstrated that the real text as well as the interface itself were subtle even when participants were aware that a writer might be engaged in suspicious activities. We also evaluated the feasibility of CloakingNote through a performance test and categorized the users' layout strategies.\n",
      "=============================\n",
      "Hand Gesture and On-body Touch Recognition by Active Acoustic Sensing throughout the Human Body\n",
      "In this paper, we present a novel acoustic sensing technique that recognizes two convenient input actions: hand gestures and on-body touch. We achieved them by observing the frequency spectrum of the wave propagated in the body, around the periphery of the wrist. Our approach can recognize hand gestures and on-body touch concurrently in real-time and is expected to obtain rich input variations by combining them. We conducted a user study that showed classification accuracy of 97%, 96%, and 97% for hand gestures, touches on the forearm, and touches on the back of the hand.\n",
      "=============================\n",
      "Next-Point Prediction Metrics for Perceived Spatial Errors\n",
      "Touch screens have a delay between user input and corresponding visual interface feedback, called input 'latency' (or 'lag'). Visual latency is more noticeable during continuous input actions like dragging, so methods to display feedback based on the most likely path for the next few input points have been described in research papers and patents. Designing these 'next-point prediction' methods is challenging, and there have been no standard metrics to compare different approaches. We introduce metrics to quantify the probability of 7 spatial error 'side-effects' caused by next-point prediction methods. Types of side-effects are derived using a thematic analysis of comments gathered in a 12 participants study covering drawing, dragging, and panning tasks using 5 state-of-the-art next-point predictors. Using experiment logs of actual and predicted input points, we develop quantitative metrics that correlate positively with the frequency of perceived side-effects. These metrics enable practitioners to compare next-point predictors using only input logs.\n",
      "=============================\n",
      "AuraSense: Enabling Expressive Around-Smartwatch Interactions with Electric Field Sensing\n",
      "Existing smartwatches rely on touchscreens for display and input, which inevitably leads to finger occlusion and confines interactivity to a small area. In this work, we introduce AuraSense, which enables rich, around-device, smartwatch interactions using electric field sensing as an adapted device. To explore how this sensing approach could enhance smartwatch interactions, we considered different antenna configurations and how they could enable useful interaction modalities. We identified four configurations that can support six well-known modalities of particular interest and utility, including gestures above or in close proximity to watches, and touchscreen-like finger tracking on the skin. We quantify the feasibility of these input modalities, suggesting that AuraSense can be low latency and robust across users and environments.\n",
      "=============================\n",
      "Telescope: Fine-Tuned Discovery of Interactive Web UI Feature Implementation\n",
      "Professional websites contain rich interactive features that developers can learn from, yet understanding their implementation remains a challenge due to the nature of unfamiliar code. Existing tools provide affordances to analyze source code, but feature-rich websites reveal tens of thousands of lines of code and can easily overwhelm the user. We thus present Telescope, a platform for discovering how JavaScript and HTML support a website interaction. Telescope helps users understand unfamiliar website code through a composite view they control by adjusting JavaScript detail, scoping the runtime timeline, and triggering relational links between JS, HTML, and website components. To support these affordances on the open web, Telescope instruments the JavaScript in a website without request intercepts using a novel sleight-of-hand technique, then watches for traces emitted from the website. In a case study across seven popular websites, Telescope helped identify less than 150 lines of front-end code out of tens of thousands that accurately describe the desired interaction in six of the sites. In an exploratory user study, we observed users identifying difficult programming concepts by developing strategies to analyze relatively small amounts of unfamiliar website source code with Telescope.\n",
      "=============================\n",
      "Resolving Spatial Variation And Allowing Spectator Participation In Multiplayer VR\n",
      "Multiplayer virtual reality (VR) games introduce the problem of variations in the physical size and shape of each user's space for mapping into a shared virtual space. We propose an asymmetric approach to solve the spatial variation problem, by allowing people to choose roles based on the size of their space. We demonstrate this concept through the implementation of a virtual snowball fight where players can choose from multiple roles, namely the shooter, the target, or an onlooker depending on whether the game is played remotely or together in one large space. In the co-located version, the target stands behind an actuated cardboard fort that responds to events in VR, providing non-VR spectators a way to participate in the experience. During preliminary deployment, users showed extremely positive reactions and the spectators were thrilled.\n",
      "=============================\n",
      "Facial Expression Mapping inside Head Mounted Display by Embedded Optical Sensors\n",
      "Head Mounted Display (HMD) provides an immersive ex-perience in virtual environments for various purposes such as for games and communication. However, it is difficult to capture facial expression in a HMD-based virtual environ-ment because the upper half of user's face is covered up by the HMD. In this paper, we propose a facial expression mapping technology between user and a virtual avatar using embedded optical sensors and machine learning. The distance between each sensor and surface of the face is measured by the optical sensors that are attached inside the HMD. Our system learns the sensor values of each facial expression by neural network and creates a classifier to estimate the current facial expression.\n",
      "=============================\n",
      "Habitsourcing: Sensing the Environment through Immersive, Habit-Building Experiences\n",
      "Citizen science and communitysensing applications allow everyday citizens to collect data about the physical world to benefit science and society. Yet despite successes, current approaches are still limited by the number of domain-interested volunteers who are willing and able to contribute useful data. In this paper we introduce habitsourcing, an alternative approach that harnesses the habit-building practices of millions of people to collect environmental data. To support the design and development of habitsourcing apps, we present (1) interaction techniques and design principles for sensing through actuation, a method for acquiring sensing data from cued interactions; and (2) ExperienceKit, an iOS library that makes it easy for developers to build and test habitsourcing applications. In two experiments, we show that our two proof-of-concept apps, ZenWalk and Zombies Interactive, compare favorably to their non-data collecting counterparts, and that we can effectively extract environmental data using simple detection techniques.\n",
      "=============================\n",
      "Analysis of Sequential Tasks in Use Context of Mobile Apps\n",
      "Most of the work on context-aware systems has focused on the context of time, location, and activity. Previous studies on the context flow have been primarily conducted on a qualitative basis. This paper proposes a new approach from a quantitative perspective. We gathered the data from automated task service, \"If This Then That (IFTTT)\", and analyzed the sequential tasks in terms of event occurrence in smart devices through association rule mining. We found out three consecutive tasks in cross-applications. The results of analysis have potential to find hidden use patterns as telling what kinds of services and channels are associated with each other. The findings provide some insights on the development of design guidelines for context-aware services.\n",
      "=============================\n",
      "WristWhirl: One-handed Continuous Smartwatch Input using Wrist Gestures\n",
      "We propose and study a new input modality, WristWhirl, that uses the wrist as an always-available joystick to perform one-handed continuous input on smartwatches. We explore the influence of the wrist's bio-mechanical properties for performing gestures to interact with a smartwatch, both while standing still and walking. Through a user study, we examine the impact of performing 8 distinct gestures (4 directional marks, and 4 free-form shapes) on the stability of the watch surface. Participants were able to perform directional marks using the wrist as a joystick at an average rate of half a second and free-form shapes at an average rate of approximately 1.5secs. The free-form shapes could be recognized by a $1 gesture recognizer with an accuracy of 93.8% and by three human inspectors with an accuracy of 85%. From these results, we designed and implemented a proof-of-concept device by augmenting the watchband using an array of proximity sensors, which can be used to draw gestures with high quality. Finally, we demonstrate a number of scenarios that benefit from one-handed continuous input on smartwatches using WristWhirl.\n",
      "=============================\n",
      "Predicting Finger-Touch Accuracy Based on the Dual Gaussian Distribution Model\n",
      "Accurately predicting the accuracy of finger-touch target acquisition is crucial for designing touchscreen UI and for modeling complex and higher level touch interaction behaviors. Despite its importance, there has been little theoretical work on creating such models. Building on the Dual Gaussian Distribution Model[3], we derived an accuracy model that predicts the success rate of target acquisition based on the target size. We evaluated the model by comparing the predicted success rates with empirical measures for three types of targets including 1-dimensional vertical and horizontal, and 2-dimensional circular targets. The predictions matched the empirical data very well: the differences between predicted and observed success rates were under 5% for 4.8 mm and 7.2 mm targets, and under 10% for 2.4 mm targets. The evaluation results suggest that our simple model can reliably predict touch accuracy.\n",
      "=============================\n",
      "Stretchis: Fabricating Highly Stretchable User Interfaces\n",
      "Recent advances in materials science research allow production of highly stretchable sensors and displays. Such technologies, however, are still not accessible to non-expert makers. We present a novel and inexpensive fabrication method for creating Stretchis, highly stretchable user interfaces that combine sensing capabilities and visual output. We use Polydimethylsiloxan (PDMS) as the base material for a Stretchi and show how to embed stretchable touch and proximity sensors and stretchable electroluminescent displays. Stretchis can be ultra-thin (≈ 200μm), flexible, and fully customizable, enabling non-expert makers to add interaction to elastic physical objects, shape-changing surfaces, fabrics, and the human body. We demonstrate the usefulness of our approach with three application examples that range from ubiquitous computing to wearables and on-skin interaction.\n",
      "=============================\n",
      "Applications of Switchable Permanent Magnetic Actuators in Shape Change and Tactile Display\n",
      "Systems realizing shape change and tactile display remain hindered by the power, cost, and size limitations of current actuation technology. We describe and evaluate a novel use of switchable permanent magnets as a bistable actuator for haptic feedback which draws power only when switching states. Because of their efficiency, low cost, and small size, these actuators show promise in realizing tactile display within mobile, wearable, and embedded systems. We present several applications demonstrating potential uses in the mobile, automotive, and desktop computing domains, and perform a technical evaluation of the actuators used in these systems.\n",
      "=============================\n",
      "The Toastboard: Ubiquitous Instrumentation and Automated Checking of Breadboarded Circuits\n",
      "The recent proliferation of easy to use electronic components and toolkits has introduced a large number of novices to designing and building electronic projects. Nevertheless, debugging circuits remains a difficult and time-consuming task. This paper presents a novel debugging tool for electronic design projects, the Toastboard, that aims to reduce debugging time by improving upon the standard paradigm of point-wise circuit measurements. Ubiquitous instrumentation allows for immediate visualization of an entire breadboard's state, meaning users can diagnose problems based on a wealth of data instead of having to form a single hypothesis and plan before taking a measurement. Basic connectivity information is displayed visually on the circuit itself and quantitative data is displayed on the accompanying web interface. Software-based testing functions further lower the expertise threshold for efficient debugging by diagnosing classes of circuit errors automatically. In an informal study, participants found the detailed, pervasive, and context-rich data from our tool helpful and potentially time-saving.\n",
      "=============================\n",
      "OctaRing: Examining Pressure-Sensitive Multi-Touch Input on a Finger Ring Device\n",
      "In this paper, we introduce OctaRing, an octagon-shaped finger ring device that facilitates pressure-sensitive multi- touch gestures. To explore the feasibility of its prototype, we conducted an experiment and investigated users' sensorimotor skills in exerting different levels of pressure on the ring with more than one finger. The results of the experiment indicate that users are comfortable with the two-finger touch configuration with two levels of pressure. Based on this result, future work will explore novel gestures involving a finger ring device.\n",
      "=============================\n",
      "Reconstruction of Scene from Multiple Sketches\n",
      "This paper discusses the feasibility of extension of expressive style with multiple 3D sketches drawn by a sketching tool that enables its users to draw and paint on 3D structured surfaces. Users of our proposed system take a picture of target objects and sketch with reference to the taken picture. They can not only sketch on the pictures but can also change their viewpoint of the sketched environment, since the system captures 3D structure by using a depth sensor as well as RGB data. Trial usage of the system shows that our users can rapidly extract their target objects/space and extend their ideas by taking pictures and drawing/painting on them. This paper presents examples of system usage, and discusses the feasibility of extension of sketches.\n",
      "=============================\n",
      "AquaCAVE: Augmented Swimming Environment with Immersive Surround-Screen Virtual Reality\n",
      "AquaCAVE is a system for enhancing the swimming experience. Although swimming is considered to be one of the best exercises to maintain our health, swimming in a pool is normally monotonous; thus, maintaining its motivation is sometimes difficult. AquaCAVE is a computer-augmented swimming pool with rear-projection acrylic walls that surround a swimmer, providing a CAVE-like immersive stereoscopic projection environment. The swimmer wears goggles with liquid-crystal display (LCD) shutter glasses, and cameras installed in the pool tracks swimmer's head position. Swimmers can be immersed into synthetic scenes such as coral reefs, outer space, or any other computer generated environments. The system can also provide swimming training with projections such as record lines and swimming forms as 3D virtual characters in the 3D space.\n",
      "=============================\n",
      "Changing the Appearance of Physical Interfaces Through Controlled Transparency\n",
      "We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n",
      "=============================\n",
      "VidCrit: Video-based Asynchronous Video Review\n",
      "Video production is a collaborative process in which stakeholders regularly review drafts of the edited video to indicate problems and offer suggestions for improvement. Although practitioners prefer in-person feedback, most reviews are conducted asynchronously via email due to scheduling and location constraints. The use of this impoverished medium is challenging for both providers and consumers of feedback. We introduce VidCrit, a system for providing asynchronous feedback on drafts of edited video that incorporates favorable qualities of an in-person review. This system consists of two separate interfaces: (1) A feedback recording interface captures reviewers' spoken comments, mouse interactions, hand gestures and other physical reactions. (2) A feedback viewing interface transcribes and segments the recorded review into topical comments so that the video author can browse the review by either text or timelines. Our system features novel methods to automatically segment a long review session into topical text comments, and to label such comments with additional contextual information. We interviewed practitioners to inform a set of design guidelines for giving and receiving feedback, and based our system's design on these guidelines. Video reviewers using our system preferred our feedback recording interface over email for providing feedback due to the reduction in time and effort. In a fixed amount of time, reviewers provided 10.9 (σ=5.09) more local comments than when using text. All video authors rated our feedback viewing interface preferable to receiving feedback via e-mail.\n",
      "=============================\n",
      "VizLens: A Robust and Interactive Screen Reader for Interfaces in the Real World\n",
      "The world is full of physical interfaces that are inaccessible to blind people, from microwaves and information kiosks to thermostats and checkout terminals. Blind people cannot independently use such devices without at least first learning their layout, and usually only after labeling them with sighted assistance. We introduce VizLens - an accessible mobile application and supporting backend that can robustly and interactively help blind people use nearly any interface they encounter. VizLens users capture a photo of an inaccessible interface and send it to multiple crowd workers, who work in parallel to quickly label and describe elements of the interface to make subsequent computer vision easier. The VizLens application helps users recapture the interface in the field of the camera, and uses computer vision to interactively describe the part of the interface beneath their finger (updating 8 times per second). We show that VizLens provides accurate and usable real-time feedback in a study with 10 blind participants, and our crowdsourcing labeling workflow was fast (8 minutes), accurate (99.7%), and cheap ($1.15). We then explore extensions of VizLens that allow it to (i) adapt to state changes in dynamic interfaces, (ii) combine crowd labeling with OCR technology to handle dynamic displays, and (iii) benefit from head-mounted cameras. VizLens robustly solves a long-standing challenge in accessibility by deeply integrating crowdsourcing and computer vision, and foreshadows a future of increasingly powerful interactive applications that would be currently impossible with either alone.\n",
      "=============================\n",
      "Mining Controller Inputs to Understand Gameplay\n",
      "Today's game analytics systems are powered by event logs, which reveal information about what players are doing but offer little insight about the types of gameplay that games foster. Moreover, the concept of gameplay itself is difficult to define and quantify. In this paper, we show that analyzing players' controller inputs using probabilistic topic models allows game developers to describe the types of gameplay -- or action -- in games in a quantitative way. More specifically, developers can discover the types of action that a game fosters and the extent that each game level fosters each type of action, all in an unsupervised manner. They can use this information to verify that their levels feature the appropriate style of gameplay and to recommend levels with gameplay that is similar to levels that players like. We begin with latent Dirichlet allocation (LDA), the simplest topic model, then develop the player-gameplay action (PGA) model to make the same types of discoveries about gameplay in a way that is independent of each player's play style. We train a player recognition system on the PGA model's output to verify that its discoveries about gameplay are in fact independent of each player's play style. The system recognizes players with over 90% accuracy in about 20 seconds of playtime.\n",
      "=============================\n",
      "Interactive Volume Segmentation with Threshold Field Painting\n",
      "An interactive method for segmentation and isosurface extraction of medical volume data is proposed. In conventional methods, users decompose a volume into multiple regions iteratively, segment each region using a threshold, and then manually clean the segmentation result by removing clutter in each region. However, this is tedious and requires many mouse operations from different camera views. We propose an alternative approach whereby the user simply applies painting operations to the volume using tools commonly seen in painting systems, such as flood fill and brushes. This significantly reduces the number of mouse and camera control operations. Our technical contribution is in the introduction of the threshold field, which assigns spatially-varying threshold values to individual voxels. This generalizes discrete decomposition of a volume into regions and segmentation using a constant threshold in each region, thereby offering a much more flexible and efficient workflow. This paper describes the details of the user interaction and its implementation. Furthermore, the results of a user study are discussed. The results indicate that the proposed method can be a few times faster than a conventional method.\n",
      "=============================\n",
      "Virtual Sweet: Simulating Sweet Sensation Using Thermal Stimulation on the Tip of the Tongue\n",
      "Being a pleasurable sensation, sweetness is recognized as the most preferred sensation among the five primary taste sensations. In this paper, we present a novel method to virtually simulate the sensation of sweetness by applying thermal stimulation to the tip of the human tongue. To digitally simulate the sensation of sweetness, the system delivers rapid heating and cooling stimuli to the tongue via a 2x2 grid of Peltier elements. To achieve distinct, controlled, and synchronized temperature variations in the stimuli, a control module is used to regulate each of the Peltier elements. Results from our preliminary experiments suggest that the participants were able to perceive mild sweetness on the tip of their tongue while using the proposed system.\n",
      "=============================\n",
      "WithYou: An Interactive Shadowing Coach with Speech Recognition\n",
      "Speech shadowing, in which the subject listens to native narration sound and tries to repeat it immediately while listening, is a proven way of practicing speaking skills when learning foreign languages. However, since the narration is independent of user's speech, the playback cannot make an adjustment when the learner fails to catch up, and this makes shadowing difficult. We propose WithYou, a system based on Automated Speech Recognition (ASR) that is able to adjust narration playback during a live shadowing speech. WithYou compares the student's live speech with the narration playback to detect shadowing mistakes. In addition, WithYou is able to handle pauses and recognize repetitive phrases in shadowing practice. A user study shows that practicing shadowing with WithYou is easier and more effective compared with conventional methods.\n",
      "=============================\n",
      "Developing fMRI-Compatible Interaction Systems through Air Pressure\n",
      "We leverage the use of air pressure to expand the interaction space within fMRI (functional magnetic resonance imaging). We present three example applications that are not previously possible in conventional fMRI interaction devices: 1) pedal interface that can record continuous pressure value pressed by users, 2) wrist tactile interface that can provide various tactile patterns or stimuli, 3) adjustable resistance joystick that can provide feedback through different resistance levels. Our work shows that the use of air pressure can enable new research opportunities for fMRI researchers.\n",
      "=============================\n",
      "NormalTouch and TextureTouch: High-fidelity 3D Haptic Shape Rendering on Handheld Virtual Reality Controllers\n",
      "We present an investigation of mechanically-actuated hand-held controllers that render the shape of virtual objects through physical shape displacement, enabling users to feel 3D surfaces, textures, and forces that match the visual rendering. We demonstrate two such controllers, NormalTouch and TextureTouch, which are tracked in 3D and produce spatially-registered haptic feedback to a user's finger. NormalTouch haptically renders object surfaces and provides force feedback using a tiltable and extrudable platform. TextureTouch renders the shape of virtual objects including detailed surface structure through a 4×4 matrix of actuated pins. By moving our controllers around while keeping their finger on the actuated platform, users obtain the impression of a much larger 3D shape by cognitively integrating output sensations over time. Our evaluation compares the effectiveness of our controllers with the two de-facto standards in Virtual Reality controllers: device vibration and visual feedback only. We find that haptic feedback significantly increases the accuracy of VR interaction, most effectively by rendering high-fidelity shape output as in the case of our controllers.\n",
      "=============================\n",
      "Wrap & Sense: Grasp Capture by a Band Sensor\n",
      "This paper proposes a bare hand grasp observation system named Wrap & Sense. We built a band type sensing equipment composed of infrared distance sensors placed in an array. The sensor band is attached to a target object with all sensors directed along the object surface and detects the hand side edge with respect to the object. Assuming type of grasp as 'power grasp', the whole hand posture can be determined according to the 3D shape of the object. Three types of application are shown as proof-of-concept.\n",
      "=============================\n",
      "Music Composition with Recommendation\n",
      "Creating a piece of music requires deep knowledge of composition, and is time-consuming even for experts. Algorithmic composition systems can generate pieces in an existing style. However, these systems are not interactive. Therefore, it is difficult for them to express the user's intention. We propose a system that recommends a continuation melody in accordance with a melody expressed by the user. Recommendation uses the style of the piece of the composer, thus users give the system a piece of the style in which they want to compose. With this system, users can compose pieces tailored to their needs, and composers can get assistance with composition.\n",
      "=============================\n",
      "Design and Evaluation of EdgeWrite Alphabets for Round Face Smartwatches\n",
      "This study presents a project aimed at designing and evaluating a unistroke gesture set of alphanumeric characters targeting round-face smartwatches. We conducted a user study with 10 participants to generate the basic gesture design for 40 characters. For each character, we measured the preference and agreement scores and uncovered any challenges faced in designing unistroke gestures for round-face smartwatches. We developed a gesture recognizer using machine learning, which used a backpropagation mechanism to evaluate the designed gestures. Using the gesture recognizer, we collected 80,000 gesture data, and evaluated them with 5-fold cross-validation. The obtained mean recognition rate was 92.14%.\n",
      "=============================\n",
      "ScalableBody: A Telepresence Robot Supporting Socially Acceptable Interactions and Human Augmentation through Vertical Actuation\n",
      "Most telepresence robots have a fixed-size body, and are unable to change the camera or display position. Therefore, although making eye contact is important in human expression, current fixed-size telepresence robots fail to achieve this. We propose a novel telepresence robot called ScalableBody, which enables users to make eye contact during conversations by changing its height. ScalableBody extends its body to modify the position of its camera or display. This approach provides eye contacts in remote conversations, thus creating almost same situation when the remote and local users make conversation like a real meeting. As for the remote users, this approach also enables them to experience having a conversation from different heights, such as being a giant or a dwarf. This technique extends the possibilities of remote communication by telepresence robots.\n",
      "=============================\n",
      "UnlimitedHand: Input and Output Hand Gestures with Less Calibration Time\n",
      "Numerous devices that either track hand gestures or provide haptic feedback have been developed with the aim of manipulating objects within Virtual Reality(VR) and Augmented Reality(AR) environments. However, these devices implement lengthy calibration processes to ease out individual differences. In this research, a wearable device that simultaneously recognizes hand gestures and outputs haptic feedback: UnlimitedHand is suggested. Photo-reflectors are placed over specific muscle groups on the forearm to read in hand gestures. For output, electrodes are placed over the same muscles to control the user's hand movements. Both sensors and electrodes target main muscle groups responsible for moving the hand. Since the positions of these muscle groups are common between humans, UnlimitedHand is able to reduce the time spent on performing calibration.\n",
      "=============================\n",
      "Fluxa: Body Movements as a Social Display\n",
      "This paper presents Fluxa, a compact wearable device that exploits body movements, as well as the visual effects of persistence of vision (POV), to generate mid-air displays on and around the body. When the user moves his/her limb, Fluxa displays a pattern that, due to retinal afterimage, can be perceived by the surrounding people. We envision Fluxa as a wearable display to foster social interactions. It can be used to enhance existing social gestures such as hand-waving to get attention, as a communicative tool that displays the speed and distance covered by joggers, and as a self-expression device that generates images while dancing. We discuss the advantages of Fluxa: a display size that could be much larger than the device itself, a semi-transparent display that allows users and others to see though it and promotes social interaction.\n",
      "=============================\n",
      "Exploring the Design Space for Energy-Harvesting Situated Displays\n",
      "We explore the design space of energy-neutral situated displays, which give physical presence to digital information. We investigate three central dimensions: energy sources, display technologies, and wireless communications. Based on the power implications from our analysis, we present a thin, wireless, photovoltaic-powered display that is quick and easy to deploy and capable of indefinite operation in indoor lighting conditions. The display uses a low-resolution e-paper architecture, which is 35 times more energy-efficient than smaller-sized high-resolution displays. We present a detailed analysis on power consumption, photovoltaic energy harvesting performance, and a detailed comparison to other display-driving architectures. Depending on the ambient lighting, the display can trigger an update every 1 -- 25 minutes and communicate to a PC or smartphone via Bluetooth Low-Energy.\n",
      "=============================\n",
      "ExtendedHand on Wheelchair\n",
      "In this paper, we present a novel welfare system which utilizes a spatial augmented reality technique. Hand is a crucial component in human-human communication. For example, we can intuitively indicate an object or place by reaching and pointing it to nearby partners. Unfortunately, for wheelchair users, such communication is often limited because their reaching ranges are narrow, and moving their bodies to the target is tiresome. To solve this issue, we propose a novel wheelchair system on which a battery-powered mobile projector is mounted. A user manipulates the projected virtual hand as an extension of the real one using a touch panel equipped on an armrest of the wheelchair. We implement our proposed system and demonstrate the effectiveness.\n",
      "=============================\n",
      "Interacting with Soli: Exploring Fine-Grained Dynamic Gesture Recognition in the Radio-Frequency Spectrum\n",
      "This paper proposes a novel machine learning architecture, specifically designed for radio-frequency based gesture recognition. We focus on high-frequency (60]GHz), short-range radar based sensing, in particular Google's Soli sensor. The signal has unique properties such as resolving motion at a very fine level and allowing for segmentation in range and velocity spaces rather than image space. This enables recognition of new types of inputs but poses significant difficulties for the design of input recognition algorithms. The proposed algorithm is capable of detecting a rich set of dynamic gestures and can resolve small motions of fingers in fine detail. Our technique is based on an end-to-end trained combination of deep convolutional and recurrent neural networks. The algorithm achieves high recognition rates (avg 87%) on a challenging set of 11 dynamic gestures and generalizes well across 10 users. The proposed model runs on commodity hardware at 140 Hz (CPU only).\n",
      "=============================\n",
      "Watch Commander: A Gesture-based Invocation System for Rectangular Smartwatches using B2B-Swipe\n",
      "We present Watch Commander, a gesture-based invocation system for rectangular smartwatches. Watch Commander allows the user to invoke functions easily and quickly by using Bezel to Bezel-Swipe (B2B-Swipe). This is because B2B-Swipe does not conflict with other swipe gestures such as flick and bezel swipe and can be performed in an eyes-free manner. Moreover, by providing GUIs that display functions assigned with B2B-Swipe, Watch Commander helps the user memorize those functions.\n",
      "=============================\n",
      "Thickness Control Technique for Printing Tactile Sheets with Fused Deposition Modeling\n",
      "We present a printing technique that controls the thickness of objects by increasing and decreasing the amount of material extruded during printing. Using this technique, printers can dynamically control thickness and output thicker objects without a staircase effect. This technique allows users to print aesthetic pattern sheets and objects that are tactile without requiring any new hardware. This extends the capabilities of fused deposition modeling (FDM) 3D printers in a simple way. We describe a method of generating and calculating a movement path for printing tactile sheets, and demonstrate the usage and processing of example objects.\n",
      "=============================\n",
      "A Novel Real Time Monitor System of 3D Printing Layers for Better Slicing Parameter Setting\n",
      "We proposed a novel real time monitor system of 3D printer with dual cameras, which capture and reconstruct the printed result layer by layer. With the reconstructed image, we can apply computer vision technique to evaluate the difference with the ideal path generate by G-code. The difference gives us clues to classify which might be the possible factor of the result. Hence we can produce advice to user for better slicing parameter settings. We believe that this system can give helps to beginner or users of 3D printer that struggle in parameter settings in the future.\n",
      "=============================\n",
      "Phones on Wheels: Exploring Interaction for Smartphones with Kinetic Capabilities\n",
      "This paper introduces novel interaction and applications using smartphones with kinetic capabilities. We develop an accessory module with robot wheels for a smartphone. With this module, the smartphone can move in a linear direction or rotate with sufficient power. The module also includes rotary encoders, allowing us to use the wheels as an input modality. We demonstrate a series of novel mobile interaction for mobile devices with kinetic capabilities through three applications.\n",
      "=============================\n",
      "Porous Interfaces for Small Screen Multitasking using Finger Identification\n",
      "The lack of dedicated multitasking interface features in smartphones has resulted in users attempting a sequential form of multitasking via frequent app switching. In addition to the obvious temporal cost, it requires physical and cognitive effort which increases multifold as the back and forth switching becomes more frequent. We propose porous interfaces, a paradigm that combines the concept of translucent windows with finger identification to support efficient multitasking on small screens. Porous interfaces enable partially transparent app windows overlaid on top of each other, each of them being accessible simultaneously using a different finger as input. We design porous interfaces to include a broad range of multitasking interactions with and between windows, while ensuring fidelity with the existing smartphone interactions. We develop an end-to-end smartphone interface that demonstrates porous interfaces. In a qualitative study, participants found porous interfaces intuitive, easy, and useful for frequent multitasking scenarios.\n",
      "=============================\n",
      "Gaze and Touch Interaction on Tablets\n",
      "We explore how gaze can support touch interaction on tablets. When holding the device, the free thumb is normally limited in reach, but can provide an opportunity for indirect touch input. Here we propose gaze and touch input, where touches redirect to the gaze target. This provides whole-screen reachability while only using a single hand for both holding and input. We present a user study comparing this technique to direct-touch, showing that users are slightly slower but can utilise one-handed use with less physical effort. To enable interaction with small targets, we introduce CursorShift, a method that uses gaze to provide users temporal control over cursors during direct-touch interactions. Taken together, users can employ three techniques on tablets: direct-touch, gaze and touch, and cursor input. In three applications, we explore how these techniques can coexist in the same UI and demonstrate how tablet tasks can be performed with thumb-only input of the holding hand, and with it describe novel interaction techniques for gaze based tablet interaction.\n",
      "=============================\n",
      "SketchingWithHands: 3D Sketching Handheld Products with First-Person Hand Posture\n",
      "We present SketchingWithHands, a 3D sketching system that incorporates a hand-tracking sensor. The system enables product designers to easily capture desired hand postures from a first-person point of view at any time and to use the captured hand information to explore handheld product concepts by 3D sketching while keeping the proper scale and usage of the products. Based on the analysis of design practices and drawing skills in the art and design literature, we suggest novel ideas for efficiently acquiring hand postures (palm-pinning widget, front and center mirrors, responsive spangles), for quickly creating and easily adjusting sketch planes (modified tick-triggered, orientable and shiftable sketch planes), for appropriately starting 3D sketching products with hand information (hand skeleton, grip axis), and for practically increasing user throughput (intensifier, rough and precise erasers)---all of which are coherently and consistently integrated in our system. A user test by ten industrial design students and an in-depth discussion show that our system is both useful and usable in designing handheld products.\n",
      "=============================\n",
      "GyroVR: Simulating Inertia in Virtual Reality using Head Worn Flywheels\n",
      "We present GyroVR, head worn flywheels designed to render inertia in Virtual Reality (VR. Motions such as flying, diving or floating in outer space generate kinesthetic forces onto our body which impede movement and are currently not represented in VR. We simulate those kinesthetic forces by attaching flywheels to the users head, leveraging the gyroscopic effect of resistance when changing the spinning axis of rotation. GyroVR is an ungrounded, wireless and self contained device allowing the user to freely move inside the virtual environment. The generic shape allows to attach it to different positions on the users body. We evaluated the impact of GyroVR onto different mounting positions on the head (back and front) in terms of immersion, enjoyment and simulator sickness. Our results show, that attaching GyroVR onto the users head (front of the Head Mounted Display (HMD)) resulted in the highest level of immersion and enjoyment and therefore can be built into future VR HMDs, enabling kinesthetic forces in VR.\n",
      "=============================\n",
      "SoEs: Attachable Augmented Haptic on Gaming Controller for Immersive Interaction\n",
      "We present SoEs (Sword of Elements), an attachable augmented haptic device for enhancing gaming controller in the immersive first-person game. Generally, Player can easily receive visual and auditory feedback through head-mounted displays (HMD) and headphones from first-person perspective in virtual world. However, the tactile feedback is less than those feedbacks in immersive environment. Although gaming controller, i.e. VIVE or Oculus controller, can provide tactile feedback by some vibration sensors, the haptic feedback is more complicated and various, it includes kinesthesia and cutaneous feedback. Our key idea is to provide a low-cost approach to simulate the haptic feedback of player manipulation in the immersive environment such as striking while the iron is hot which the player could feel the heat and reaction force. Eventually, the game makers could utilize the attachable device into their games for providing haptic feedback.\n",
      "=============================\n",
      "Prevention of Unintentional Input While Using Wrist Rotation for Device Configuration\n",
      "We describe the design of the safeguard interface that helps users avoid unintentional input while using wrist rotation. When configuring the parameters of various devices, our interface helps reduce the chance of making accidental changes by delaying the result of input and allowing the users to make deliberate attempt to change the parameters to their desired value. We evaluated our methods with a set of user experience and found that our methods were more preferred when the end-results of configurational changes of the devices become more critical and can cause irreversible damage.\n",
      "=============================\n",
      "Expressive Keyboards: Enriching Gesture-Typing on Mobile Devices\n",
      "Gesture-typing is an efficient, easy-to-learn, and errortolerant technique for entering text on software keyboards. Our goal is to \"recycle\" users' otherwise-unused gesture variation to create rich output under the users' control, without sacrificing accuracy. Experiment 1 reveals a high level of existing gesture variation, even for accurate text, and shows that users can consciously vary their gestures under different conditions. We designed an Expressive Keyboard for a smart phone which maps input gesture features identified in Experiment 1 to a continuous output parameter space, i.e. RGB color. Experiment 2 shows that users can consciously modify their gestures, while retaining accuracy, to generate specific colors as they gesture-type. Users are more successful when they focus on output characteristics (such as red) rather than input characteristics (such as curviness). We designed an app with a dynamic font engine that continuously interpolates between several typefaces, as well as controlling weight and random variation. Experiment 3 shows that, in the context of a more ecologically-valid conversation task, users enjoy generating multiple forms of rich output. We conclude with suggestions for how the Expressive Keyboard approach can enhance a wide variety of gesture recognition applications.\n",
      "=============================\n",
      "Partial Bookmarking: A Structure-independent Mechanism of Transclusion for a Portion of any Web Page\n",
      "A novel mechanism of transclusion for collecting and producing information on the Web, named partial bookmarking, is proposed. Partial bookmarking allows a user to collect portions of any web page by making it able to use for a spatial hypertext, like a web document element, without the need to duplicate its contents. Whereas the previous studies involving transclusion required pre-designed linkable objects, such as XML elements or HTML objects, partial bookmarking does not rely on any document structure. To accomplish partial bookmarking, we enhanced a conventional web browser with multiple tabs by introducing the technology of mirroring to display only a portion of a web page appropriately while factoring in potential copyright issues.\n",
      "=============================\n",
      "Luminescent Tentacles: A Scalable SMA Motion Display\n",
      "The Luminescent Tentacles system is a scalable kinetic surface system for kinetic art, ambient display, and animatronics. The 256 shape-memory alloy actuators react to hand movement by fluid dynamics and Kinect. These actuators behave like waving tentacles of sea anemones under the sea, and the top of the actuator softly glows like a bioluminescent organism. To precisely control a large number of actuators simultaneously, the system utilizes one microcontroller per actuator for distributed processing. In addition, it provides a scalable platform, which can be easily built into various forms.\n",
      "=============================\n",
      "ChainFORM: A Linear Integrated Modular Hardware System for Shape Changing Interfaces\n",
      "This paper presents ChainFORM: a linear, modular, actuated hardware system as a novel type of shape changing interface. Using rich sensing and actuation capability, this modular hardware system allows users to construct and customize a wide range of interactive applications. Inspired by modular and serpentine robotics, our prototype comprises identical modules that connect in a chain. Modules are equipped with rich input and output capability: touch detection on multiple surfaces, angular detection, visual output, and motor actuation. Each module includes a servo motor wrapped with a flexible circuit board with an embedded microcontroller. Leveraging the modular functionality, we introduce novel interaction capability with shape changing interfaces, such as rearranging the shape/configuration and attaching to passive objects and bodies. To demonstrate the capability and interaction design space of ChainFORM, we implemented a variety of applications for both computer interfaces and hands-on prototyping tools.\n",
      "=============================\n",
      "Fitter: A System for Easily Printing Objects that Fit Real Objects\n",
      "When printing both self-making and existing 3D models, users often create models to fit to a real object within it. Fitting models to the size of a real object is a delicate problem. To address it, we present a concept to capture the size of a real object, create or modify a model that conforms to the captured image, and print the model on the spot. We create a 3D printer to realize this concept by installing a touch panel display in the build plate system. In this paper, we focus on creating containers that fit accessories. We create containers for a pair of scissors, a smart watch, a drone, a pair of glasses, and a pen holder.\n",
      "=============================\n",
      "Activity-Aware Video Stabilization for BallCam\n",
      "We present a video stabilization algorithm for ball camera systems that undergo extreme egomotion during sports play. In particular, we focus on the BallCam system which is an American football embedded with an action camera at the tip of the ball. We propose an activity-aware video stabilization algorithm which is able to understand the current activity of the BallCam, which uses estimated activity labels to inform a robust video stabilization algorithm. Activity recognition is performed with a deep convolutional neural network, which uses optical flow.\n",
      "=============================\n",
      "Boomerang: Rebounding the Consequences of Reputation Feedback on Crowdsourcing Platforms\n",
      "Paid crowdsourcing platforms suffer from low-quality work and unfair rejections, but paradoxically, most workers and requesters have high reputation scores. These inflated scores, which make high-quality work and workers difficult to find, stem from social pressure to avoid giving negative feedback. We introduce Boomerang, a reputation system for crowdsourcing platforms that elicits more accurate feedback by rebounding the consequences of feedback directly back onto the person who gave it. With Boomerang, requesters find that their highly-rated workers gain earliest access to their future tasks, and workers find tasks from their highly-rated requesters at the top of their task feed. Field experiments verify that Boomerang causes both workers and requesters to provide feedback that is more closely aligned with their private opinions. Inspired by a game-theoretic notion of incentive-compatibility, Boomerang opens opportunities for interaction design to incentivize honest reporting over strategic dishonesty.\n",
      "=============================\n",
      "AggreGaze: Collective Estimation of Audience Attention on Public Displays\n",
      "Gaze is frequently explored in public display research given its importance for monitoring and analysing audience attention. However, current gaze-enabled public display interfaces require either special-purpose eye tracking equipment or explicit personal calibration for each individual user. We present AggreGaze, a novel method for estimating spatio-temporal audience attention on public displays. Our method requires only a single off-the-shelf camera attached to the display, does not require any personal calibration, and provides visual attention estimates across the full display. We achieve this by 1) compensating for errors of state-of-the-art appearance-based gaze estimation methods through on-site training data collection, and by 2) aggregating uncalibrated and thus inaccurate gaze estimates of multiple users into joint attention estimates. We propose different visual stimuli for this compensation: a standard 9-point calibration, moving targets, text and visual stimuli embedded into the display content, as well as normal video content. Based on a two-week deployment in a public space, we demonstrate the effectiveness of our method for estimating attention maps that closely resemble ground-truth audience gaze distributions.\n",
      "=============================\n",
      "RadarCat: Radar Categorization for Input & Interaction\n",
      "In RadarCat we present a small, versatile radar-based system for material and object classification which enables new forms of everyday proximate interaction with digital devices. We demonstrate that we can train and classify different types of materials and objects which we can then recognize in real time. Based on established research designs, we report on the results of three studies, first with 26 materials (including complex composite objects), next with 16 transparent materials (with different thickness and varying dyes) and finally 10 body parts from 6 participants. Both leave one-out and 10-fold cross-validation demonstrate that our approach of classification of radar signals using random forest classifier is robust and accurate. We further demonstrate four working examples including a physical object dictionary, painting and photo editing application, body shortcuts and automatic refill based on RadarCat. We conclude with a discussion of our results, limitations and outline future directions.\n",
      "=============================\n",
      "Friend*Chip: A Bracelet with Digital Pet for Socially Inclusive Games for Children\n",
      "Learning in groups have different potential benefits for children. They have the opportunity to solve problems together, to share experiences and to develop social skills. However, from teachers point of view, creating a safe and inclusive positive environment for children is not an simple task since each child has differences that represent a challenge for implementing effectively group dynamics. The focus of this work is the design of a system that motivates children to approach to others and create opportunities of social interaction. The system creates a fun and enjoyable situation that is always supervised by the teacher, who can monitor and change the group dynamics at any moment during the activity.\n",
      "=============================\n",
      "Towards Understanding Collaboration around Interactive Surfaces: Exploring Joint Visual Attention\n",
      "In this abstract, we present a novel method for exploring the visual behavior of multiple users engaged in a collaborative task around an interactive surface. The proposed method synchronizes input from multiple eye trackers, describes the visual behavior of individual users over time, and identifies joint attention across multiple users. We applied this method to analyze the visual behavior of four users collaborating using a large-scale multi-touch tabletop.\n",
      "=============================\n",
      "IdeaHound: Improving Large-scale Collaborative Ideation with Crowd-Powered Real-time Semantic Modeling\n",
      "Prior work on creativity support tools demonstrates how a computational semantic model of a solution space can enable interventions that substantially improve the number, quality and diversity of ideas. However, automated semantic modeling often falls short when people contribute short text snippets or sketches. Innovation platforms can employ humans to provide semantic judgments to construct a semantic model, but this relies on external workers completing a large number of tedious micro tasks. This requirement threatens both accuracy (external workers may lack expertise and context to make accurate semantic judgments) and scalability (external workers are costly). In this paper, we introduce IdeaHound, an ideation system that seamlessly integrates the task of defining semantic relationships among ideas into the primary task of idea generation. The system combines implicit human actions with machine learning to create a computational semantic model of the emerging solution space. The integrated nature of these judgments allows IDEAHOUND to leverage the expertise and efforts of participants who are already motivated to contribute to idea generation, overcoming the issues of scalability inherent to existing approaches. Our results show that participants were equally willing to use (and just as productive using) IDEAHOUND compared to a conventional platform that did not require organizing ideas. Our integrated crowdsourcing approach also creates a more accurate semantic model than an existing crowdsourced approach (performed by external crowds). We demonstrate how this model enables helpful creative interventions: providing diverse inspirational examples, providing similar ideas for a given idea and providing a visual overview of the solution space.\n",
      "=============================\n",
      "Rovables: Miniature On-Body Robots as Mobile Wearables\n",
      "We introduce Rovables, a miniature robot that can move freely on unmodified clothing. The robots are held in place by magnetic wheels, and can climb vertically. The robots are untethered and have an onboard battery, microcontroller, and wireless communications. They also contain a low-power localization system that uses wheel encoders and IMU, allowing Rovables to perform limited autonomous navigation on the body. In the technical evaluations, we found that Rovables can operate continuously for 45 minutes and can carry up to 1.5N. We propose an interaction space for mobile on-body devices spanning sensing, actuation, and interfaces, and develop application scenarios in that space. Our applications include on-body sensing, modular displays, tactile feedback and interactive clothing and jewelry.\n",
      "=============================\n",
      "Rig Animation with a Tangible and Modular Input Device\n",
      "We propose a novel approach to digital character animation, combining the benefits of modular and tangible input devices and sophisticated rig animation algorithms. With a symbiotic software and hardware approach, we overcome limitations inherent to all previous tangible devices. It allows users to directly control complex rigs with 5-10 physical controls only. These compact input device configurations - optimized for a specific rig and a set of sample poses - are automatically generated by our algorithm. This avoids oversimplification of the pose space and excessively bulky devices.\n",
      "=============================\n",
      "Histogram: Spatiotemporal Photo-Displaying Interface\n",
      "As the smartphone has become more widely available, we easily take photos and upload them online to share with others. Photographs are abundant, but they are not used properly, even though they provide meaningful information about the social scenes of our daily lives. To address this issue, Histogram was created as a new interface for displaying and sharing location-related photographs chronologically to trace the changes in a location. The prototype of this system is mobile-optimized to encourage users to easily upload photos with their smartphones, so that the system can be run through social cooperative work.\n",
      "=============================\n",
      "Hilbert Curves: A Tool for Resolution Independent Haptic Texture\n",
      "Haptic systems usually stimulate the kinesthetic aspects of the sense of touch, i.e. force feedback systems. But more and more devices aim to stimulate the cutaneous part of the sense of touch to reproduce more complex tactile sensations. To do so, they stimulate one's fingertip in different locations, usually in the fashion of a matrix pattern. In this paper we investigate the new possibilities that are offered by such a framework and present an ongoing project that investigates the benefits of Hilbert curves to display resolution independent mid-air haptic textures in comparison with other implementation approaches.\n",
      "=============================\n",
      "Object-Oriented Interaction: Enabling Direct Physical Manipulation of Abstract Content via Objectification\n",
      "Touch input promises intuitive interactions with digital content as it employs our experience of manipulating physical objects: digital content can be rotated, scaled, and translated using direct manipulation gestures. However, the reliance on analog also confines the scope of direct physical manipulation: the physical world provides no mechanism to interact with digital abstract content. As such, applications on touchscreen devices either only include limited functionalities or fallback on the traditional form-filling paradigm, which is tedious, slow, and error prone for touch input. My research focuses on designing a new UI framework to enable complex functionalities on touch screen devices by expanding direct physical manipulation to abstract content via objectification. I present two research projects, objectification of attributes and selection, which demonstrate considerable promises.\n",
      "=============================\n",
      "Dynamic Authoring of Audio with Linked Scripts\n",
      "Speech recordings are central to modern media from podcasts to audio books to e-lectures and voice-overs. Authoring these recordings involves an iterative back and forth process between script writing/editing and audio recording/editing. Yet, most existing tools treat the script and the audio separately, making the back and forth workflow very tedious. We present Voice Script, an interface to support a dynamic workflow for script writing and audio recording/editing. Our system integrates the script with the audio such that, as the user writes the script or records speech, edits to the script are translated to the audio and vice versa. Through informal user studies, we demonstrate that our interface greatly facilitates the audio authoring process in various scenarios.\n",
      "=============================\n",
      "Toward a Compact Device to Interact with a Capacitive Touch Screen\n",
      "Capacitive touch screens are widely used in various products. Touch screens have an advantage that an input system and output system can be integrated into a single module. We consider this advantage could make it possible to realize a new universal interface for both human-to-machine (H2M) and machine-to-machine (M2M). For a M2M interface, some sort of method to simulate finger touching is needed. Therefore, we propose an alternative method to interact with a touch screen using two electrical approaches. Our proposal is effective in automating touch screen operations, modality conversion device for people with disabilities, and so on. We assembled a prototype to confirm the principle to control a touch screen with the electrical methods. We believe that our proposal will complement the weakness of touch screens and expand their possibility.\n",
      "=============================\n",
      "Uniformity Based Haptic Alert Network\n",
      "We experience haptic feedback on a wide variety of devices in the modern day, including cellphones, tablets, and smartwatches. However haptic alerts can quickly become disruptive rather than helpful to a user when multiple devices are providing feedback simultaneously or consecutively. Thus in this paper, we propose an intercommunicating, turn-based local network between a user's devices. This will allow a guaranteed minimal time span between device alerts. Additionally, when multiple devices provide a notification-based haptic alert, devices often produce different feedback due to the varying materials they are placed on. To address this, our framework allows devices to self-regulate their levels of haptic responses based on the material density of the surface they are placed on. This allows the framework to enforce a uniform level of haptic feedback across all the surface-device combinations. Finally, we will also utilize this common network to eliminate redundant alerts across devices.\n",
      "=============================\n",
      "HoloFlex: A Flexible Light-Field Smartphone with a Microlens Array and a P-OLED Touchscreen\n",
      "We present HoloFlex, a 3D flexible smartphone featuring a light-field display consisting of a high-resolution P-OLED display and an array of 16,640 microlenses. HoloFlex allows mobile users to interact with 3D images featuring natural visual cues such as motion parallax and stereoscopy without glasses or head tracking. Its flexibility allows the use of bend input for interacting with 3D objects along the z axis. Images are rendered into 12-pixel wide circular blocks-pinhole views of the 3D scene-which enable ~80 unique viewports at an effective resolution of 160 × 104. The microlens array distributes each pixel from the display in a direction that preserves the angular information of light rays in the 3D scene. We present a preliminary study evaluating the effect of bend input vs. a vertical touch screen slider on 3D docking performance. Results indicate that bend input significantly improves movement time in this task. We also present 3D applications including a 3D editor, a 3D Angry Birds game and a 3D teleconferencing system that utilize bend input.\n",
      "=============================\n",
      "The UIST Video Browser: Creating Shareable Playlists of Video Previews\n",
      "We introduce the UIST Video Browser which provides a rapid overview of the UIST 30-second video previews, based on the conference schedule. Attendees can see an overview of upcoming talks, search by topic, and create personalized, shareable video playlists that capture the most interesting or relevant papers.\n",
      "=============================\n",
      "Making Fabrication Real\n",
      "Low-cost, easy-to-use 3D printers have promised to empower everyday users with the ability to fabricate physical objects of their own design. While these printers specialize in building objects from scratch, they are innately oblivious to the real world in which the printed objects will be situated and in use. In my thesis research, I develop fabrication techniques with tool integration to enable users to expressively specify how a design can be attached to, augment, adapt, support, or otherwise function with existing real world objects. In this paper, I describe projects to date as well as ongoing work that explores this space of research.\n",
      "=============================\n",
      "Peripersonal Space in Virtual Reality: Navigating 3D Space with Different Perspectives\n",
      "We introduce the concept of \"peripersonal space\" of an avatar in 3D virtual reality and discuss how it plays an important role on 3D navigation with different perspectives. By analyzing the eye-gaze data of avatar-based navigation with first-person perspective and third-person perspective, we examine the effects of an avatar's peripersonal space on the users' perceptual scopes within 3D virtual environments. We propose that manipulating peripersonal space of an avatar with various perspectives has the immediate effects on the users' scopes of perception as well as the patterns of attentional capture. This study provides a helpful guideline for designing more effective navigation system with an avatar in 3D virtual environment.\n",
      "=============================\n",
      "Haptic Learning of Semaphoric Finger Gestures\n",
      "Haptic learning of gesture shortcuts has never been explored. In this paper, we investigate haptic learning of a freehand semaphoric finger tap gesture shortcut set using haptic rings. We conduct a two-day study of 30 participants where we couple haptic stimuli with visual and audio stimuli, and compare their learning performance with wholly visual learning. The results indicate that with <30 minutes of learning, haptic learning of finger tap semaphoric gestures is comparable to visual learning and maintains its recall on the second day.\n",
      "=============================\n",
      "TRing: Instant and Customizable Interactions with Objects Using an Embedded Magnet and a Finger-Worn Device\n",
      "We present TRing, a finger-worn input device which provides instant and customizable interactions. TRing offers a novel method for making plain objects interactive using an embedded magnet and a finger-worn device. With a particle filter integrated magnetic sensing technique, we compute the fingertip's position relative to the embedded magnet. We also offer a magnet placement algorithm that guides the magnet installation location based upon the user's interface customization. By simply inserting or attaching a small magnet, we bring interactivity to both fabricated and existing objects. In our evaluations, TRing shows an average tracking error of 8.6 mm in 3D space and a 2D targeting error of 4.96 mm, which are sufficient for implementing average-sized conventional controls such as buttons and sliders. A user study validates the input performance with TRing on a targeting task (92% accuracy within 45 mm distance) and a cursor control task (91% accuracy for a 10 mm target). Furthermore, we show examples that highlight the interaction capability of our approach.\n",
      "=============================\n",
      "Depth Based Shadow Pointing Interface for Public Displays\n",
      "We propose a robust pointing detection with virtual shadow representation for interacting with a public display. Using a depth camera, our shadow is generated by a model with an angled virtual sun light and detects the nearest point as a pointer. The position of the shadow becomes higher when user walks closer, which conveys the notion of correct distance to control the pointer and offers accessibility to the higher area of the display.\n",
      "=============================\n",
      "LaserStroke: Mid-air Tactile Experiences on Contours Using Indirect Laser Radiation\n",
      "This demonstration presents a novel form of mid-air tactile display, LaserStroke, that makes use of a laser irradiated on the elastic medium attached to the skin. LaserStroke extends a laser device with an orientation control platform and a magnetic tracker so that it can elicit tapping and stroking sensations to a user's palm from a distance. LaserStroke offers unique tactile experiences while a user freely moves his/her hand in midair.\n",
      "=============================\n",
      "Transparent Reality: Using Eye Gaze Focus Depth as Interaction Modality\n",
      "We present a novel, eye gaze based interaction technique, using focus depth as an input modality for virtual reality (VR) applications. We also show custom hardware prototype implementation. Comparing the focus depth based interaction to a scroll wheel interface, we find no statistically significant difference in performance (the focus depth works slightly better) and a subjective preference of the users in a user study with 10 participants playing a simple VR game. This indicates that it is a suitable interface modality that should be further explored. Finally, we give some application scenarios and guidelines for using focus depth interactions in VR applications.\n",
      "=============================\n",
      "The Elements of Fashion Style\n",
      "The outfits people wear contain latent fashion concepts capturing styles, seasons, events, and environments. Fashion theorists have proposed that these concepts are shaped by design elements such as color, material, and silhouette. A dress may be \"bohemian\" because of its pattern, material, trim, or some combination of them: it is not always clear how low-level elements translate to high-level styles. In this paper, we use polylingual topic modeling to learn latent fashion concepts jointly in two languages capturing these elements and styles. Using this latent topic formation we can translate between these two languages through topic space, exposing the elements of fashion style. We train the polylingual topic model (PLTM) on a set of more than half a million outfits collected from Polyvore, a popular fashion-based social net- work. We present novel, data-driven fashion applications that allow users to express their needs in natural language just as they would to a real stylist and produce tailored item recommendations for these style needs.\n",
      "=============================\n",
      "Reading and Learning Smartfonts\n",
      "As small displays on devices like smartwatches become increasingly common, many people have difficulty reading the text on these displays. Vision conditions like presbyopia that result in blurry near vision make reading small text particularly hard. We design multiple different scripts for displaying English text, legible at small sizes even when blurry, for small screens such as smartphones and smartwatches. These \"smartfonts\" redesign visual character presentations to improve the reading experience. Like cursive, Grade 1 Braille, and ordinary fonts, they preserve orthography and spelling. They have the potential to enable people to read more text comfortably on small screens, e.g., without reading glasses. To simulate presbyopia, we blur images and evaluate their legibility using paid crowdsourcing. We also evaluate the difficulty of learning to read smartfonts and observe a learnability/legibility trade-off. Our most learnable smartfont can be read at roughly half the speed of Latin after two thousand practice sentences. It is also legible smaller than half the size of traditional Latin (i.e. \"English\") when blurry.\n",
      "=============================\n",
      "Estimating Contact Force of Fingertip and Providing Tactile Feedback Simultaneously\n",
      "This study proposes a method for estimating the contact force of the fingertip by inputting vibrations actively. The use of active bone-conducted sound sensing has been limited to estimating the joint angle of the elbow and the finger. We applied it to the method for estimating the contact force of the fingertip. Unlike related works, it is not necessary to mount the device on a fingertip, and tactile feedback is enabled using tangible vibrations.\n",
      "=============================\n",
      "WhammyPhone: Exploring Tangible Audio Manipulation Using Bend Input on a Flexible Smartphone\n",
      "We present WhammyPhone, a novel audio interface that supports physical manipulation of digital audio through bend gestures. WhammyPhone combines a high-resolution flexible display, bend sensors, and a set of intuitive interaction techniques that enable novice users to manipulate sound in a tangible fashion. With WhammyPhone, bend gestures can control both discrete (e.g. triggering a note) and continuous parameters (e.g. pitch bend). We showcase application scenarios that leverage the unique input modalities of WhammyPhone and discuss its potential for digital audio manipulation.\n",
      "=============================\n",
      "Physiological Signal-Driven Virtual Reality in Social Spaces\n",
      "Virtual and augmented reality are becoming the new medium that transcend the way we interact with virtual content, paving the way for many immersive and interactive forms of applications. The main purpose of my research is to create a seamless combination of physiological sensing with virtual reality to provide users with a new layer of input modality or as a form of implicit feedback. To achieve this, my research focuses in novel augmented reality (AR) and virtual reality (VR) based application for a multi-user, multi-view, multi-modal system augmented by physiological sensing methods towards an increased public and social acceptance.\n",
      "=============================\n",
      "ViBand: High-Fidelity Bio-Acoustic Sensing Using Commodity Smartwatch Accelerometers\n",
      "Smartwatches and wearables are unique in that they reside on the body, presenting great potential for always-available input and interaction. Their position on the wrist makes them ideal for capturing bio-acoustic signals. We developed a custom smartwatch kernel that boosts the sampling rate of a smartwatch's existing accelerometer to 4 kHz. Using this new source of high-fidelity data, we uncovered a wide range of applications. For example, we can use bio-acoustic data to classify hand gestures such as flicks, claps, scratches, and taps, which combine with on-device motion tracking to create a wide range of expressive input modalities. Bio-acoustic sensing can also detect the vibrations of grasped mechanical or motor-powered objects, enabling passive object recognition that can augment everyday experiences with context-aware functionality. Finally, we can generate structured vibrations using a transducer, and show that data can be transmitted through the human body. Overall, our contributions unlock user interface techniques that previously relied on special-purpose and/or cumbersome instrumentation, making such interactions considerably more feasible for inclusion in future consumer devices.\n",
      "=============================\n",
      "EdgeVib: Effective Alphanumeric Character Output Using a Wrist-Worn Tactile Display\n",
      "This paper presents EdgeVib, a system of spatiotemporal vibration patterns for delivering alphanumeric characters on wrist-worn vibrotactile displays. We first investigated spatiotemporal pattern delivery through a watch-back tactile display by performing a series of user studies. The results reveal that employing a 2×2 vibrotactile array is more effective than employing a 3×3 one, because the lower-resolution array creates clearer tactile sensations in less time consumption. We then deployed EdgeWrite patterns on a 2×2 vibrotactile array to determine any difficulties of delivering alphanumerical characters, and then modified the unistroke patterns into multistroke EdgeVib ones on the basis of the findings. The results of a 24-participant user study reveal that the recognition rates of the modified multistroke patterns were significantly higher than the original unistroke ones in both alphabet (85.9% vs. 70.7%) and digits (88.6% vs. 78.5%) delivery, and a further study indicated that the techniques can be generalized to deliver two-character compound messages with recognition rates higher than 83.3%. The guidelines derived from our study can be used for designing watch-back tactile displays for alphanumeric character output.\n",
      "=============================\n",
      "Authoring Illustrations of Human Movements by Iterative Physical Demonstration\n",
      "Illustrations of human movements are used to communicate ideas and convey instructions in many domains, but creating them is time-consuming and requires skill. We introduce DemoDraw, a multi-modal approach to generate these illustrations as the user physically demonstrates the movements. In a Demonstration Interface, DemoDraw segments speech and 3D joint motion into a sequence of motion segments, each characterized by a key pose and salient joint trajectories. Based on this sequence, a series of illustrations is automatically generated using a stylistically rendered 3D avatar annotated with arrows to convey movements. During demonstration, the user can navigate using speech and amend or re-perform motions if needed. Once a suitable sequence of steps has been created, a Refinement Interface enables fine control of visualization parameters. In a three-part evaluation, we validate the effectiveness of the generated illustrations and the usability of DemoDraw. Our results show 4 to 7-step illustrations can be created in 5 or 10 minutes on average.\n",
      "=============================\n",
      "Designing a Haptic Feedback System for Hearing-Impaired to Experience Tap Dance\n",
      "In this study, we have designed a system to enable hearing-impaired to enjoy the performance of tap dancers. This system transfers the haptic sensation of tap dancing from the stage to the audience and helps hearing-impaired people enjoy the vibration of the taps even if they cannot hear the sound. We organized an event to verify the effectiveness of the system. To do this, we collaborated with a tap dance unit and science museum. We found that our system succeeded in helping the tap dancers share the fun and enjoyment of dance with the audience comprising people with hearing disabilities.\n",
      "=============================\n",
      "RealPen: Providing Realism in Handwriting Tasks on Touch Surfaces using Auditory-Tactile Feedback\n",
      "We present RealPen, an augmented stylus for capacitive tablet screens that recreates the physical sensation of writing on paper with a pencil, ball-point pen or marker pen. The aim is to create a more engaging experience when writing on touch surfaces, such as screens of tablet computers. This is achieved by regenerating the friction-induced oscillation and sound of a real writing tool in contact with paper. To generate realistic tactile feedback, our algorithm analyzes the frequency spectrum of the friction oscillation generated when writing with traditional tools, extracts principal frequencies, and uses the actuator's frequency response profile for an adjustment weighting function. We enhance the realism by providing the sound feedback aligned with the writing pressure and speed. Furthermore, we investigated the effects of superposition and fluctuation of several frequencies on human tactile perception, evaluated the performance of RealPen, and characterized users' perception and preference of each feedback type.\n",
      "=============================\n",
      "DriftBoard: A Panning-Based Text Entry Technique for Ultra-Small Touchscreens\n",
      "Emerging ultra-small wearables like smartwatches pose a design challenge for touch-based text entry. This is due to the \"fat-finger problem,\" wherein users struggle to select elements much smaller than their fingers. To address this challenge, we developed DriftBoard, a panning-based text entry technique where the user types by positioning a movable qwerty keyboard on an interactive area with respect to a fixed cursor point. In this paper, we describe the design and implementation of DriftBoard and report results of a user study on a watch-size touchscreen. The study compared DriftBoard to two ultra-small keyboards, ZoomBoard (tapping-based) and Swipeboard (swiping-based). DriftBoard performed comparably (no significant difference) to ZoomBoard in the major metrics of text entry speed and error rate, and outperformed Swipeboard, which suggests that panning-based typing is a promising input method for text entry on ultra-small touchscreens.\n",
      "=============================\n",
      "waveSense: Ultra Low Power Gesture Sensing Based on Selective Volumetric Illumination\n",
      "We present waveSense, a low power hand gestures recogni- tion system suitable for mobile and wearable devices. A novel Selective Volumetric Illumination (SVI) approach using off-the-shelf infrared (IR) emitters and non-focused IR sensors were introduced to achieve the power efficiency. Our current implementation consumes 8.65mW while sensing hand gestures within 60cm radius from the sensors. In this demo, we introduce the concept and the theoretical background of waveSense, details of the prototype implementation, and application possibilities.\n",
      "=============================\n",
      "Optical Marionette: Graphical Manipulation of Human's Walking Direction\n",
      "We present a novel manipulation method that subconsciously changes the walking direction of users via visual processing on a head mounted display (HMD). Unlike existing navigation systems that require users to recognize information and then follow directions as two separate, conscious processes, the proposed method guides users without them needing to pay attention to the information provided by the navigation system and also allows them to be graphically manipulated by controllers. In the proposed system, users perceive the real world by means of stereo images provided by a stereo camera and the HMD. Specifically, while walking, the navigation system provides users with real-time feedback by processing the images they have just perceived and giving them visual stimuli. This study examined two image-processing methods for manipulation of human's walking direction: moving stripe pattern and changing focal region. Experimental results indicate that the changing focal region method most effectively leads walkers as it changes their walking path by approximately 200 mm/m on average.\n",
      "=============================\n",
      "Digital Gastronomy: Methods & Recipes for Hybrid Cooking\n",
      "Several recent projects have introduced digital machines to the kitchen, yet their impact on culinary culture is limited. We envision a culture of Digital Gastronomy that enhances traditional cooking with new interactive capabilities, rather than replacing the chef with an autonomous machine. Thus, we deploy existing digital fabrication instruments in traditional kitchen and integrate them into cooking via hybrid recipes. This concept merges manual and digital procedures, and imports parametric design tools into cooking, allowing the chef to personalize the tastes, flavors, structures and aesthetics of dishes. In this paper we present our hybrid kitchen and the new cooking methodology, illustrated by detailed recipes with degrees of freedom that can be set digitally prior to cooking. Lastly, we discuss future work and conclude with thoughts on the future of hybrid gastronomy.\n",
      "=============================\n",
      "proCover: Sensory Augmentation of Prosthetic Limbs Using Smart Textile Covers\n",
      "Today's commercially available prosthetic limbs lack tactile sensation and feedback. Recent research in this domain focuses on sensor technologies designed to be directly embedded into future prostheses. We present a novel concept and prototype of a prosthetic-sensing wearable that offers a non-invasive, self-applicable and customizable approach for the sensory augmentation of present-day and future low to mid-range priced lower-limb prosthetics. From consultation with eight lower-limb amputees, we investigated the design space for prosthetic sensing wearables and developed novel interaction methods for dynamic, user-driven creation and mapping of sensing regions on the foot to wearable haptic feedback actuators. Based on a pilot-study with amputees, we assessed the utility of our design in scenarios brought up by the amputees and we summarize our findings to establish future directions for research into using smart textiles for the sensory enhancement of prosthetic limbs.\n",
      "=============================\n",
      "CircuitStack: Supporting Rapid Prototyping and Evolution of Electronic Circuits\n",
      "For makers and developers, circuit prototyping is an integral part of building electronic projects. Currently, it is common to build circuits based on breadboard schematics that are available on various maker and DIY websites. Some breadboard schematics are used as is without modification, and some are modified and extended to fit specific needs. In such cases, diagrams and schematics merely serve as blueprints and visual instructions, but users still must physically wire the breadboard connections, which can be time-consuming and error-prone. We present CircuitStack, a system that combines the flexibility of breadboarding with the correctness of printed circuits, for enabling rapid and extensible circuit construction. This hybrid system enables circuit reconfigurability, component reusability, and high efficiency at the early stage of prototyping development.\n",
      "=============================\n",
      "Smart Headlight: An Application of Projector-Camera Vision\n",
      "A projector manipulates outgoing light rays, while a camera records incoming ones. Combining these optically inverse devices, especially in a coaxial manner, creates the possibility of a new computer-vision technology. The \"Smart Headlight,\" currently under development at Carnegie Mellon's Robotics Institute, is one example: a device that can \"erase\" raindrops or snowflakes from a driver's sight, allowing for continuous use of the \"high beams\" mode while not causing glare against oncoming drivers, and enhance the appearance of important objects, such as pedestrians. In that sense, it constitutes a \"genuine\" augmented reality, manipulating the reality for how it appears to a viewer, rather than merely overlaying objects on the image of the reality. This talk will present the state of the Smart Headlight project and discuss further possible applications of projector-camera systems.\n",
      "=============================\n",
      "On Suggesting Phrases vs. Predicting Words for Mobile Text Composition\n",
      "A system capable of suggesting multi-word phrases while someone is writing could supply ideas about content and phrasing and allow those ideas to be inserted efficiently. Meanwhile, statistical language modeling has provided various approaches to predicting phrases that users type. We introduce a simple extension to the familiar mobile keyboard suggestion interface that presents phrase suggestions that can be accepted by a repeated-tap gesture. In an extended composition task, we found that phrases were interpreted as suggestions that affected the content of what participants wrote more than conventional single-word suggestions, which were interpreted as predictions. We highlight a design challenge: how can a phrase suggestion system make valuable suggestions rather than just accurate predictions'\n",
      "=============================\n",
      "Thermocons: Evaluating the Thermal Haptic Perception of the Forehead\n",
      "Thermocons describes our work in progress for evaluating thermal haptic feedback on the forehead as a viable feedback modality for integration with head mounted devices. The purpose was to identify the thermal perception for simultaneous feedback at three locations of the forehead. We provided hot-only, cold-only and hot/cold-mixed thermal stimulations at these location to identify the sensitivity for accurate perception. Our evaluation with 9 participants indicated that perceiving cold-only stimulations were significantly better with an accuracy of 88%. The perception accuracy for hot-only and hot/cold-mixed stimulations were 66% and 65% respectively.\n",
      "=============================\n",
      "Reprise: A Design Tool for Specifying, Generating, and Customizing 3D Printable Adaptations on Everyday Objects\n",
      "Everyday tools and objects often need to be customized for an unplanned use or adapted for specific user, such as adding a bigger pull to a zipper or a larger grip for a pen. The advent of low-cost 3D printing offers the possibility to rapidly construct a wide range of such adaptations. However, while 3D printers are now affordable enough for even home use, the tools needed to design custom adaptations normally require skills that are beyond users with limited 3D modeling experience. In this paper, we describe Reprise--a design tool for specifying, generating, customizing and fitting adaptations onto existing household objects. Reprise allows users to express at a high level what type of action is applied to an object. Based on this high level specification, Reprise automatically generates adaptations. Users can use simple sliders to customize the adaptations to better suit their particular needs and preferences, such as increasing the tightness for gripping, enhancing torque for rotation, or making a larger base for stability. Finally, Reprise provides a toolkit of fastening methods and support structures for fitting the adaptations onto existing objects. To validate our approach, we used Reprise to replicate 15 existing adaptation examples, each of which represents a specific category in a design space distilled from an analysis of over 3000 cases found in the literature and online communities. We believe this work would benefit makers and designers for prototyping lifehacking solutions and assistive technologies.\n",
      "=============================\n",
      "Meta: Enabling Programming Languages to Learn from the Crowd\n",
      "Collectively authored programming resources such as Q&A sites and open-source libraries provide a limited window into how programs are constructed, debugged, and run. To address these limitations, we introduce Meta: a language extension for Python that allows programmers to share functions and track how they are used by a crowd of other programmers. Meta functions are shareable via URL and instrumented to record runtime data. Combining thousands of Meta functions with their collective runtime data, we demonstrate tools including an optimizer that replaces your function with a more efficient version written by someone else, an auto-patcher that saves your program from crashing by finding equivalent functions in the community, and a proactive linter that warns you when a function fails elsewhere in the community. We find that professional programmers are able to use Meta for complex tasks (creating new Meta functions that, for example, cross-validate a logistic regression), and that Meta is able to find 44 optimizations (for a 1.45 times average speedup) and 5 bug fixes across the crowd.\n",
      "=============================\n",
      "Foundry: Hierarchical Material Design for Multi-Material Fabrication\n",
      "We demonstrate a new approach for designing functional material definitions for multi-material fabrication using our system called Foundry. Foundry provides an interactive and visual process for hierarchically designing spatially-varying material properties (e.g., appearance, mechanical, optical). The resulting meta-materials exhibit structure at the micro and macro level and can surpass the qualities of traditional composites. The material definitions are created by composing a set of operators into an operator graph. Each operator performs a volume decomposition operation, remaps space, or constructs and assigns a material composition. The operators are implemented using a domain-specific language for multi-material fabrication; users can easily extend the library by writing their own operators. Foundry can be used to build operator graphs that describe complex, parameterized, resolution-independent, and reusable material definitions. We also describe how to stage the evaluation of the final material definition which in conjunction with progressive refinement, allows for interactive material evaluation even for complex designs. We show sophisticated and functional parts designed with our system.\n",
      "=============================\n",
      "H-Studio: an authoring tool for adding haptic and motion effects to audiovisual content\n",
      "Haptic and motion effects have been widely used for virtual reality applications in order to provide a physical feedback from the virtual world. Such feedback was recently studied to improve the user experience in audiovisual entertainment applications. But the creation of haptic and motion effects is a main issue and requires dedicated editing tool. This paper describes a user-friendly authoring tool to create and synchronize such effects with audiovisual content. More precisely we focus on the edition of motion effects. Authoring is simplified thanks to a dedicated graphical user interface, allowing either to import external data or to synthesize effects thanks to a force-feedback device. Another key feature of this editor is the playback function which enables to preview the motion effect. Hence this new tool allows non expert users to create immersive haptic-audiovisual experiences.\n",
      "=============================\n",
      "TapLaptop: Expansion of the Operating Area of a Laptop by Detection Taps Using a Single Embedded Microphone\n",
      "Unique sounds are produced by tapping each area of the keyboard or palm rest on a laptop PC. These different sounds are attributable to the internal structure of the computer or the position of the microphone used to detect these sounds. Various approaches are available for enriching the input area by identifying the operating sounds produced by users, but most require an additional microphone (e.g., piezoelectric microphone) to detect the structure-borne sounds carried by the laptop structure. In this study, we propose the expansion of the operating area of a laptop by distinguishing tapping sounds using the embedded microphone. Our method does not require any external sensors, thereby maintaining the advantages of laptops in terms of mobility and low weight, and it is readily deployed on a laptop using the embedded microphone.\n",
      "=============================\n",
      "AirCode: Unobtrusive Physical Tags for Digital Fabrication\n",
      "We present AirCode, a technique that allows the user to tag physically fabricated objects with given information. An AirCode tag consists of a group of carefully designed air pockets placed beneath the object surface. These air pockets are easily produced during the fabrication process of the object, without any additional material or postprocessing. Meanwhile, the air pockets affect only the scattering light transport under the surface, and thus are hard to notice to our naked eyes. But, by using a computational imaging method, the tags become detectable. We present a tool that automates the design of air pockets for the user to encode information. AirCode system also allows the user to retrieve the information from captured images via a robust decoding algorithm. We demonstrate our tagging technique with applications for metadata embedding, robotic grasping, as well as conveying object affordances.\n",
      "=============================\n",
      "Learning Visual Importance for Graphic Designs and Data Visualizations\n",
      "Knowing where people look and click on visual designs can provide clues about how the designs are perceived, and where the most important or relevant content lies. The most important content of a visual design can be used for effective summarization or to facilitate retrieval from a database. We present automated models that predict the relative importance of different elements in data visualizations and graphic designs. Our models are neural networks trained on human clicks and importance annotations on hundreds of designs. We collected a new dataset of crowdsourced importance, and analyzed the predictions of our models with respect to ground truth importance and human eye movements. We demonstrate how such predictions of importance can be used for automatic design retargeting and thumbnailing. User studies with hundreds of MTurk participants validate that, with limited post-processing, our importance-driven applications are on par with, or outperform, current state-of-the-art methods, including natural image saliency. We also provide a demonstration of how our importance predictions can be built into interactive design tools to offer immediate feedback during the design process.\n",
      "=============================\n",
      "Interacting with Acoustic Simulation and Fabrication\n",
      "Incorporating accurate physics-based simulation into interactive design tools is challenging. However, adding the physics accurately becomes crucial to several emerging technologies. For example, in virtual/augmented reality (VR/AR) videos, the faithful reproduction of surrounding audios is required to bring the immersion to the next level. Similarly, as personal fabrication is made possible with accessible 3D printers, more intuitive tools that respect the physical constraints can help artists to prototype designs. One main hurdle is the sheer amount of computation complexity to accurately reproduce the real-world phenomena through physics-based simulation. In my thesis research, I develop interactive tools that implement efficient physics-based simulation algorithms for automatic optimization and intuitive user interaction.\n",
      "=============================\n",
      "One Reality: Augmenting How the Physical World is Experienced by combining Multiple Mixed Reality Modalities\n",
      "Most of our daily activities take place in the physical world, which inherently imposes physical constraints. In contrast, the digital world is very flexible, but usually isolated from its physical counterpart. To combine these two realms, many Mixed Reality (MR) techniques have been explored, at different levels in the continuum. In this work we present an integrated Mixed Reality ecosystem that allows users to incrementally transition from pure physical to pure virtual experiences in a unique reality. This system stands on a conceptual framework composed of 6 levels. This paper presents these levels as well as the related interaction techniques.\n",
      "=============================\n",
      "PhyShare: Sharing Physical Interaction in Virtual Reality\n",
      "We present PhyShare, a new haptic user interface based on actuated robots. Virtual reality has recently been gaining wide adoption, and an effective haptic feedback in these scenarios can strongly support user's sensory in bridging virtual and physical world. Since participants do not directly observe these robotic proxies, we investigate the multiple mappings between physical robots and virtual proxies that can utilize the resources needed to provide a well rounded VR experience. PhyShare bots can act either as directly touchable objects or invisible carriers of physical objects, depending on different scenarios. They also support distributed collaboration, allowing remotely located VR collaborators to share the same physical feedback.\n",
      "=============================\n",
      "Non-Linear Editor for Text-Based Screencast\n",
      "Screencasts, where computer screen is broadcast to a large audience on the web, are becoming popular as an online educational tool. Among various types of screencast content, popular are the contents that involve text editing, including computer programming. There are emerging platforms that support such text-based screencasts by recording every character insertion/deletion from the creator and reconstructing its playback on the viewer's screen. However, these platforms lack rich support for creating and editing the screencast itself, mainly due to the difficulty of manipulating recorded text changes; the changes are tightly coupled in sequence, thus modifying arbitrary part of the sequence is not trivial. We present a non-linear editing tool for text-based screencasts. With the proposed selective history rewrite process, our editor allows users to substitute an arbitrary part of a text-based screencast while preserving overall consistency of the rest of the text-based screencast.\n",
      "=============================\n",
      "Everyday Eye Contact Detection Using Unsupervised Gaze Target Discovery\n",
      "Eye contact is an important non-verbal cue in social signal processing and promising as a measure of overt attention in human-object interactions and attentive user interfaces. However, robust detection of eye contact across different users, gaze targets, camera positions, and illumination conditions is notoriously challenging. We present a novel method for eye contact detection that combines a state-of-the-art appearance-based gaze estimator with a novel approach for unsupervised gaze target discovery, i.e. without the need for tedious and time-consuming manual data annotation. We evaluate our method in two real-world scenarios: detecting eye contact at the workplace, including on the main work display, from cameras mounted to target objects, as well as during everyday social interactions with the wearer of a head-mounted egocentric camera. We empirically evaluate the performance of our method in both scenarios and demonstrate its effectiveness for detecting eye contact independent of target object type and size, camera position, and user and recording environment.\n",
      "=============================\n",
      "Neuroanatomical Correlates of Perceived Usability\n",
      "Usability has a distinct subjective component, yet surprisingly little is known about its neural basis and relation to the neuroanatomy of aesthetics. To begin closing this gap, we conducted two functional magnetic resonance imaging studies in which participants were shown static webpages (in the first study) and videos of interaction with webpages (in the second study). The webpages were controlled so as to exhibit high and low levels of perceived usability and perceived aesthetics. Our results show unique links between perceived usability and brain areas involved in functions such as emotional processing (left fusiform gyrus, superior frontal gyrus), anticipation of physical interaction (precentral gyrus), task intention (anterior cingulate cortex), and linguistic processing (medial and bilateral superior frontal gyri). We use these findings to discuss the brain correlates of perceived usability and the use of fMRI for usability evaluation and for generating new user experiences.\n",
      "=============================\n",
      "Erg-O: Ergonomic Optimization of Immersive Virtual Environments\n",
      "Interaction in VR involves large body movements, easily inducing fatigue and discomfort. We propose Erg-O, a manipulation technique that leverages visual dominance to maintain the visual location of the elements in VR, while making them accessible from more comfortable locations. Our solution works in an open-ended fashion (no prior knowledge of the object the user wants to touch), can be used with multiple objects, and still allows interaction with any other point within user's reach. We use optimization approaches to compute the best physical location to interact with each visual element, and space partitioning techniques to distort the visual and physical spaces based on those mappings and allow multi-object retargeting. In this paper we describe the Erg-O technique, propose two retargeting strategies and report the results from a user study on 3D selection under different conditions, elaborating on their potential and application to specific usage scenarios.\n",
      "=============================\n",
      "WhichFingers: Identifying Fingers on Touch Surfaces and Keyboards using Vibration Sensors\n",
      "HCI researchers lack low latency and robust systems to support the design and development of interaction techniques using finger identification. We developed a low cost prototype using piezo based vibration sensors attached to each finger. By combining the events from an input device with the information from the vibration sensors we demonstrate how to achieve low latency and robust finger identification. Our prototype was evaluated in a controlled experiment, using two keyboards and a touchpad, showing recognition rates of 98.2% for the keyboard and, for the touchpad, 99.7% for single touch and 94.7% for two simultaneous touches. These results were confirmed in an additional laboratory style experiment with ecologically valid tasks. Last we present new interactions techniques made possible using this technology.\n",
      "=============================\n",
      "EyeScout: Active Eye Tracking for Position and Movement Independent Gaze Interaction with Large Public Displays\n",
      "While gaze holds a lot of promise for hands-free interaction with public displays, remote eye trackers with their confined tracking box restrict users to a single stationary position in front of the display. We present EyeScout, an active eye tracking system that combines an eye tracker mounted on a rail system with a computational method to automatically detect and align the tracker with the user's lateral movement. EyeScout addresses key limitations of current gaze-enabled large public displays by offering two novel gaze-interaction modes for a single user: In \"Walk then Interact\" the user can walk up to an arbitrary position in front of the display and interact, while in \"Walk and Interact\" the user can interact even while on the move. We report on a user study that shows that EyeScout is well perceived by users, extends a public display's sweet spot into a sweet line, and reduces gaze interaction kick-off time to 3.5 seconds -- a 62% improvement over state of the art solutions. We discuss sample applications that demonstrate how EyeScout can enable position and movement-independent gaze interaction with large public displays.\n",
      "=============================\n",
      "Designing Vibrotactile Widgets with Printed Actuators and Sensors\n",
      "Physical controls are fabricated through complicated assembly of parts requiring expensive machinery and are prone to mechanical wear. One solution is to embed controls directly in interactive surfaces, but the proprioceptive part of gestural interaction that makes physical controls discoverable and usable solely by hand gestures is lost and has to be compensated, by vibrotactile feedback for instance. Vibrotactile actuators face the same aforementioned issues as for physical controls. We propose printed vibrotactile actuators and sensors. They are printed on plastic sheets, with piezoelectric ink for actuation, and with silver ink for conductive elements, such as wires and capacitive sensors. These printed actuators and sensors make it possible to design vibrotactile widgets on curved surfaces, without complicated mechanical assembly.\n",
      "=============================\n",
      "Characterizing Latency in Touch and Button-Equipped Interactive Systems\n",
      "We present a low cost method to measure and characterize the end-to-end latency when using a touch system (tap latency) or an input device equipped with a physical button. Our method relies on a vibration sensor attached to a finger and a photo-diode to detect the screen response. Both are connected to a micro-controller connected to a host computer using a low-latency USB communication protocol in order to combine software and hardware probes to help determine where the latency comes from. We present the operating principle of our method before investigating the main sources of latency in several systems. We show that most of the latency originates from the display side. Our method can help application designers characterize and troubleshoot latency on a wide range of interactive systems.\n",
      "=============================\n",
      "Ultrasonic Cuisine: Proposal of Ultrasonic Non-contact Stirring Methods\n",
      "In this paper we propose a method of non-contact stirring. Ultrasonic waves have been studied for various applications. However, devices using ultrasound have been devised only to specialize in one role up to now. In recent years, we aim at generalization of aerial ultrasonic equipment used for various applications such as tactile presentation and super directional speaker, and propose applications closely our real life.\n",
      "=============================\n",
      "HaptI/O: Physical Node for the Internet of Haptics\n",
      "We propose the concept of the \"Internet of Haptics\" (IoH) that enables sharing experiences of others with the sense of touch. IoH allows to multicast haptic sensation from one Sensor-Node (Inter-Node) to multiple Actuator-Node (Ceive-Node) and to multicast from multiple Inter-Node to multiple Ceive-Node via the Internet. As a proof of concept, we developed the \"HaptI/O\" device which is a physical network node that can perform as a gateway to input or output the haptic information to/from the human body or tangible objects. We use the WebRTC as the baseline protocol for communication. Users can gain access IoH Web using a smartphone or PC and experience the haptic sensation by selecting the Inter-Node and Ceive-Node from a web browser. Multiple HaptI/O would be connected on the IoH server and transmit the haptic information from one node to multiple nodes as well as one to one mutual connection so that HaptI/O enables us to share our experiences with the sense of touch.\n",
      "=============================\n",
      "AutoDub: Automatic Redubbing for Voiceover Editing\n",
      "Redubbing is an extensively used technique to correct errors in voiceover recordings. It involves re-recording a part of a voiceover, identifying the corresponding section of audio in the original recording that needs to be replaced, and using low level audio tools to replace the audio. Although this sequence of steps can be performed using traditional audio editing tools, the process can be tedious when dealing with long voiceover recordings and prohibitively difficult for users not familiar with such tools. To address this issue, we present AutoDub, a novel system for redubbing voiceover recordings. Using our system, a user simply needs to re-record the part of the voiceover that needs to be replaced. Our system automatically locates the corresponding part in the original recording and performs the low level audio processing to replace it. The system can be easily incorporated in any existing sophisticated audio editor or can be employed as a functionality in an audio-guided user interface. User studies involving participation from novice, knowledgeable and expert users indicate that our tool is preferred to a traditional audio editor based redubbing approach by all categories of users due to its faster and easier redubbing capabilities.\n",
      "=============================\n",
      "A Digital Pen and Paper Email System for Older Adults\n",
      "As communication technologies continue to rapidly evolve, older adults face challenges to access systems and devices, which may increase their social isolation. Our project investigates the design of a digital pen and paper-based communication system that allows users to connect to their family and friends' e-mail inboxes. Given the unique needs of older adults, we opted for a participatory design approach, prototyping the system with 22 older adults through a series of design workshops in two locations. Four individuals used our resulting system over a period of two weeks. Based on their feedback and a review of design workshops, we are currently in the process of refining our interface and preparing for a larger deployment study.\n",
      "=============================\n",
      "CircuitSense: Automatic Sensing of Physical Circuits and Generation of Virtual Circuits to Support Software Tools.\n",
      "The rise of Maker communities and open-source electronic prototyping platforms have made electronic circuit projects increasingly popular around the world. Although there are software tools that support the debugging and sharing of circuits, they require users to manually create the virtual circuits in software, which can be time-consuming and error-prone. We present CircuitSense, a system that automatically recognizes the wires and electronic components placed on breadboards. It uses a combination of passive sensing and active probing to detect and generate the corresponding circuit representation in software in real-time. CircuitSense bridges the gap between the physical and virtual representations of circuits. It enables users to interactively construct and experiment with physical circuits while gaining the benefits of using software tools. It also dramatically simplifies the sharing of circuit designs with online communities.\n",
      "=============================\n",
      "Crowd Research: Open and Scalable University Laboratories\n",
      "Research experiences today are limited to a privileged few at select universities. Providing open access to research experiences would enable global upward mobility and increased diversity in the scientific workforce. How can we coordinate a crowd of diverse volunteers on open-ended research? How could a PI have enough visibility into each person's contributions to recommend them for further study? We present Crowd Research, a crowdsourcing technique that coordinates open-ended research through an iterative cycle of open contribution, synchronous collaboration, and peer assessment. To aid upward mobility and recognize contributions in publications, we introduce a decentralized credit system: participants allocate credits to each other, which a graph centrality algorithm translates into a collectively-created author order. Over 1,500 people from 62 countries have participated, 74% from institutions with low access to research. Over two years and three projects, this crowd has produced articles at top-tier Computer Science venues, and participants have gone on to leading graduate programs.\n",
      "=============================\n",
      "Contour: An Efficient Voice-enabled Workflow for Producing Text-to-Speech Content\n",
      "Voice assistant technology has expanded the design space for voice-activated consumer products and audio-centric user experience. To navigate this emerging design space, Speech Synthesis Markup Language (SSML) provides a standard to characterize synthetic speech based on parametric control of the prosody elements, i.e. pitch, rate, volume, contour, range, and duration. However, the existing voice assistants utilizing Text-to-Speech (TTS) lack expressiveness. The need of a new production workflow for more efficient and emotional audio content using TTS is discussed. A prototype that allows a user to produce TTS-based content in any emotional tone using voice input is presented. To evaluate the new workflow enabled by the prototype, an initial comparative study is conducted against the parametric approach. Preliminary quantitative and qualitative results suggest the new workflow is more efficient based on time to complete tasks and number of design iterations, while maintaining the same level of user preferred production quality.\n",
      "=============================\n",
      "Scanalog: Interactive Design and Debugging of Analog Circuits with Programmable Hardware\n",
      "Analog circuit design is a complex, error-prone task in which the processes of gathering observations, formulating reasonable hypotheses, and manually adjusting the circuit raise significant barriers to an iterative workflow. We present Scanalog, a tool built on programmable analog hardware that enables users to rapidly explore different circuit designs using direct manipulation, and receive immediate feedback on the resulting behaviors without manual assembly, calculation, or probing. Users can interactively tune modular signal transformations on hardware with real inputs, while observing real-time changes at all points in the circuit. They can create custom unit tests and assertions to detect potential issues. We describe three interactive applications demonstrating the expressive potential of Scanalog. In an informal evaluation, users successfully conditioned analog sensors and described Scanalog as both enjoyable and easy to use.\n",
      "=============================\n",
      "SweepCanvas: Sketch-based 3D Prototyping on an RGB-D Image\n",
      "The creation of 3D contents still remains one of the most crucial problems for the emerging applications such as 3D printing and Augmented Reality. In Augmented Reality, how to create virtual contents that seamlessly overlay with the real environment is a key problem for human-computer interaction and many subsequent applications. In this paper, we present a sketch-based interactive tool, which we term emph{SweepCanvas}, for rapid exploratory 3D modeling on top of an RGB-D image. Our aim is to offer end-users a simple yet efficient way to quickly create 3D models on an image. We develop a novel sketch-based modeling interface, which takes a pair of user strokes as input and instantly generates a curved 3D surface by sweeping one stroke along the other. A key enabler of our system is an optimization procedure that extracts pairs of spatial planes from the context to position and sweep the strokes. We demonstrate the effectiveness and power of our modeling system on various RGB-D data sets and validate the use cases via a pilot study.\n",
      "=============================\n",
      "Fading into the Background: Unleashing Ubiquitous and Unobtrusive Context Sensing\n",
      "As computing becomes increasingly embedded into the fabric of everyday life, systems that understand people's context of use are of paramount importance. Regardless of whether the platform is a mobile device, wearable, or smart infrastructure, context offers an implicit dimension that is vital to increasing the richness of human-computer interaction. In my thesis work, I introduce multiple enabling technologies that greatly enhance context awareness in highly dynamic platforms, all without costly or invasive instrumentation. My systems have been deployed across long periods and multiple environments, the results of which show the versatility, accuracy and potential for robust context sensing. By combining novel sensing with machine learning, my work transforms raw signals into intelligent abstractions that can power rich, context-sensitive applications, unleashing the potential of next-generation computing platforms.\n",
      "=============================\n",
      "CollaVR: Collaborative In-Headset Review for VR Video\n",
      "Collaborative review and feedback is an important part of conventional filmmaking and now Virtual Reality (VR) video production as well. However, conventional collaborative review practices do not easily translate to VR video because VR video is normally viewed in a headset, which makes it difficult to align gaze, share context, and take notes. This paper presents CollaVR, an application that enables multiple users to review a VR video together while wearing headsets. We interviewed VR video professionals to distill key considerations in reviewing VR video. Based on these insights, we developed a set of networked tools that enable filmmakers to collaborate and review video in real-time. We conducted a preliminary expert study to solicit feedback from VR video professionals about our system and assess their usage of the system with and without collaboration features.\n",
      "=============================\n",
      "CanalSense: Face-Related Movement Recognition System based on Sensing Air Pressure in Ear Canals\n",
      "We present a jaw, face, or head movement (face-related movement) recognition system called CanalSense. It recognizes face-related movements using barometers embedded in earphones. We find that face-related movements change air pressure inside the ear canals, which shows characteristic changes depending on the type and degree of the movement. We also find that such characteristic changes can be used to recognize face-related movements. We conduct an experiment to measure the accuracy of recognition. As a result, random forest shows per-user recognition accuracies of 87.6% for eleven face-related movements and 87.5% for four OpenMouth levels.\n",
      "=============================\n",
      "Interactive Room Capture on 3D-Aware Mobile Devices\n",
      "We propose a novel interactive system to simplify the process of indoor 3D CAD room modeling. Traditional room modeling methods require users to measure room and furniture dimensions, and manually select models that match the scene from large catalogs. Users then employ a mouse and keyboard interface to construct walls and place the objects in their appropriate locations. In contrast, our system leverages the sensing capabilities of a 3D aware mobile device, recent advances in object recognition, and a novel augmented reality user interface, to capture indoor 3D room models in-situ. With a few taps, a user can mark the surface of an object, take a photo, and the system retrieves and places a matching 3D model into the scene, from a large online database. User studies indicate that this modality is significantly quicker, more accurate, and requires less effort than traditional desktop tools.\n",
      "=============================\n",
      "Eye Tracking Using Built-in Camera for Smartphone-based HMD\n",
      "Virtual reality (VR) using head-mounted displays (HMDs) is becoming popular. Smartphone-based HMDs (SbHMDs) are so low cost that users can easily experience VR. Unfortunately, their input modality is quite limited. We propose a real-time eye tracking technique that uses the built-in front facing camera to capture the user's eye. It realizes stand-alone pointing functionality without any additional device.\n",
      "=============================\n",
      "Omnicode: A Novice-Oriented Live Programming Environment with Always-On Run-Time Value Visualizations\n",
      "Visualizations of run-time program state help novices form proper mental models and debug their code. We push this technique to the extreme by posing the following question: What if a live programming environment for an imperative language always displays the entire history of all run-time values for all program variables all the time? To explore this question, we built a prototype live IDE called Omnicode (\"Omniscient Code\") that continually runs the user's Python code and uses a scatterplot matrix to visualize the entire history of all of its numerical values, along with meaningful numbers derived from other data types. To filter the visualizations and hone in on specific points of interest, the user can brush and link over the scatterplots or select portions of code. They can also zoom in to view detailed stack and heap visualizations at each execution step. An exploratory study on 10 novice programmers discovered that they found Omnicode to be useful for debugging, forming mental models, explaining their code to others, and discovering moments of serendipity that would not have been likely within an ordinary IDE.\n",
      "=============================\n",
      "Shall We Fabricate?: Collaborative, Bidirectional, Incremental Fabrication\n",
      "The recent emergence of digital fabrication allows everyday designers to make, deploy, and enjoy their creation. However, the excitement over digital fabrication presumes that users have sufficient domain knowledge to create complex models by understanding the underlying principles, can be self-creative without computational supports. This paper presents a new fabrication framework that lowers the boundary of solving everyday issues with fabrication. A formalism and accompanying finite state machine (FSM) model that help assign a fabrication machine intelligence to appreciate humans' design actions was proposed, with a view towards a new fabrication framework empowering collaborative, incremental fabrication. Empowered by the novel framework, this paper envisions a future of fabrication that pushes the ceiling, a collaborative fabrication, processing intermittent, unpredictable events as live input and reflect them in the emerging outcomes by co-design process between a designer and a machine.\n",
      "=============================\n",
      "Torta: Generating Mixed-Media GUI and Command-Line App Tutorials Using Operating-System-Wide Activity Tracing\n",
      "Tutorials are vital for helping people perform complex software-based tasks in domains such as programming, data science, system administration, and computational research. However, it is tedious to create detailed step-by-step tutorials for tasks that span multiple interrelated GUI and command-line applications. To address this challenge, we created Torta, an end-to-end system that automatically generates step-by-step GUI and command-line app tutorials by demonstration, provides an editor to trim, organize, and add validation criteria to these tutorials, and provides a web-based viewer that can validate step-level progress and automatically run certain steps. The core technical insight that underpins Torta is that combining operating-system-wide activity tracing and screencast recording makes it easier to generate mixed-media (text+video) tutorials that span multiple GUI and command-line apps. An exploratory study on 10 computer science teaching assistants (TAs) found that they all preferred the experience and results of using Torta to record programming and sysadmin tutorials relevant to classes they teach rather than manually writing tutorials. A follow-up study on 6 students found that they all preferred following the Torta tutorials created by those TAs over the manually-written versions.\n",
      "=============================\n",
      "Playful Interactions with Body Channel Communication: Conquer it!\n",
      "Conquer it! is a lightweight proof-of-concept exertion game that demonstrates Body Channel Communication (BCC) in a smart environment. BCC employs the human body as communication medium to transfer digital data between physical objects by using electric fields that are coupled to the body. During the game participants are provided with BCC wearables, each of which represents a specific RGB color. When the user stands, walks on, or touches with a hand the BCC tiles, communication is automatically established: the corresponding sensor area decodes the message (RGB value) originating from the wearable and lights up according to that color for two seconds. The goal of the game is to try to light up as many tile cells simultaneously as possible. Participants can try to keep alive the colors by continuously moving around on the tiles. In the multiuser version, by stepping on or touching a blinking cell, users can immediately claim the area and overwrite the color of that subtile.\n",
      "=============================\n",
      "Filum: A Sewing Technique to Alter Textile Shapes\n",
      "We propose a novel shape-changing technique called Filum, which makes it possible to alter the shapes of textiles to better suit the requirements of people and the environment. Using strings and various sewing methods, ordinary textiles can be automatically shortened or shrunk into curved shapes. We demonstrate a series of novel interactions between people and textiles via three applications.\n",
      "=============================\n",
      "Towards Intermanual Apparent Motion of Thermal Pulses\n",
      "Perceptual illusions enable designers to go beyond hardware limitations to create rich haptic content. Nevertheless, spatio-temporal interactions for thermal displays have not been studied thoroughly. We focus on the apparent motion of hot and cold thermal pulses delivered at the thenar eminence of both hands. Here we show that 1000 ms hot and cold thermal pulses overlapping for about 40% of their actuation time are likely to produce a continuous apparent motion sensation. Furthermore, we show that the quality of the illusion (defined as the motion's temporal continuity) was more sensitive to changes in SOA for cold pulses in relation to hot pulses.\n",
      "=============================\n",
      "Printing System Reflecting User's Intent in Real Time Using a Handheld Printer\n",
      "We propose a new type of printing system that incorporates sensors in a handheld printer to reflect in real time user intent in the results of printing on paper. This system achieves two key functions: \"real-time embellishment\" for altering printed content by reading user hand movements by pressure and optical sensors, and \"local transcription\" for selecting content to be output by tracing existing content on paper with a linear camera. We performed experiments to measure the accuracy of both techniques and evaluate their usefulness.\n",
      "=============================\n",
      "Systems for Improving Online Discussion\n",
      "More and more of the discussion that happens now takes place on the web, whether it be for work, communities of interest, political and civic discourse, or education. However, little has changed in the design of online discussion systems, such as email, forums, and chat, in the decades they have been available, even as discussions grow in size and scope. As a result, online communities continue to struggle with issues stemming from growing participation, a diversity of participants, and new application domains. To solve these problems, my research is on building novel online discussion systems that give members of a community direct control over their experiences and information within these systems. Specifically, I focus on: 1) tools to make sense of large discussions and extract usable knowledge from them, 2) tools to situate conversations in the context what is being discussed, as well as 3) tools to give users more fine-grained control over the delivery of content, so that messages only go to those who wish to receive it.\n",
      "=============================\n",
      "KinToon: A Kinect Facial Projector for Communication Enhancement for ASD Children\n",
      "Children with ASD (Autism Spectrum Disorder) have social communication difficulties partly due to their abnormal avoidance of eye contact on human faces, yet they have a normal visual processing strategy on cartoon face. In this paper, we present KinToon, a face-to-face communication enhancement system to help ASD children in their training lessons. Our system use Kinect to scan human face and extract key points from facial contour, and match them to corresponding key points of a cartoon face. A modified cartoon face is projected to the communicator's face to achieve the effect of dynamic \"makeup\". ASD children will finally talk to the communicator with dynamic cartoon makeup, which would reduce their stress of interacting with people and make them easier to understand emotions. The interactive devices were applied to an ASD training lesson, and our creative approach was examined to be relatively effective in encouraging ASD children to fetch more emotional information and have more eye contact with people by eye tracking.\n",
      "=============================\n",
      "Frictio: Passive Kinesthetic Force Feedback for Smart Ring Output\n",
      "Smart rings have a unique form factor suitable for many applications, however, they offer little opportunity to provide the user with natural output. We propose passive kinesthetic force feedback as a novel output method for rotational input on smart rings. With this new output channel, friction force profiles can be designed, programmed, and felt by a user when they rotate the ring. This modality enables new interactions for ring form factors. We demonstrate the potential of this new haptic output method though Frictio, a prototype smart ring. In a controlled experiment, we determined the recognizability of six force profiles, including Hard Stop, Ramp-Up, Ramp-Down, Resistant Force, Bump, and No Force. The results showed that participants could distinguish between the force profiles with 94% accuracy. We conclude by presenting a set of novel interaction techniques that Frictio enables, and discuss insights and directions for future research.\n",
      "=============================\n",
      "The Feedback Block Model for an Adaptive E-Book\n",
      "The emergence of social reading services has enabled readers to participate actively in reading activities by means of sharing and feedback. Readers can state their opinion on a book by providing feedback. However, because current e-books are published with fixed, unchangeable content, it is difficult to reflect the reader's feedback on them. In this paper, we propose a system for an adaptive e-book that dynamically updates itself on user participation. To achieve this, we designed a Feedback Block Model and a Feedback Engine. In the Feedback Block Model, at the time of publication, the author defines the type of feedback expected from readers. After publication, the Feedback Engine collects and aggregates the readers? feedback. The Feedback Engine can be configured with drag-and-drop block programming, and hence, even authors inexperienced in programming can create an adaptive e-book.\n",
      "=============================\n",
      "CritiqueKit: A Mixed-Initiative, Real-Time Interface For Improving Feedback\n",
      "We present CritiqueKit, a mixed-initiative machine-learning system that helps students give better feedback to peers by reusing prior feedback, reducing it to be useful in a general context, and retraining the system about what is useful in real time. CritiqueKit exploits the fact that novices often make similar errors, leading reviewers to reuse the same feedback on many different submissions. It takes advantage of all prior feedback, and classifies feedback as the reviewer types it. CritiqueKit continually updates the corpus of feedback with new comments that are added, and it guides reviewers to improve their feedback, and thus the entire corpus, over time.\n",
      "=============================\n",
      "INVISO: A Cross-platform User Interface for Creating Virtual Sonic Environments\n",
      "The predominant interaction paradigm of current audio spatialization tools, which are primarily geared towards expert users, imposes a design process in which users are characterized as stationary, limiting the application domain of these tools. Navigable 3D sonic virtual realities, on the other hand, can support many applications ranging from soundscape prototyping to spatial data representation. Although modern game engines provide a limited set of audio features to create such sonic environments, the interaction methods are inherited from the graphical design features of such systems, and are not specific to the auditory modality. To address such limitations, we introduce INVISO, a novel web-based user interface for designing and experiencing rich and dynamic sonic virtual realities. Our interface enables both novice and expert users to construct complex immersive sonic environments with 3D dynamic sound components. INVISO is platform-independent and facilitates a variety of mixed reality applications, such as those where users can simultaneously experience and manipulate a virtual sonic environment. In this paper, we detail the interface design considerations for our audio-specific VR tool. To evaluate the usability of INVISO, we conduct two user studies: The first demonstrates that our visual interface effectively facilitates the generation of creative audio environments; the second demonstrates that both expert and non-expert users are able to use our software to accurately recreate complex 3D audio scenes.\n",
      "=============================\n",
      "Wind Tactor: An Airflow-based Wearable Tactile Display\n",
      "Traditional wearable tactile displays transfer information through a firm contact between the tactile stimulator (tactor) and the skin. The firm contact, however, might limit the location of wearable tactile displays and might be the source of discomfort when the skin is being exposed to prolonged contact. This motivated us to find a non-contact wearable tactile display, which is able to transfer information without a contact. Based on the literature review, we concluded that we should focus on airflow-based tactile displays among various non-contact stimulation methods. In my previous work, I proposed the concept of a non-contact wearable tactile display using airflows and explored its feasibility. Focusing on an airflow-based wearable tactile display, I am investigating the expressivity and the feasibility of wearable airflow displays in real-world environments. I expect my dissertation will provide empirical grounds and guidelines for the design of an airflow-based wearable tactile display.\n",
      "=============================\n",
      "Trigger-Action-Circuits: Leveraging Generative Design to Enable Novices to Design and Build Circuitry\n",
      "The dramatic decrease in price and increase in availability of hobbyist electronics has led to a wide array of embedded and interactive devices. While electronics have become more widespread, developing and prototyping the required circuitry for these devices is still difficult, requiring knowledge of electronics, components, and programming. In this paper, we present Trigger-Action-Circuits (TAC), an interactive system that leverages generative design to produce circuitry, firmware, and assembly instructions, based on high-level, behavioural descriptions. TAC is able to generate multiple candidate circuits from a behavioural description, giving the user a number of alternative circuits that may be best suited to their use case (e.g., based on cost, component availability or ease of assembly). The generated circuitry uses off-the-shelf, commodity electronics, not specialized hardware components, enabling scalability and extensibility. TAC supports a range of common components and behaviors that are frequently required for prototyping electronic circuits. A user study demonstrated that TAC helps users avoid problems encountered during circuit design and assembly, with users completing their circuits significantly faster than with traditional methods.\n",
      "=============================\n",
      "Optically Dynamic Interfaces\n",
      "In the virtual world, changing properties of objects such as their color, size or shape is one of the main means of communication. Objects are hidden or revealed when needed, or undergo changes in color or size to communicate importance. I am interested in how these features can be brought into the real world by modifying the optical properties of physical objects and devices, and how this dynamic appearance influences interaction and behavior. The interplay of creating functional prototypes of interactive artifacts and devices, and studying them in controlled experiments forms the basis of my research. During my research I created a three level model describing how physical artifacts and interfaces can be appropriated to allow for dynamic appearance: (1) dynamic objects, (2) augmented objects, and (3) augmented surroundings. This position paper outlines these three levels and details instantiations of each level that were created in the context of this thesis research.\n",
      "=============================\n",
      "Mutual Human Actuation\n",
      "Human actuation is the idea of using people to provide large-scale force feedback to users. The Haptic Turk system, for example, used four human actuators to lift and push a virtual reality user; TurkDeck used ten human actuators to place and animate props for a single user. While the experience of human actuators was decent, it was still inferior to the experience these people could have had, had they participated as a user. In this paper, we address this issue by making everyone a user. We introduce mutual human actuation, a version of human actuation that works without dedicated human actuators. The key idea is to run pairs of users at the same time and have them provide human actuation to each other. Our system, Mutual Turk, achieves this by (1) offering shared props through which users can exchange forces while obscuring the fact that there is a human on the other side, and (2) synchronizing the two users' timelines such that their way of manipulating the shared props is consistent across both virtual worlds. We demonstrate mutual human actuation with an example experience in which users pilot kites though storms, tug fish out of ponds, are pummeled by hail, battle monsters, hop across chasms, push loaded carts, and ride in moving vehicles.\n",
      "=============================\n",
      "SkinBot: A Wearable Skin Climbing Robot\n",
      "We introduce SkinBot; a lightweight robot that moves over the skin surface with a two-legged suction-based locomotion mechanism and that captures a wide range of body parameters with an exchangeable multipurpose sensing module. We believe that robots that live on our skin such as SkinBot will enable a more systematic study of the human body and will offer great opportunities to advance many areas such as telemedicine, human-computer interfaces, body care, and fashion.\n",
      "=============================\n",
      "Designing and Evaluating Livefonts\n",
      "The emergence of personal computing devices offers both a challenge and opportunity for displaying text: small screens can be hard to read, but also support higher resolution. To fit content on a small screen, text must be small. This small text size can make computing devices unusable, in particular to low-vision users, whose vision is not correctable with glasses. Usability is also decreased for sighted users straining to read the small letters, especially without glasses at hand. We propose animated scripts called livefonts for displaying English with improved legibility for all users. Because paper does not support animation, traditional text is static. However, modern screens support animation, and livefonts capitalize on this capability. We evaluate our livefont variations' legibility through a controlled lab study with low-vision and sighted participants, and find our animated scripts to be legible across vision types at approximately half the size (area) of traditional letters, while previous smartfonts (static alternate scripts) did not show a significant legibility advantage for low-vision users. We evaluate the learnability of our livefont with low-vision and sighted participants, and find it to be comparably learnable to static smartfonts after two thousand practice sentences.\n",
      "=============================\n",
      "StrutModeling: A Low-Fidelity Construction Kit to Iteratively Model, Test, and Adapt 3D Objects\n",
      "We present StrutModeling, a computationally enhanced construction kit that enables users without a 3D modeling background to prototype 3D models by assembling struts and hub primitives in physical space. Physical 3D models are immediately captured in software and result in readily available models for 3D printing. Given the concrete physical format of StrutModels, modeled objects can be tested and fine tuned in the presence of existing objects and specific needs of users. StrutModeling avoids puzzling with pieces by contributing an adjustable strut and universal hub design. Struts can be adjusted in length and snap to magnetic hubs in any configuration. As such, arbitrarily complex models can be modeled, tested, and adjusted during the design phase. In addition, the embedded sensing capabilities allow struts to be used as measuring devices for lengths and angles, and tune physical mesh models according to existing physical objects.\n",
      "=============================\n",
      "HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\n",
      "We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n",
      "=============================\n",
      "FlexStylus: Leveraging Bend Input for Pen Interaction\n",
      "FlexStylus, a flexible stylus, detects deformation of the barrel as a vector with both a rotational and an absolute value, providing two degrees of freedom with the goal of improving the expressivity of digital art using a stylus device. We outline the construction of the prototype and the principles behind the sensing method, which uses a cluster of four fibre-optic based deformation sensors. We propose interaction techniques using the FlexStylus to improve menu navigation and tool selection. Finally, we describe a study comparing users' ability to match a changing target value using a commercial pressure stylus and the FlexStylus' absolute deformation. When using the FlexStylus, users had a significantly higher accuracy overall. This suggests that deformation may be a useful input method for future work considering stylus augmentation.\n",
      "=============================\n",
      "Triggering Artwork Swaps for Live Animation\n",
      "Live animation of 2D characters is a new form of storytelling that has started to appear on streaming platforms and broadcast TV. Unlike traditional animation, human performers control characters in real time so that they can respond and improvise to live events. Current live animation systems provide a range of animation controls, such as camera input to drive head movements, audio for lip sync, and keyboard shortcuts to trigger discrete pose changes via artwork swaps. However, managing all of these controls during a live performance is challenging. In this work, we present a new interactive system that specifically addresses the problem of triggering artwork swaps in live settings. Our key contributions are the design of a multi-touch triggering interface that overlays visual triggers around a live preview of the character, and a predictive triggering model that leverages practice performances to suggest pose transitions during live performances. We evaluate our system with quantitative experiments, a user study with novice participants, and interviews with professional animators.\n",
      "=============================\n",
      "\"Smellization\" of Warnings against Overuse Power used to Promote Energy Saving Behavior\n",
      "In Japan, the necessity of saving energy is rising due to the nuclear accident caused by the Great East Japan Earthquake that occurred on March 11, 2011. Reduction of energy usage is required due to rapid increases in electricity consumption due to the scorching summer heat in recent years. The common ways to provide information on energy consumption mainly occur through \"visualization\" of information. On the contrary, olfactory stimulation can be performed while working, and it is effective also when the degree of arousal is low. This study considers applications on the basis of the concept of \"smellization\" of information using olfactive stimulation. In this paper, we introduce the configuration and operation examples of a system developed for evoking public energy conservation behavior using smell.\n",
      "=============================\n",
      "Tactile Element with Double-sided Inkjet Printing to Generate Electrostatic Forces and Electrostimuli\n",
      "We propose a tactile element that can generate both an electrostatic force and an electrostimulus, and can be used to provide tactile feedback on a wide area of human skin such as the palm of the hand. Touching the flat surface through the our proposed tactile element allow the user to feel both uneven and rough textures. In addition, the element can be fabricated using double-sided inkjet printing with conductive ink. Use of a flexible substrate, such as a PET film or paper, allows the user to design a free-formed tactile element. In this demonstration, we describe the implementation of the proposed stimuli element and show examples of applications.\n",
      "=============================\n",
      "Codestrates: Literate Computing with Webstrates\n",
      "We introduce Codestrates, a literate computing approach to developing interactive software. Codestrates blurs the distinction between the use and development of applications. It builds on the literate computing approach, commonly found in interactive notebooks such as Jupyter notebook. Literate computing weaves together prose and live computation in the same document. However, literate computing in interactive notebooks are limited to computation and it is challenging to extend their user interface, reprogram their functionality, or develop stand-alone applications. Codestrates builds literate computing capabilities on top of Webstrates and demonstrates how it can be used for (i) collaborative interactive notebooks, (ii) extending its functionality from within itself, and (iii) developing reprogrammable applications.\n",
      "=============================\n",
      "Rico: A Mobile App Dataset for Building Data-Driven Design Applications\n",
      "Data-driven models help mobile app designers understand best practices and trends, and can be used to make predictions about design performance and support the creation of adaptive UIs. This paper presents Rico, the largest repository of mobile app designs to date, created to support five classes of data-driven applications: design search, UI layout generation, UI code generation, user interaction modeling, and user perception prediction. To create Rico, we built a system that combines crowdsourcing and automation to scalably mine design and interaction data from Android apps at runtime. The Rico dataset contains design data from more than 9.7k Android apps spanning 27 categories. It exposes visual, textual, structural, and interactive design properties of more than 72k unique UI screens. To demonstrate the kinds of applications that Rico enables, we present results from training an autoencoder for UI layout similarity, which supports query- by-example search over UIs.\n",
      "=============================\n",
      "JDLED: Towards Visio-Tactile Displays Based on Electrochemical Locomotion of Liquid-Metal Janus Droplets\n",
      "An actuated shape-changing interface with faster response and smaller pixel size using a liquid material can provide real time tangible interaction with the digital world in physical space. To this end, we demonstrate an interface that displays user-defined patterns dynamically using liquid metal droplets as programmable micro robots on a flat surface. We built a prototype using an array of embedded electrodes and a switching circuit to control the jump of the droplets from electrode to electrode. The actuation and dynamics of the droplets under the finger provides mild tactile feedback to the user. Our demo is the first to show a planar visio-tactile display using liquid metal, and is a first step to make shape-changing physical ephemeral widgets on a tabletop interface.\n",
      "=============================\n",
      "Secondary Motion for Performed 2D Animation\n",
      "When bringing animated characters to life, artists often augment the primary motion of a figure by adding secondary animation -- subtle movement of parts like hair, foliage or cloth that complements and emphasizes the primary motion. Traditionally, artists add secondary motion to animated illustrations only through arduous manual effort, and often eschew it entirely. Emerging ``live' performance applications allow both novices and experts to perform the primary motion of a character, but only a virtuoso performer could manage the degrees of freedom needed to specify both primary and secondary motion together. This paper introduces physically-inspired rigs that propagate the primary motion of layered, illustrated characters to produce plausible secondary motion. These composable elements are rigged and controlled via a small number of parameters to produce an expressive range of effects. Our approach supports a variety of the most common secondary effects, which we demonstrate with an assortment of characters of varying complexity.\n",
      "=============================\n",
      "Projective Windows: Arranging Windows in Space Using Projective Geometry\n",
      "In augmented and virtual reality, there may be many 3D planar windows with 2D texts, images, and videos on them. Projective Windows is a technique using projective geometry to bring any near or distant window instantly to the fingertip and then to scale and position it simultaneously with a single, continuous flow of hand motion.\n",
      "=============================\n",
      "A Capacitive Touch Sensing Technique with Series-connected Sensing Electrodes\n",
      "Touch sensing with multiple electrodes allows expressive touch interactions. The adaptability and flexibility of the sensor are important in efficiently prototyping touch based systems. The proposed technique uses capacitive touch sensing and simplifies the connections as the electrodes are connected in series via capacitors and the interface circuit is connected to the electrode array by just two wires. The touched electrode is recognized by measuring the capacitance changes while switching the polarity of the signal. We show that the technique is capable of detecting different touches through simulations and actual measurements. User tests show that ten electrodes are successfully recognized after user calibration. They also show the proposal's other novel capabilities of multi-touch (2-touch) and `capacitor-free' design. Various forms of electrodes and applications are examined to elucidate the application range.\n",
      "=============================\n",
      "WearMail: On-the-Go Access to Information in Your Email with a Privacy-Preserving Human Computation Workflow\n",
      "Email is more than just a communication medium. Email serves as an external memory for people---it contains our reservation numbers, meeting details, phone numbers, and more. Often, people need access to this information while on the go, which is cumbersome from mobile devices with limited I/O bandwidth. In this paper, we introduce WearMail, a conversational interface to retrieve specific information in email. WearMail is mostly automated but is made robust to information extraction tasks via a novel privacy-preserving human computation workflow. In WearMail, crowdworkers never have direct access to emails, but rather (i) generate an email filter to help the system find messages that may contain the desired information, and (ii) generate examples of the requested information that are then used to create custom, low-level information extractors that run automatically within the set of filtered emails. We explore the impact of varying levels of obfuscation on result quality, demonstrating that workers are able to deal with highly-obfuscated information nearly as well as with the original. WearMail introduces general mechanisms that let the crowd search and select private data without having direct access to the data itself.\n",
      "=============================\n",
      "SmoothMoves: Smooth Pursuits Head Movements for Augmented Reality\n",
      "SmoothMoves is an interaction technique for augmented reality (AR) based on smooth pursuits head movements. It works by computing correlations between the movements of on-screen targets and the user's head while tracking those targets. The paper presents three studies. The first suggests that head based input can act as an easier and more affordable surrogate for eye-based input in many smooth pursuits interface designs. A follow-up study grounds the technique in the domain of augmented reality, and captures the error rates and acquisition times on different types of AR devices: head-mounted (2.6%, 1965ms) and hand-held (4.9%, 2089ms). Finally, the paper presents an interactive lighting system prototype that demonstrates the benefits of using smooth pursuits head movements in interaction with AR interfaces. A final qualitative study reports on positive feedback regarding the technique's suitability for this scenario. Together, these results indicate show SmoothMoves is viable, efficient and immediately available for a wide range of wearable devices that feature embedded motion sensing.\n",
      "=============================\n",
      "Creating Haptic Illusion of Compliance for Tangential Force Input using Vibrotactile Actuator\n",
      "We demonstrate a haptic feedback method that generates a compliance illusion on a rigid surface with a tangential force sensor and a vibrotactile actuator. The method assumes a conceptual model where a virtual object is placed on a textured surface and stringed to four walls with four springs. A two dimensional tangential force vector measured from the rigid surface is mapped to the virtual position of the virtual object on the textured surface. By playing vibration patterns that simulate the friction-induced vibrations made from the movement of the virtual object, we could make an illusion that the rigid surface feels like moving. We also demonstrate that the perceptual properties of the illusion, such as the stiffness of the virtual spring and the maximum travel distance of the virtual object, can be programmatically controlled.\n",
      "=============================\n",
      "iSphere: Self-Luminous Spherical Drone Display\n",
      "We present iSphere, a flying spherical display that can display high resolution and bright images in all directions from anywhere in 3D space. Our goal is to build a new platform which can physically and directly emerge arbitrary bodies in the real world. iSphere flies by itself using a built-in drone and creates a spherical display by rotating arcuate multi light-emitting diode (LED) tapes around the drone. As a result of the persistence of human vision, we see it as a spherical display flying in the sky. The proposed method yields large display surfaces, high resolution, drone mobility, high visibility and 360° field of view. Previous approaches fail to match these characteristics, because of problems with aerodynamics and payload. We construct a prototype and validate the proposed method. The unique characteristics and benefits of flying spherical display surfaces are discussed and we describe application scenarios based on iSphere such as guidance, signage and telepresence.\n",
      "=============================\n",
      "Reinventing the Wheel: Transforming Steering Wheel Systems for Autonomous Vehicles\n",
      "In this paper, we introduce two different transforming steering wheel systems that can be utilized to augment user experience for future partially autonomous and fully autonomous vehicles. The first one is a robotic steering wheel that can mechanically transform by using its actuators to move the various components into different positions. The second system is a LED steering wheel that can visually transform by using LEDs embedded along the rim of wheel to change colors. Both steering wheel systems contain onboard microcontrollers developed to interface with our driving simulator. The main function of these two systems is to provide emergency warnings to drivers in a variety of safety critical scenarios, although the design space that we propose for these steering wheel systems also includes the use as interactive user interfaces. To evaluate the effectiveness of the emergency alerts, we conducted a driving simulator study examining the performance of participants (N=56) after an abrupt loss of autonomous vehicle control. Drivers who experienced the robotic steering wheel performed significantly better than those who experienced the LED steering wheel. The results of this study suggest that alerts utilizing mechanical movement are more effective than purely visual warnings.\n",
      "=============================\n",
      "Ani-Bot: A Mixed-Reality Modular Robotics System\n",
      "We present Ani-Bot, a modular robotics system that allows users to construct Do-It-Yourself (DIY) robots and use mixed-reality approach to interact with them. Ani-Bot enables novel user experience by embedding Mixed-Reality Interaction (MRI) in the three phases of interacting with a modular construction kit, namely, Creation, Tweaking, and Usage. In this paper, we first present the system design that allows users to instantly perform MRI once they finish assembling the robot. Further, we discuss the augmentations offered by MRI in the three phases in specific.\n",
      "=============================\n",
      "An EEG-based Adaptive Training System for ASD Children\n",
      "Children with ASD (Autism Spectrum Disorder) have difficulties in expressing their feelings and needs, their teachers have to be very familiar with them to adjust teaching contents in related training lessons. In this paper, we present an adaptive training system with EEG (Electroencephalogram) devices for autistic children. We designed an EEG helmet to monitor children's attention levels, and a video chat system with virtual cartoon faces covered on teacher's face. Cartoon faces are synchronized with the performer's facial movements to help trainers express themselves in an exaggerated way. When the attention reduction is detected by the EEG helmet, cartoon face will adjust automatically, and try to draw their attention back through changing cartoon types, colors, brightness, etc. Each change and feedback from children will be traced by the helmet and analyzed for improvements. By continuous iterative learning, the system will become smarter in avoiding children's physical exhaustion. The system was introduced in the form of a specific training lesson to an ASD school, and preliminary experiment has indicated an encouraging result.\n",
      "=============================\n",
      "DodecaPen: Accurate 6DoF Tracking of a Passive Stylus\n",
      "We propose a system for real-time six degrees of freedom (6DoF) tracking of a passive stylus that achieves sub-millimeter accuracy, which is suitable for writing or drawing in mixed reality applications. Our system is particularly easy to implement, requiring only a monocular camera, a 3D printed dodecahedron, and hand-glued binary square markers. The accuracy and performance we achieve are due to model-based tracking using a calibrated model and a combination of sparse pose estimation and dense alignment. We demonstrate the system performance in terms of speed and accuracy on a number of synthetic and real datasets, showing that it can be competitive with state-of-the-art multi-camera motion capture systems. We also demonstrate several applications of the technology ranging from 2D and 3D drawing in VR to general object manipulation and board games.\n",
      "=============================\n",
      "Multiplanes: Assisted Freehand VR Drawing\n",
      "Multiplanes is a virtual reality (VR) drawing system that provides users with the flexibility of freehand drawing and the ability to draw perfect shapes. Through the combination of both beautified and 2D drawing, Multiplanes addresses challenges in creating 3D VR drawings. To achieve this, the system beautifies user's strokes based on the most probable, intended shapes while the user is drawing them. It also automatically generates snapping planes and beautification trigger points based on previous and current strokes and the current controller pose. Based on geometrical relationships to previous strokes, beautification trigger points act as guides inside the virtual environment. Users can hit these points to (explicitly) trigger a stroke beautification. In contrast to other systems, when using Multiplanes, users do not need to manually set or do any kind of special gesture to activate, such guides allowing the user to focus on the creative process.\n",
      "=============================\n",
      "Bifröst: Visualizing and Checking Behavior of Embedded Systems across Hardware and Software\n",
      "The Maker movement has encouraged more people to start working with electronics and embedded processors. A key challenge in developing and debugging custom embedded systems is understanding their behavior, particularly at the boundary between hardware and software. Existing tools such as step debuggers and logic analyzers only focus on software or hardware, respectively. This paper presents a new development environment designed to illuminate the boundary between embedded code and circuits. Bifröst automatically instruments and captures the progress of the user's code, variable values, and the electrical and bus activity occurring at the interface between the processor and the circuit it operates in. This data is displayed in a linked visualization that allows navigation through time and program execution, enabling comparisons between variables in code and signals in circuits. Automatic checks can detect low-level hardware configuration and protocol issues, while user-authored checks can test particular application semantics. In an exploratory study with ten participants, we investigated how Bifröst influences debugging workflows.\n",
      "=============================\n",
      "Hand Development Kit: Soft Robotic Fingers as Prosthetic Augmentation of the Hand\n",
      "Recent developments in wearable robots and human augmentation open up new possibilities of designing computational interfaces integrated to the body. Particularly, supernumerary robot is a recently established field of research that investigates a radical idea of adding robotic limbs to users. Such augmentations, however, pose a limit in how much we can add to the body due to weight or interference with other body parts. To address that, we explore the use of soft robots as supernumerary robotic fingers. We present a pair of soft robotic fingers driven by cables and servomotors, and applications using the robotic fingers in various contexts.\n",
      "=============================\n",
      "Data Storage and Interaction using Magnetized Fabric\n",
      "This paper enables data storage and interaction with smart fabric, without the need for onboard electronics or batteries. To do this, we present the first smart fabric design that harnesses the ferromagnetic properties of conductive thread. Specifically, we manipulate the polarity of magnetized fabric and encode different forms of data including 2D images and bit strings. These bits can be read by swiping a commodity smartphone across the fabric, using its inbuilt magnetometer. Our results show that magnetized fabric retains its data even after washing, drying and ironing. Using a glove made of magnetized fabric, we can also perform six gestures in front of a smartphone, with a classification accuracy of 90.1%. Finally, using magnetized thread, we create fashion accessories like necklaces, ties, wristbands and belts with data storage capabilities as well as enable authentication applications.\n",
      "=============================\n",
      "Pepper's Cone: An Inexpensive Do-It-Yourself 3D Display\n",
      "This paper describes a simple 3D display that can be built from a tablet computer and a plastic sheet folded into a cone. This display allows naturally viewing a three-dimensional object from any direction over a 360-degree path of travel without the use of a head mount or special glasses. Inspired by the classic Pepper's Ghost illusion, our approach uses a curved transparent surface to reflect the image displayed on a 2D display. By properly pre-distorting the displayed image our system can produce a perspective-correct image to the viewer that appears to be suspended inside the reflector. We use the gyroscope integrated into modern tablet computers to adjust the rendered image based on the relative orientation of the viewer. The end result is a natural and intuitive interface for inspecting a 3D object. Our choice of a cone reflector is obtained by analyzing optical performance and stereo-compatibility over rotationally-symmetric conic reflector shapes. We also present the prototypes we built and measure the performance of our display through side-by-side comparisons with reference images.\n",
      "=============================\n",
      "Grip Force Estimation by Emitting Vibration\n",
      "We propose a method for determining grip force based on active bone-conducted sound sensing, which is an active acoustic sensing. In our previous studies, we estimated the joint angle, hand pose, and contact force by emitting a vibration to the body. We aspired to expand to an additional application of an active bone-conducted sound sensing, thus, we tried to estimate the grip force by creating a wrist-type device. The grip force was determined by using the power spectral density as the features, and gradient boosted regression trees (GBRT). Through evaluation experiments, the average error of the estimated grip force was around 15 N. Moreover, we confirmed that the grip strength could be determined with high accuracy.\n",
      "=============================\n",
      "Exploring of Simulating Passing through Feeling on the Wrist: Using Thermal Feedback\n",
      "Wearable devices combining with VR/AR technology become a research hotspot these years. In some research, tactile displays are put on the skin and synchronized with VR/AR environment. Researchers try to use these display to simulate varied embodied feeling to enhance the immersion in the VR/AR environment. In the field of game entertainment, based on the scenario, sometimes the feeling of passing through the body need to be presented to the user. However this is physically impossible. Thus we make a exploration attempting to simulate this feeling by thermal feedback. Here we use two thermal modules bonding on the two side of the wrist( inside and outside). When we actuate two modules sequentially, user would perceive the stimuli and interpret this into a feeling of passing though. In the paper, we will introduce the interface and describe the experiment to determine the principle for thermo-tactile illusion of passing through.\n",
      "=============================\n",
      "Reflector: Distance-Independent, Private Pointing on a Reflective Screen\n",
      "Reflector is a novel direct pointing method that utilizes hidden design space on reflective screens. By aligning a part of the user's onscreen reflection with objects rendered on the screen, Reflector enables (1) distance-independent and (2) private pointing on commodity screens. Reflector can be implemented easily in both desktop and mobile conditions through a single camera installed at the edge of the screen. Reflector's pointing performance was compared to today's major direct input devices: eye trackers and touchscreens. We demonstrate that Reflector allows the user to point more reliably, regardless of distance from the screen, compared to an eye tracker. Further, due to the private nature of an onscreen reflection, Reflector shows a shoulder surfing success rate 20 times lower than that of touchscreens for the task of entering a 4-digit PIN.\n",
      "=============================\n",
      "Walk-In Music: Walking Experience with Synchronized Music and Its Effect of Pseudo-gravity\n",
      "\"Walk-In Music\" is a system that provides a new walking experience through synchronized music and pseudo-gravity. This system synchronizes each step with the music being listened to and creates a feeling of generating music through walking. It creates a Walk-In state where music and walking are consistent at all times. In this state, when changing the speed of music, the pedestrian may feel pseudo-gravity based on pseudo-haptics. Our results indicate that by changing the speed of music during the Walk-In state, the walking speed became faster and slower. We call this a Walk-Shift. This demonstrated the possibility of controlling personal walking by music. Walk-In Music has created a pleasant experience by music, and proposed a new relationship between people and music that leads to behavior changes.\n",
      "=============================\n",
      "Automatically Visualizing Audio Travel Podcasts\n",
      "Audio Podcasts have gained popularity because they are a compelling form of storytelling and are easy to consume. However, they are not as easy to produce since resources are invested in the research, recording, and editing process and the average length of an episode is over an hour. Some audio podcasts could benefit from visuals to increase engagement and learning, but manually curating them can be arduous and time-consuming. We introduce a tool for automatically visualizing audio podcasts, currently focused on the genre of travelogues. Our system works by first time-aligning the transcript of a given podcast, using NLP techniques to extract entities and track how interesting or relevant they are throughout the podcast, and then retrieving visual data appropriately to describe them, either through transitions on a map or professionally taken photographs with captions. By automatically creating a visual narrative to accompany a podcast, we hope our tool can provide listeners with a better sense of the podcast's topic.\n",
      "=============================\n",
      "Shot Orientation Controls for Interactive Cinematography with 360 Video\n",
      "Virtual reality filmmakers creating 360-degree video currently rely on cinematography techniques that were developed for traditional narrow field of view film. They typically edit together a sequence of shots so that they appear at a fixed-orientation irrespective of the viewer's field of view. But because viewers set their own camera orientation they may miss important story content while looking in the wrong direction. We present new interactive shot orientation techniques that are designed to help viewers see all of the important content in 360-degree video stories. Our viewpoint-oriented technique reorients the shot at each cut so that the most important content lies in the the viewer's current field of view. Our active reorientation technique, lets the viewer press a button to immediately reorient the shot so that important content lies in their field of view. We present a 360-degree video player which implements these techniques and conduct a user study which finds that users spend 5.2-9.5% more time viewing the important points (manually labelled) of the scene with our techniques compared to the traditional fixed-orientation cuts. In practice, 360-degree video creators may label important content, but we also provide an automatic method for determining important content in existing 360-degree videos.\n",
      "=============================\n",
      "FoamSense: Design of Three Dimensional Soft Sensors with Porous Materials\n",
      "Here we report the new soft sensor \"FoamSense\" that can measure the deformation state of a volumetric soft object such as compressed, bent, twisted and sheared (Figure 1). This sensor is made by impregnating a porous soft object with conductive ink. The design process of FoamSense is explained. We then summarized the features and basic characteristics of some porous materials for designing these sensors appropriately. We also proposed the potential of using digital fabrication for controlling the carrier structure of FoamSense. Proposed porous structure showed an anisotropic sensor characteristic. We discussed the potential and limitation of this approach. Three possible applications are proposed by using FoamSense. FoamSense supports a richer interaction between the user and soft objects.\n",
      "=============================\n",
      "A Thermally Enhanced Weather Checking System in VR\n",
      "In this project, by combining thermal feedback with Virtual Reality (VR) and utilizing thermal stimuli to present temperature data of weather, we attempted to provide a multi-sensory experience for enhancing users' perception of environment in virtual space. By integrating thermal modules with the current VR head mounted display to provide thermal feedback directly on the face, and by setting thermal stimulus to provide similar feeling towards real air temperature, we developed an application in which users are able to \"feel\" the weather in VR environment. An user experiment was also conducted to evaluate our design, according to which we verified that thermal feedback can improve users' experience in perceiving environment, and this research also provided a new approach for setting thermal feedback for presenting environmental information in virtual space.\n",
      "=============================\n",
      "Outside-In: Visualizing Out-of-Sight Regions-of-Interest in a 360° Video Using Spatial Picture-in-Picture Previews\n",
      "360-degree video contains a full field of environmental content. However, browsing these videos, either on screens or through head-mounted displays (HMDs), users consume only a subset of the full field of view per a natural viewing experience. This causes a search problem when a region-of-interest (ROI) in a video is outside of the current field of view (FOV) on the screen, or users may search for non-existing ROIs. We propose Outside-In, a visualization technique which re-introduces off-screen regions-of-interest (ROIs) into the main screen as spatial picture-in-picture (PIP) previews. The geometry of the preview windows further encodes a ROI's relative location vis-à-vis the main screen view, allowing for effective navigation. In an 18-participant study, we compare Outside-In with traditional arrow-based guidance within three types of 360-degree video. Results show that Outside-In outperforms in regard to understanding spatial relationship, the storyline of the content and overall preference. Two applications are demonstrated for use with Outside-In in 360-degree video navigation with touchscreens, and live telepresence.\n",
      "=============================\n",
      "Demonstrating TrussFab's Editor: Designing Sturdy Large-Scale Structures\n",
      "We demonstrate TrussFab's editor for creating large-scale structures that are sturdy enough to carry human weight. TrussFab achieves the large scale by using plastic bottles as beams that form structurally sound node-link structures, also known as trusses, allowing it to handle the forces resulting from scale and load. During this hands-on demo at UIST, attendees will use the TrussFab software to design their own structures, validate their design using integrated structural analysis, and export their designs for 3D printing.\n",
      "=============================\n",
      "MatchPoint: Spontaneous Spatial Coupling of Body Movement for Touchless Pointing\n",
      "Pointing is a fundamental interaction technique where user movement is translated to spatial input on a display. Conventionally, this is based on a rigid configuration of a display coupled with a pointing device that determines the types of movement that can be sensed, and the specific ways users can affect pointer input. Spontaneous spatial coupling is a novel input technique that instead allows any body movement, or movement of tangible objects, to be appropriated for touchless pointing on an ad hoc basis. Pointer acquisition is facilitated by the display presenting graphical objects in motion, to which users can synchronise to define a temporary spatial coupling with the body part or tangible object they used in the process. The technique can be deployed using minimal hardware, as demonstrated by MatchPoint, a generic computer vision-based implementation of the technique that requires only a webcam. We explore the design space of spontaneous spatial coupling, demonstrate the versatility of the technique with application examples, and evaluate MatchPoint performance using a multi-directional pointing task.\n",
      "=============================\n",
      "SoundCraft: Enabling Spatial Interactions on Smartwatches using Hand Generated Acoustics\n",
      "We present SoundCraft, a smartwatch prototype embedded with a microphone array, that localizes angularly, in azimuth and elevation, acoustic signatures: non-vocal acoustics that are produced using our hands. Acoustic signatures are common in our daily lives, such as when snapping or rubbing our fingers, tapping on objects or even when using an auxiliary object to generate the sound. We demonstrate that we can capture and leverage the spatial location of such naturally occurring acoustics using our prototype. We describe our algorithm, which we adopt from the MUltiple SIgnal Classification (MUSIC) technique [31], that enables robust localization and classification of the acoustics when the microphones are required to be placed at close proximity. SoundCraft enables a rich set of spatial interaction techniques, including quick access to smartwatch content, rapid command invocation, in-situ sketching, and also multi-user around device interaction. Via a series of user studies, we validate SoundCraft's localization and classification capabilities in non-noisy and noisy environments.\n",
      "=============================\n",
      "Qoom: An Interactive Omnidirectional Ball Display\n",
      "We present a sphere-shaped interactive display system, named Qoom, as a new input and output device. Unlike existing sphere-shaped displays, Qoom is a perfectly spherical ball that can be rotated, thrown, or even kicked. First, we discuss how spherical displays can be used in daily life and describe how users interact with spheres. Then, we show how we developed the Qoom prototype that uses touch and rotation detection, real-time object tracking, and spherical projection mapping. We implemented actions including touching, rotating, bouncing and throwing as controls. We also developed applications for Qoom that utilize the unique advantages of ball displays.\n",
      "=============================\n",
      "GestAKey: Get More Done with Just-a-Key on a Keyboard\n",
      "The computer keyboard is a widely used input device to operate computers, such as text entry and command execution. Typically, keystrokes are detected as binary states (e.g. \"pressed\" vs. \"not pressed\"). Due to this, more complex input commands need multiple key presses that could go up to pressing four keys at the same time, such as pressing \"Cmd + Shift + Opt + 4\" to take a screenshot to the clipboard on macOS. We present GestAKey, a technique to enable multifunctional keystrokes on a single key, providing new interaction possibilities on the familiar keyboards. The system consists of touch sensitive keycaps and a software backend that recognizes micro-gestures performed on individual keys to perform system commands or input special characters. In this demo, attendees will get the chance to interact with several GestAKey-enabled proof-of-concept applications.\n",
      "=============================\n",
      "Towards a Universal Knowledge Accelerator\n",
      "The human mind remains an unparalleled engine of innovation, with its unique ability to make sense of complex information and find deep analogical connections driving progress in science and technology over the past millennia. The recent explosion of online information available in virtually every domain should present an opportunity for accelerating this engine; instead, it threatens to slow it as the information processing limits of individual minds are reached. In this talk I discuss our efforts towards building a universal knowledge accelerator: a system in which the sensemaking people engage in online is captured and made useful for others, leading to virtuous cycles of constantly improving information sources that in turn help people more effectively synthesize and innovate. Approximately 70 billion hours per year in the U.S. alone are spent on complex online sensemaking in domains ranging from scientific literature to health; capturing even a fraction of this could provide significant benefits. We discuss three integrated levels of research that are needed to realize this vision: at the individual level in understanding and capturing higher order cognition; at the computational level in developing new interaction systems and AI partners for human cognition; and at the social level in developing complex and creative crowdsourcing and social computing systems.\n",
      "=============================\n",
      "Panning and Zooming High-Resolution Panoramas in Virtual Reality Devices\n",
      "Two recent innovations in immersive media include the ability to capture very high resolution panoramic imagery, and the rise of consumer level heads-up displays for virtual reality. Unfortunately, zooming to examine the high resolution in VR breaks the basic contract with the user, that the FOV of the visual field matches the FOV of the imagery. In this paper, we study methods to overcome this restriction to allow high resolution panoramic imagery to be able to be explored in VR. We introduce and test new interface modalities for exploring high resolution panoramic imagery in VR. In particular, we demonstrate that limiting the visual FOV of the zoomed in imagery to the central portion of the visual field, and modulating the transparency or zoom level of the imagery during rapid panning, reduce simulator sickness and help with targeting tasks.\n",
      "=============================\n",
      "You as a Puppet: Evaluation of Telepresence User Interface for Puppetry\n",
      "We propose an immersive telepresence system for puppetry that transmits a human performer's body and facial movements into a puppet with audiovisual feedback to the performer. The cameras carried in place of puppet's eyes stream live video to the HMD worn by the performer, so that performers can see the images from the puppet's eyes with their own eyes and have a visual understanding of the puppet's ambience. In conventional methods to manipulate a puppet (a hand-puppet, a string-puppet, and a rod-puppet), there is a need to practice manipulating puppets, and there is difficulty carrying out interactions with the audience. Moreover, puppeteers must be positioned exactly where the puppet is. The proposed system addresses these issues by enabling a human performer to manipulate the puppet remotely using his or her body and facial movements. We conducted several user studies with both beginners and professional puppeteers. The results show that, unlike the conventional method, the proposed system facilitates the manipulation of puppets especially for beginners. Moreover, this system allows performers to enjoy puppetry and fascinate audiences.\n",
      "=============================\n",
      "Information Identification Support Method for Areas with Densely Located Signboards\n",
      "We developed methods and implemented a system prototype to help people find specific signboards in areas with densely located signboards. In addition, we examined whether the proposed methods would reduce the search time of a specific signboard. The result showed that the proposed method was superior in cases where there were multiple signboards to be searched and background saturation was low.\n",
      "=============================\n",
      "Enabling Sensing and Interaction with Everyday Objects\n",
      "The Internet of Things (IoT) promises to revolutionize the way people interact with their environment and the objects within it by creating a ubiquitous network of physical devices. However, current advancement in IoT has been heavily targeted towards creating battery-powered electronics. There remains a huge gap between the collection of smart devices integrated into the IoT and the massive number of everyday physical objects. The goal of my research is to bridge this gap in the current IoT framework to enable novel sensing and interactive applications with daily objects.\n",
      "=============================\n",
      "Thermal-Comfort Design of Personalized Casts\n",
      "This paper introduces a novel method for designing personalized orthopedic casts which are aware of thermal-comfort while satisfying mechanical requirements. Our pipeline starts from thermal images taken by an infrared camera, by which the distribution of thermal-comfort sensitivity is generated on the surface of a 3D scanned model. We formulate a hollowed Voronoi tessellation pattern to represent the covered region for a web-like cast design. The pattern is further optimized according to the thermal-comfort sensitivity calculated from thermal images. Working together with a thickness variation method, we generate a solid model for a personalized cast maximizing both thermal comfort and mechanical stiffness. To demonstrate the effectiveness of our approach, 3D printed models of personalized casts are tested on body parts of different individuals.\n",
      "=============================\n",
      "Hacking Computer Science History: A Cultural Intervention\n",
      "One cannot understand computer hacking without delving into the history of computer science but is the converse true? In this talk, I examine zones of collaboration as well as points of tension between the fields of computer science/engineering and computer hacking. By drawing on my anthropological research on hackers and turning to a set of cases around voting machines vulnerabilities, the Unix operating system, cryptography, and the Bit Torrent protocol, this talk suggests that while the object and practices around computers help account for a range of important similarities, the differences end there. Significant cultural differences and institutional constraints account for distinct moral and even technical trajectories evident in these two domains. I end reflecting on a slice of the important differences to asses what computer science may have to offer hacking and what hacking may have to offer the field of computer science.\n",
      "=============================\n",
      "Atypical: A Type System for Live Performances\n",
      "Chalktalk is a computer-based visual language based around real-time interaction with virtual objects in a blackboard-style environment. Its aim is to be a presentation and communication tool, using animation and interactivity to allow easy visualization of complex ideas and concepts. This demonstration will show Chalktalk in action, with focus on its ability to link these objects together to send data between them, as well as the flexible type system, named Atypical, that underpins this feature.\n",
      "=============================\n",
      "SceneCtrl: Mixed Reality Enhancement via Efficient Scene Editing\n",
      "Due to the development of 3D sensing and modeling techniques, the state-of-the-art mixed reality devices such as Microsoft Hololens have the ability of digitalizing the physical world. This unique feature bridges the gap between virtuality and reality and largely elevates the user experience. Unfortunately, the current solution only performs well if the virtual contents complement the real scene. It can easily cause visual artifacts when the reality needs to be modified due to the virtuality (e.g., remove real objects to offer more space for virtual objects), a common scenario in mixed reality applications such as room redecoration and environment design. We present a novel system, called emph{SceneCtrl}, that allows the user to interactively edit the real scene sensed by Hololens, such that the reality can be adapted to suit virtuality. Our proof-of-concept prototype employs scene reconstruction and understanding to enable efficient editing such as deleting, moving, and copying real objects in the scene. We also demonstrate emph{SceneCtrl} on a number of example scenarios in mixed reality, verifying the enhanced experience by resolving conflicts between virtuality and reality.\n",
      "=============================\n",
      "Grabity: A Wearable Haptic Interface for Simulating Weight and Grasping in Virtual Reality\n",
      "Ungrounded haptic devices for virtual reality (VR) applications lack the ability to convincingly render the sensations of a grasped virtual object's rigidity and weight. We present Grabity, a wearable haptic device designed to simulate kinesthetic pad opposition grip forces and weight for grasping virtual objects in VR. The device is mounted on the index finger and thumb and enables precision grasps with a wide range of motion. A unidirectional brake creates rigid grasping force feedback. Two voice coil actuators create virtual force tangential to each finger pad through asymmetric skin deformation. These forces can be perceived as gravitational and inertial forces of virtual objects. The rotational orientation of the voice coil actuators is passively aligned with the real direction of gravity through a revolute joint, causing the virtual forces to always point downward. This paper evaluates the performance of Grabity through two user studies, finding promising ability to simulate different levels of weight with convincing object rigidity. The first user study shows that Grabity can convey various magnitudes of weight and force sensations to users by manipulating the amplitude of the asymmetric vibration. The second user study shows that users can differentiate different weights in a virtual environment using Grabity.\n",
      "=============================\n",
      "SketchExpress: Remixing Animations for More Effective Crowd-Powered Prototyping of Interactive Interfaces\n",
      "Low-fidelity prototyping at the early stages of user interface (UI) design can help designers and system builders quickly explore their ideas. However, interactive behaviors in such prototypes are often replaced by textual descriptions because it usually takes even professionals hours or days to create animated interactive elements due to the complexity of creating them. In this paper, we introduce SketchExpress, a crowd-powered prototyping tool that enables crowd workers to create reusable interactive behaviors easily and accurately. With the system, a requester-designers or end-users-describes aloud how an interface should behave and crowd workers make the sketched prototype interactive within minutes using a demonstrate-remix-replay approach. These behaviors are manually demonstrated, refined using remix functions, and then can be replayed later. The recorded behaviors persist for future reuse to help users communicate with the animated prototype. We conducted a study with crowd workers recruited from Mechanical Turk, which demonstrated that workers could create animations using SketchExpress in 2.9 minutes on average with 27% gain in the quality of animations compared to the baseline condition of manual demonstration.\n",
      "=============================\n",
      "DreamSketch: Early Stage 3D Design Explorations with Sketching and Generative Design\n",
      "We present DreamSketch, a novel 3D design interface that combines the free-form and expressive qualities of sketching with the computational power of generative design algorithms. In DreamSketch, a user coarsely defines the problem by sketching the design context. Then, a generative design algorithm produces multiple solutions that are augmented as 3D objects in the sketched context. The user can interact with the scene to navigate through the generated solutions. The combination of sketching and generative algorithms enables designers to explore multiple ideas and make better informed design decisions during the early stages of design. Design study sessions with designers and mechanical engineers demonstrate the expressive nature and creative possibilities of DreamSketch.\n",
      "=============================\n",
      "Hybrid Use of Asynchronous and Synchronous Interaction for Collaborative Creation\n",
      "My dissertation is aimed at enabling people to collaborate to create complex artifacts: for example, to develop software, sketch GUI prototypes, play music together, or write a novel. Such creative processes are not well defined and can evolve dynamically. We introduce interactive systems that help users collaborate and communicate in the open-ended process. In particular, we investigate the benefits of both integrating asynchronous interactions into real-time collaborations and of having real time components in asynchronous collaborative settings. The systems provide tools that combine the two different types of interaction techniques, and we validate them via user study, participatory performing arts, and the online deployments of systems and crowdsourced tasks. The hybrid methods are designed to help users recover collaborative context, make the process approachable to nonexperts, collaborate online crowds on demand in real-time, and sustain liveness during collaboration. The dissertation will result in cross-domain knowledge in designing collaborative systems and it will help us create a framework for future intelligent systems that will help people solve more complex tasks effectively.\n",
      "=============================\n",
      "GraspForm: Shape and Hardness Rendering on Handheld Device toward Universal Interface for 3D Haptic TV\n",
      "The main goal of our research is to develop a haptic display that conveys the shapes, hardness, and textures of objects displayed on near-future 3D haptic TVs. We present a novel handheld device, GraspForm. This device renders the surface shapes and hardness of a virtual object that is represented in an absolute position in real space. GraspForm has a 2×2 matrix of actuated hemispheres for one fingertip, two actuated pads for the palm, and a force feedback actuator for the thumb. Our first experimental results showed that eight participants succeeded in recognizing the side geometries of a cylinder and a square prism regardless of the availability of visual information.\n",
      "=============================\n",
      "Raising the Heat: Electrical Muscle Stimulation for Simulated Heat Withdrawal Response\n",
      "Virtual Reality (VR) has numerous mechanisms for making a virtual scene more compellingly real. Most effort has been focused on visual and auditory techniques for immersive environments, although some commercial systems now include relatively crude haptic effects through handheld controllers or haptic suits. We present results from a pilot experiment demonstrating the use of Electrical Muscle Stimulation (EMS) to trick participants into thinking a surface is dangerously hot even though it is below 50C. This is accomplished by inducing an artificial heat withdrawal reflex by contracting the participant's bicep shortly after contact with the virtual hot surface. Although the effects of multiple experimental confounds need to be quantified in future work, results so far suggest that EMS could potentially be used to modify temperature perception in VR and AR contexts. Such an illusion has applications for VR gaming as well as emergency response and workplace training and simulation, in addition to providing new insights into the human perceptual system.\n",
      "=============================\n",
      "HapticDrone: An Encountered-Type Kinesthetic Haptic Interface with Controllable Force Feedback: Initial Example for 1D Haptic Feedback\n",
      "We present HapticDrone, a concept to generate controllable and comparable force feedback for direct haptic interaction with a drone. As a proof-of-concept study this paper focuses on creating haptic feedback only in 1D direction. To this end, an encountered-type, safe and un-tethered haptic display is implemented. An overview of the system and details on how to control the force output of drones is provided. Our current prototype generates forces up to 1.53 N upwards and 2.97 N downwards. This concept serves as a first step towards introducing drones as mainstream haptic devices.\n",
      "=============================\n",
      "Dwell+: Multi-Level Mode Selection Using Vibrotactile Cues\n",
      "We present Dwell+, a method that boosts the effectiveness of typical dwell selection by augmenting the passive dwell duration with active haptic ticks which promptly drives rapid switches of modes forward through the user's skin sensations. In this way, Dwell+ enables multi-level dwell selection using rapid haptic ticks. To select a mode from a button, users dwell-touch the button until the mode of selection is haptically prompted. Our haptic stimulation design consists of a short 10ms vibrotacile feedback that indicates a mode arriving and a break that separates consecutive modes. We first tested the effectiveness of 170ms, 150ms, 130ms, and 110ms intervals between modes for a 10-level selection. The results reveal that 3-beats-per-chunk rhythm design, e.g., displaying longer 25ms vibrations initially for all three modes, could potentially achieve higher accuracy. The second study reveals significant improvement wherein a 94.5% accuracy was achieved for a 10-level Dwell+ selection using the 170ms interval with 3-beats-per-chunk design, and a 93.82% rate of accuracy using the more frequent 150ms interval with similar chunks for 5-level selection. The performance of conducting touch and receiving vibration from disparate hands was investigated for our final study to provide a wider range of usage. Our applications demonstrated implementing Dwell+ across interfaces, such as text input on a smartwatch, enhancing touch space for HMDs, boosting modalities of stylus-based tool selection, and extending the input vocabulary of physical interfaces.\n",
      "=============================\n",
      "Jaguar: Indoor Navigation System for Organizations\n",
      "Global Positioning System (GPS) technology is widely used for outdoor navigation, but it is still challenging to apply this technology to a mid-scale or indoor environment. Using GPS in this way raises issues, such as reliability, deployment cost, and maintenance. Recently, companies like Google have begun to provide accurate indoor mapping. However, current implementations rely on both Wi-Fi and cellular technologies which have a hard time identifying the user's exact location in an indoor environment. There are two research questions in this paper: (1) How do we design a flexible and cost efficient indoor navigation system for organizations? (2) How to find an optimized path in a mid-scale/local environment. Here we propose Jaguar, which is a novel navigation system that utilizes a customized KML map with NFC technologies to address above questions. Our system includes an Android mobile application, a web-based map authoring tool and an implementation of a Cartesian plane based path finding algorithm. The initial testing of the system shows successful adaptation for a school campus environment.\n",
      "=============================\n",
      "Markit and Talkit: A Low-Barrier Toolkit to Augment 3D Printed Models with Audio Annotations\n",
      "As three-dimensional printers become more available, 3D printed models can serve as important learning materials, especially for blind people who perceive the models tactilely. Such models can be much more powerful when augmented with audio annotations that describe the model and their elements. We present Markit and Talkit, a low-barrier toolkit for creating and interacting with 3D models with audio annotations. Makers (e.g., hobbyists, teachers, and friends of blind people) can use Markit to mark model elements and associate then with text annotations. A blind user can then print the augmented model, launch the Talkit application, and access the annotations by touching the model and following Talkit's verbal cues. Talkit uses an RGB camera and a microphone to sense users' inputs so it can run on a variety of devices. We evaluated Markit with eight sighted \"makers\" and Talkit with eight blind people. On average, non-experts added two annotations to a model in 275 seconds (SD=70) with Markit. Meanwhile, with Talkit, blind people found a specified annotation on a model in an average of 7 seconds (SD=8).\n",
      "=============================\n",
      "CommandBoard: Creating a General-Purpose Command Gesture Input Space for Soft Keyboard\n",
      "CommandBoard offers a simple, efficient and incrementally learnable technique for issuing gesture commands from a soft keyboard. We transform the area above the keyboard into a command-gesture input space that lets users draw unique command gestures or type command names followed by execute. Novices who pause see an in-context dynamic guide, whereas experts simply draw. Our studies show that CommandBoard's inline gesture shortcuts are significantly faster (almost double) than markdown symbols and significantly preferred by users. We demonstrate additional techniques for more complex commands, and discuss trade-offs with respect to the user's knowledge and motor skills, as well as the size and structure of the command space.\n",
      "=============================\n",
      "Mobile Brain-Computer Interface for Dance and Somatic Practice\n",
      "Sensor technologies have been adapted to performing arts, and owing to the recent advancement of low-cost mobile electroencephalography devices, brain-computer interface (BCI) is integrated to dance performances as well. Nevertheless, BCI is less accessible to artists compared to other sensors because signal processing and machine learning are required. This paper proposes a work-in-progress example of BCI applications for performances that has been designed in collaboration with contemporary dancers. Its contribution is that the piece is not an add-on to a performance, but the implementation reflects practices of contemporary dance.\n",
      "=============================\n",
      "BlowFab: Rapid Prototyping for Rigid and Reusable Objects using Inflation of Laser-cut Surfaces\n",
      "This study proposes BlowFab, a prototyping method used to create a 2.5-dimensional prototype in a short time by combining laser cutting and blow molding techniques. The user creates adhesive areas and inflatable areas by engraving and cutting multilayered plastic sheets using a laser cutter. These adhesive areas are fused automatically by overlapping two crafted sheets and softening them with a heater. The user can then create hard prototypes by injecting air into the sheets. Objects can be bent in any direction by cutting incisions or engraving a resistant resin. The user can create uneven textures by engraving a pattern with a heat-resistant film. These techniques can be used for prototyping various strong inflatable objects. The finished prototype is strong and can be collapsed readily for storage when not required. In this study, the design process is described using the proposed method. The study also evaluates possible bending mechanisms and texture expression methods along with various usage scenarios and discusses the resolution, strength, and reusability of the prototype developed.\n",
      "=============================\n",
      "shapeShift: A Mobile Tabletop Shape Display for Tangible and Haptic Interaction\n",
      "shapeShift is a compact, high-resolution (7 mm pitch), mobile tabletop shape display. We explore potential interaction techniques in both passive and active mobile scenarios. In the passive case, the user is able to freely move and spin the display as it renders elements. We introduce use cases for rendering lateral I/O elements, exploring volumetric datasets, and grasping and manipulating objects. On an active omnidirectional-robot platform, shapeShift can display moving objects and provide both vertical and lateral kinesthetic feedback. We use the active platform as an encounter-type haptic device combined with a head-mounted display to dynamically simulate the presence of virtual content.\n",
      "=============================\n",
      "DS.js: Turn Any Webpage into an Example-Centric Live Programming Environment for Learning Data Science\n",
      "Data science courses and tutorials have grown popular in recent years, yet they are still taught using production-grade programming tools (e.g., R, MATLAB, and Python IDEs) within desktop computing environments. Although powerful, these tools present high barriers to entry for novices, forcing them to grapple with the extrinsic complexities of software installation and configuration, data file management, data parsing, and Unix-like command-line interfaces. To lower the barrier for novices to get started with learning data science, we created DS.js, a bookmarklet that embeds a data science programming environment directly into any existing webpage. By transforming any webpage into an example-centric IDE, DS.js eliminates the aforementioned complexities of desktop-based environments and turns the entire web into a rich substrate for learning data science. DS.js automatically parses HTML tables and CSV/TSV data sets on the target webpage, attaches code editors to each data set, provides a data table manipulation and visualization API designed for novices, and gives instructional scaffolding in the form of bidirectional previews of how the user's code and data relate.\n",
      "=============================\n",
      "Grouping Applications Using Geometrical Information of Applications on Tabletop Systems\n",
      "In this paper, we propose a grouping scheme that classifies applications into groups for individual users by utilizing their geometrical information on a tabletop system. The proposed scheme investigates the geometrical information of each application, such as its position on the display and its rotational information, and then groups the applications of each individual user by utilizing a classifier with the geometrical information. We evaluate the proposed scheme with lab experiments, and the results show that, on average, 95.6% of applications are well classified into their users.\n",
      "=============================\n",
      "A Modular Smartphone for Lending\n",
      "We motivate, design, and prototype a modular smartphone designed to make temporary device lending trustworthy and convenient. The concept is that the phone can be separated into pieces, so a child, friend, or even stranger can begin an access-controlled interaction with one piece, while the own-er retains another piece to continue their tasks and monitor activity. This is grounded in a survey capturing attitudes towards device lending, and an exploratory study probing how people might lend pieces of different kinds of modular smartphones. Design considerations are generated for a hardware form factor and software interface to support different lending scenarios. A functional prototype combining three smartphones into a single modular device is described and used to demonstrate a lending interaction design. A usability test validates the concept using the prototype.\n",
      "=============================\n",
      "NaviFields: Relevance fields for adaptive VR navigation\n",
      "Virtual Reality allow users to explore virtual environments naturally, by moving their head and body. However, the size of the environments they can explore is limited by real world constraints, such as the tracking technology or the physical space available. Existing techniques removing these limitations often break the metaphor of natural navigation in VR (e.g. steering techniques), involve control commands (e.g., teleporting) or hinder precise navigation (e.g., scaling user's displacements). This paper proposes NaviFields, which quantify the requirements for precise navigation of each point of the environment, allowing natural navigation within relevant areas, while scaling users' displacements when travelling across non-relevant spaces. This expands the size of the navigable space, retains the natural navigation metaphor and still allows for areas with precise control of the virtual head. We present a formal description of our NaviFields technique, which we compared against two alternative solutions (i.e., homogeneous scaling and natural navigation). Our results demonstrate our ability to cover larger spaces, introduce minimal disruption when travelling across bigger distances and improve very significantly the precise control of the viewpoint inside relevant areas.\n",
      "=============================\n",
      "Pyro: Thumb-Tip Gesture Recognition Using Pyroelectric Infrared Sensing\n",
      "We present Pyro, a micro thumb-tip gesture recognition technique based on thermal infrared signals radiating from the fingers. Pyro uses a compact, low-power passive sensor, making it suitable for wearable and mobile applications. To demonstrate the feasibility of Pyro, we developed a self-contained prototype consisting of the infrared pyroelectric sensor, a custom sensing circuit, and software for signal processing and machine learning. A ten-participant user study yielded a 93.9% cross-validation accuracy and 84.9% leave-one-session-out accuracy on six thumb-tip gestures. Subsequent lab studies demonstrated Pyro's robustness to varying light conditions, hand temperatures, and background motion. We conclude by discussing the insights we gained from this work and future research questions.\n",
      "=============================\n",
      "CurrentViz: Sensing and Visualizing Electric Current Flows of Breadboarded Circuits\n",
      "Electric current and voltage are fundamental to learning, understanding, and debugging circuits. Although both can be measured using tools such as multimeters and oscilloscopes, electric current is much more difficult to measure because users have to unplug parts of a circuit and then insert the measuring tools in serial. Furthermore, users need to restore the circuits back to its original state after measurements have been taken. In practice, this cumbersome process poses a formidable barrier to knowing how current flows throughout a circuit. We present CurrentViz, a system that can sense and visualize the electric current flowing through a circuit, which helps users quickly understand otherwise invisible circuit behavior. It supports fully automatic, ubiquitous, and real-time collection of amperage information of breadboarded circuits. It also supports visualization of the amperage data on a circuit schematic to provide an intuitive view into the current state of a circuit.\n",
      "=============================\n",
      "ImAxes: Immersive Axes as Embodied Affordances for Interactive Multivariate Data Visualisation\n",
      "We introduce ImAxes immersive system for exploring multivariate data using fluid, modeless interaction. The basic interface element is an embodied data axis. The user can manipulate these axes like physical objects in the immersive environment and combine them into sophisticated visualisations. The type of visualisation that appears depends on the proximity and relative orientation of the axes with respect to one another, which we describe with a formal grammar. This straight-forward composability leads to a number of emergent visualisations and interactions, which we review, and then demonstrate with a detailed multivariate data analysis use case.\n",
      "=============================\n",
      "SmartSleeve: Real-time Sensing of Surface and Deformation Gestures on Flexible, Interactive Textiles, using a Hybrid Gesture Detection Pipeline\n",
      "Over the last decades, there have been numerous efforts in wearable computing research to enable interactive textiles. Most work focus, however, on integrating sensors for planar touch gestures, and thus do not fully take advantage of the flexible, deformable and tangible material properties of textile. In this work, we introduce SmartSleeve, a deformable textile sensor, which can sense both surface and deformation gestures in real-time. It expands the gesture vocabulary with a range of expressive interaction techniques, and we explore new opportunities using advanced deformation gestures, such as, Twirl, Twist, Fold, Push and Stretch. We describe our sensor design, hardware implementation and its novel non-rigid connector architecture. We provide a detailed description of our hybrid gesture detection pipeline that uses learning-based algorithms and heuristics to enable real-time gesture detection and tracking. Its modular architecture allows us to derive new gestures through the combination with continuous properties like pressure, location, and direction. Finally, we report on the promising results from our evaluations which demonstrate real-time classification.\n",
      "=============================\n",
      "SensIR: Detecting Hand Gestures with a Wearable Bracelet using Infrared Transmission and Reflection\n",
      "Gestures have become an important tool for natural interaction with computers and thus several wearables have been developed to detect hand gestures. However, many existing solutions are unsuitable for practical use due to low accuracy, high cost or poor ergonomics. We present SensIR, a bracelet that uses near-infrared sensing to infer hand gestures. The bracelet is composed of pairs of infrared emitters and receivers that are used to measure both the transmission and reflection of light through/off the wrist. SensIR improves the accuracy of existing infrared gesture sensing systems through the key idea of taking measurements with all possible combinations of emitters and receivers. Our study shows that SensIR is capable of detecting 12 discrete gestures with 93.3% accuracy. SensIR has several advantages compared to other systems such as high accuracy, low cost, robustness against bad skin coupling and thin form-factor.\n",
      "=============================\n",
      "More than a Feeling: The MiFace Framework for Defining Facial Communication Mappings\n",
      "Facial expressions transmit a variety of social, grammatical, and affective signals. For technology to leverage this rich source of communication, tools that better model the breadth of information they convey are required. MiFace is a novel framework for creating expression lexicons that map signal values to parameterized facial muscle movements. In traditional mapping paradigms using posed photographs, naïve judges select from predetermined label sets and movements are inferred by trained experts. The set of generally accepted expressions established in this way is limited to six basic displays of affect. In contrast, our approach generatively simulates muscle movements on a 3D avatar. By applying natural language processing techniques to crowdsourced free-response labels for the resulting images, we efficiently converge on an expression's value across signal categories. Two studies returned 218 discriminable facial expressions with 51 unique labels. The six basic emotions are included, but we additionally define such nuanced expressions as embarrassed, curious, and hopeful.\n",
      "=============================\n",
      "AccelTag: A Passive Smart ID Tag with Acceleration Sensor for Interactive Applications\n",
      "There are many everyday situations in which users need to enter their user identification (user ID), such as logging in to computer systems and entering secure offices. In such situations, contactless passive IC cards are convenient because users can input their user ID simply by passing the card over a reader. However, these cards cannot be used for successive interactions. To address this issue, we propose AccelTag, a contactless IC card equipped with an acceleration sensor and a liquid crystal display (LCD). AccelTag utilizes high-function RFID technology so that the acceleration sensor and the LCD can also be driven by a wireless power supply. With its built-in acceleration sensor, AccelTag can acquire its direction and movement when it is waved over the reader. We demonstrate several applications using AccelTag, such as displaying several types of information in the card depending on the user's requirements.\n",
      "=============================\n",
      "Designing 3D-Printed Deformation Behaviors Using Spring-Based Structures: An Initial Investigation\n",
      "Recent work in 3D printing has focused on tools and techniques to design deformation behaviors using mechanical structures such as joints and metamaterials. In this poster, we explore how to embed and control mechanical springs to create deformable 3D-printed objects. We propose an initial design space of 3D-printable spring-based structures to support a wide range of expressive behaviors, including stretch and compress, bend, twist, and all possible combinations. The poster concludes with a brief feasibility test and enumerates future work.\n",
      "=============================\n",
      "Auditory Overview of Web Pages for Screen Reader Users\n",
      "Blind users browse the web using screen readers. Screen readers read the content on a web page sequentially via synthesized speech. The linear nature of this process makes it difficult to obtain an overview of the web page, which creates navigation challenges. To alleviate this problem, we have developed ScreenTrack, a browser extension that summarizes a web page's accessibility features into a short, dynamically generated soundtrack. Users can quickly gain an overview of the presence of web elements useful for navigation on a web page. Here we describe ScreenTrack and discuss future research plans.\n",
      "=============================\n",
      "Carpacio: Repurposing Capacitive Sensors to Distinguish Driver and Passenger Touches on In-Vehicle Screens\n",
      "Standard vehicle infotainment systems often include touch screens that allow the driver to control their mobile phone, navigation, audio, and vehicle configurations. For the driver's safety, these interfaces are often disabled or simplified while the car is in motion. Although this reduced functionality aids in reducing distraction for the driver, it also disrupts the usability of infotainment systems for passengers. Current infotainment systems are unaware of the seating position of their user and hence, cannot adapt. We present Carpacio, a system that takes advantage of the capacitive coupling created between the touchscreen and the electrode present in the seat when the user touches the capacitive screen. Using this capacitive coupling phenomenon, a car infotainment system can intelligently distinguish who is interacting with the screen seamlessly, and adjust its user interface accordingly. Manufacturers can easily incorporate Carpacio into vehicles since the included seat occupancy detection sensor or seat heating coils can be used as the seat electrode. We evaluated Carpacio in eight different cars and five mobile devices and found that it correctly detected over 2600 touches with an accuracy of 99.4%.\n",
      "=============================\n",
      "MagTics: Flexible and Thin Form Factor Magnetic Actuators for Dynamic and Wearable Haptic Feedback\n",
      "We present MagTics, a novel flexible and wearable haptic interface based on magnetically actuated bidirectional tactile pixels (taxels). MagTics' thin form factor and flexibility allows for rich haptic feedback in mobile settings. We propose a novel actuation mechanism based on bistable electromagnetic latching that combines high frame rate and holding force with low energy consumption and a soft and flexible form factor. We overcome limitations of traditional soft actuators by placing several hard actuation cells, driven by flexible printed electronics, in a soft 3D printed case. A novel EM-shielding prevents magnet-magnet interactions and allows for high actuator densities. A prototypical implementation comprising of 4 actuated pins on a 1.7 cm pitch, with 2 mm travel, and generating 160 mN to 200 mN of latching force is used to implement a number of compelling application scenarios including adding haptic and tactile display capabilities to wearable devices, to existing input devices and to provide localized haptic feedback in virtual reality. Finally, we report results of a psychophysical study, conducted to inform future developments and to identify possible application domains.\n",
      "=============================\n",
      "HapticClench: Investigating Squeeze Sensations using Memory Alloys\n",
      "Squeezing sensations are one of the most common and intimate forms of human contact. In this paper, we investigate HapticClench, a device that generates squeezing sensations using shape memory alloys. We define squeezing feedback in terms of it perceptual properties and conduct a psychophysical evaluation of HapticClench. HapticClench is capable of generating up to four levels of distinguishable load and works well in distracted scenarios. HapticClench has a high spatial acuity and can generate spatial patterns on the wrist that the user can accurately recognize. We also demonstrate the use of HapticClench for communicating gradual progress of an activity, and for generating squeezing sensations using rings and loose bracelets.\n",
      "=============================\n",
      "Attaching Objects to Smartphones Back Side for a Modular Interface\n",
      "This paper proposes a new approach to enhancing interaction with general applications on smartphones. Physical objects held on back surface of the smartphone, which can be captured by the rear camera with a mirror, work as input devices or controllers. It does not require any additional electronic devices but offers tactile feedback. The occlusion problem does not occur when using smartphone's back side, in terms of both display and camera viewing. We implemented on an Android smartphone and confirmed that it provides richer interaction and low latency (100 ms).\n",
      "=============================\n",
      "Sonoliards: Rendering Audible Sound Spots by Reflecting the Ultrasound Beams\n",
      "This paper proposes a dynamic acoustic field generation system for a spot audio towards particular person indoors. Spot audio techniques have been explored by generating the ultrasound beams toward the target person in certain area however everyone in this area can hear the sound. Our system recognizes the positions of each person indoor using motion capture and 3D model data of the room. After that we control direction of parametric speaker in real-time so that sound reach only particular person by calculating the reflection of sound on surfaces such as wall and ceiling. We calculate direction of parametric speaker using a beam tracing method. We present generating methods of dynamic acoustic field in our system and conducted the experiments on human factor to evaluate performance of proposed system.\n",
      "=============================\n",
      "ZIPT: Zero-Integration Performance Testing of Mobile App Designs\n",
      "To evaluate the performance of mobile app designs, designers and researchers employ techniques such as A/B, usability, and analytics-driven testing. While these are all useful strategies for evaluating known designs, comparing many divergent solutions to identify the most performant remains a costly and difficult problem. This paper introduces a design performance testing approach that leverages existing app implementations and crowd workers to enable comparative testing at scale. This approach is manifest in ZIPT, a zero-integration performance testing platform that allows designers to collect detailed design and interaction data over any Android app -- including apps they do not own and did not build. Designers can deploy scripted tests via ZIPT to collect aggregate user performance metrics (e.g., completion rate, time on task) and qualitative feedback over third-party apps. Through case studies, we demonstrate that designers can use ZIPT's aggregate data and visualizations to understand the relative performance of interaction patterns found in the wild, and identify usability issues in existing Android apps.\n",
      "=============================\n",
      "RetroShape: Leveraging Rear-Surface Shape Displays for 2.5D Interaction on Smartwatches\n",
      "The small screen size of a smartwatch limits user experience when watching or interacting with media. We propose a supplementary tactile feedback system to enhance the user experience with a method unique to the smartwatch form factor. Our system has a deformable surface on the back of the watch face, allowing the visual scene on screen to extend into 2.5D physical space. This allows the user to watch and feel virtual objects, such as experiencing a ball bouncing against the wrist. We devised two controlled experiments to analyze the influence of tactile display resolution on the illusion of virtual object presence. Our first study revealed that on average, a taxel can render virtual objects between 70% and 138% of its own size without shattering the illusion. From the second study, we found visual and haptic feedback can be separated by 4.5mm to 16.2mm for the tested taxels. Based on the results, we developed a prototype (called RetroShape) with 4×4 10mm taxels using micro servo motors, and demonstrated its unique capability through a set of tactile-enhanced games and videos. A preliminary user evaluation showed that participants welcome RetroShape as a useful addition to existing smartwatch output.\n",
      "=============================\n",
      "iSoft: A Customizable Soft Sensor with Real-time Continuous Contact and Stretching Sensing\n",
      "We present iSoft, a single volume soft sensor capable of sensing real-time continuous contact and unidirectional stretching. We propose a low-cost and an easy way to fabricate such piezoresistive elastomer-based soft sensors for instant interactions. We employ an electrical impedance tomography (EIT) technique to estimate changes of resistance distribution on the sensor caused by fingertip contact. To compensate for the rebound elasticity of the elastomer and achieve real-time continuous contact sensing, we apply a dynamic baseline update for EIT. The baseline updates are triggered by fingertip contact and movement detections. Further, we support unidirectional stretching sensing using a model-based approach which works separately with continuous contact sensing. We also provide a software toolkit for users to design and deploy personalized interfaces with customized sensors. Through a series of experiments and evaluations, we validate the performance of contact and stretching sensing. Through example applications, we show the variety of examples enabled by iSoft.\n",
      "=============================\n",
      "EYE DEAR: Smartphone Text Resizing Interaction for the Eye Health of the Presbyopia Population\n",
      "Presbyopia is a symptom in which the elasticity of the lens is weakened and the image is not formed. However, as the use of smart phones increases, the age at which presbyopia symptoms appear is gradually decreasing. The closer the distance is from the smartphone, the less the focus of the eyes will be which making the letters and pictures on the smartphone screen appear blurred. In this study, we conducted a study on the interactions that helped to improve health of eye for people with presbyopia or those who have a habit that can facilitating presbyopia. As the distance of the smartphone from the eye is increased, the font size is increased to upgrading readability and the prototype is tested by 20 experimenters.\n",
      "=============================\n",
      "Demonstrating Interactive Systems based on Electrical Muscle Stimulation\n",
      "We provide a hands-on demonstration of the potential of interactive systems based on electrical muscle stimulation (EMS). These wearable devices allow attendees, for example, to physically learn how to manipulate objects they never seen before, feel walls and forces in virtual reality, and so forth. In our demo we plan to not only demonstrate several of these EMS-based prototypes but also to provide instructions and free hardware for people to conduct their first projects using EMS.\n",
      "=============================\n",
      "Scaling Notifications Beyond Alerts: From Subtly Drawing Attention up to Forcing the User to Take Action\n",
      "Research has been done in sophisticated notifications, still, devices today mainly stick to a binary level of information, while they are either attention drawing or silent. We propose scalable notifications, which adjust the intensity level reaching from subtle to obtrusive and even going beyond that level while forcing the user to take action. To illustrate the technical feasibility and validity of this concept, we developed three prototypes. The prototypes provided mechano-pressure, thermal, and electrical feedback, which were evaluated in different lab studies. Our first prototype provides subtle poking through to high and frequent pressure on the user's spine, which significantly improves back posture. In a second scenario, the user is able to perceive the overuse of a drill by an increased temperature on the palm of a hand until the heat is intolerable, forcing the user to eventually put down the tool. The last application comprises of a speed control in a driving simulation, while electric muscle stimulation on the users' legs, conveys information on changing the car's speed by a perceived tingling until the system forces the foot to move involuntarily. In conclusion, all studies' findings support the feasibility of our concept of a scalable notification system, including the system forcing an intervention.\n",
      "=============================\n",
      "SoundBender: Dynamic Acoustic Control Behind Obstacles\n",
      "Ultrasound manipulation is growing in popularity in the HCI community with applications in haptics, on-body interaction, and levitation-based displays. Most of these applications share two key limitations: a) the complexity of the sound fields that can be produced is limited by the physical size of the transducers, and b) no obstacles can be present between the transducers and the control point. We present SoundBender, a hybrid system that overcomes these limitations by combining the versatility of phased arrays of Transducers (PATs) with the precision of acoustic metamaterials. In this paper, we explain our approach to design and implement such hybrid modulators (i.e. to create complex sound fields) and methods to manipulate the field dynamically (i.e. stretch, steer). We demonstrate our concept using self-bending beams enabling both levitation and tactile feedback around an obstacle and present example applications enabled by SoundBender.\n",
      "=============================\n",
      "MoSculp: Interactive Visualization of Shape and Time\n",
      "We present a system that visualizes complex human motion via 3D motion sculptures-a representation that conveys the 3D structure swept by a human body as it moves through space. Our system computes a motion sculpture from an input video, and then embeds it back into the scene in a 3D-aware fashion. The user may also explore the sculpture directly in 3D or physically print it. Our interactive interface allows users to customize the sculpture design, for example, by selecting materials and lighting conditions. To provide this end-to-end workflow, we introduce an algorithm that estimates a human's 3D geometry over time from a set of 2D images, and develop a 3D-aware image-based rendering approach that inserts the sculpture back into the original video. By automating the process, our system takes motion sculpture creation out of the realm of professional artists, and makes it applicable to a wide range of existing video material. By conveying 3D information to users, motion sculptures reveal space-time motion information that is difficult to perceive with the naked eye, and allow viewers to interpret how different parts of the object interact over time. We validate the effectiveness of motion sculptures with user studies, finding that our visualizations are more informative about motion than existing stroboscopic and space-time visualization methods.\n",
      "=============================\n",
      "Shape-Aware Material: Interactive Fabrication with ShapeMe\n",
      "Makers often create both physical and digital prototypes to explore a design, taking advantage of the subtle feel of physical materials and the precision and power of digital models. We introduce ShapeMe, a novel smart material that captures its own geometry as it is physically cut by an artist or designer. ShapeMe includes a software toolkit that lets its users generate customized, embeddable sensors that can accommodate various object shapes. As the designer works on a physical prototype, the toolkit streams the artist's physical changes to its digital counterpart in a 3D CAD environment. We use a rapid, inexpensive and simple-to-manufacture inkjet printing technique to create embedded sensors. We successfully created a linear predictive model of the sensors' lengths, and our empirical tests of ShapeMe show an average accuracy of 2 to 3 mm. We present two application scenarios for modeling multi-object constructions, such as architectural models, and 3D models consisting of multiple layers stacked one on top of each other. ShapeMe demonstrates a novel technique for integrating digital and physical modeling, and suggests new possibilities for creating shape-aware materials.\n",
      "=============================\n",
      "FingerArc and FingerChord: Supporting Novice to Expert Transitions with Guided Finger-Aware Shortcuts\n",
      "Keyboard shortcuts can be more efficient than graphical input, but they are underused by most users. To alleviate this, we present \"Guided Finger-Aware Shortcuts\" to reduce the gulf between graphical input and shortcut activation. The interaction technique works by recognising when a special hand posture is used to press a key, then allowing secondary finger movements to select among related shortcuts if desired. Novice users can learn the mappings through dynamic visual guidance revealed by holding a key down, but experts can trigger shortcuts directly without pausing. Two variations are described: FingerArc uses the angle of the thumb, and FingerChord uses a second key press. The techniques are motivated by an interview study identifying factors hindering the learning, use, and exploration of keyboard shortcuts. A controlled comparison with conventional keyboard shortcuts shows the techniques encourage overall shortcut usage, make interaction faster, less error-prone, and provide advantages over simply adding visual guidance to standard shortcuts.\n",
      "=============================\n",
      "AmbientLetter: Letter Presentation Method for Discreet Notification of Unknown Spelling when Handwriting\n",
      "We propose a technique to support writing activity in a confidential manner with a pen-based device. Autocorrect and predictive conversion do not work when writing by hand, and looking up unknown spelling is sometimes embarrassing. Therefore, we propose AmbientLetter which seamlessly and discretely presents the forgotten spelling to the user in scenarios where handwriting is necessary. In this work, we describe the system structure and the technique used to conceal the user\"s getting the information.\n",
      "=============================\n",
      "Mixed-Reality for Object-Focused Remote Collaboration\n",
      "In this paper we outline the design of a mixed-reality system to support object-focused remote collaboration. Here, being able to adjust collaborators' perspectives on the object as well as understand one another's perspective is essential to support effective collaboration over distance. We propose a low-cost mixed-reality system that allows users to: (1) quickly align and understand each other's perspective; (2) explore objects independently from one another, and (3) render gestures in the remote's workspace. In this work, we focus on the expert's role and we introduce an interaction technique allowing users to quickly manipulation 3D virtual objects in space.\n",
      "=============================\n",
      "Juggling 4.0: Learning Complex Motor Skills with Augmented Reality Through the Example of Juggling\n",
      "Learning new motor skills is a problem that people are constantly confronted with (e.g. to learn a new kind of sport). In our work, we investigate to which extent the learning process of a motor sequence can be optimized with the help of Augmented Reality as a technical assistant. Therefore, we propose an approach that divides the problem into three tasks: (1) the tracking of the necessary movements, (2) the creation of a model that calculates possible deviations and (3) the implementation of a visual feedback system. To evaluate our approach, we implemented the idea by using infrared depth sensors and an Augmented Reality head-mounted device (HoloLens). Our results show that the system can give an efficient assistance for the correct height of a throw with one ball. Furthermore, it provides a basis for the support of a complete juggling sequence.\n",
      "=============================\n",
      "Fostering Design Process of Shape-Changing Interfaces\n",
      "Shape-changing interfaces match forms and haptics with functions and bring affordances to devices. I believe that shape-changing interfaces will be increasingly available to end-users in the future. To increase acceptance of shape-changing interfaces by end-users, we need to provide designers with design criteria and framework closely grounded on their current skills and needs. Also, we need to provide them with prototyping tools to enable quick assessment of ideas in the physical world. In this paper, I introduce the three threads of my Ph.D. research in the direction of providing the design tools. First, I advance existing shape-changing interface taxonomies to broaden design vocabulary and systemize design framework, based on the classification of everyday objects. Second, I conduct a study with end-users to suggest interaction techniques and design guidelines for shape-changing interfaces from their current practice. Lastly, I develop a physical prototyping tool for shape-changing interfaces to shorten prototyping iterations based on well-known Lego-like bricks.\n",
      "=============================\n",
      "Screen-Camera Communication via Matrix Barcode Utilizing Imperceptible Color Vibration\n",
      "Communication between screens and cameras has attracted attention as a ubiquitous information source, motivated by the widespread use of smartphones and the increase of public advertising and information screens. We propose embedding matrix barcodes into images projected on displays by utilizing imperceptible color vibration. This approach maintains the visual experience as the barcodes are imperceptible and can be implemented on almost any display and camera for the technology to be pervasive. In fact, the color vibration can be generated by ordinary 60 Hz LCDs and captured by 120 fps smartphone cameras. To illustrate the technology capabilities, we present scenarios of potential practical applications.\n",
      "=============================\n",
      "Hybrid Watch User Interfaces: Collaboration Between Electro-Mechanical Components and Analog Materials\n",
      "We introduce programmable material and electro-mechanical control to enable a set of hybrid watch user interfaces that symbiotically leverage the joint strengths of electro-mechanical hands and a dynamic watch dial. This approach enables computation and connectivity with existing materials to preserve the inherent physical qualities and abilities of traditional analog watches. We augment the watch's mechanical hands with micro-stepper motors for control, positioning and mechanical expressivity. We extend the traditional watch dial with programmable pigments for non-emissive dynamic patterns. Together, these components enable a unique set of interaction techniques and user interfaces beyond their individual capabilities.\n",
      "=============================\n",
      "Enabling Single-Handed Interaction in Mobile and Wearable Computing\n",
      "Mobile and wearable computing are increasingly pervasive as people carry and use personal devices in everyday life. Screen sizes of such devices are becoming larger and smaller to accommodate both intimate and practical uses. Some mobile device screens are becoming larger to accommodate new experiences (e.g., phablet, tablet, eReader), whereas screen sizes on wearable devices are becoming smaller to allow them to fit into more places (e.g., smartwatch, wrist-band and eye-wear). However, these trends are making it difficult to use such devices with only one hand due to their placement, limited thumb reach and the fat-finger problem. This is especially true as there are many occasions when a user's other hand is occupied (encumbered) or not available. This thesis work explores, creates and studies novel interaction techniques that enable effective single-hand usage on mobile and wearable devices, empowering users to achieve more with their smart devices when only one hand is available.\n",
      "=============================\n",
      "OmniEyeball: Spherical Display Equipped With Omnidirectional Camera And Its Application For 360-Degree Video Communication\n",
      "We propose OmniEyeball (OEB), which is a novel interactive 360° image I/O system. It integrates the spherical display system with an omnidirectional camera to enable both capturing the 360° panoramic live streaming video as well as displaying it. We also present its unique application for symmetric 360° video communication by utilizing two OEB terminals, which may solve the narrow field-of-view problem in video communication. In addition, we designed a vision-based touch detection technique as well as some features to support 360° video communication.\n",
      "=============================\n",
      "resources2city Explorer: A System for Generating Interactive Walkable Virtual Cities out of File Systems\n",
      "We present resources2city Explorer (R2CE), a tool for representing file systems as interactive, walkable virtual cities. R2CE visualizes file systems based on concepts of spatial, 3D information processing. For this purpose, it extends the range of functions of conventional file browsers considerably. Visual elements in a city generated by R2CE represent (relations of) objects of the underlying file system. The paper describes the functional spectrum of R2CE and illustrates it by visualizing a sample of 940 files.\n",
      "=============================\n",
      "Scout: Mixed-Initiative Exploration of Design Variations through High-Level Design Constraints\n",
      "Although the exploration of variations is a key part of interface design, current processes for creating variations are mostly manual. We present Scout, a system that helps designers explore many variations rapidly through mixed-initiative interaction with high-level constraints and design feedback. Past constraint-based layout systems use low-level spatial constraints and mostly produce only a single design. Scout advances upon these systems by introducing high-level constraints based on design concepts (e.g. emphasis). With Scout, we have formalized several high-level constraints into their corresponding low-level spatial constraints to enable rapidly generating many designs through constraint solving and program synthesis.\n",
      "=============================\n",
      "DynamicSlide: Reference-based Interaction Techniques for Slide-based Lecture Videos\n",
      "Presentation slides play an important role in online lecture videos. Slides convey the main points of the lecture visually, while the instructor's narration adds detailed verbal explanations to each item in the slide. We call the link between a slide item and the corresponding part of the narration a reference. In order to assess the feasibility of reference-based interaction techniques for watching videos, we introduce DynamicSlide, a video processing system that automatically extracts references from slide-based lecture videos and a video player. The system incorporates a set of reference-based techniques: emphasizing the current item in the slide that is being explained, enabling item-based navigation, and enabling item-based note-taking. Our pipeline correctly finds 79% of the references in a set of five videos with 141 references. Results from a user study suggest that DynamicSlide's features improve the learner's video browsing and navigation experience.\n",
      "=============================\n",
      "Fusion: Opportunistic Web Prototyping with UI Mashups\n",
      "Modern web development is rife with complexity at all layers, ranging from needing to configure backend services to grappling with frontend frameworks and dependencies. To lower these development barriers, we introduce a technique that enables people to prototype opportunistically by borrowing pieces of desired functionality from across the web without needing any access to their underlying codebases, build environments, or server backends. We implemented this technique in a browser extension called Fusion, which lets users create web UI mashups by extracting components from existing unmodified webpages and hooking them together using transclusion and JavaScript glue code. We demonstrate the generality and versatility of Fusion via a case study where we used it to create seven UI mashups in domains such as programming tools, data science, web design, and collaborative work. Our mashups include replicating portions of prior HCI systems (Blueprint for in-situ code search and DS.js for in-browser data science), extending the p5.js IDE for Processing with real-time collaborative editing, and integrating Python Tutor code visualizations into static tutorials. These UI mashups each took less than 15 lines of JavaScript glue code to create with Fusion.\n",
      "=============================\n",
      "Believe it or not: Designing a Human-AI Partnership for Mixed-Initiative Fact-Checking\n",
      "Fact-checking, the task of assessing the veracity of claims, is an important, timely, and challenging problem. While many automated fact-checking systems have been recently proposed, the human side of the partnership has been largely neglected: how might people understand, interact with, and establish trust with an AI fact-checking system? Does such a system actually help people better assess the factuality of claims? In this paper, we present the design and evaluation of a mixed-initiative approach to fact-checking, blending human knowledge and experience with the efficiency and scalability of automated information retrieval and ML. In a user study in which participants used our system to aid their own assessment of claims, our results suggest that individuals tend to trust the system: participant accuracy assessing claims improved when exposed to correct model predictions. However, this trust perhaps goes too far: when the model was wrong, exposure to its predictions often degraded human accuracy. Participants given the option to interact with these incorrect predictions were often able improve their own performance. This suggests that transparent models are key to facilitating effective human interaction with fallible AI models.\n",
      "=============================\n",
      "A Demonstration of VRSpinning: Exploring the Design Space of a 1D Rotation Platform to Increase the Perception of Self-Motion in VR\n",
      "In this demonstration we introduce VRSpinning, a seated locomotion approach based around stimulating the user's vestibular system using a rotational impulse to induce the perception of linear self-motion. Currently, most approaches for locomotion in VR use either concepts like teleportation for traveling longer distances or present a virtual motion that creates a visual-vestibular conflict, which is assumed to cause simulator sickness. With our platform we evaluated two designs for using the rotation of a motorized swivel chair to alleviate this, wiggle and impulse. Our evaluation showed that impulse, using short rotation bursts matched with the visual acceleration, can significantly reduce simulator sickness and increase the perception of self-motion compared to no physical motion.\n",
      "=============================\n",
      "Assembly-aware Design of Printable Electromechanical Devices\n",
      "From smart toys and household appliances to personal robots, electromechanical devices play an increasingly important role in our daily lives. Rather than relying on gadgets that are mass-produced, our goal is to enable casual users to custom-design such devices based on their own needs and preferences. To this end, we present a computational design system that leverages the power of digital fabrication and the emergence of affordable electronics such as sensors and microcontrollers. The input to our system consists of a 3D representation of the desired device's shape, and a set of user-preferred off-the-shelf components. Based on this input, our method generates an optimized, 3D printable enclosure that can house the required components. To create these designs automatically, we formalize a new spatio-temporal model that captures the entire assembly process, including the placement of the components within the device, mounting structures and attachment strategies, the order in which components must be inserted, and collision-free assembly paths. Using this model as a technical core, we then leverage engineering design guidelines and efficient numerical techniques to optimize device designs. In a user study, which also highlights the challenges of designing such devices, we find our system to be effective in reducing the entry barriers faced by casual users in creating such devices. We further demonstrate the versatility of our approach by designing and fabricating three devices with diverse functionalities.\n",
      "=============================\n",
      "DroneCTRL: A Tangible Remote Input Control for Quadcopters\n",
      "Recent research has presented quadcopters to enable mid-air interaction. Using quadcopters to provide tactile feedback, navigation, or user input are the current scope of related work. However, most quadcopter steering systems are complicated to use for non-expert users or require an expensive tracking system for autonomous flying. Safety-critical scenarios require trained and expensive personnel to navigate quadcopters through crucial flight paths within narrow spaces. To simplify the input and manual operation of quadcopters, we present DroneCTRL, a tangible pointing device to navigate quadcopters. DroneCTRL resembles a remote control including optional visual feedback by a laser pointer and tangibility to improve the quadcopter control usability for non-expert users. In a preliminary user study, we compare the efficiency of hardware and software-based controller with DroneCTRL. Our results favor the usage of DroneCTRL with and without visual feedback to achieve more precision and accuracy.\n",
      "=============================\n",
      "D-Aquarium: A Digital Aquarium to Reduce Perceived Waiting Time at Children's Hospital\n",
      "Patients waiting for long to use medical services become more physically and psychologically anxious than do people waiting to use general services. Since children feel more anxiety and fear in a hospital, it is necessary to reduce their perceived waiting time by disturbing their awareness of time and dispersing their attention. We present the D-Aquarium, a computer-based digital aquarium that provides psychological stability to pediatric patients and reduces their perceived waiting time by using distractions to alleviate their psychological anxiety and interfere with their perception of time.\n",
      "=============================\n",
      "Magneto-Haptics: Embedding Magnetic Force Feedback for Physical Interactions\n",
      "We present magneto-haptics, a design approach of haptic sensations powered by the forces present among permanent magnets during active touch. Magnetic force has not been efficiently explored in haptic design because it is not intuitive and there is a lack of methods to associate or visualize magnetic force with haptic sensations, especially for complex magnetic patterns. To represent the haptic sensations of magnetic force intuitively, magneto-haptics formularizes haptic potential from the distribution of magnetic force along the path of motion. It provides a rapid way to compute the relationship between the magnetic phenomena and the haptic mechanism. Thus, we can convert a magnetic force distribution into a haptic sensation model, making the design of magnet-embedded haptic sensations more efficient. We demonstrate three applications of magneto-haptics through interactive interfaces and devices. We further verify our theory by evaluating some magneto-haptic designs through experiments.\n",
      "=============================\n",
      "Crowdsourcing Similarity Judgments for Agreement Analysis in End-User Elicitation Studies\n",
      "End-user elicitation studies are a popular design method, but their data require substantial time and effort to analyze. In this paper, we present Crowdsensus, a crowd-powered tool that enables researchers to efficiently analyze the results of elicitation studies using subjective human judgment and automatic clustering algorithms. In addition to our own analysis, we asked six expert researchers with experience running and analyzing elicitation studies to analyze an end-user elicitation dataset of 10 functions for operating a web-browser, each with 43 voice commands elicited from end-users for a total of 430 voice commands. We used Crowdsensus to gather similarity judgments of these same 430 commands from 410 online crowd workers. The crowd outperformed the experts by arriving at the same results for seven of eight functions and resolving a function where the experts failed to agree. Also, using Crowdsensus was about four times faster than using experts.\n",
      "=============================\n",
      "Perceptual Switch for Gaze Selection\n",
      "One of the main drawbacks of the fixation-based gaze interfaces is that they are unable to distinguish top-down attention (or selection, a gaze with a purpose) from stimulus driven bottom-up attention (or navigation, a stare without any intentions) without time durations or unnatural eye movements. We found that using the bistable image called the Necker's cube as a button user interface (UI) helps to remedy the limitation. When users switch two rivaling percepts of the Necker's cube at will, unique eye movements are triggered and these characteristics can be used to indicate a button press or a selecting action. In this paper, we introduce (1) the cognitive phenomenon called \"percept switch\" for gaze interaction, and (2) propose \"perceptual switch\" or the Necker's cube user interface (UI) which uses \"percept switch\" as the indication of a selection. Our preliminary experiment confirms that perceptual switch can be used to distinguish voluntary gaze selection from random navigation, and discusses that the visual elements of the Necker's cube such as size and biased visual cues could be adjusted for the optimal use of individual users.\n",
      "=============================\n",
      "Blocks-to-CAD: A Cross-Application Bridge from Minecraft to 3D Modeling\n",
      "Learning a new software application can be a challenge, requiring the user to enter a new environment where their existing knowledge and skills do not apply, or worse, work against them. To ease this transition, we propose the idea of cross-application bridges that start with the interface of a familiar application, and gradually change their interaction model, tools, conventions, and appearance to resemble that of an application to be learned. To investigate this idea, we developed Blocks-to-CAD, a cross-application bridge from Minecraft-style games to 3D solid modeling. A user study of our system demonstrated that our modifications to the game did not hurt enjoyment or increase cognitive load, and that players could successfully apply knowledge and skills learned in the game to tasks in a popular 3D solid modeling application. The process of developing Blocks-to-CAD also revealed eight design strategies that can be applied to design cross-application bridges for other applications and domains.\n",
      "=============================\n",
      "TrussFormer: 3D Printing Large Kinetic Structures\n",
      "We present TrussFormer, an integrated end-to-end system that allows users to 3D print large-scale kinetic structures, i.e., structures that involve motion and deal with dynamic forces. TrussFormer builds on TrussFab, from which it inherits the ability to create static large-scale truss structures from 3D printed connectors and PET bottles. TrussFormer adds movement to these structures by placing linear actuators into them: either manually, wrapped in reusable components called assets, or by demonstrating the intended movement. TrussFormer verifies that the resulting structure is mechanically sound and will withstand the dynamic forces resulting from the motion. To fabricate the design, TrussFormer generates the underlying hinge system that can be printed on standard desktop 3D printers. We demonstrate TrussFormer with several example objects, including a 6 legged walking robot and a 4m tall animatronics dinosaur with 5 degrees of freedom.\n",
      "=============================\n",
      "OptRod: Constructing Interactive Surface with Multiple Functions and Flexible Shape by Projected Image\n",
      "In this demonstration, we propose OptRod, constructing interactive surface with multiple functions and flexible shape by projected image. A PC generates images as control signals and projects them to the bottom of OptRods by a projector or LCD. An OptRod receives the light and converts its brightness into a control signal for the attached output device. By using multiple OptRods, the PC can simultaneously operate many output devices without any signal lines. Moreover, we can arrange surfaces of various shapes easily by combining multiple OptRods. OptRod supports various functions by replacing the device unit connected to OptRod.\n",
      "=============================\n",
      "Spacetime: Enabling Fluid Individual and Collaborative Editing in Virtual Reality\n",
      "Virtual Reality enables users to explore content whose physics are only limited by our creativity. Such limitless environments provide us with many opportunities to explore innovative ways to support productivity and collaboration. We present Spacetime, a scene editing tool built from the ground up to explore the novel interaction techniques that empower single user interaction while maintaining fluid multi-user collaboration in immersive virtual environment. We achieve this by introducing three novel interaction concepts: the Container, a new interaction primitive that supports a rich set of object manipulation and environmental navigation techniques, Parallel Objects, which enables parallel manipulation of objects to resolve interaction conflicts and support design workflows, and Avatar Objects, which supports interaction among multiple users while maintaining an individual users' agency. Evaluated by professional Virtual Reality designers, Spacetime supports powerful individual and fluid collaborative workflows.\n",
      "=============================\n",
      "Unlimited Electric Gum: A Piezo-based Electric Taste Apparatus Activated by Chewing\n",
      "Herein, we propose \"unlimited electric gum,\" an electric taste device that will enable users to perceive taste for as long the user is chewing the gum. We developed an in-mouth type novel electric taste-imparting apparatus using a piezoelectric element so that the piezoelectric effect is stimulated by chewing. This enabled the design of a device that does not require cables around a user's lips or batteries in their mouth. In this paper, we introduce this device and report our experimental and exhibition results.\n",
      "=============================\n",
      "Ownershift: Facilitating Overhead Interaction in Virtual Reality with an Ownership-Preserving Hand Space Shift\n",
      "We present Ownershift, an interaction technique for easing overhead manipulation in virtual reality, while preserving the illusion that the virtual hand is the user's own hand. In contrast to previous approaches, this technique does not alter the mapping of the virtual hand position for initial reaching movements towards the target. Instead, the virtual hand space is only shifted gradually if interaction with the overhead target requires an extended amount of time. While users perceive their virtual hand as operating overhead, their physical hand moves gradually to a less strained position at waist level. We evaluated the technique in a user study and show that Ownershift significantly reduces the physical strain of overhead interactions, while only slightly reducing task performance and the sense of body ownership of the virtual hand.\n",
      "=============================\n",
      "Artificial Motion Guidance: an Intuitive Device based on Pneumatic Gel Muscle (PGM)\n",
      "We present a wearable soft exoskeleton sleeve based on PGM. The sleeve consists of 4 PGMs is controlled by a computing system and can actuate 4 different movements (hand extension, flexion, pronation and supination). Depending on how strong the actuation is, the user feels a slight force (haptic feedback) or the hand moves (if the users relaxes the muscles). The paper gives details about the system implementation, the interaction space and some ideas about application scenarios.\n",
      "=============================\n",
      "Haptopus: Haptic VR Experience Using Suction Mechanism Embedded in Head-mounted Display\n",
      "With the spread of VR experiences using HMD, many proposals have been made to improve the experiences by providing tactile information to the fingertips. However, there are problems, such as difficulty attaching and detaching the devices and hindrances to free finger movement. To solve these issues, we developed \"Haptopus,\" which embeds a tactile display in the HMD and presents tactile sensations to the face. In this paper, we conducted a preliminary investigation on the best suction pressure and compared Haptopus to conventional tactile presentation approaches. As a result, we confirmed that Haptopus improves the quality of the VR experience.\n",
      "=============================\n",
      "Vizir: A Domain-Specific Graphical Language for Authoring and Operating Airport Automations\n",
      "Automation is one of the key solutions proposed and adopted by international Air Transport research programs to meet the challenges of increasing air traffic. For automation to be safe and usable, it needs to be suitable to the activity it supports, both when authoring it and when operating it. Here we present Vizir, a Domain-Specific Graphical Language and an Environment for authoring and operating airport automations. We used a participatory-design process with Air Traffic Controllers to gather requirements for Vizir and to design its features. Vizir combines visual interaction-oriented programming constructs with activity-related geographic areas and events. Vizir offers explicit human-control constructs, graphical substrates and means to scale-up with multiple automations. We propose a set of guidelines to inspire designers of similar usable hybrid human-automation systems.\n",
      "=============================\n",
      "Comfortable and Efficient Travel Techniques in VR\n",
      "Locomotion,the most basic interaction in Virtual Environments (VE), enables users to move around the virtual world. Locomotion in Virtual Reality (VR) is a problem which has not been solved completely since existing techniques have a specific set of requirements and limitations. In addition, the uncertainty about the impact that virtual cues have on users perception complicates the development of better locomotion interfaces. A broadly applicable locomotion technique that is easy to use and addresses the issues of presence, cybersickness and fatigue has yet to be developed. Though optical flow and vestibular cues are dominant in navigation, other cues such as auditory, arm feedback, wind, etc. play a role. The proposed research aims to evaluate and improve upon a set of locomotion techniques for different modes of locomotion in virtual scenarios, as well as the transitions between them. The outcome measures of the evaluations of the different scenarios are usefulness for spatial orientation, presence, fatigue, cybersickness and user preference. The envisioned contribution of my thesis is research towards the design of a locomotion technique that is easy to use and addresses the shortcomings of current implementations.\n",
      "=============================\n",
      "Vibrosight: Long-Range Vibrometry for Smart Environment Sensing\n",
      "Smart and responsive environments rely on the ability to detect physical events, such as appliance use and human activities. Currently, to sense these types of events, one must either upgrade to \"smart\" appliances, or attach aftermarket sensors to existing objects. These approaches can be expensive, intrusive and inflexible. In this work, we present Vibrosight, a new approach to sense activities across entire rooms using long-range laser vibrometry. Unlike a microphone, our approach can sense physical vibrations at one specific point, making it robust to interference from other activities and noisy environments. This property enables detection of simultaneous activities, which has proven challenging in prior work. Through a series of evaluations, we show that Vibrosight can offer high accuracies at long range, allowing our sensor to be placed in an inconspicuous location. We also explore a range of additional uses, including data transmission, sensing user input and modes of appliance operation, and detecting human movement and activities on work surfaces.\n",
      "=============================\n",
      "Game Design for Users with Constraint: Exergame for Older Adults with Cognitive Impairment\n",
      "In order to design serious games, attention needs to be paid to the target users. One important application of serious games is the design of games for older adults with dementia. Interfaces and activities in games designed for this group of users should be conducted by considering both the cognitive and physical limitations of these people, which may be challenging. We overcome these challenges by using the advantages of new head mounted display virtual reality (HMD-VR) technology and the knowledge of experts. The results of a preliminary three-week exercise involving participants with dementia shows that our design approach has been successful in achieving an interesting environment and could engage participants in the game.\n",
      "=============================\n",
      "cARe: An Augmented Reality Support System for Dementia Patients\n",
      "Symptoms of progressing dementia like memory loss, impaired executive function and decreasing motivation can gradually undermine instrumental activities of daily living (IADL) such as cooking. Assisting technologies in form of augmented reality (AR) have previously been applied to support cognitively impaired users during IADLs. In most cases, instructions were provided locally via projection or a head-mounted display (HMD) but lacked an incentive mechanism and the flexibility to support a broad range of use-cases. To provide users and therapists with a holistic solution, we propose cARe, a framework that can be easily adapted by therapists to various use-cases without any programming knowledge. Users are then guided through manual processes with localized visual and auditory cues that are rendered by an HMD. Our ongoing user study indicates that users are more comfortable and successful in cooking with cARe as compared to a printed recipe, which promises a more dignified and autonomous living for dementia patients.\n",
      "=============================\n",
      "Designing Groundless Body Channel Communication Systems: Performance and Implications\n",
      "Novel interactions that capacitively couple electromagnetic (EM) fields between devices and the human body are gaining more attention in the human-computer interaction community. One class of these techniques is Body Channel Communication (BCC), a method that overlays physical touch with digital information. Despite the number of published capacitive sensing and communication prototypes, there exists no guideline on how to design such hardware or what are the application limitations and possibilities. Specifically, wearable (groundless) BCC has been proven in the past to be extremely challenging to implement. Additionally, the exact behavior of the human body as an EM-field medium is still not fully understood today. Consequently, the application domain of BCC technology could not be fully explored. This paper addresses this problem. Based on a recently published general purpose wearable BCC system, we first present a thorough evaluation of the impact of various technical parameter choices and an exhaustive channel characterization of the human body as a host for BCC. Second, we discuss the implications of these results for the application design space and present guidelines for future wearable BCC systems and their applications. Third, we point out an important observation of the measurements, namely that BCC can employ the whole body as user interface (and not just hands or feet). We sketch several applications with these novel interaction modalities.\n",
      "=============================\n",
      "Face/On: Actuating the Facial Contact Area of a Head-Mounted Display for Increased Immersion\n",
      "In this demonstration, we introduce Face/On, an embedded feedback device that leverages the contact area between the user's face and a virtual reality (VR) head-mounted display (HMD) to provide rich haptic feedback in virtual environments (VEs). Head-worn haptic feedback devices have been explored in previous work to provide directional cues via grids of actuators and localized feedback on the users' skin. Most of these solutions were immersion breaking due to their encumbering and uncomfortable design and build around a single actuator type, thus limiting the overall fidelity and flexibility of the haptic feedback. We present Face/On, a VR HMD face cushion with three types of discreetly embedded actuators that provide rich haptic feedback without encumbering users with invasive instrumentation on the body. By combining vibro-tactile and thermal feedback with electrical muscle stimulation (EMS), Face/On can simulate a wide range of scenarios and benefit from synergy effects between these feedback types.\n",
      "=============================\n",
      "Rousillon: Scraping Distributed Hierarchical Web Data\n",
      "Programming by Demonstration (PBD) promises to enable data scientists to collect web data. However, in formative interviews with social scientists, we learned that current PBD tools are insufficient for many real-world web scraping tasks. The missing piece is the capability to collect hierarchically-structured data from across many different webpages. We present Rousillon, a programming system for writing complex web automation scripts by demonstration. Users demonstrate how to collect the first row of a 'universal table' view of a hierarchical dataset to teach Rousillon how to collect all rows. To offer this new demonstration model, we developed novel relation selection and generalization algorithms. In a within-subject user study on 15 computer scientists, users can write hierarchical web scrapers 8 times more quickly with Rousillon than with traditional programming.\n",
      "=============================\n",
      "Wearable Kinesthetic I/O Device for Sharing Muscle Compliance\n",
      "In this paper, we present a wearable kinesthetic I/O device, which is able to measure and intervene in multiple muscle activities simultaneously through the same electrodes. The developed system includes an I/O module, capable of measuring the electromyogram (EMG) of four muscle tissues, while applying electrical muscle stimulation (EMS) at the same time. The developed wearable system is configured in a scalable manner for achieving 1) high stimulus frequency (up to 70 Hz), 2) wearable dimensions in which the device can be placed along the limbs, and 3) flexibility of the number of I/O electrodes (up to 32 channels). In a pilot user study, which shared the wrist compliance between two persons, participants were able to recognize the level of their confederate's wrist joint compliance using a 4-point Likert scale. The developed system would benefit a physical therapist and a patient, during hand rehabilitation, using a peg board for sharing their wrist compliance and grip force, which are usually difficult to be observed in a visual contact.\n",
      "=============================\n",
      "reMi: Translating Ambient Sounds of Moment into Tangible and Shareable Memories through Animated Paper\n",
      "We present a tangible memory notebook--reMi--that records the ambient sounds and translates them into a tangible and shareable memory using animated paper. The paper replays the recorded sounds and deforms its shape to generate synchronized motions with the sounds. Computer-mediated communication interfaces have allowed us to share, record and recall memories easily through visual records. However, those digital visual-cues that are trapped behind the device's 2D screen are not the only means to recall a memory we experienced with more than the sense of vision. To develop a new way to store, recall and share a memory, we investigate how tangible motion of a paper that represents sound can enhance the \"reminiscence\".\n",
      "=============================\n",
      "A Stretch-Flexible Textile Multitouch Sensor for User Input on Inflatable Membrane Structures & Non-Planar Surfaces\n",
      "We present a textile sensor, capable of detecting multi-touch and multi-pressure input on non-planar surfaces and demonstrate how such sensors can be fabricated and integrated into pressure stabilized membrane envelopes (i.e. inflatables). Our sensor design is both stretchable and flexible/bendable and can conform to various three-dimensional surface geometries and shape-changing surfaces. We briefly outline an approach for basic signal acquisition from such sensors and how they can be leveraged to measure internal air-pressure of inflatable objects without specialized air-pressure sensors. We further demonstrate how standard electronic circuits can be integrated with malleable inflatable objects without the need for rigid enclosures for mechanical protection.\n",
      "=============================\n",
      "InfiniTouch: Finger-Aware Interaction on Fully Touch Sensitive Smartphones\n",
      "Smartphones are the most successful mobile devices and offer intuitive interaction through touchscreens. Current devices treat all fingers equally and only sense touch contacts on the front of the device. In this paper, we present InfiniTouch, the first system that enables touch input on the whole device surface and identifies the fingers touching the device without external sensors while keeping the form factor of a standard smartphone. We first developed a prototype with capacitive sensors on the front, the back and on three sides. We then conducted a study to train a convolutional neural network that identifies fingers with an accuracy of 95.78% while estimating their position with a mean absolute error of 0.74cm. We demonstrate the usefulness of multiple use cases made possible with InfiniTouch, including finger-aware gestures and finger flexion state as an action modifier.\n",
      "=============================\n",
      "Augmenting Human Hearing Through Interactive Auditory Mediated Reality\n",
      "To filter and shut out an increasingly loud environment, many resort to the use of personal audio technology. They drown out unwanted sounds, by wearing headphones. This uniform interaction with all surrounding sounds can have a negative impact on social relations and situational awareness. Leveraging mediation through smarter headphones, users gain more agency over their sense of hearing: For instance by being able to selectively alter the volume and other features of specific sounds, without losing the ability to add media. In this work, we propose the vision of interactive auditory mediated reality (AMR). To understand users' attitude and requirements, we conducted a week-long event sampling study (n = 12), where users recorded and rated sources (n = 225) which they would like to mute, amplify or turn down. The results indicate that besides muting, a distinct, \"quiet-but-audible\" volume exists. It caters to two requirements at the same time: aesthetics/comfort and information acquisition.\n",
      "=============================\n",
      "Authoring and Verifying Human-Robot Interactions\n",
      "As social agents, robots designed for human interaction must adhere to human social norms. How can we enable designers, engineers, and roboticists to design robot behaviors that adhere to human social norms and do not result in interaction breakdowns? In this paper, we use automated formal-verification methods to facilitate the encoding of appropriate social norms into the interaction design of social robots and the detection of breakdowns and norm violations in order to prevent them. We have developed an authoring environment that utilizes these methods to provide developers of social-robot applications with feedback at design time and evaluated the benefits of their use in reducing such breakdowns and violations in human-robot interactions. Our evaluation with application developers (N=9) shows that the use of formal-verification methods increases designers' ability to identify and contextualize social-norm violations. We discuss the implications of our approach for the future development of tools for effective design of social-robot applications.\n",
      "=============================\n",
      "Robots For Us: Organizational and Community Perspectives on the Collaborative Design of Ubiquitous Robots\n",
      "Robots are expected to become ubiquitous in the near future, working with people in various environments, including homes, schools, hospitals, and offices. As physically and socially interactive technologies, robots present new opportunities for embodied interaction and active as well as passive sensing in these contexts. They have also been shown to psychologically impact individuals, affect group and organizational dynamics, and modify our concepts and experiences of work, care, and social relationships. Designing robots for increasingly ubiquitous everyday use requires understanding how robots are perceived, and can be adopted and supported in open-ended, natural social circumstances. This, in turn, calls for design and evaluation methodologies that go beyond the dyadic and small group interactions in laboratories that have largely been the focus of research in human-robot interaction. In this talk, I will present alternative perspectives on the design and evaluation of socially interactive robotic technologies in real-world contexts, focusing on several case studies of socially assistive robots in eldercare. I will first discuss how older adults make sense of robots for use in their homes, in relation to the broader social contexts in which they live, as part of collaborative design activities, and in the course of month-long implementations of robots in their homes. These in-home studies bring up various issues relating to the types of data older adults and the clinicians who work with them would like to collect, related privacy concerns, impacts on other people in the home, and how robot designs can support the relationships older adults hope to have with and through robots. Secondly, I will explore the institutional and community-based use and design of robots in different eldercare facilities, including a nursing home, a retirement community, and an intergenerational daycare. These studies bring out how robots fit into and affect the institutional and group dynamics of interaction, and also allow us to explore how robots might be envisioned as technologies that can support not only individual, but community-level goals. Through these case studies of robots, as emerging ubiquitous interactive technologies, I will bring out themes that can inform the design and study of pervasive systems more broadly, including collaborative design, the use of data collected during social interactions with and around technologies, related ethical concerns, and the need for incorporating the aims of groups, institutions, and communities in the design of intelligent interactive technologies.\n",
      "=============================\n",
      "Immersive Trip Reports\n",
      "Since the advent of consumer photography, tourists and hikers have made photo records of their trips to share later. Aside from being kept as memories, photo presentations such as slideshows are also shown to others who have not visited the location to try to convey the experience.However, a slideshow alone is limited in conveying the broader spatial context, and thus the feeling of presence in beautiful natural scenery is lost. We address this by presenting the photographs as part of an immersive experience. We introduce an automated pipeline for aligning photographs with a digital terrain model. From this geographic registration, we produce immersive presentations which are viewed either passively as a video, or interactively in virtual reality. Our experimental evaluation verifies that this new mode of presentation successfully conveys the spatial context of the scene and is enjoyable to users.\n",
      "=============================\n",
      "An Interactive Pipeline for Creating Visual Blends\n",
      "Visual blends are an advanced graphic design technique to draw users' attention to a message. They blend together two objects in a way that is novel and useful in conveying a message symbolically. This demo presents an interactive pipeline for creating visual blends that follows the iterative design process. Our pipeline decomposes the process into both computational techniques and human microtasks. It allows users to collaboratively generate visual blends with steps involving brainstorming, synthesis, and iteration. Our demo allows individual users to see how existing visual blends were made, edit or improve existing visual blends, and create new visual blends.\n",
      "=============================\n",
      "Ultra-Low-Power Mode for Screenless Mobile Interaction\n",
      "Smartphones are now a central technology in the daily lives of billions, but it relies on its battery to perform. Battery optimization is thereby a crucial design constraint in any mobile OS and device. However, even with new low-power methods, the ever-growing touchscreen remains the most power-hungry component. We propose an Ultra-Low-Power Mode (ULPM) for mobile devices that allows for touch interaction without visual feedback and exhibits significant power savings of up to 60% while allowing to complete interactive tasks. We demonstrate the effectiveness of the screenless ULPM in text-entry tasks, camera usage, and listening to videos, showing only a small decrease in usability for typical users.\n",
      "=============================\n",
      "Gaze-guided Image Classification for Reflecting Perceptual Class Ambiguity\n",
      "Despite advances in machine learning and deep neural networks, there is still a huge gap between machine and human image understanding. One of the causes is the annotation process used to label training images. In most image categorization tasks, there is a fundamental ambiguity between some image categories and the underlying class probability differs from very obvious cases to ambiguous ones. However, current machine learning systems and applications usually work with discrete annotation processes and the training labels do not reflect this ambiguity. To address this issue, we propose an new image annotation framework where labeling incorporates human gaze behavior. In this framework, gaze behavior is used to predict image labeling difficulty. The image classifier is then trained with sample weights defined by the predicted difficulty. We demonstrate our approach's effectiveness on four-class image classification tasks.\n",
      "=============================\n",
      "CamTrackPoint: Camera-Based Pointing Stick Using Transmitted Light through Finger\n",
      "We present CamTrackPoint, a new input interface that can be controlled by finger gestures captured by front or rear cameras of a mobile device. CamTrackPoint mounts a 3D-printed ring on the camera's bezel, and it senses the movements of the user's finger by tracking the light passed through the finger. The proposed method provides mobile devices with a new input interface that offers physical force feedback like a pointing stick. The cost of our method is low as it needs only a simple ring-shaped part on the camera bezel. Moreover, the ring doesn't disturb the functions of the camera, unless a user uses the interface. We implement a prototype for a smartphone; two CamTrackPoint rings are made for the front and rear cameras. We evaluate its performance and characteristics in an experiment. The proposed technique provides smooth scrolling and would give better game experience on the available smartphone.\n",
      "=============================\n",
      "Pop-up Robotics: Facilitating HRI in Public Spaces\n",
      "Human-Robot Interaction (HRI) research in public spaces often encounters delays and restrictions due to several factors, including the need for sophisticated technology, regulatory approvals, and public or community support. To remedy these concerns, we suggest HRI can apply the core philosophy of Tactical Urbanism, a concept from urban planning, to catalyze HRI in public spaces, provide community feedback and information on the feasibility of future implementations of robots in the public, and also create social impact and forge connections with the community while spreading awareness about robots as a public resource. As a case study, we share tactics used and strategies followed to conduct a pop-up style study of 'A robotic mailbox to support and raise awareness about homelessness.' We discuss benefits and challenges of the pop-up approach and recommend using it to enable the social studies of HRI not only to match but to precede, the fast-paced technological advancement and deployment of robots.\n",
      "=============================\n",
      "Aalto Interface Metrics (AIM): A Service and Codebase for Computational GUI Evaluation\n",
      "Aalto Interface Metrics (AIM) pools several empirically validated models and metrics of user perception and attention into an easy-to-use online service for the evaluation of graphical user interface (GUI) designs. Users input a GUI design via URL, and select from a list of 17 different metrics covering aspects ranging from visual clutter to visual learnability. AIM presents detailed breakdowns, visualizations, and statistical comparisons, enabling designers and practitioners to detect shortcomings and possible improvements. The web service and code repository are available at interfacemetrics.aalto.fi.\n",
      "=============================\n",
      "Touch+Finger: Extending Touch-based User Interface Capabilities with \"Idle\" Finger Gestures in the Air\n",
      "In this paper, we present Touch+Finger, a new interaction technique that augments touch input with multi-finger gestures for rich and expressive interaction. The main idea is that while one finger is engaged in a touch event, a user can leverage the remaining fingers, the \"idle\" fingers, to perform a variety of hand poses or in-air gestures to extend touch-based user interface capabilities. To fully understand the use of these idle fingers, we constructed a design space based on conventional touch gestures (i.e., single- and multi-touch gestures) and inter- action period (i.e., before and during touch). Considering the design space, we investigated the possible movement of the idle fingers and developed a total of 20 Touch+Finger gestures. Using ring-like devices to track the motion of the idle fingers in the air, we evaluated the Touch+Finger gestures on both recognition accuracy and ease of use. They were classified with a recognition accuracy of over 99% and received positive and negative comments from 8 participants. We suggested 8 interaction techniques with Touch+Finger gestures that demonstrate extended touch-based user interface capabilities.\n",
      "=============================\n",
      "ZEUSSS: Zero Energy Ubiquitous Sound Sensing Surface Leveraging Triboelectric Nanogenerator and Analog Backscatter Communication\n",
      "ZEUSSS (Zero Energy Ubiquitous Sound Sensing Surface), allows physical objects and surfaces to be instrumented with a thin, self-sustainable material that provides acoustic sensing and communication capabilities. We have built a prototype ZEUSSS tag using minimal hardware and flexible electronic components, extending our original self-sustaining SATURN microphone with a printed, flexible antenna to support passive communication via analog backscatter. ZEUSSS enables objects to have ubiquitous wire-free battery-free audio based context sensing, interaction, and surveillance capabilities.\n",
      "=============================\n",
      "HydroRing: Supporting Mixed Reality Haptics Using Liquid Flow\n",
      "Current haptic devices are often bulky and rigid, making them unsuitable for ubiquitous interaction and scenarios where the user must also interact with the real world. To address this gap, we propose HydroRing, an unobtrusive, finger-worn device that can provide the tactile sensations of pressure, vibration, and temperature on the fingertip, enabling mixed-reality haptic interactions. Different from previous explorations, HydroRing in active mode delivers sensations using liquid travelling through a thin, flexible latex tube worn across the fingerpad, and has minimal impact on a user's dexterity and their perception of stimuli in passive mode. Two studies evaluated participants' ability to perceive and recognize sensations generated by the device, as well as their ability to perceive physical stimuli while wearing the device. We conclude by exploring several applications leveraging this mixed-reality haptics approach.\n",
      "=============================\n",
      "Designing Socially Acceptable Hand-to-Face Input\n",
      "Wearable head-mounted displays combine rich graphical output with an impoverished input space. Hand-to-face gestures have been proposed as a way to add input expressivity while keeping control movements unobtrusive. To better understand how to design such techniques, we describe an elicitation study conducted in a busy public space in which pairs of users were asked to generate unobtrusive, socially acceptable hand-to-face input actions. Based on the results, we describe five design strategies: miniaturizing, obfuscating, screening, camouflaging and re-purposing. We instantiate these strategies in two hand-to-face input prototypes, one based on touches to the ear and the other based on touches of the thumbnail to the chin or cheek. Performance assessments characterize time and error rates with these devices. The paper closes with a validation study in which pairs of users experience the prototypes in a public setting and we gather data on the social acceptability of the designs and reflect on the effectiveness of the different strategies.\n",
      "=============================\n",
      "The Right Content at the Right Time: Contextual Examples for Just-in-time Creative Learning\n",
      "People often run into barriers when doing creative tasks with software because it is difficult to translate goals into concrete actions. While expert-made tutorials, examples, and documentation abound online, finding the most relevant content and adapting it to one's own situation and task is a challenge. My research introduces techniques for exposing relevant examples to novices in the context of their own workflows. These techniques are embodied in three systems. The first, RePlay, helps people find solutions when stuck by automatically locating relevant moments from expert-made videos. The second, DiscoverySpace, helps novices get started by mining and recommending expert-made software macros. The third, CritiqueKit, helps novices improve their work by providing ambient guidance and recommendations. Preliminary experiments with RePlay suggest that contextual video clips help people complete targeted tasks. Controlled experiments with DiscoverySpace and CritiqueKit demonstrate that software macros prevent novices from losing confidence, and ambient guidance improves novice output. My research illustrates the power of user communities to support creative learning.\n",
      "=============================\n",
      "Interactive Tangrami: Rapid Prototyping with Modular Paper-folded Electronics\n",
      "Prototyping interactive objects with personal fabrication tools like 3D printers requires the maker to create subsequent design artifacts from scratch which produces unnecessary waste and does not allow to reuse functional components. We present Interactive Tangrami, paper-folded and reusable building blocks (Tangramis) that can contain various sensor input and visual output capabilities. We propose a digital design toolkit that lets the user plan the shape and functionality of a design piece. The software manages the communication to the physical artifact and streams the interaction data via the Open Sound protocol (OSC) to an application prototyping system (e.g. MaxMSP). The building blocks are fabricated digitally with a rapid and inexpensive ink-jet printing method. Our systems allows to prototype physical user interfaces within minutes and without knowledge of the underlying technologies. We demo its usefulness with two application examples.\n",
      "=============================\n",
      "Trans-scale Playground: An Immersive Visual Telexistence System for Human Adaptation\n",
      "In this paper, we present a novel telexistence system and design methods for telexistence studies to explore spatialscale deconstruction. There have been studies on the experience of dwarf-sized or giant-sized telepresence have been conducted over a period of many years. In this study, we discuss the scale of movements, image transformation, technical components of telepresence robots, and user experiences of telexistence-based spatial transformations. We implemented two types of telepresence robots with an omnidirectional stereo camera setup for a spatial trans-scale experience, wheeled robots, and quadcopters. These telepresence robots provide users with a trans-scale experience for a distance ranging from 15 cm to 30 m. We conducted user studies for different camera positions on robots and for different image transformation method.\n",
      "=============================\n",
      "SilentVoice: Unnoticeable Voice Input by Ingressive Speech\n",
      "SilentVoice is a new voice input interface device that penetrates the speech-based natural user interface (NUI) in daily life. The proposed \"ingressive speech\" method enables placement of a microphone very close to the front of the mouth without suffering from pop-noise, capturing very soft speech sounds with a good S/N ratio. It realizes ultra-small (less than 39dB(A)) voice leakage, allowing us to use voice input without annoying surrounding people in public and mobile situations as well as offices and homes. By measuring airflow direction, SilentVoice can easily be separated from normal utterances with 98.8% accuracy; no activation words are needed. It can be used for voice-activated systems with a specially trained voice recognizer; evaluation results yield word error rates (WERs) of 1.8% (speaker-dependent condition), and 7.0% (speaker-independent condition) with a limited dictionary of 85 command sentences. A whisper-like natural voice can also be used for real-time voice communication.\n",
      "=============================\n",
      "4DMesh: 4D Printing Morphing Non-Developable Mesh Surfaces\n",
      "We present 4DMesh, a method of combining shrinking and bending thermoplastic actuators with customized geometric algorithms to 4D print and morph centimeter- to meter-sized functional non-developable surfaces. We will share two end-to-end inverse design algorithms. With our tools, users can input CAD models of target surfaces and produce respective printable files. The flat sheet printed can morph into target surfaces when triggered by heat. This system saves shipping and packaging costs, in addition to enabling customizability for the design of relatively large non-developable structures. We designed a few functional artifacts to leverage the advantage of non-developable surfaces for their unique functionalities in aesthetics, mechanical strength, geometric ergonomics and other functionalities. In addition, we demonstrated how this technique can potentially be adapted to customize molds for industrial parts (e.g., car, boat, etc.) in the future.\n",
      "=============================\n",
      "The Immersive Bubble Chart: a Semantic and Virtual Reality Visualization for Big Data\n",
      "In this paper, we introduce the Immersive Bubble Chart, a visualization for hierarchical datasets presented in a virtual reality (VR) world. Users get immersed into the visualization and interact with the bubbles using gestures with a view to overcoming some limitations of 2D visualizations due to the capabilities and interaction affordances of the devices. The technological advances in VR give the possibility to design malleable and extensible representations and more natural and engaging interactions. Using the Oculus Touch controllers, the users can grab and move the bubbles, throw them away or bump two of them for creating a cluster. We have tested the Immersive Bubble Chart with the hierarchical clusters of semantically related terms generated from Twitter.\n",
      "=============================\n",
      "EyeExpress: Expanding Hands-free Input Vocabulary using Eye Expressions\n",
      "The muscles surrounding the human eye are capable of performing a wide range of expressions such as squinting, blinking, frowning, and raising eyebrows. This work explores the use of these ocular expressions to expand the input vocabularies of hands-free interactions. We conducted a series of user studies: 1) to understand which eye expressions users could consistently perform among all possible expressions, 2) to explore how these expressions can be used for hands-free interactions through a user-defined design process. Our study results showed that most participants could consistently perform 9 of the 18 possible eye expressions. Also, in the user define study the participants used the eye expressions to create hands-free interactions for the state-of-the-art augmented reality (AR) head-mounted displays.\n",
      "=============================\n",
      "The Science and Practice of Transitions\n",
      "We tend to set ourselves up to thrive in a particular state while ignoring the transitions between states. But there is magic in the transitions; they are where unexpected and interesting things happen. There is an opportunity for our user interfaces to better support the transitions we make. In this talk I will share some of what I have learned from years of productivity research about how to successfully transition between tasks over the course of a day, and reflect on how these findings might be extended to help us understand how we, as academics and practitioners, can successfully transition through the various contexts and roles that we hold in a lifetime.\n",
      "=============================\n",
      "Multitasking with Play Write, a Mobile Microproductivity Writing Tool\n",
      "Mobile devices offer people the opportunity to get useful tasks done during time previously thought to be unusable. Because mobile devices have small screens and are often used in divided attention scenarios, people are limited to using them for short, simple tasks; complex tasks like editing a document present significant challenges in this environment. In this paper we demonstrate how a complex task requiring focused attention can be adapted to the fragmented way people work while mobile by decomposing the task into smaller, simpler microtasks. We introduce Play Write, a microproductivity tool that allows people to edit Word documents from their phones via such microtasks. When participants used Play Write while simultaneously watching a video, we found that they strongly preferred its microtask-based editing approach to the traditional editing experience offered by Mobile Word. Play Write made participants feel more productive and less stressed, and they completed more edits with it. Our findings suggest microproductivity tools like Play Write can help people be productive in divided attention scenarios.\n",
      "=============================\n",
      "Indutivo: Contact-Based, Object-Driven Interactions with Inductive Sensing\n",
      "We present Indutivo, a contact-based inductive sensing technique for contextual interactions. Our technique recognizes conductive objects (metallic primarily) that are commonly found in households and daily environments, as well as their individual movements when placed against the sensor. These movements include sliding, hinging, and rotation. We describe our sensing principle and how we designed the size, shape, and layout of our sensor coils to optimize sensitivity, sensing range, recognition and tracking accuracy. Through several studies, we also demonstrated the performance of our proposed sensing technique in environments with varying levels of noise and interference conditions. We conclude by presenting demo applications on a smartwatch, as well as insights and lessons we learned from our experience.\n",
      "=============================\n",
      "Tacttoo: A Thin and Feel-Through Tattoo for On-Skin Tactile Output\n",
      "This paper introduces Tacttoo, a feel-through interface for electro-tactile output on the user's skin. Integrated in a temporary tattoo with a thin and conformal form factor, it can be applied on complex body geometries, including the fingertip, and is scalable to various body locations. At less than 35µm in thickness, it is the thinnest tactile interface for wearable computing to date. Our results show that Tacttoo retains the natural tactile acuity similar to bare skin while delivering high-density tactile output. We present the fabrication of customized Tacttoo tattoos using DIY tools and contribute a mechanism for consistent electro-tactile operation on the skin. Moreover, we explore new interactive scenarios that are enabled by Tacttoo. Applications in tactile augmented reality and on-skin interaction benefit from a seamless augmentation of real-world tactile cues with computer-generated stimuli. Applications in virtual reality and private notifications benefit from high-density output in an ergonomic form factor. Results from two psychophysical studies and a technical evaluation demonstrate Tacttoo's functionality, feel-through properties and durability.\n",
      "=============================\n",
      "Unimanual Pen+Touch Input Using Variations of Precision Grip Postures\n",
      "We introduce a new pen input space by forming postures with the same hand that also grips the pen while writing, drawing, or selecting. The postures contact the multitouch surface around the pen to enable detection without special sensors. A formative study investigates the effectiveness, accuracy, and comfort of 33 candidate postures in controlled tasks. The results indicate a useful subset of postures. Using raw capacitive sensor data captured in the study, a convolutional neural network is trained to recognize 10 postures in real time. This recognizer is used to create application demonstrations for pen-based document annotation and vector drawing. A small usability study shows the approach is feasible.\n",
      "=============================\n",
      "Porta: Profiling Software Tutorials Using Operating-System-Wide Activity Tracing\n",
      "It can be hard for tutorial creators to get fine-grained feedback about how learners are actually stepping through their tutorials and which parts lead to the most struggle. To provide such feedback for technical software tutorials, we introduce the idea of tutorial profiling, which is inspired by software code profiling. We prototyped this idea in a system called Porta that automatically tracks how users navigate through a tutorial webpage and what actions they take on their computer such as running shell commands, invoking compilers, and logging into remote servers. Porta surfaces this trace data in the form of profiling visualizations that augment the tutorial with heatmaps of activity hotspots and markers that expand to show event details, error messages, and embedded screencast videos of user actions. We found through a user study of 3 tutorial creators and 12 students who followed their tutorials that Porta enabled both the tutorial creators and the students to provide more specific, targeted, and actionable feedback about how to improve these tutorials. Porta opens up possibilities for performing user testing of technical documentation in a more systematic and scalable way.\n",
      "=============================\n",
      "Head Pose Classification by using Body-Conducted Sound\n",
      "Vibrations generated by human activity have been used for recognizing human behavior and developing user interfaces; however, it is difficult to estimate static poses that do not generate a vibration. This can be solved using active acoustic sensing; however, this method is not suitable for emitting some vibrations around the head in terms of the influence of audition. Therefore, we propose a method for estimating head poses using body-conducted sound naturally and regularly generated in the human body. The support vector classification recognizes vertical and horizontal directions of the head, and we confirmed the feasibility of the proposed method through experiments.\n",
      "=============================\n",
      "Sprout: Crowd-Powered Task Design for Crowdsourcing\n",
      "While crowdsourcing enables data collection at scale, ensuring high-quality data remains a challenge. In particular, effective task design underlies nearly every reported crowdsourcing success, yet remains difficult to accomplish. Task design is hard because it involves a costly iterative process: identifying the kind of work output one wants, conveying this information to workers, observing worker performance, understanding what remains ambiguous, revising the instructions, and repeating the process until the resulting output is satisfactory. To facilitate this process, we propose a novel meta-workflow that helps requesters optimize crowdsourcing task designs and Sprout, our open-source tool, which implements this workflow. Sprout improves task designs by (1) eliciting points of confusion from crowd workers, (2) enabling requesters to quickly understand these misconceptions and the overall space of questions, and (3) guiding requesters to improve the task design in response. We report the results of a user study with two labeling tasks demonstrating that requesters strongly prefer Sprout and produce higher-rated instructions compared to current best practices for creating gated instructions (instructions plus a workflow for training and testing workers). We also offer a set of design recommendations for future tools that support crowdsourcing task design.\n",
      "=============================\n",
      "Haptic Feedback to the Palm and Fingers for Improved Tactile Perception of Large Objects\n",
      "When one manipulates a large or bulky object, s/he utilizes tactile information at both fingers and the palm. Our goal is to efficiently convey contact information to a user's hand during interaction with a virtual object. We propose a haptic system that can provide haptic feedback to thumb/middle finger/index finger and on a palm. Our interface design utilizes a novel compact mechanism to provide haptic information to the palm. Also, we propose a haptic rendering strategy to calculate haptic feedback continuously. We demonstrate that cutaneous feedback on the palm improves the haptic perception of a large virtual object compared to when there is only kinesthetic feedback to the fingers.\n",
      "=============================\n",
      "Wall-based Space Manipulation Technique for Efficient Placement of Distant Objects in Augmented Reality\n",
      "We present a wall-based space manipulation (WSM) technique that enables users to efficiently select and move distant objects by dynamically squeezing their surrounding space in augmented reality. Users can bring a target object closer by dragging a solid plane behind the object and squeezing the space between them and the plane so that they can select and move the object more delicately and efficiently. We furthermore discuss the unique design challenges of WSM, including the dimension of space reduction and the recognition of the reduced space in relation to the real space. We conducted a user evaluation to verify how WSM improves the performance of the hand-centered object manipulation technique on the HoloLens for moving near objects far away and vice versa. The results indicate that WSM overall performed consistently well and significantly improved efficiency while alleviating arm fatigue.\n",
      "=============================\n",
      "Maestro: Designing a System for Real-Time Orchestration of 3D Modeling Workshops\n",
      "Instructors of 3D design workshops for children face many challenges, including maintaining awareness of students' progress, helping students who need additional attention, and creating a fun experience while still achieving learning goals. To help address these challenges, we developed Maestro, a workshop orchestration system that visualizes students' progress, automatically detects and draws attention to common challenges faced by students, and provides mechanisms to address common student challenges as they occur. We present the design of Maestro, and the results of a case-study evaluation with an experienced facilitator and 13 children. The facilitator appreciated Maestro's real-time indications of which students were successfully following her tutorial demonstration, and recognized the system's potential to \"extend her reach\" while helping struggling students. Participant interaction data from the study provided support for our follow-along detection algorithm, and the capability to remind students to use 3D navigation.\n",
      "=============================\n",
      "Asterisk and Obelisk: Motion Codes for Passive Tagging\n",
      "Machine readable passive tags for tagging physical objects are ubiquitous today. We propose Motion Codes, a passive tagging mechanism that is based on the kinesthetic motion of the user's hand. Here, the tag comprises of a visual pattern that is displayed on a physical surface. To scan the tag and receive the encoded information, the user simply traces their finger over the pattern. The user wears an inertial motion sensing (IMU) ring on the finger that records the traced pattern. We design two motion code schemes, Asterisk and Obelisk that rely on directional vector data processed from the IMU. We evaluate both schemes for the effects of orientation, size, and data density on their accuracies. We further conduct an in-depth analysis of the sources of motion deviations in the ring data as compared to the ground truth finger movement data. Overall, Asterisk achieves a 95% accuracy for an information capacity of 16.8 million possible sequences.\n",
      "=============================\n",
      "GridDrones: A Self-Levitating Physical Voxel Lattice for Interactive 3D Surface Deformations\n",
      "We present GridDrones, a self-levitating programmable matter platform that can be used for representing 2.5D voxel grid relief maps capable of rendering unsupported structures and 3D transformations. GridDrones consists of cube-shaped nanocopters that can be placed in a volumetric 1xnxn mid-air grid, which is demonstrated here with 15 voxels. The number of voxels and scale is only limited by the size of the room and budget. Grid deformations can be applied interactively to this voxel lattice by manually selecting a set of voxels, then assigning a continuous topological relationship between voxel sets that determines how voxels move in relation to each other and manually drawing out selected voxels from the lattice structure. Using this simple technique, it is possible to create unsupported structures that can be translated and oriented freely in 3D. Shape transformations can also be recorded to allow for simple physical shape morphing animations. This work extends previous work on selection and editing techniques for 3D user interfaces.\n",
      "=============================\n",
      "The Exploratory Labeling Assistant: Mixed-Initiative Label Curation with Large Document Collections\n",
      "In this paper, we define the concept of exploratory labeling: the use of computational and interactive methods to help analysts categorize groups of documents into a set of unknown and evolving labels. While many computational methods exist to analyze data and build models once the data is organized around a set of predefined categories or labels, few methods address the problem of reliably discovering and curating such labels in the first place. In order to move first steps towards bridging this gap, we propose an interactive visual data analysis method that integrates human-driven label ideation, specification and refinement with machine-driven recommendations. The proposed method enables the user to progressively discover and ideate labels in an exploratory fashion and specify rules that can be used to automatically match sets of documents to labels. To support this process of ideation, specification, as well as evaluation of the labels, we use unsupervised machine learning methods that provide suggestions and data summaries. We evaluate our method by applying it to a real-world labeling problem as well as through controlled user studies to identify and reflect on patterns of interaction emerging from exploratory labeling activities.\n",
      "=============================\n",
      "A Mixed-Initiative Interface for Animating Static Pictures\n",
      "We present an interactive tool to animate the visual elements of a static picture, based on simple sketch-based markup. While animated images enhance websites, infographics, logos, e-books, and social media, creating such animations from still pictures is difficult for novices and tedious for experts. Creating automatic tools is challenging due to ambiguities in object segmentation, relative depth ordering, and non-existent temporal information. With a few user drawn scribbles as input, our mixed initiative creative interface extracts repetitive texture elements in an image, and supports animating them. Our system also facilitates the creation of multiple layers to enhance depth cues in the animation. Finally, after analyzing the artwork during segmentation, several animation processes automatically generate kinetic textures that are spatio-temporally coherent with the source image. Our results, as well as feedback from our user evaluation, suggest that our system effectively allows illustrators and animators to add life to still images in a broad range of visual styles.\n",
      "=============================\n",
      "ShareSpace: Facilitating Shared Use of the Physical Space by both VR Head-Mounted Display and External Users\n",
      "Currently, \"walkable\" virtual reality (VR) is achieved by dedicating a room-sized space for VR activities, which is not shared with non-HMD users engaged in their own activities. To achieve the goal of allowing shared use of space for all users while overcoming the obvious difficulty of integrating use with those immersed in a VR experience, we present ShareSpace, a system that allows external users to communicate their needs for physical space to those wearing an HMD and immersed in their VR experience. ShareSpace works by allowing external users to place \"shields\" in the virtual environment by using a set of physical shield tools. A pad visualizer helps this process by allowing external users to examine the arrangement of virtual shields. We also discuss interaction techniques that minimize the interference between the respective activities of the HMD wearers and the other users of the same physical space. To evaluate our design, a user study was conducted to collect user feedback from participants in four trial scenarios. The results indicate that our ShareSpace system allows users to perform their respective activities with improved engagement and safety. In addition, this study shows that while the HMD users did perceive a considerable degree of interference due to the internal visual indications from the ShareSpace system, they were still more engaged in their VR experience than when interrupted by direct external physical interference initiated by external users.\n",
      "=============================\n",
      "Transparent Mask: Face-Capturing Head-Mounted Display with IR Pass Filters\n",
      "Virtual reality (VR) using a head-mounted display (HMD) have been rapidly becoming popular. Lots of HMD products and various VR applications such as games, training tools and communication services have been released in recent years. However, there is a well-known problem that the user's face is covered by the HMD preventing the facial expression from being captured. This strongly restricts VR applications. For example, users wearing HMDs normally cannot exchange their face images. This degrades communication quality in virtual spaces because facial expressions are an important element of human communication.\n",
      "=============================\n",
      "Reversing Voice-Related Biases Through Haptic Reinforcement\n",
      "Biased perceptions of others are known to negatively influence the outcomes of social and professional interactions in many regards. Theses biases can be informed by a multitude of non-verbal cues such as voice pitch and voice volume. This project explores how haptic effects, generated from speech, could attenuate listeners' perceived voice-related biases formed from a speaker's voice pitch. Promising preliminary results collected during a decision-making task suggest that the speech to haptic mapping and vibration delivery mechanism employed does attenuate voice-related biases. Accordingly, it is anticipated that such a system could be introduced in the workplace to equalize people's contribution opportunities and to create a more inclusive environment by reversing voice-related biases.\n",
      "=============================\n",
      "SynchronizAR: Instant Synchronization for Spontaneous and Spatial Collaborations in Augmented Reality\n",
      "We present SynchronizAR, an approach to spatially register multiple SLAM devices together without sharing maps or involving external tracking infrastructures. SynchronizAR employs a distance based indirect registration which resolves the transformations between the separate SLAM coordinate systems. We attach an Ultra-Wide Bandwidth~(UWB) based distance measurements module on each of the mobile AR devices which is capable of self-localization with respect to the environment. As users move on independent paths, we collect the positions of the AR devices in their local frames and the corresponding distance measurements. Based on the registration, we support to create a spontaneous collaborative AR environment to spatially coordinate users' interactions. We run both technical evaluation and user studies to investigate the registration accuracy and the usability towards spatial collaborations. Finally, we demonstrate various collaborative AR experience using SynchronizAR.\n",
      "=============================\n",
      "Active Authentication on Smartphone using Touch Pressure\n",
      "Smartphone user authentication is still an open challenge because the balance between both security and usability is indispensable. To balance between them, active authentication is one way to overcome the problem. In this paper, we tackle to improve the accuracy of active authentication by adopting online learning with touch pressure. In recent years, it becomes easy to use the smartphones equipped with pressure sensor so that we have confirmed the effectiveness of adopting the touch pressure as one of the features to authenticate. Our experiments adopting online AROW algorithm with touch pressure show that equal error rate (EER), where the miss rate and false rate are equal, is reduced up to one-fifth by adding touch pressure feature. Moreover, we have confirmed that training with the data from both sitting posture and prone posture archives the best when testing variety of postures including sitting, standing and prone, which achieves EER up to 0.14%.\n",
      "=============================\n",
      "Orecchio: Extending Body-Language through Actuated Static and Dynamic Auricular Postures\n",
      "In this paper, we propose using the auricle - the visible part of the ear - as a means of expressive output to extend body language to convey emotional states. With an initial exploratory study, we provide an initial set of dynamic and static auricular postures. Using these results, we examined the relationship between emotions and auricular postures, noting that dynamic postures involving stretching the top helix in fast (e.g., 2Hz) and slow speeds (1Hz) conveyed intense and mild pleasantness while static postures involving bending the side or top helix towards the center of the ear were associated with intense and mild unpleasantness. Based on the results, we developed a prototype (called Orrechio) with miniature motors, custom-made robotic arms and other electronic components. A preliminary user evaluation showed that participants feel more comfortable using expressive auricular postures with people they are familiar with, and that it is a welcome addition to the vocabulary of human body language.\n",
      "=============================\n",
      "Designing Inherent Interactions on Wearable Devices\n",
      "Wearable devices are becoming important computing devices to personal users. They have shown promising applications in multiple domains. However, designing interactions on smartwears remains challenging as the miniature sized formfactors limit both its input and output space. My thesis research proposes a new paradigm of Inherent Interaction on smartwears, with the idea of seeking interaction opportunities from users daily activities. This is to help bridging the gap between novel smartwear interactions and real-life experiences shared among users. This report introduces the concept of Inherent Interaction with my previous and current explorations in the category.\n",
      "=============================\n",
      "Touch180: Finger Identification on Mobile Touchscreen using Fisheye Camera and Convolutional Neural Network\n",
      "We present Touch180, a computer vision based solution for identifying fingers on a mobile touchscreen with a fisheye camera and deep learning algorithm. As a proof-of-concept research, this paper focused on robustness and high accuracy of finger identification. We generated a new dataset for Touch180 configuration, which is named as Fisheye180. We trained a CNN (Convolutional Neural Network)-based network utilizing touch locations as auxiliary inputs. With our novel dataset and deep learning algorithm, finger identification result shows 98.56% accuracy with VGG16 model. Our study will serve as a step stone for finger identification on a mobile touchscreen.\n",
      "=============================\n",
      "RollingStone: Using Single Slip Taxel for Enhancing Active Finger Exploration with a Virtual Reality Controller\n",
      "We propose using a single slip tactile pixel on virtual reality controllers to produce sensations of finger sliding and textures. When a user moves the controller on a virtual surface, we add a slip opposite to the movement, creating an illusion of a finger that is sliding on the surface, while varying the slip feedback changes lateral forces on fingertip. When coupled with hand motion the lateral forces can be used to create perceptions of artificial textures. RollingStone has been implemented as a prototype VR controller consisting of a ball-based slip display positioned under the user's fingertip. Within the slip display, a pair of motors actuates the ball, which is capable of gener- ating both short- and long-term two-degree-of-freedom slip feedback. An exploratory study was conducted to ensure that changing the relative motion between the finger and the ball could alter the perceptions conveying the properties of a tex- ture. The following two perception-based studies examined the minimum changes in speed of slip and angle of slip that are detectable by users. The results help us to design haptic patterns as well as our prototype applications. Finally, our preliminary user evaluation indicated that participants wel- comed RollingStone as a useful addition to the range of VR controllers.\n",
      "=============================\n",
      "AccordionFab: Fabricating Inflatable 3D Objects by Laser Cutting and Welding Multi-Layered Sheets\n",
      "In this paper, we propose a method to create 3D inflatable objects by laminating plastic layers. AccordionFab is a fabrication method in which the user can prototype multi-layered inflatable structures rapidly with a common laser cutter. Our key finding is that it is possible to selectively weld the two uppermost plastic sheets out of the stacked sheets by defocusing the laser and inserting the heat-resistant paper below the desired welding layer. As the contribution of our research, we investigated the optimal distance between the lens and the workpiece for cutting and welding and developed an attachment which supports welding process. Next, we developed a mechanism of changing the thickness and bending angle of multi-layered objects and created a simulation software. Using these techniques, the user can create various prototypes such as personal furniture that fits user's body and packing containers that fit the contents.\n",
      "=============================\n",
      "Wearable Haptic Device that Presents the Haptics Sensation Corresponding to Three Fingers on the Forearm\n",
      "In this demonstration, as an attempt of a new haptic presentation method for objects in virtual reality (VR) environment, we show a device that presents the haptic sensation of the fingertip on the forearm, not on the fingertip. This device adopts a five-bar linkage mechanism and it is possible to present the strength, direction of force. Compared with a fingertip mounted type displays, it is possible to address the issues of their weight and size which hinder the free movement of fingers. We have confirmed that the experiences in the VR environment is improved compared with without haptics cues situation regardless of without presenting haptics information directly to the fingertip.\n",
      "=============================\n",
      "Facilitating Document Reading by Linking Text and Tables\n",
      "Document authors commonly use tables to support arguments presented in the text. But, because tables are usually separate from the main body text, readers must split their attention between different parts of the document. We present an interactive document reader that automatically links document text with corresponding table cells. Readers can select a sentence (or tables cells) and our reader highlights the relevant table cells (or sentences). We provide an automatic pipeline for extracting such references between sentence text and table cells for existing PDF documents that combines structural analysis of tables with natural language processing and rule-based matching. On a test corpus of 330 (sentence, table) pairs, our pipeline correctly extracts 48.8% of the references. An additional 30.5% contain only false negatives (FN) errors -- the reference is missing table cells. The remaining 20.7% contain false positives (FP) errors -- the reference includes extraneous table cells and could therefore mislead readers. A user study finds that despite such errors, our interactive document reader helps readers match sentences with corresponding table cells more accurately and quickly than a baseline document reader.\n",
      "=============================\n",
      "TakeToons: Script-driven Performance Animation\n",
      "Performance animation is an expressive method for animating characters through human performance. However, character motion is only one part of creating animated stories. The typical workflow also involves writing a script, coordinating actors, and editing recorded performances. In most cases, these steps are done in isolation with separate tools, which introduces friction and hinders iteration. We propose TakeToons, a script-driven approach that allows authors to annotate standard scripts with relevant animation events like character actions, camera positions, and scene backgrounds. We compile this script into a story model that persists throughout the production process and provides a consistent structure for organizing and assembling recorded performances and propagating script or timing edits to existing recordings. TakeToons enables writing, performing and editing to happen in an integrated and interleaved manner that streamlines production and facilitates iteration. Informal feedback from professional animators suggests that our approach can benefit many existing workflows supporting individual authors and production teams with many different contributors.\n",
      "=============================\n",
      "Mindgame: Mediating People's EEG Alpha Band Power through Reinforcement Learning\n",
      "This paper presents Mindgame, a reinforcement learning optimized neurofeedback mindfulness system. To avoid the potential bias and difficulties of designing mapping between neural signal and output, we adopt a trial-and-error learning method to explore the preferred mapping. In a pilot study we assess the effectiveness of Mindgame in mediating people's EEG alpha band. All participants' alpha band change towards the desired direction.\n",
      "=============================\n",
      "PrintMotion: Actuating Printed Objects Using Actuators Equipped in a 3D Printer\n",
      "We introduce a novel use for desktop 3D printers using actuators equipped in the printers. The actuators control an extruder and a build-plate mounted on a fused deposition modeling (FDM) 3D printer, moving them horizontally or vertically. Our technique enables actuation of 3D-printed objects on the build-plate by controlling the actuators, and people can interact with them by connecting interface devices to the 3D printer. In this work, we describe how to actuate printed objects using the actuators and present several objects illustrated by our technique.\n",
      "=============================\n",
      "SweatSponse: Closing the Loop on Notification Delivery Using Skin Conductance Responses\n",
      "Today\"s smartphone notification systems are incapable of determining whether a notification has been successfully perceived without explicit interaction from the user. When the system incorrectly assumes that a notification has not been perceived, it may repeat it redundantly, disrupting the user (e.g., phone ringing). Or, when it assumes that a notification was perceived, and therefore fails to repeat it, the notification will be missed altogether (e.g., text message). We introduce SweatSponse, a feedback loop using skin conductance responses (SCR) to infer the perception of smartphone notifications just after their presentation. Early results from a laboratory study suggest that notifications induce SCR and that they could be used to better infer perception of smartphone notifications in real-time.\n",
      "=============================\n",
      "Scenograph: Fitting Real-Walking VR Experiences into Various Tracking Volumes\n",
      "When developing a real-walking virtual reality experience, designers generally create virtual locations to fit a specific tracking volume. Unfortunately, this prevents the resulting experience from running on a smaller or differently shaped tracking volume. To address this, we present a software system called Scenograph. The core of Scenograph is a tracking volume-independent representation of real-walking experiences. Scenograph instantiates the experience to a tracking volume of given size and shape by splitting the locations into smaller ones while maintaining narrative structure. In our user study, participants' ratings of realism decreased significantly when existing techniques were used to map a 25m2 experience to 9m2 and an L-shaped 8m2 tracking volume. In contrast, ratings did not differ when Scenograph was used to instantiate the experience.\n",
      "=============================\n",
      "One Button to Rule Them All: Rendering Arbitrary Force-Displacement Curves\n",
      "Physical buttons provide rich force characteristics during the travel range, which are commonly described in the form of force-displacement curves. These force characteristics play an important role in the users' experiences while pressing a button. However, due to lack of proper tools to dynamically render various force-displacement curves, little literature has tried iterative button design improvement. This paper presents Button Simulator, a low-cost 3D printed physical button capable of displaying any force-displacement curves, with limited average error offset around .034 N. By reading the force-displacement curves of existing push-buttons, we can easily replicate the force characteristics from any buttons onto our Button Simulator. One can even go beyond existing buttons and design non-existent ones as the form of arbitrary force-displacement curves; then use Button Simulator to render the sensation. This project will be open-sourced and the implementation details will be released. Our system can be a useful tool for future researchers, designers, and makers to investigate rich and dynamic button\"s force design.\n",
      "=============================\n",
      "CrowdMuse: An Adaptive Crowd Brainstorming System\n",
      "Online crowds, with their large numbers and diversity, show great potential for creativity, particularly during large-scale brainstorming sessions. Research has explored different ways of augmenting this creativity, such as showing ideators some form of inspiration to get them to explore more categories or generate more ideas. The mechanisms used to select which inspirations are shown to ideators thus far have been focused on characteristics of the inspirations rather than on ideators. This can hinder their effect, as creativity research has shown that ideators have unique cognitive structures and may therefore be better inspired by some ideas rather than others. We introduce CrowdMuse, an adaptive system for supporting large scale brainstorming. The system models ideators based on their past ideas and adapts the system views and inspiration mechanisms accordingly. An evaluation of this system could inform how to better individually support ideators.\n",
      "=============================\n",
      "Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation\n",
      "This paper introduces Dynamic 3D Printing, a fast and reconstructable shape formation system. Dynamic 3D Printing can assemble an arbitrary three-dimensional shape from a large number of small physical elements. Also, it can disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbitrary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and implementation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long-term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper, we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.\n",
      "=============================\n",
      "Towards a Symbiotic Human-Machine Depth Sensor: Exploring 3D Gaze for Object Reconstruction\n",
      "Eye tracking is expected to become an integral part of future augmented reality (AR) head-mounted displays (HMDs) given that it can easily be integrated into existing hardware and provides a versatile interaction modality. To augment objects in the real world, AR HMDs require a three-dimensional understanding of the scene, which is currently solved using depth cameras. In this work we aim to explore how 3D gaze data can be used to enhance scene understanding for AR HMDs by envisioning a symbiotic human-machine depth camera, fusing depth data with 3D gaze information. We present a first proof of concept, exploring to what extend we are able to recognise what a user is looking at by plotting 3D gaze data. To measure 3D gaze, we implemented a vergence-based algorithm and built an eye tracking setup consisting of a Pupil Labs headset and an OptiTrack motion capture system, allowing us to measure 3D gaze inside a 50x50x50 cm volume. We show first 3D gaze plots of \"gazed-at\" objects and describe our vision of a symbiotic human-machine depth camera that combines a depth camera and human 3D gaze information.\n",
      "=============================\n",
      "Robust Annotation of Mobile Application Interfaces in Methods for Accessibility Repair and Enhancement\n",
      "Accessibility issues in mobile apps make those apps difficult or impossible to access for many people. Examples include elements that fail to provide alternative text for a screen reader, navigation orders that are difficult, or custom widgets that leave key functionality inaccessible. Social annotation techniques have demonstrated compelling approaches to such accessibility concerns in the web, but have been difficult to apply in mobile apps because of the challenges of robustly annotating interfaces. This research develops methods for robust annotation of mobile app interface elements. Designed for use in runtime interface modification, our methods are based in screen identifiers, element identifiers, and screen equivalence heuristics. We implement initial developer tools for annotating mobile app accessibility metadata, evaluate our current screen equivalence heuristics in a dataset of 2038 screens collected from 50 mobile apps, present three case studies implementing runtime repair of common accessibility issues, and examine repair of real-world accessibility issues in 26 apps. These contributions overall demonstrate strong opportunities for social annotation in mobile accessibility.\n",
      "=============================\n",
      "Designing Interactive Behaviours Beyond the Desktop\n",
      "As interactions move beyond the desktop, interactive behaviours (effects of actions as they happen, or once they happen) are becoming increasingly complex. This complexity is due to the variety of forms that objects might take, and the different inputs and sensors capturing information, and the ability to create nuanced responses to those inputs. Current interaction design tools do not support much of this rich behaviour authoring. In my work I create prototyping tools that examine ways in which designers can create interactive behaviours. Thus far, I have created two prototyping tools: Pineal and Astral, which examine how to create physical forms based on a smart object's behaviour, and how to reuse existing desktop infrastructures to author different kinds of interactive behaviour. I also contribute conceptual elements, such as how to create smart objects using mobile devices, their sensors and outputs, instead of using custom electronic circuits, as well as devising evaluation strategies used in HCI toolkit research which directly informs my approach to evaluating my tools.\n",
      "=============================\n",
      "A WOZ Study of Feedforward Information on an Ambient Display in Autonomous Cars\n",
      "We describe the development and user testing of an ambient display for autonomous vehicles. Instead of providing feedback about driving actions, once executed, it communicates driving decisions in advance, via light signals in passengers\" peripheral vision. This ambient display was tested in an WoZ-based on-the-road-driving simulation of a fully autonomous vehicle. Findings from a preliminary study with 14 participants suggest that such a display might be particularly useful to communicate upcoming inertia changes for passengers.\n",
      "=============================\n",
      "Non-Linear Editing of Text-Based Screencasts\n",
      "Screencasts, where recordings of a computer screen are broadcast to a large audience on the web, are becoming popular as an online educational tool. To provide rich interactions with the text within screencasts, there are emerging platforms that support text-based screencasts by recording every character insertion and deletion from the creator and reconstructing its playback on the viewer's screen. However, these platforms lack support for non-linear editing of screencasts, which involves manipulating a sequence of text editing operations. Since text editing operations are tightly coupled in sequence, modifying an arbitrary part of the sequence often creates ambiguity that yields multiple possible results that require user's choice for resolution. We present an editing tool with a non-linear editing algorithm for text-based screencasts. The tool allows users to edit any arbitrary part of a text-based screencast while preserving the overall consistency of the screencast. In an exploratory user study, all subjects successfully carried out a variety of screencast editing tasks using our prototype screencast editor.\n",
      "=============================\n",
      "Lip-Interact: Improving Mobile Device Interaction with Silent Speech Commands\n",
      "We present Lip-Interact, an interaction technique that allows users to issue commands on their smartphone through silent speech. Lip-Interact repurposes the front camera to capture the user's mouth movements and recognize the issued commands with an end-to-end deep learning model. Our system supports 44 commands for accessing both system-level functionalities (launching apps, changing system settings, and handling pop-up windows) and application-level functionalities (integrated operations for two apps). We verify the feasibility of Lip-Interact with three user experiments: evaluating the recognition accuracy, comparing with touch on input efficiency, and comparing with voiced commands with regards to personal privacy and social norms. We demonstrate that Lip-Interact can help users access functionality efficiently in one step, enable one-handed input when the other hand is occupied, and assist touch to make interactions more fluent.\n",
      "=============================\n",
      "Augmented Collaboration in Shared Space Design with Shared Attention and Manipulation\n",
      "Augmented collaboration in a shared house design scenario has been studied widely with various approaches. However, those studies did not consider human perception. Our goal is to lower the user's perceptual load for augmented collaboration in shared space design scenarios. Applying attention theories, we implemented shared head gaze, shared selected object, and collaborative manipulation features in our system in two different versions with HoloLens. To investigate whether user perceptions of the two different versions differ, we conducted an experiment with 18 participants (9 pairs) and conducted a survey and semi-structured interviews. The results did not show significant differences between the two versions, but produced interesting insights. Based on the findings, we provide design guidelines for collaborative AR systems.\n",
      "=============================\n",
      "Investigation into Natural Gestures Using EMG for \"SuperNatural\" Interaction in VR\n",
      "Can natural interaction requirements be fulfilled while still harnessing the \"supernatural\" fantasy of Virtual Reality (VR)? In this work we used off the shelf Electromyogram (EMG) sensors as an input device which can afford natural gestures to preform the \"supernatural\" task of growing your arm in VR. We recorded 18 participants preforming a simple retrieval task in two phases; an initial and a learning phase where the stretch arm was disabled and enabled respectively. The results show that the gestures used in the initial phase are different than the main gestures used to retrieve an object in our system and that the times taken to complete the learning phase are highly variable across participants.\n",
      "=============================\n",
      "Artistic Vision: Providing Contextual Guidance for Capture-Time Decisions\n",
      "With the increased popularity of cameras, more and more people are interested in learning photography. People are willing to invest in expensive cameras as a medium for their artistic expression, but few have access to in-person classes. Inspired by critique sessions common in in-person art practice classes, we propose design principles for creative learning. My dissertation research focuses on designing new interfaces and interactions that provide contextual in-camera feedback to aid users in learning visual elements of photography. We interactively visualize results of image processing algorithms as additional information for the user to make more informed and intentional decisions during capture. In this paper, we describe our design principles, and apply these principles in the design of two guided photography interfaces: one to explore lighting options for a portrait, and one to refine contents and composition of a photo.\n",
      "=============================\n",
      "Evaluation of Interaction Techniques for a Virtual Reality Reading Room in Diagnostic Radiology\n",
      "Today, radiologists diagnose three dimensional medical data using two dimensional displays. When designing environments with optimal conditions for such a process various aspects like contrast, screen reflection and background light have to be considered. As shown in previous research, applying virtual environments in combination with a Head-Mounted Display for diagnostic imaging provides potential benefits to reduce issues of bad posture and diagnostic mistakes. However, there is little research in exploring the usability and user experience of such beneficial environments. In this work we designed and evaluated different means of interaction to increase radiologists' performance. Therefore we created a virtual reality radiology reading room and employed it to evaluate three different interaction techniques. These allow a direct, semi-direct and indirect manipulation for performing scrolling- and windowing- tasks which are the most important for a radiologist. A study including nine radiologists was conducted and evaluated using the User Experience Questionnaire. Results indicate that direct manipulation is the preferred interaction technique, it outscored the other two control possibilities in attractiveness and pragmatic quality.\n",
      "=============================\n",
      "DualPanto: A Haptic Device that Enables Blind Users to Continuously Interact with Virtual Worlds\n",
      "We present a new haptic device that enables blind users to continuously interact with spatial virtual environments that contain moving objects, as is the case in sports or shooter games. Users interact with DualPanto by operating the me handle with one hand and by holding on to the it handle with the other hand. Each handle is connected to a pantograph haptic input/output device. The key feature is that the two handles are spatially registered with respect to each other. When guiding their avatar through a virtual world using the me handle, spatial registration enables users to track moving objects by having the device guide the output hand. This allows blind players of a 1-on-1 soccer game to race for the ball or evade an opponent; it allows blind players of a shooter game to aim at an opponent and dodge shots. In our user study, blind participants reported very high enjoyment when using the device to play (6.5/7).\n",
      "=============================\n",
      "MetaArms: Body Remapping Using Feet-Controlled Artificial Arms\n",
      "We introduce MetaArms, wearable anthropomorphic robotic arms and hands with six degrees of freedom operated by the user's legs and feet. Our overall research goal is to re-imagine what our bodies can do with the aid of wearable robotics using a body-remapping approach. To this end, we present an initial exploratory case study. MetaArms' two robotic arms are controlled by the user's feet motion, and the robotic hands can grip objects according to the user's toes bending. Haptic feedback is also presented on the user's feet that correlate with the touched objects on the robotic hands, creating a closed-loop system. We present formal and informal evaluations of the system, the former using a 2D pointing task according to Fitts' Law. The overall throughput for 12 users of the system is reported as 1.01 bits/s (std 0.39). We also present informal feedback from over 230 users. We find that MetaArms demonstrate the feasibility of body-remapping approach in designing robotic limbs that may help us re-imagine what the human body could do.\n",
      "=============================\n",
      "Engagement Learning: Expanding Visual Knowledge by Engaging Online Participants\n",
      "Most artificial intelligence (AI) systems to date have focused entirely on performance, and rarely if at all on their social interactions with people and how to balance the AIs' goals against their human collaborators'. Learning quickly from interactions with people poses both social challenges and is unresolved technically. In this paper, we introduce engagement learning: a training approach that learns to trade off what the AI needs---the knowledge value of a label to the AI---against what people are interested to engage with---the engagement value of the label. We realize our goal with ELIA (Engagement Learning Interaction Agent), a conversational AI agent who's goal is to learn new facts about the visual world by asking engaging questions of people about the photos they upload to social media. Our current deployment of ELIA on Instagram receives a response rate of 26%.\n",
      "=============================\n",
      "Idyll: A Markup Language for Authoring and Publishing Interactive Articles on the Web\n",
      "The web has matured as a publishing platform: news outlets regularly publish rich, interactive stories while technical writers use animation and interaction to communicate complex ideas. This style of interactive media has the potential to engage a large audience and more clearly explain concepts, but is expensive and time consuming to produce. Drawing on industry experience and interviews with domain experts, we contribute design tools to make it easier to author and publish interactive articles. We introduce Idyll, a novel \"compile-to-the-web\" language for web-based interactive narratives. Idyll implements a flexible article model, allowing authors control over document style and layout, reader-driven events (such as button clicks and scroll triggers), and a structured interface to JavaScript components. Through both examples and first-use results from undergraduate computer science students, we show how Idyll reduces the amount of effort and custom code required to create interactive articles.\n",
      "=============================\n",
      "I/O Braid: Scalable Touch-Sensitive Lighted Cords Using Spiraling, Repeating Sensing Textiles and Fiber Optics\n",
      "We introduce I/O Braid, an interactive textile cord with embedded sensing and visual feedback. I/O Braid senses proximity, touch, and twist through a spiraling, repeating braiding topology of touch matrices. This sensing topology is uniquely scalable, requiring only a few sensing lines to cover the whole length of a cord. The same topology allows us to embed fiber optic strands to integrate co-located visual feedback. We provide an overview of the enabling braiding techniques, design considerations, and approaches to gesture detection. These allow us to derive a set of interaction techniques, which we demonstrate with different form factors and capabilities. Our applications illustrate how I/O Braid can invisibly augment everyday objects, such as touch-sensitive headphones and interactive drawstrings on garments, while enabling discoverability and feedback through embedded light sources.\n",
      "=============================\n",
      "RESi: A Highly Flexible, Pressure-Sensitive, Imperceptible Textile Interface Based on Resistive Yarns\n",
      "We present RESi (Resistive tExtile Sensor Interfaces), a novel sensing approach enabling a new kind of yarn-based, resistive pressure sensing. The core of RESi builds on a newly designed yarn, which features conductive and resistive properties. We run a technical study to characterize the behaviour of the yarn and to determine the sensing principle. We demonstrate how the yarn can be used as a pressure sensor and discuss how specific issues, such as connecting the soft textile sensor with the rigid electronics can be solved. In addition, we present a platform-independent API that allows rapid prototyping. To show its versatility, we present applications developed with different textile manufacturing techniques, including hand sewing, machine sewing, and weaving. RESi is a novel technology, enabling textile pressure sensing to augment everyday objects with interactive capabilities.\n",
      "=============================\n",
      "Pushables: A DIY Approach for Fabricating Customizable and Self-Contained Tactile Membrane Dome Switches\n",
      "Momentary switches are important building blocks to prototype novel physical user interfaces and enable tactile, explicit and eyes-free interactions. Unfortunately, typical representatives, such as push-buttons or pre-manufactured membrane switches, often do not fulfill individual design requirements and lack customization options for rapid prototyping. With this work, we present Pushables, a DIY fabrication approach for producing thin, bendable and highly customizable membrane dome switches. Therefore, we contribute a three-stage fabrication pipeline that describes the production and assembly on the basis of prototyping methods with different skill levels making our approach suitable for technology-enthusiastic makers, researchers, fab labs and others who require custom membrane switches in small quantities. To demonstrate the wide applicability of Pushables, we present application examples from ubiquitous, mobile and wearable computing.\n",
      "=============================\n",
      "VR Grabbers: Ungrounded Haptic Retargeting for Precision Grabbing Tools\n",
      "Haptic feedback in VR is important for realistic simulation in virtual reality. However, recreating the haptic experience for hand tools in VR traditionally requires hardware with precise actuators, adding complexity to the system. We propose Ungrounded Haptic Retargeting, an interaction technique that provides a realistic haptic experience for grabbing tools using only passive mechanisms. This technique leverages the ungrounded feedback inherent in grabbing tools combined with dynamic visual adjustments of their position in virtual reality to create an illusion of physical presence for virtual objects. To demonstrate the capabilities of this technique, we created VR Grabbers, an exemplary passive VR controller, similar to training chopsticks, with haptic feedback for precise object selection and manipulation. We conducted two user studies based on VR Grabbers. The first study probed the perceptual limits of the illusion; we found that the maximum position difference between the virtual and physical world acceptable to the user is (-1.48, 1.95) cm. The second study showed that task performance of the VR Grabbers controller with Ungrounded Haptic Retargeting enabled outperforms the same controller with Ungrounded Haptic Retargeting disabled.\n",
      "=============================\n",
      "Wireless Analytics for 3D Printed Objects\n",
      "We present the first wireless physical analytics system for 3D printed objects using commonly available conductive plastic filaments. Our design can enable various data capture and wireless physical analytics capabilities for 3D printed objects, without the need for electronics. To achieve this goal, we make three key contributions: (1) demonstrate room scale backscatter communication and sensing using conductive plastic filaments, (2) introduce the first backscatter designs that detect a variety of bi-directional motions and support linear and rotational movements, and (3) enable data capture and storage for later retrieval when outside the range of the wireless coverage, using a ratchet and gear system. We validate our approach by wirelessly detecting the opening and closing of a pill bottle, capturing the joint angles of a 3D printed e-NABLE prosthetic hand, and an insulin pen that can store information to track its use outside the range of a wireless receiver.\n",
      "=============================\n",
      "ElectroTutor: Test-Driven Physical Computing Tutorials\n",
      "A wide variety of tools for creating physical computing systems have been developed, but getting started in this domain remains challenging for novices. In this paper, we introduce test-driven physical computing tutorials, a novel application of interactive tutorial systems to better support users in building and programming physical computing systems. These tutorials inject interactive tests into the tutorial process to help users verify and understand individual steps before proceeding. We begin by presenting a taxonomy of the types of tests that can be incorporated into physical computing tutorials. We then present ElectroTutor, a tutorial system that implements a range of tests for both the software and physical aspects of a physical computing system. A user study suggests that ElectroTutor can improve users' success and confidence when completing a tutorial, and save them time by reducing the need to backtrack and troubleshoot errors made on previous tutorial steps.\n",
      "=============================\n",
      "Sense.Seat: Inducing Improved Mood and Cognition through Multisensorial Priming\n",
      "User interface software and technologies have been evolving significantly and rapidly. This poster presents a breakthrough user experience that leverages multisensorial priming and embedded interaction and introduces an interactive piece of furniture called Sense.Seat. Sensory stimuli such as calm colors, lavender and other scents as well as ambient soundscapes have been traditionally used to spark creativity and promote well-being. Sense.Seat is the first computational multisensorial seat that can be digitally controlled and vary the frequency and intensity of visual, auditory and olfactory stimulus. It is a new user interface shaped as a seat or pod that primes the user for inducing improved mood and cognition, therefore improving the work environment.\n",
      "=============================\n",
      "Shared Autonomy for an Interactive AI System\n",
      "Across many domains, interactive systems either make decisions for us autonomously or yield decision-making authority to us and play a supporting role. However, many settings, such as those in education or the workplace, benefit from sharing this autonomy between the user and the system, and thus from a system that adapts to them over time. In this paper, we pursue two primary research questions: (1) How do we design interfaces to share autonomy between the user and the system? (2) How does shared autonomy alter a user\"s perception of a system? We present SharedKeys, an interactive shared autonomy system for piano instruction that plays different video segments of a piece for students to emulate and practice. Underlying our approach to shared autonomy is a mixed-observability Markov decision process that estimates a user\"s desired autonomy level based on her performance and attentiveness. Pilot studies revealed that students sharing autonomy with the system learned more quickly and perceived the system as more intelligent.\n",
      "=============================\n",
      "DextrES: Wearable Haptic Feedback for Grasping in VR via a Thin Form-Factor Electrostatic Brake\n",
      "We introduce DextrES, a flexible and wearable haptic glove which integrates both kinesthetic and cutaneous feedback in a thin and light form factor (weight is less than 8g). Our approach is based on an electrostatic clutch generating up to 20 N of holding force on each finger by modulating the electrostatic attraction between flexible elastic metal strips to generate an electrically-controlled friction force. We harness the resulting braking force to rapidly render on-demand kinesthetic feedback. The electrostatic brake is mounted onto the the index finger and thumb via modular 3D printed articulated guides which allow the metal strips to glide smoothly. Cutaneous feedback is provided via piezo actuators at the fingertips. We demonstrate that our approach can provide rich haptic feedback under dexterous articulation of the user's hands and provides effective haptic feedback across a variety of different grasps. A controlled experiment indicates that DextrES improves the grasping precision for different types of virtual objects. Finally, we report on results of a psycho-physical study which identifies discrimination thresholds for different levels of holding force.\n",
      "=============================\n",
      "Adasa: A Conversational In-Vehicle Digital Assistant for Advanced Driver Assistance Features\n",
      "Advanced Driver Assistance Systems (ADAS) come equipped on most modern vehicles and are intended to assist the driver and enhance the driving experience through features such as lane keeping system and adaptive cruise control. However, recent studies show that few people utilize these features for several reasons. First, ADAS features were not common until recently. Second, most users are unfamiliar with these features and do not know what to expect. Finally, the interface for operating these features is not intuitive. To help drivers understand ADAS features, we present a conversational in-vehicle digital assistant that responds to drivers' questions and commands in natural language. With the system prototyped herein, drivers can ask questions or command using unconstrained natural language in the vehicle, and the assistant trained by using advanced machine learning techniques, coupled with access to vehicle signals, responds in real-time based on conversational context. Results of our system prototyped on a production vehicle are presented, demonstrating its effectiveness in improving driver understanding and usability of ADAS.\n",
      "=============================\n",
      "WiFröst: Bridging the Information Gap for Debugging of Networked Embedded Systems\n",
      "The rise in prevalence of Internet of Things (IoT) technologies has encouraged more people to prototype and build custom internet connected devices based on low power microcontrollers. While well-developed tools exist for debugging network communication for desktop and web applications, it can be difficult for developers of networked embedded systems to figure out why their network code is failing due to the limited output affordances of embedded devices. This paper presents WiFröst, a new approach for debugging these systems using instrumentation that spans from the device itself, to its communication API, to the wireless router and back-end server. WiFröst automatically collects this data, displays it in a web-based visualization, and highlights likely issues with an extensible suite of checks based on analysis of recorded execution traces.\n",
      "=============================\n",
      "Arboretum and Arbility: Improving Web Accessibility Through a Shared Browsing Architecture\n",
      "Many web pages developed today require navigation by visual interaction-seeing, hovering, pointing, clicking, and dragging with the mouse over dynamic page content. These forms of interaction are increasingly popular as developer trends have moved from static, logically structured pages to dynamic, interactive pages. However, they are also often inaccessible to blind web users who tend to rely on keyboard-based screen readers to navigate the web. Despite existing web accessibility standards, engineering web pages to be equally accessible via both keyboard and visuomotor mouse-based interactions is often not a priority for developers. Improving access to this kind of visual and interactive web content has been a long-standing goal of HCI researchers, but the barriers have proven to be too varied and unpredictable to be overcome by some of the proposed solutions: promoting guidelines and best practices, automatically generating accessible versions of pre-exisiting web pages, or developing human-assisted solutions, such as screen and cursor-sharing, which tend to diminish an end user's agency. In this paper we present a real-time, collaborative approach to helping blind web users overcome inaccessible parts of existing web pages. We introduce *Arboretum*, a new architecture that enables any web user to seamlessly hand off controlled parts of their browsing session to remote users, while maintaining control over the interface via a \"propose and accept/reject\" mechanism. We illustrate the benefit of Arboretum by using it to implement *Arbility*, a browser that allows blind users to hand off targeted visual interaction tasks to remote crowd workers. We evaluate the entire system in a study with 9 blind web users, showing that Arbility allows them to interact with web content that was previously difficult to access via a screen reader alone.\n",
      "=============================\n",
      "FTIR-based Touch Pad for Smartphone-based HMD Enhancement\n",
      "We propose to equip smartphone-based HMDs (SbHMDs) with an additional touch pad. SbHMDs are a low cost approach to allowing users to experience virtual reality (VR). Current SbHMDs, however, provide poor input functionality and sometimes external devices are necessary to enhance the VR experience. Our proposal uses frustrated total internal reflection (FTIR) to realize a touch pad on the external surfaces of the HMD case; no special devices are needed. As simple FTIR approaches do not suit SbHMDs due to the spatial relation between camera and light, we design an arrangement of acrylic plates and mirror suitable for smartphone's built-in camera and torch-light. It extends the input vocabulary SbHMDs to include touch location, gestures, and also pressure.\n",
      "=============================\n",
      "Phonoscape: Auralization of Photographs using Stereophonic Auditory Icons\n",
      "In this paper, we developed an auditory display method which improves the comprehension of photograph to apply the support system for person with visual impairment. The auralization method is constructed by object recognition, auditory iconization and stereophonic techniques. Through the experiments, the enhancement of intelligibility and discriminability was confirmed compared to the image-to-speech reading machine method.\n",
      "=============================\n",
      "Ubicoustics: Plug-and-Play Acoustic Activity Recognition\n",
      "Despite sound being a rich source of information, computing devices with microphones do not leverage audio to glean useful insights about their physical and social context. For example, a smart speaker sitting on a kitchen countertop cannot figure out if it is in a kitchen, let alone know what a user is doing in a kitchen - a missed opportunity. In this work, we describe a novel, real-time, sound-based activity recognition system. We start by taking an existing, state-of-the-art sound labeling model, which we then tune to classes of interest by drawing data from professional sound effect libraries traditionally used in the entertainment industry. These well-labeled and high-quality sounds are the perfect atomic unit for data augmentation, including amplitude, reverb, and mixing, allowing us to exponentially grow our tuning data in realistic ways. We quantify the performance of our approach across a range of environments and device categories and show that microphone-equipped computing devices already have the requisite capability to unlock real-time activity recognition comparable to human accuracy.\n",
      "=============================\n",
      "Montage: A Video Prototyping System to Reduce Re-Shooting and Increase Re-Usability\n",
      "Video prototypes help capture and communicate interaction with paper prototypes in the early stages of design. However, designers sometimes find it tedious to create stop-motion videos for continuous interactions and to re-shoot clips as the design evolves. We introduce Montage, a proof-of-concept implementation of a computer-assisted process for video prototyping. Montage lets designers progressively augment video prototypes with digital sketches, facilitating the creation, reuse and exploration of dynamic interactions. Montage uses chroma keying to decouple the prototyped interface from its context of use, letting designers reuse or change them independently. We describe how Montage enhances video prototyping by combining video with digital animated sketches, encourages the exploration of different contexts of use, and supports prototyping of different interaction styles.\n",
      "=============================\n",
      "Effects of an Adaptive Modality Selection Algorithm for Navigation Systems\n",
      "Portable electronic navigation systems are often used for directional guidance when humans need to navigate terrain quickly and accurately. Prior work in this field has focused on using either the visual or haptic sensory modality for providing such guidance, and results have indicated that either option may be preferable depending upon the user's specific needs. However, conventional methods involve selecting a single modality based on which will work best with the task the user is most likely to perform and using this modality throughout the duration of the navigation. In this paper, we describe the design and results of a study intended to evaluate the effectiveness of an adaptive modality selection algorithm that dynamically selects a navigation system's directional guidance modality while considering both task-specific benefits and the time-varying effects of switching cost, stimulus-specific adaptation, and habituation. Our findings indicate that use of this algorithm can improve user performance in the presence of multiple simultaneous tasks.\n",
      "=============================\n",
      "Ply: A Visual Web Inspector for Learning from Professional Webpages\n",
      "While many online resources teach basic web development, few are designed to help novices learn the CSS concepts and design patterns experts use to implement complex visual features. Professional webpages embed these design patterns and could serve as rich learning materials, but their stylesheets are complex and difficult for novices to understand. This paper presents Ply, a CSS inspection tool that helps novices use their visual intuition to make sense of professional webpages. We introduce a new visual relevance testing technique to identify properties that have visual effects on the page, which Ply uses to hide visually irrelevant code and surface unintuitive relationships between properties. In user studies, Ply helped novice developers replicate complex web features 50% faster than those using Chrome Developer Tools, and allowed novices to recognize and explain unfamiliar concepts. These results show that visual inspection tools can support learning from complex professional webpages, even for novice developers.\n",
      "=============================\n",
      "SurfaceStreams: A Content-Agnostic Streaming Toolkit for Interactive Surfaces\n",
      "We present SurfaceStreams, an open-source toolkit for recording and sharing visual content among multiple heterogeneous display-camera systems. SurfaceStreams clients support on-the-fly background removal and rectification on a range of different capture devices (Kinect & RealSense depth cameras, SUR40 sensor, plain webcam). After preprocessing, the raw data is compressed and sent to the SurfaceStreams server, which can dynamically receive streams from multiple clients, overlay them using the removed background as mask, and deliver the merged result back to the clients for display. We discuss an exemplary usage scenario (3-way shared interactive tabletop surface) and present results from a preliminary performance evaluation.\n",
      "=============================\n",
      "Kaleidoscope: An RDF-based Exploratory Data Analysis Tool for Ideation Outcomes\n",
      "Evaluating and selecting ideas is a critical and time-consuming step in collaborative ideation, making computational support for this task a desired research goal. However, existing automatic approaches to idea selection might eliminate valuable ideas. In this work we combine automatic approaches with human sensemaking. Kaleidoscope is an exploratory data analytics tool based on semantic technologies. It supports users in exploring and annotating existing ideas interactively. In the following, we present key design principles of Kaleidoscope. Based on qualitative feedback collected on a prototype, we identify potential improvements and describe future work.\n",
      "=============================\n",
      "FacePush: Introducing Normal Force on Face with Head-Mounted Displays\n",
      "This paper presents FacePush, a Head-Mounted Display (HMD) integrated with a pulley system to generate normal forces on a user's face in virtual reality (VR). The mechanism of FacePush is obtained by shifting torques provided by two motors that press upon a user's face via utilization of a pulley system. FacePush can generate normal forces of varying strengths and apply those to the surface of the face. To inform our design of FacePush for noticeable and discernible normal forces in VR applications, we conducted two studies to iden- tify the absolute detection threshold and the discrimination threshold for users' perception. After further consideration in regard to user comfort, we determined that two levels of force, 2.7 kPa and 3.375 kPa, are ideal for the development of the FacePush experience via implementation with three applications which demonstrate use of discrete and continuous normal force for the actions of boxing, diving, and 360 guidance in virtual reality. In addition, with regards to a virtual boxing application, we conducted a user study evaluating the user experience in terms of enjoyment and realism and collected the user's feedback.\n",
      "=============================\n",
      "RFIMatch: Distributed Batteryless Near-Field Identification Using RFID-Tagged Magnet-Biased Reed Switches\n",
      "This paper presents a technique enabling distributed batteryless near-field identification (ID) between two passive radio frequency ID (RFID) tags. Each conventional ultra-high-frequency (UHF) RFID tag is modified by connecting its antenna and chip to a reed switch and then attaching a magnet to one of the reed switch's terminals, thus transforming it into an always-on switch. When the two modules approach each other, the magnets counteract each other and turn off both switches at the same time. The coabsence of IDs thus indicates a unique interaction event. In addition to sensing, the module also provides native haptic feedback through magnetic repulsion force, enabling users to perceive the system's state eyes-free, without physical constraints. Additional visual feedback can be provided through an energy-harvesting module and a light emitting diode. This specific hardware design supports contactless, orientation-invariant sensing, with a form factor compact enough for embedded and wearable use in ubiquitous computing applications.\n",
      "=============================\n",
      "FDSense: Estimating Young's Modulus and Stiffness of End Effectors to Facilitate Kinetic Interaction on Touch Surfaces\n",
      "We make touch input by physically colliding an end effector (e.g., a body part or a stylus) with a touch surface. Prior studies have examined the use of kinematic variables of collision between objects, such as position, velocity, force, and impact. However, the nature of the collision can be understood more thoroughly by considering the known physical relationships that exist between directly measurable variables (i.e., kinetics). Based on this collision kinetics, this study proposes a novel touch technique called FDSense. By simultaneously observing the force and contact area measured from the touchpad, FDSense allows estimation of the Young's modulus and stiffness of the object being contacted. Our technical evaluation showed that FDSense could effectively estimate the Young's modulus of end effectors made of various materials, and the stiffness of each part of the human hand. Two applications using FDSense were demonstrated, for digital painting and digital instruments, where the result of the expression varies significantly depending on the elasticity of the end effector. In a following informal study, participants assessed the technique positively.\n",
      "=============================\n",
      "Extending a Reactive Expression Language with Data Update Actions for End-User Application Authoring\n",
      "Mavo is a small extension to the HTML language that empowers non-programmers to create simple web applications. Authors can mark up any normal HTML document with attributes that specify data elements that Mavo makes editable and persists. But while applications authored with Mavo allow users to edit individual data items, they do not offer any programmatic data actions that can act in customizable ways on large collections of data simultaneously or that modify data according to a computation. We explore an extension to the Mavo language that enables non-programmers to author these richer data update actions. We show that it lets authors create a more powerful set of applications than they could previously, while adding little additional complexity to the authoring process. Through user evaluations, we assess how closely our data update syntax matches how novice authors would instinctively express such actions, and how well they are able to use the syntax we provided.\n",
      "=============================\n",
      "Haptic Interface Using Tendon Electrical Stimulation\n",
      "This demonstration corresponds to our previous paper, which deals with our finding that a proprioceptive force sensation can be presented by electrical stimulation from the skin surface to the tendon region (Tendon Electrical Stimulation: TES). We showed that TES can elicit a force sensation, and adjusting the current parameters can control the amount of the sensation. Unlike electrical muscle stimulation (EMS), which can also present force sensation by stimulating motor nerves to contract muscles, TES is thought to present a proprioceptive force sensation by stimulating receptors or sensory nerves responsible for recognizing the magnitude of the muscle contraction existing inside the tendon. In the demo, we offer the occasion for trying TES.\n",
      "=============================\n",
      "Next-Point Prediction for Direct Touch Using Finite-Time Derivative Estimation\n",
      "End-to-end latency in interactive systems is detrimental to performance and usability, and comes from a combination of hardware and software delays. While these delays are steadily addressed by hardware and software improvements, it is at a decelerating pace. In parallel, short-term input prediction has shown promising results in recent years, in both research and industry, as an addition to these efforts. We describe a new prediction algorithm for direct touch devices based on (i) a state-of-the-art finite-time derivative estimator, (ii) a smoothing mechanism based on input speed, and (iii) a post-filtering of the prediction in two steps. Using both a pre-existing dataset of touch input as benchmark, and subjective data from a new user study, we show that this new predictor outperforms the predictors currently available in the literature and industry, based on metrics that model user-defined negative side-effects caused by input prediction. In particular, we show that our predictor can predict up to 2 or 3 times further than existing techniques with minimal negative side-effects.\n",
      "=============================\n",
      "MobiLimb: Augmenting Mobile Devices with a Robotic Limb\n",
      "In this paper, we explore the interaction space of MobiLimb, a small 5-DOF serial robotic manipulator attached to a mobile device. It (1) overcomes some limitations of mobile devices (static, passive, motionless); (2) preserves their form factor and I/O capabilities; (3) can be easily attached to or removed from the device; (4) offers additional I/O capabilities such as physical deformation and (5) can support various modular elements such as sensors, lights or shells. We illustrate its potential through three classes of applications: As a tool, MobiLimb offers tangible affordances and an expressive controller that can be manipulated to control virtual and physical objects. As a partner, it reacts expressively to users' actions to foster curiosity and engagement or assist users. As a medium, it provides rich haptic feedback such as strokes, pat and other tactile stimuli on the hand or the wrist to convey emotions during mediated multimodal communications.\n",
      "=============================\n",
      "Collaborative Virtual Reality for Low-Latency Interaction\n",
      "In collaborative virtual environments, users must often perform tasks requiring coordinated action between multiple parties. Some cases are symmetric, in which users work together on equal footing, while others are asymmetric, in which one user may have more experience or capabilities than another (e.g., one may guide another in completing a task). We present a multi-user virtual reality system that supports interactions of both these types. Two collaborating users, whether co-located or remote, simultaneously manipulate the same virtual objects in a physics simulation, in tasks that require low latency networking to perform successfully. We are currently applying this approach to motor rehabilitation, in which a therapist and patient work together.\n",
      "=============================\n",
      "Demonstrating Gamepad with Programmable Haptic Texture Analog Buttons\n",
      "We demonstrate a haptic feedback method to generate multiple virtual textures on analog buttons of the gamepad. The method utilizes the haptic illusion evoked from proper haptic cues in respect of the analog button's movement to change the perceived physical property of the button. Two types of analog buttons, joystick and trigger button on the gamepad is augmented with localized haptic feedback. We implemented two virtual textures for each type of analog button, and these textures could be programmatically controlled reflecting the dynamic game situations. We also demonstrate a two-player shooter game to show the dynamic texture representation of customized gamepad could enrich the game experience.\n",
      "=============================\n",
      "Self-Powered Gesture Recognition with Ambient Light\n",
      "We present a self-powered module for gesture recognition that utilizes small, low-cost photodiodes for both energy harvesting and gesture sensing. Operating in the photovoltaic mode, photodiodes harvest energy from ambient light. In the meantime, the instantaneously harvested power from individual photodiodes is monitored and exploited as clues for sensing finger gestures in proximity. Harvested power from all photodiodes are aggregated to drive the whole gesture-recognition module including the micro-controller running the recognition algorithm. We design robust, lightweight algorithm to recognize finger gestures in the presence of ambient light fluctuations. We fabricate two prototypes to facilitate user's interaction with smart glasses and smart watch. Results show 99.7%/98.3% overall precision/recall in recognizing five gestures on glasses and 99.2%/97.5% precision/recall in recognizing seven gestures on the watch. The system consumes 34.6 µW/74.3 µW for the glasses/watch and thus can be powered by the energy harvested from ambient light. We also test system's robustness under varying light intensities, light directions, and ambient light fluctuations, where the system maintains high recognition accuracy (> 96%) in all tested settings.\n",
      "=============================\n",
      "DisplayBowl: A Bowl-Shaped Display for Omnidirectional Videos\n",
      "We introduce DisplayBowl which is a concept of a bowl shaped hemispherical display for showing omnidirectional images. This display provides three-way observation for omnidirectional images. DisplayBowl allows users to observe an omnidirectional image by looking the image from above. In addition, users can see it with a first-person-viewpoint, by looking into the inside of the hemispherical surface from diagonally above. Furthermore, by observing both the inside and the outside of the hemispherical surface at the same time from obliquely above, it is possible to observe it by a pseudo third-person-viewpoint, like watching the drone obliquely from behind. These ways of viewing solve the problem of inability of pilots controlling a remote vehicle such as a drone to notice what happens behind them, which happen with conventional displays such as flat displays and head mounted displays.\n",
      "=============================\n",
      "I Know What You Want: Using Gaze Metrics to Predict Personal Interest\n",
      "In daily communications, we often use interpersonal cues - telltale facial expressions and body language - to moderate responses to our conversation partners. While we are able to interpret gaze as a sign of interest or reluctance, conventional user interfaces do not yet possess this possible benefit. In our work, we evaluate to what degree fixation-based gaze metrics can be used to infer a user's personal interest in the displayed content. We report on a study (N=18) where participants were presented with a grid array of different images, whilst being recorded for gaze behavior. Our system calculated a ranking for shown images based on gaze metrics. We found that all metrics are effective indicators of the participants' interest by analyzing their agreement with regard to the system's ranking. In an evaluation in a museum, we found that this translates to in-the-wild scenarios despite environmental constraints, such as limited data accuracy.\n",
      "=============================\n",
      "ElectricItch: Skin Irritation as a Feedback Modality\n",
      "Grabbing users' attention is a fundamental aspect of interactive systems. However, there is a disconnect between the ways our devices notify us and how our bodies do so naturally. In this paper, we explore the body's modality of itching as a way to provide such natural feedback. We create itching sensations via low-current electric stimulation, which allows us to quickly generate this sensation on demand. In a first study we explore the design space around itching and how changes in stimulation parameters influence the resulting sensation. In a second study we compare vibration feedback and itching integrated in a smartwatch form factor. We find that we can consistently induce itching sensations and that these are perceived as more activating and interrupting than vibrotactile stimuli.\n",
      "=============================\n",
      "Turbulence Ahead - A 3D Web-Based Aviation Weather Visualizer\n",
      "Although severe aircraft accidents have been reduced in the last decades, the number of injuries and fatalities caused by turbulence is still rising. Current aviation weather products are unable to provide a holistic and intuitive view of the overall weather situation, especially in terms of turbulence forecasts. This work introduces an interactive 3D prototype developed with a user-centered design approach. The prototype focuses on the visualization of significant weather charts, which are utilized during flight preparation. An online user study is conducted to compare the prototype with today's 2D paper maps. A total of 64 pilots from an internationally operating airline participated in the study. Among the major findings of the study is that the prototype significantly decreased the cognitive load, and enhanced spatial awareness and usability. To determine the spatial awareness, a novel similarity measure for spatial configurations of aviation weather data is introduced.\n",
      "=============================\n",
      "Knobology 2.0: Giving Shape to the Haptic Force Feedback of Interactive Knobs\n",
      "We present six rotary knobs, each with a distinct shape, that provide haptic force feedback on rotation. The knob shapes were evaluated in relation to twelve haptic feedback stimuli. The stimuli were designed as a combination of the most relevant perceptual parameters of force feedback; acceleration, friction, detent amplitude and spacing. The results indicate that there is a relationship between the shape of a knob and its haptic feedback. The perceived functionality can be dynamically altered by changing its shape and haptic feedback. This work serves as basis for the design of dynamic interface controls that can adapt their shape and haptic feel to the content that is controlled. In our demonstration, we show the six distinct knobs shapes with the different haptic feedback stimuli. Attendees can experience the interaction with the different knob shapes in relation the stimuli and design stimuli with a graphical editor.\n",
      "=============================\n",
      "Companion - A Software Toolkit for Digitally Aided Pen-and-Paper Tabletop Roleplaying\n",
      "We present Companion, a software tool tailored towards improving and digitally supporting the pen-and-paper tabletop role-playing experience. Pen-and-paper role-playing games (P&P RPG) are a concept known since the early 1970s. Since then, the genre has attracted a massive community of players while branching out into several genres and P&P RPG systems to choose from. Due to the highly interactive and dynamic nature of the game, a participants individual impact on narrative and interactive aspects of the game is extremely high. The diversity of scenarios within this context unfold a variety of players needs, as well as factors limiting and enhancing game-play. Companion offers an audio management workspace for creation and playback of soundscapes based on visual layouting. It supports interactive image presentation and map exploration which can incorporate input from any device providing TUIO tracking data. Additionally, a mobile app was developed to be used as a remote control for media activation on the desktop host.\n",
      "=============================\n",
      "Learning Design Semantics for Mobile Apps\n",
      "Recently, researchers have developed black-box approaches to mine design and interaction data from mobile apps. Although the data captured during this interaction mining is descriptive, it does not expose the design semantics of UIs: what elements on the screen mean and how they are used. This paper introduces an automatic approach for generating semantic annotations for mobile app UIs. Through an iterative open coding of 73k UI elements and 720 screens, we contribute a lexical database of 25 types of UI components, 197 text button concepts, and 135 icon classes shared across apps. We use this labeled data to learn code-based patterns to detect UI components and to train a convolutional neural network that distinguishes between icon classes with 94% accuracy. To demonstrate the efficacy of our approach at scale, we compute semantic annotations for the 72k unique UIs in the Rico dataset, assigning labels for 78% of the total visible, non-redundant elements.\n",
      "=============================\n",
      "PuPoP: Pop-up Prop on Palm for Virtual Reality\n",
      "The sensation of being able to feel the shape of an object when grasping it in Virtual Reality (VR) enhances a sense of presence and the ease of object manipulation. Though most prior works focus on force feedback on fingers, the haptic emulation of grasping a 3D shape requires the sensation of touch using the entire hand. Hence, we present Pop-up Prop on Palm (PuPoP), a light-weight pneumatic shape-proxy interface worn on the palm that pops several airbags up with predefined primitive shapes for grasping. When a user's hand encounters a virtual object, an airbag of appropriate shape, ready for grasping, is inflated by way of the use of air pumps; the airbag then deflates when the object is no longer in play. Since PuPoP is a physical prop, it can provide the full sensation of touch to enhance the sense of realism for VR object manipulation. For this paper, we first explored the design and implementation of PuPoP with multiple shape structures. We then conducted two user studies to further understand its applicability. The first study shows that, when in conflict, visual sensation tends to dominate over touch sensation, allowing a prop with a fixed size to represent multiple virtual objects with similar sizes. The second study compares PuPoP with controllers and free-hand manipulation in two VR applications. The results suggest that utilization of dynamically-changing PuPoP, when grasped by users in line with the shapes of virtual objects, enhances enjoyment and realism. We believe that PuPoP is a simple yet effective way to convey haptic shapes in VR.\n",
      "=============================\n",
      "Post-literate Programming: Linking Discussion and Code in Software Development Teams\n",
      "The literate programming paradigm presents a program interleaved with natural language text explaining the code's rationale and logic. While this is great for program readers, the labor of creating literate programs deters most program authors from providing this text at authoring time. Instead, as we determine through interviews, developers provide their design rationales after the fact, in discussions with collaborators. We propose to capture these discussions and incorporate them into the code. We have prototyped a tool to link online discussion of code directly to the code it discusses. Incorporating these discussions incrementally creates post-literate programs that convey information to future developers.\n",
      "=============================\n",
      "Increasing Walking in VR using Redirected Teleportation\n",
      "Teleportation is a popular locomotion technique that lets users safely navigate beyond the confines of available positional tracking space without inducing VR sickness. Because available walking space is limited and teleportation is faster than walking, a risk with using teleportation is that users might end up abandoning walking input and only relying on teleportation, which is considered detrimental to presence. We present redirected teleportation; an improved version of teleportation that uses iterative non-obtrusive reorientation and repositioning using a portal to redirect the user back to the center of the tracking space, where available walking space is larger. A user study compares the effectiveness, accuracy, and usability of redirected teleportation with regular teleportation using a navigation task in three different environments. Results show that redirected teleportation allows for a better utilization of available tracking space than regular teleportation, as it requires significantly fewer teleportations, while users walk more and use a larger portion of the available tracking space.\n",
      "=============================\n",
      "Crowd-AI Systems for Non-Visual Information Access in the Real World\n",
      "The world is full of information, interfaces and environments that are inaccessible to blind people. When navigating indoors, blind people are often unaware of key visual information, such as posters, signs, and exit doors. When accessing specific interfaces, blind people cannot independently do so without at least first learning their layout and labeling them with sighted assistance. My work investigates interactive systems that integrates computer vision, on-demand crowdsourcing, and wearables to amplify the abilities of blind people, offering solutions for real-time environment and interface navigation. My work provides more options for blind people to access information and increases their freedom in navigating the world.\n",
      "=============================\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from pprint import pprint\n",
    "def save_json(data, filepath):\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "csv_file = open('HCI_paper_abstracts/HCI_paper_per_record_release.csv', 'r')\n",
    "reader = csv.DictReader(csv_file)\n",
    "keys = ['paperid', 'abstract', 'title']\n",
    "extracted_data = []\n",
    "for row in reader:\n",
    "    # exclusion criteria:\n",
    "    # 1. title and abstract are empty\n",
    "    if row['title'] == '' or row['abstract'] == '':\n",
    "        continue\n",
    "    # 2. is not UIST paper\n",
    "    if row['venue'] != 'UIST': \n",
    "        continue\n",
    "    # 3. is not a publication\n",
    "    if row['title'].startswith(\"Proceedings of the\"):\n",
    "        continue\n",
    "    extracted_data.append({key: row[key] for key in keys})\n",
    "\n",
    "for index, paper in enumerate(extracted_data):\n",
    "    # paper['id'] = paper['paperid']\n",
    "    paper['id'] = str(index)\n",
    "    paper['content'] = paper['title'] + '\\n' + paper['abstract']\n",
    "    print(paper['content'])\n",
    "    print(\"=============================\")\n",
    "    del paper['paperid']\n",
    "    del paper['title']\n",
    "    del paper['abstract']\n",
    "save_json(extracted_data, 'HCI_paper_abstracts/papers.json')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1723\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "def save_json(data, filepath):\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "paper_data = json.load(open(\"HCI_paper_abstracts/papers.json\"))\n",
    "print(len(paper_data))\n",
    "paper_data_small = paper_data[:100]\n",
    "save_json(paper_data_small, 'HCI_paper_abstracts/papers_small.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "import traceback\n",
    "\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "async def multithread_embeddings(texts, api_key):\n",
    "    tasks = [get_embedding(text, api_key) for text in texts]\n",
    "    # results = await asyncio.gather(*tasks)  # Runs network requests concurrently\n",
    "    results = await tqdm_asyncio.gather(*tasks, desc=\"generating embeddings\")\n",
    "    return results\n",
    "\n",
    "\n",
    "async def get_embedding(text, api_key, model=\"text-embedding-3-small\"):\n",
    "    return await asyncio.to_thread(_get_embedding_block, text, api_key, model)\n",
    "\n",
    "def _get_embedding_block(text, api_key, model=\"text-embedding-3-small\"):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    # print(\"tokens: \", len(enc.encode(text)), len(enc.encode(text)) > 8191)\n",
    "    while len(enc.encode(text)) > 8191:\n",
    "        text = text[:-100]\n",
    "        print(\"truncated: \", len(enc.encode(text)))\n",
    "    try:\n",
    "        return client.embeddings.create(input=text, model=model).data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc()) \n",
    "        # return get_embedding(text, api_key, model)\n",
    "\n",
    "# api_key = open(\"../api_key\").read().strip()\n",
    "# paper_data = json.load(open(\"HCI_paper_abstracts/papers.json\"))\n",
    "# paper_data = paper_data[:2]\n",
    "# texts = [paper['content'] for paper in paper_data]\n",
    "# embeddings = await multithread_embeddings(texts, api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "\n",
    "async def call_agent(agent, user_message):\n",
    "    response = await agent.on_messages(\n",
    "        [TextMessage(content=user_message, source=\"user\")],\n",
    "        cancellation_token=CancellationToken(),\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "async def parallel_call_agents(agent, user_messages):\n",
    "    tasks = [call_agent(agent, user_message) for user_message in user_messages]\n",
    "\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "def generate_topic_assignment_agent(model: str, api_key: str):\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=model,\n",
    "        api_key=api_key,\n",
    "        temperature=0.0,\n",
    "        model_capabilities={\n",
    "            \"vision\": False,\n",
    "            \"function_calling\": False,\n",
    "            \"json_output\": True,\n",
    "        },\n",
    "    )\n",
    "    agent = AssistantAgent(\n",
    "        name=\"goal_decomposition_agent\",\n",
    "        model_client=model_client,\n",
    "        system_message=\"\"\"You are a topic assignment system. The user will provide you with a list of texts. You need to assign one topic to summarize all of them. \n",
    "            The topic should be a simple noun-phrase. Only one topic should be generated.\n",
    "            Reply with the JSON format: \n",
    "            {{\n",
    "                topic: string \n",
    "            }}\"\"\",\n",
    "    )\n",
    "    return agent\n",
    "\n",
    "\n",
    "paper_data = json.load(open(\"HCI_paper_abstracts/papers.json\"))\n",
    "all_texts = [paper['content'] for paper in paper_data]\n",
    "cluster_texts = {\n",
    "    \"0\": all_texts[:180]\n",
    "}\n",
    "model=\"gpt-4o-mini\"\n",
    "api_key = open(\"../api_key\").read().strip()\n",
    "topic_assignment_agent = generate_topic_assignment_agent(model, api_key)\n",
    "user_messages = []\n",
    "for cluster, texts in cluster_texts.items():\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "    if (\n",
    "        len(enc.encode(\"\\n\".join(texts))) > 100000\n",
    "    ):  # 128000 is the max token limit for GPT-4o-mini\n",
    "        texts = random.sample(texts, len(texts) // 2)\n",
    "    user_messages.append(\"\\n\".join(texts))\n",
    "\n",
    "responses = await parallel_call_agents(topic_assignment_agent, user_messages)\n",
    "responses = [\n",
    "    json.loads(response.chat_message.content)[\"topic\"] for response in responses\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Multimodal Interaction and User Interface Design']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering, OPTICS\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from scipy.optimize import minimize\n",
    "from collections import defaultdict\n",
    "\n",
    "def circular_dr(X):\n",
    "    print(\"calculating pairwise distances\")\n",
    "    D = cosine_distances(X)\n",
    "    print(\"optimizing positions\")\n",
    "    angles = optimize_positions(D)\n",
    "    return angles\n",
    "\n",
    "\n",
    "def optimize_positions(D, initial_theta=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Optimize the positions of points on a circle to preserve the distances in D.\n",
    "\n",
    "    Parameters:\n",
    "    - D: n x n distance matrix.\n",
    "    - initial_theta: Optional initial guess for the angles.\n",
    "    - verbose: If True, print optimization details.\n",
    "\n",
    "    Returns:\n",
    "    - Optimized angles theta (in radians).\n",
    "    \"\"\"\n",
    "    n = D.shape[0]\n",
    "\n",
    "    d_min = np.min(D)\n",
    "    d_max = np.max(D)\n",
    "    D = (D - d_min) / (d_max - d_min)\n",
    "    if initial_theta is None:\n",
    "        # Initialize theta randomly between 0 and 2pi\n",
    "        initial_theta = np.random.uniform(0, 2 * np.pi, n)\n",
    "\n",
    "    # Define bounds for each theta: [0, 2pi]\n",
    "    bounds = [(0, 2 * np.pi) for _ in range(n)]\n",
    "\n",
    "    # Optionally, fix the first angle to 0 to remove rotational symmetry\n",
    "    # Uncomment the following lines if you want to fix theta_0 = 0\n",
    "    # bounds = [(0, 0)] + [(0, 2 * np.pi) for _ in range(n - 1)]\n",
    "\n",
    "    # Define the objective function with D fixed\n",
    "    obj = lambda theta: objective(theta, D)\n",
    "\n",
    "    # Perform the optimization\n",
    "    result = minimize(\n",
    "        obj,\n",
    "        initial_theta,\n",
    "        method=\"L-BFGS-B\",\n",
    "        bounds=bounds,\n",
    "        options={\"disp\": True, \"maxfun\": 9999999},\n",
    "    )\n",
    "\n",
    "    if not result.success:\n",
    "        raise ValueError(\"Optimization failed: \" + result.message)\n",
    "\n",
    "    optimized_theta = result.x\n",
    "    return optimized_theta\n",
    "\n",
    "\n",
    "def objective(theta, D):\n",
    "    \"\"\"\n",
    "    Objective function to minimize: sum of squared differences between\n",
    "    circular distances and the given distance matrix D.\n",
    "\n",
    "    Parameters:\n",
    "    - theta: array of angles (in radians) for each point.\n",
    "    - D: n x n distance matrix.\n",
    "\n",
    "    Returns:\n",
    "    - Sum of squared differences.\n",
    "    \"\"\"\n",
    "    n = len(theta)\n",
    "    # Compute pairwise circular distances\n",
    "    d = 1 - np.cos(\n",
    "        np.minimum(\n",
    "            np.abs(theta[:, np.newaxis] - theta[np.newaxis, :]),\n",
    "            2 * np.pi - np.abs(theta[:, np.newaxis] - theta[np.newaxis, :]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Compute the difference only for i < j to avoid double counting and zero diagonals\n",
    "    mask = np.triu(np.ones((n, n)), k=1).astype(bool)\n",
    "    diff = d[mask] - 2 * D[mask]\n",
    "\n",
    "    return np.sum(diff**2)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tf_idf_embeddings(texts):\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    return tfidf_matrix.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1723, 1000)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_data = json.load(open(\"HCI_paper_abstracts/papers.json\"))\n",
    "all_texts = [paper['content'] for paper in paper_data]\n",
    "api_key = open(\"../api_key\").read().strip()\n",
    "# embeddings = await multithread_embeddings(all_texts, api_key)\n",
    "embeddings = tf_idf_embeddings(all_texts)\n",
    "embeddings = np.array(embeddings)\n",
    "embeddings.shape\n",
    "# for paper, embedding in zip(paper_data, embeddings):\n",
    "#     paper['embedding'] = embedding\n",
    "# save_json(paper_data, 'HCI_paper_abstracts/embeddings.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 1536)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_w_embeddings = json.load(open(\"HCI_paper_abstracts/embeddings.json\"))\n",
    "X = np.array([paper['embedding'] for paper in paper_w_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating pairwise distances\n",
      "optimizing positions\n",
      "[4.56809429 5.05492282 0.         4.59478144 4.50997288 3.82893479\n",
      " 2.18112397 2.54690795 1.67421592 6.28318531 0.         6.14540623\n",
      " 6.28318531 2.17731706 0.5060088  0.04009305 4.97195877 6.28318531\n",
      " 0.         2.88038295 2.12074357 1.97749716 4.27047173 0.\n",
      " 3.96875512 0.93582299 1.48283549 0.         4.13519709 3.65379142\n",
      " 1.76577745 6.28318531 6.28318531 6.28318531 1.44226051 5.66518794\n",
      " 3.87017157 6.28318531 4.24836739 2.20909904 3.39179363 3.20165325\n",
      " 5.94499119 3.25100331 2.19394026 3.93238596 3.64091055 6.00668613\n",
      " 2.35822724 0.18927305 1.82330915 4.86166641 0.         4.12237761\n",
      " 6.1394515  3.29249681 4.76250306 4.73182337 4.64284566 6.15209357\n",
      " 0.         2.44100292 4.24946237 4.77607356 1.17183635 0.12661623\n",
      " 1.10474472 4.03468083 2.43532933 4.50645741 1.06255032 6.28318531\n",
      " 1.65540786 0.9391001  6.28318531 4.96981155 2.53055702 3.89166601\n",
      " 1.59462366 2.63412195 4.9358495  5.57117006 6.28318531 5.26913564\n",
      " 1.24756148 2.30998225 1.72668927 1.26530392 0.8828133  0.\n",
      " 4.1949307  0.         3.89031375 2.10414876 6.28318531 5.37748726\n",
      " 1.12969844 0.         1.67772769 0.         5.3026274  2.33274537\n",
      " 0.32242112 0.         4.48172189 4.21000201 6.28318531 4.99918307\n",
      " 4.39896943 4.91155358 0.         0.25438201 4.60546407 1.90013105\n",
      " 4.88366972 4.44406458 2.3018946  3.05923398 1.97571578 3.78084212\n",
      " 2.75028179 0.         1.48813169 0.22030662 4.89062197 4.34757958\n",
      " 0.74358204 2.16847041 3.897934   6.06440812 4.0686384  3.41684454\n",
      " 0.         4.49624881 0.08697453 0.         5.17347083 2.9961225\n",
      " 2.30603566 6.08686909 0.         1.16749718 6.19911479 2.97882193\n",
      " 3.2045016  0.         0.         2.10372656 2.23551591 4.3983154\n",
      " 4.75907978 6.28318531 4.3720501  4.4560786  0.59209307 2.56354866\n",
      " 1.9908549  4.44043794 1.2565903  0.20377479 5.13512315 6.28318531\n",
      " 2.81273265 6.28318531 0.         0.         5.76664223 2.10150018\n",
      " 1.24465004 2.01950833 4.90352876 2.36559407 2.61054663 3.72975825\n",
      " 4.34847523 1.65425438 4.30459686 0.         3.77904268 1.35667094\n",
      " 6.20481598 1.28910857 2.12629475 2.11707296 4.5220403  1.71889435\n",
      " 3.996491   5.93941466 4.64407749 0.         0.         3.5957051\n",
      " 4.46756854 4.07270097 1.9482538  0.70610214 0.53527407 2.78758933\n",
      " 2.86840357 3.42290202 0.         4.38271185 2.19472027 0.\n",
      " 1.36418266 0.         2.51782242 3.02003956 3.88047796 6.28318531\n",
      " 4.35070106 3.88906599 1.50573753 0.17494759 1.63162267 1.60357857\n",
      " 3.7306489  1.48729087 4.6029488  0.         6.28318531 0.20192609\n",
      " 0.         3.4221696  0.         5.68691324 2.17193191 3.65751248\n",
      " 0.89398096 5.75817054 5.97793635 1.03752977 1.50796528 3.52028577\n",
      " 4.0643198  5.02899522 1.99738627 1.62212212 4.38800063 0.\n",
      " 4.69802064 2.30055807 1.71486647 5.64882912 1.23101265 4.22860865\n",
      " 1.03809336 4.28382134 4.43159358 6.28318531 4.83720903 4.33417142\n",
      " 0.         0.         2.91483165 0.         0.         6.28318531\n",
      " 1.1734279  4.04503813 3.45566461 4.02320052 0.19985676 1.41875606\n",
      " 0.         2.36498988 2.96125728 2.19489848 4.80940467 5.01893712\n",
      " 4.39343806 2.104936   6.28318531 0.         4.2434726  6.28318531\n",
      " 5.50133656 2.2350977  3.56182075 2.19214471 1.81868142 4.26743049\n",
      " 3.7951477  2.27266979 1.87586409 6.28318531 4.31291014 0.58832691\n",
      " 2.19632072 0.         4.02838946 0.         5.01990674 4.65768737\n",
      " 2.2496298  4.1957753  4.13652682 0.63197057 2.25949119 0.96066857\n",
      " 6.268046   4.5703402  2.33292755 4.33937477 4.61629744 2.2910145\n",
      " 4.63258256 1.61213459 1.00758384 2.01787136 0.25590425 4.77376709\n",
      " 2.04987392 6.28318531 2.35549288 3.64054202 3.99281357 4.7688387\n",
      " 2.17872424 3.50772886 2.36451742 4.85722312 4.97523328 0.\n",
      " 4.57835894 5.60022088 0.59731838 4.12031951 4.55866971 4.09886584\n",
      " 4.47737221 0.         3.83138416 1.00560773 2.2269346  0.\n",
      " 2.08647003 3.62165914 3.91240142 2.85533636 0.         3.99601178\n",
      " 2.58982072 1.84208435 4.15698933 0.         0.         0.\n",
      " 5.32341696 4.6596097  3.00105621 4.99952124 4.6055933  6.28318531\n",
      " 1.91104746 4.28365906 1.89750534 0.         1.6474476  4.50612048\n",
      " 4.92113151 6.28318531 5.49295668 5.12608414 5.00369783 3.99287343\n",
      " 4.5498093  1.3395539  0.         4.27407941 1.70184088 2.77718582\n",
      " 2.30808976 4.41773303 6.28318531 2.08169356 0.62646918 2.17065167\n",
      " 4.49017623 2.88044497 2.43115601 3.6948725  6.28318531 6.17823118\n",
      " 2.99786554 1.47118544 0.         0.39208526 6.28318531 3.46601755\n",
      " 2.77385609 1.81779866 3.31217925 0.         0.         0.75402568\n",
      " 3.10524263 5.11685875 0.36161616 0.         6.28318531 3.66490878\n",
      " 4.28932976 2.75233434 1.37020218 3.76216224 3.96094208 4.03557683\n",
      " 1.25475243 2.12252757 2.36559294 3.0943354  2.4389987  6.01310966\n",
      " 1.69252502 2.6009798  2.84211241 3.97512486 4.23753218 1.37465119\n",
      " 4.64285862 1.51913247 6.28318531 3.14030789 4.13283166 1.93565117\n",
      " 1.84759902 2.45492461 3.95697564 4.90555276 5.62357395 2.96833224\n",
      " 1.49487    6.2415986  1.801549   0.         6.28318531 5.16820543\n",
      " 5.08907642 2.16194126 0.52865959 0.02896628 2.71260365 3.6629896\n",
      " 2.99810068 5.83442416 4.38367805 3.3511462  0.         4.88484101\n",
      " 3.00117785 3.24576989 1.59683984 6.28318531 4.43191947 4.43542612\n",
      " 2.0778265  4.92729115 1.21784609 2.23106646 5.87902827 2.25348743\n",
      " 2.24714797 1.80185282 1.86620115 0.         4.12366766 3.36921789\n",
      " 5.0860563  1.73533156 1.69803451 6.28318531 6.28318531 1.53174073\n",
      " 1.58777014 0.38890247 3.01946966 4.19303304 6.28318531 1.79696443\n",
      " 0.         1.59410371 2.08281469 2.14028885 4.11463889 2.05534493\n",
      " 6.28318531 6.06726005 4.66538665 6.28318531 2.01619586 4.43678825\n",
      " 0.         2.66174437 1.80559854 5.72880799 1.73984514 1.40353853\n",
      " 2.20051576 4.47276016]\n"
     ]
    }
   ],
   "source": [
    "sub_X = embeddings[:500]\n",
    "angles = circular_dr(sub_X)\n",
    "print(angles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taskdecomposition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
