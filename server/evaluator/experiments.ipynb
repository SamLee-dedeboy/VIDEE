{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_core import CancellationToken\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai_client = OpenAI(api_key=open(\"../api_key\").read().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_def_toString(task, goal):\n",
    "    return task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from portkey_ai import createHeaders, PORTKEY_GATEWAY_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_complexity_evaluation_agent(\n",
    "    goal: str,\n",
    "    node: \"MCT_Node\",\n",
    "    model: str,\n",
    "    api_key: str,\n",
    "    complexity_definition: str,\n",
    "    few_shot_examples: list[dict],\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the complexity evaluation agent to evaluate whether the node is complex.\n",
    "    Args:\n",
    "        goal: The final task goal. (not actually used in the evaluation)\n",
    "        node: The node to evaluate.\n",
    "        model: The model to use for evaluation.\n",
    "        api_key: The API key for the model.\n",
    "        complexity_definition: The definition of complexity.\n",
    "        few_shot_examples: Few-shot examples for the evaluation. (optional)\n",
    "    \"\"\"\n",
    "\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=model,\n",
    "        api_key=api_key,\n",
    "    )\n",
    "\n",
    "    complexity_evaluation_agent = AssistantAgent(\n",
    "        name=\"complexity_evaluation_agent\",\n",
    "        model_client=model_client,\n",
    "        # temperature=0.5,\n",
    "        system_message=f\"\"\"You are a text complexity evaluator. The user will provide some text that describes a task.\n",
    "Your job is to evaluate whether the text is 'complex' or 'not complex' based on the following definition:\n",
    "{complexity_definition}\n",
    "\n",
    "You must output your reasoning in a <REASONING>...</REASONING> block, then provide your final decision \n",
    "in a <RESULT>...</RESULT> block. The <RESULT> block must contain EXACTLY \"Yes\" or \"No\" (nothing else).\n",
    "\n",
    "Example format:\n",
    "<REASONING>This is my reasoning about complexity.</REASONING>\n",
    "<RESULT>Yes</RESULT>\n",
    "\"\"\",\n",
    "    )\n",
    "\n",
    "    few_shot_messages = []\n",
    "    if len(few_shot_examples) > 0:\n",
    "        for example in few_shot_examples:\n",
    "            example_reasoning = example['user_reasoning']\n",
    "            example_evaluation = example['user_evaluation']\n",
    "            few_shot_messages.append(\n",
    "                TextMessage(\n",
    "                    content=task_def_toString(\n",
    "                        # MCT_Node.model_validate(example[\"node\"]), goal\n",
    "                        example[\"node\"], goal\n",
    "                    ),\n",
    "                    source=\"user\",\n",
    "                )\n",
    "            )\n",
    "            few_shot_messages.append(\n",
    "                TextMessage(\n",
    "                    content=(\n",
    "                        f\"<REASONING>{example_reasoning}</REASONING>\\n\"\n",
    "                        f\"<RESULT>{'Yes' if example_evaluation else 'No'}</RESULT>\"\n",
    "                    ),\n",
    "                    source=\"assistant\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "    user_message = task_def_toString(node, goal)\n",
    "    messages = few_shot_messages + [TextMessage(content=user_message, source=\"user\")]\n",
    "\n",
    "    response = await complexity_evaluation_agent.on_messages(\n",
    "        messages,\n",
    "        cancellation_token=CancellationToken(),\n",
    "    )\n",
    "\n",
    "    result_text = response.chat_message.content.strip()\n",
    "\n",
    "    reasoning_match = re.search(r\"<REASONING>(.*?)</REASONING>\", result_text, re.DOTALL)\n",
    "    reasoning = reasoning_match.group(1).strip() if reasoning_match else \"\"\n",
    "\n",
    "    result_match = re.search(r\"<RESULT>(.*?)</RESULT>\", result_text, re.DOTALL)\n",
    "    final_result = result_match.group(1).strip() if result_match else \"No\"\n",
    "\n",
    "    complexity_value = 1 if final_result == \"Yes\" else 0 if final_result == \"No\" else -1\n",
    "\n",
    "    # save_json(\n",
    "    #     {\n",
    "    #         \"system_message\": complexity_evaluation_agent._system_messages[0].content,\n",
    "    #         \"few_shot_messages\": list(\n",
    "    #             map(lambda m: m.source + \": \" + m.content, few_shot_messages)\n",
    "    #         ),\n",
    "    #         \"response\": result_text,\n",
    "    #     },\n",
    "    #     \"complexity_evaluation.json\",\n",
    "    # )\n",
    "\n",
    "    print(reasoning)\n",
    "\n",
    "    return complexity_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_coherence_evaluation_agent(\n",
    "    goal: str,\n",
    "    parent_node: \"MCT_Node\",\n",
    "    child_node: \"MCT_Node\",\n",
    "    model: str,\n",
    "    api_key: str,\n",
    "    coherence_definition: str,\n",
    "    few_shot_examples: list[dict],\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the coherence evaluation agent to evaluate whether the child node is coherent with the parent node.\n",
    "    Args:\n",
    "        goal: The final task goal. (not actually used in the evaluation)\n",
    "        parent_node: The parent node.\n",
    "        child_node: The child node. (the node to evaluate)\n",
    "        model: The model to use for evaluation.\n",
    "        api_key: The API key for the model.\n",
    "        coherence_definition: The definition of coherence.\n",
    "        few_shot_examples: Few-shot examples for the evaluation. (optional)\n",
    "    \"\"\"\n",
    "\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=model,\n",
    "        api_key=api_key,\n",
    "    )\n",
    "\n",
    "    coherence_evaluation_agent = AssistantAgent(\n",
    "        name=\"coherence_evaluation_agent\",\n",
    "        model_client=model_client,\n",
    "        # temperature=0.5,\n",
    "        system_message=f\"\"\"You are a coherence evaluator. \n",
    "You will be given two parts of a sequence: a parent part and a child part.\n",
    "Evaluate whether the child part logically or thematically follows from the parent part \n",
    "according to the following definition of coherence:\n",
    "{coherence_definition}\n",
    "\n",
    "You must output your reasoning in a <REASONING>...</REASONING> block, \n",
    "then provide your final decision in a <RESULT>...</RESULT> block. \n",
    "The <RESULT> block must contain EXACTLY \"Yes\" or \"No\" (nothing else).\n",
    "\n",
    "Example format:\n",
    "<REASONING>This is my reasoning about coherence.</REASONING>\n",
    "<RESULT>Yes</RESULT>\n",
    "\"\"\",\n",
    "    )\n",
    "\n",
    "    def user_message_generator(_parent_node, _child_node):\n",
    "        return \"\"\"\n",
    "        - Parent Part: {parent_part}\n",
    "        - Child Part: {child_part}\n",
    "        \"\"\".format(\n",
    "            parent_part=task_def_toString(_parent_node, goal),\n",
    "            child_part=task_def_toString(_child_node, goal),\n",
    "        )\n",
    "\n",
    "    few_shot_messages = []\n",
    "    if len(few_shot_examples) > 0:\n",
    "        for example in few_shot_examples:\n",
    "            # example_parent = MCT_Node.model_validate(example[\"parent_node\"])\n",
    "            # example_child = MCT_Node.model_validate(example[\"node\"])\n",
    "            example_parent = example[\"node\"]\n",
    "            example_child = example[\"node\"]\n",
    "            example_reasoning = example[\"user_reasoning\"]\n",
    "            example_eval = example[\"user_evaluation\"]\n",
    "\n",
    "            few_shot_messages.append(\n",
    "                TextMessage(\n",
    "                    content=user_message_generator(example_parent, example_child),\n",
    "                    source=\"user\",\n",
    "                )\n",
    "            )\n",
    "            few_shot_messages.append(\n",
    "                TextMessage(\n",
    "                    content=(\n",
    "                        f\"<REASONING>{example_reasoning}</REASONING>\\n\"\n",
    "                        f\"<RESULT>{'Yes' if example_eval else 'No'}</RESULT>\"\n",
    "                    ),\n",
    "                    source=\"assistant\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "    user_message = user_message_generator(parent_node, child_node)\n",
    "    messages = few_shot_messages + [TextMessage(content=user_message, source=\"user\")]\n",
    "\n",
    "    response = await coherence_evaluation_agent.on_messages(\n",
    "        messages,\n",
    "        cancellation_token=CancellationToken(),\n",
    "    )\n",
    "\n",
    "    result_text = response.chat_message.content.strip()\n",
    "\n",
    "    reasoning_match = re.search(r\"<REASONING>(.*?)</REASONING>\", result_text, re.DOTALL)\n",
    "    reasoning = reasoning_match.group(1).strip() if reasoning_match else \"\"\n",
    "\n",
    "    result_match = re.search(r\"<RESULT>(.*?)</RESULT>\", result_text, re.DOTALL)\n",
    "    final_result = result_match.group(1).strip() if result_match else \"No\"\n",
    "\n",
    "    coherence_value = 1 if final_result == \"Yes\" else 0 if final_result == \"No\" else -1\n",
    "\n",
    "    # save_json(\n",
    "    #     {\n",
    "    #         \"system_message\": coherence_evaluation_agent._system_messages[0].content,\n",
    "    #         \"few_shot_messages\": list(\n",
    "    #             map(lambda m: m.source + \": \" + m.content, few_shot_messages)\n",
    "    #         ),\n",
    "    #         \"response\": result,\n",
    "    #     },\n",
    "    #     \"coherence_evaluation.json\",\n",
    "    # )\n",
    "\n",
    "    print(reasoning)\n",
    "\n",
    "    return coherence_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_importance_evaluation_agent(\n",
    "    goal: str,\n",
    "    node: \"MCT_Node\", # MCT_Node\n",
    "    model: str,\n",
    "    api_key: str,\n",
    "    importance_definition: str,\n",
    "    few_shot_examples: list[dict],\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the importance evaluation agent to evaluate whether the node is important.\n",
    "    Args:\n",
    "        goal: The final task goal. (necessary for the evaluation)\n",
    "        node: The node to evaluate.\n",
    "        model: The model to use for evaluation.\n",
    "        api_key: The API key for the model.\n",
    "        importance_definition: The definition of importance.\n",
    "        few_shot_examples: Few-shot examples for the evaluation. (optional)\n",
    "    \"\"\"\n",
    "\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=model,\n",
    "        api_key=api_key,\n",
    "    )\n",
    "\n",
    "    importance_evaluation_agent = AssistantAgent(\n",
    "        name=\"importance_evaluation_agent\",\n",
    "        model_client=model_client,\n",
    "        # temperature=0.5,\n",
    "        system_message=f\"\"\"You are an importance evaluator. \n",
    "You will be given a final task goal and a subtask description.\n",
    "Evaluate whether the subtask is important using the following definition:\n",
    "{importance_definition}\n",
    "\n",
    "You must output your reasoning in a <REASONING>...</REASONING> block, then provide your final decision \n",
    "in a <RESULT>...</RESULT> block. The <RESULT> block must contain EXACTLY \"Yes\" or \"No\" (nothing else).\n",
    "\n",
    "Example format:\n",
    "<REASONING>This is my reasoning about importance.</REASONING>\n",
    "<RESULT>Yes/No</RESULT>\n",
    "\"\"\",\n",
    "    )\n",
    "\n",
    "    def user_message_generator(_goal, _node):\n",
    "        return (\n",
    "            f\"- A final task goal: {_goal}\\n\"\n",
    "            f\"- A subtask description: {task_def_toString(_node, _goal)}\"\n",
    "        )\n",
    "\n",
    "    few_shot_messages = []\n",
    "    if len(few_shot_examples) > 0:\n",
    "        for example in few_shot_examples:\n",
    "            example_reasoning = example[\"user_reasoning\"]\n",
    "            example_evaluation = example[\"user_evaluation\"]\n",
    "            few_shot_messages.append(\n",
    "                TextMessage(\n",
    "                    content=user_message_generator(goal, example[\"node\"]),\n",
    "                    source=\"user\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "            few_shot_messages.append(\n",
    "                TextMessage(\n",
    "                    content=(\n",
    "                        f\"<REASONING>{example_reasoning}</REASONING>\\n\"\n",
    "                        f\"<RESULT>{'Yes' if example_evaluation else 'No'}</RESULT>\"\n",
    "                    ),\n",
    "                    source=\"assistant\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "    user_message = user_message_generator(goal, node)\n",
    "    messages = few_shot_messages + [TextMessage(content=user_message, source=\"user\")]\n",
    "\n",
    "    response = await importance_evaluation_agent.on_messages(\n",
    "        messages,\n",
    "        cancellation_token=CancellationToken(),\n",
    "    )\n",
    "\n",
    "    result_text = response.chat_message.content.strip()\n",
    "\n",
    "    reasoning_match = re.search(r\"<REASONING>(.*?)</REASONING>\", result_text, re.DOTALL)\n",
    "    reasoning = reasoning_match.group(1).strip() if reasoning_match else \"\"\n",
    "\n",
    "    result_match = re.search(r\"<RESULT>(.*?)</RESULT>\", result_text, re.DOTALL)\n",
    "    final_result = result_match.group(1).strip() if result_match else \"No\"\n",
    "\n",
    "    importance_value = 1 if final_result == \"Yes\" else 0 if final_result == \"No\" else -1\n",
    "\n",
    "    # save_json(\n",
    "    #     {\n",
    "    #         \"system_message\": complexity_evaluation_agent._system_messages[0].content,\n",
    "    #         \"few_shot_messages\": list(\n",
    "    #             map(lambda m: m.source + \": \" + m.content, few_shot_messages)\n",
    "    #         ),\n",
    "    #         \"response\": result,\n",
    "    #     },\n",
    "    #     \"complexity_evaluation.json\",\n",
    "    # )\n",
    "\n",
    "    print(reasoning)\n",
    "\n",
    "    return importance_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complexity = await run_complexity_evaluation_agent(\n",
    "    text = 'This step is crucial as it allows you to identify the key components (entities) and how they are related to each other, which is foundational for constructing a knowledge graph.',\n",
    "    model = 'gpt-4o-mini',\n",
    "    api_key=open(\"../api_key\").read().strip()\n",
    ")\n",
    "complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence = await run_coherence_evaluation_agent(\n",
    "    parent_part = 'This step is crucial as it allows you to identify the key components (entities) and how they are related to each other, which is foundational for constructing a knowledge graph.',\n",
    "    child_part = 'This step is crucial as it allows you to identify the key components (entities) and how they are related to each other, which is foundational for constructing a knowledge graph.',\n",
    "    model = 'gpt-4o-mini',\n",
    "    api_key=open(\"../api_key\").read().strip()\n",
    ")\n",
    "coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = await run_importance_evaluation_agent(\n",
    "    final_goal = \"Entity Extraction\",\n",
    "    subtask_description = \"This step is crucial as it allows you to identify the key components (entities) and how they are related to each other, which is foundational for constructing a knowledge graph.\",\n",
    "    model = 'gpt-4o-mini',\n",
    "    api_key=open(\"../api_key\").read().strip()\n",
    ")\n",
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0.0\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "async def run_all_evaluations(goal: str, node_str: str, parent_node_str: str, model: str, api_key: str, evaluations: list=[]):\n",
    "    tasks = [\n",
    "        run_complexity_evaluation_agent(text=node_str, model=model, api_key=api_key),\n",
    "        run_coherence_evaluation_agent(parent_part=parent_node_str, child_part=node_str, model=model, api_key=api_key),\n",
    "        run_importance_evaluation_agent(final_goal=goal, subtask_description=node_str, model=model, api_key=api_key),\n",
    "    ]\n",
    "\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "goal = \"I want to construct a knowledge graph from a dataset of Wikipedia articles.\"\n",
    "node_str = \"\"\"Task: Extract Entities\n",
    "            Description: Identify and extract relevant entities from the collection of documents.\"\"\"\n",
    "parent_node_str =  \"\"\"Task: Root Node\n",
    "            Description: Start of the process.\"\"\"\n",
    "model = \"gpt-4o-mini\"\n",
    "api_key = open(\"../api_key\").read().strip()\n",
    "[a, b, c] = await run_all_evaluations(goal, node_str, parent_node_str, model, api_key)\n",
    "print(a, b, c, (a+b+c)/3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a text complexity evaluator. The user will provide some text that describes a task.\\nYour job is to evaluate whether the text is \\'complex\\' or \\'not complex\\' based on the following definition:\\n(\"A text is considered complex if it requires advanced knowledge or expertise, contains multiple layered or specialized concepts, or requires multi-step reasoning to understand or accomplish the described goal. Otherwise, it\\'s considered not complex.\",)\\n\\nIf the text meets the complexity definition, respond with:\\n\"Yes\"\\n\\nOtherwise, respond with:\\n\"No\"\\n\\nOutput must be EXACTLY one of these words, with no additional formatting, punctuation, or explanation.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complexity_definition: str = (\n",
    "        \"A text is considered complex if it requires advanced knowledge \"\n",
    "        \"or expertise, contains multiple layered or specialized concepts, or \"\n",
    "        \"requires multi-step reasoning to understand or accomplish the described goal. \"\n",
    "        \"Otherwise, it's considered not complex.\"\n",
    "    ),\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "        model=model,\n",
    "        api_key=api_key,\n",
    "    )\n",
    "complexity_evaluation_agent = AssistantAgent(\n",
    "        name=\"complexity_evaluation_agent\",\n",
    "        model_client=model_client,\n",
    "        # temperature=0.5,\n",
    "        system_message=f\"\"\"You are a text complexity evaluator. The user will provide some text that describes a task.\n",
    "Your job is to evaluate whether the text is 'complex' or 'not complex' based on the following definition:\n",
    "{complexity_definition}\n",
    "\n",
    "If the text meets the complexity definition, respond with:\n",
    "\"Yes\"\n",
    "\n",
    "Otherwise, respond with:\n",
    "\"No\"\n",
    "\n",
    "Output must be EXACTLY one of these words, with no additional formatting, punctuation, or explanation.\n",
    "\"\"\",\n",
    "    )\n",
    "complexity_evaluation_agent._system_messages[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text requires some understanding of knowledge graphs and the relationship between entities within that context, suggesting a need for specialized knowledge in data representation or information science. Identifying key components and their relationships also implies a level of analysis that entails multi-step reasoning. Therefore, this text involves layered concepts and necessitates advanced understanding, making it complex.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complexity_definition = (\n",
    "    \"A text is considered complex if it requires advanced knowledge \"\n",
    "    \"or expertise, contains multiple layered or specialized concepts, or \"\n",
    "    \"requires multi-step reasoning to understand or accomplish the described goal. \"\n",
    "    \"Otherwise, it's considered not complex.\"\n",
    ")\n",
    "\n",
    "complexity = await run_complexity_evaluation_agent(\n",
    "    goal = \"Understanding Document Relationships\",\n",
    "    node = \"This step is crucial as it allows you to identify the key components (entities) and how they are related to each other, which is foundational for constructing a knowledge graph.\",\n",
    "    model = \"gpt-4o-mini\",\n",
    "    api_key = open(\"../../api_key\").read().strip(),\n",
    "    complexity_definition=complexity_definition,\n",
    "    few_shot_examples=[]\n",
    ")\n",
    "\n",
    "complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Parent Part discusses \"Entity Extraction,\" which typically involves identifying and categorizing key entities from a given text. The Child Part, \"Entity Relationship Tree Construction,\" logically follows as it suggests a subsequent step where the entities extracted are organized into a structured format that illustrates the relationships between them. This shows a progression in data processing, where the foundational work of entity extraction leads naturally to the construction of relationships between those entities. There is no contradiction and the concepts are related within the same domain of information processing.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_definition = (\n",
    "    \"Two text pieces are considered coherent in a sequence if the second \"\n",
    "    \"logically or thematically follows from the first, maintains consistency with it, \"\n",
    "    \"and does not present a contradictory or unrelated concept.\"\n",
    ")\n",
    "\n",
    "coherence = await run_coherence_evaluation_agent(\n",
    "    goal = \"Understanding Document Relationships\",\n",
    "    parent_node = \"Entity Extraction\",\n",
    "    child_node = \"Entity Relationship Tree Construction\",\n",
    "    model = \"gpt-4o-mini\",\n",
    "    api_key=open(\"../../api_key\").read().strip(),\n",
    "    coherence_definition=coherence_definition,\n",
    "    few_shot_examples=[]\n",
    ")\n",
    "coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity extraction is critical for understanding document relationships as it involves identifying and classifying key components and entities within the documents. This information is essential for establishing connections and understanding how different documents relate to each other. Without entity extraction, it would be challenging to analyze the relationships accurately, making it a fundamental part of achieving the overall goal. Therefore, this subtask is important.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_definition: str = (\n",
    "    (\n",
    "        \"A subtask is considered important if it is critical, essential, \"\n",
    "        \"or significantly beneficial to achieving the final goal. If it is tangential, \"\n",
    "        \"optional, or has minimal impact, then it is not important.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "importance = await run_importance_evaluation_agent(\n",
    "    goal = \"Understanding Document Relationships\",\n",
    "    node = \"Entity Extraction\",\n",
    "    model = \"gpt-4o-mini\",\n",
    "    api_key=open(\"../../api_key\").read().strip(),\n",
    "    importance_definition=importance_definition,\n",
    "    few_shot_examples=[]\n",
    ")\n",
    "importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "task-decomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
